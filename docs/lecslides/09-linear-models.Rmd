---
title: "Linear models"
author: "Prof Ellis"
date: "2021-10-22"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/cogs137-logo-hex.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
# library(ggtext)
library(knitr)
library(kableExtra)

# For nonsese...
library(emo)
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
opts_chunk$set(fig.height = 3, fig.width = 5, dpi = 300) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

## Q&A

Q: Is it possible to earn partial credit/have curved points for the midterm? I felt like sometimes the questions were a bit ambiguous so I may have misinterpreted what some of they were asking me to find (especially number 7!).   A: Partial credit is possible on every question! 

Q: Since most of the class struggled with question 7, would it be possible if you go through the thought process and code behind it? Not sure if we have time for that, but I was just curious.
A: Yes! We'll do this on Monday. I want people to have their grades, feedback, and exams back before we discuss so they know where to focus their attention. 
---

## Course Announcements

- **lab04** due tonight (11:59 PM)
- **hw02** due Monday

--

- **lab03** scores have been posted

---

class: center, middle

# Characterizing relationships with models

---

# Agenda

- Linear Models
  - Quantitative Predictor
  - Categorical Predictor (2 & \\>2 levels)
  - residuals
  - data transformations
  
---

## Data: Paris Paintings

```{r message=FALSE}
pp <- read_csv("data/paris_paintings.csv", na = c("n/a", "", "NA"))
```

- Number of observations: `r nrow(pp)`
- Number of variables: `r ncol(pp)`

---

## Goal: Predict height from width

$$\widehat{height}_{i} = \beta_0 + \beta_1 \times width_{i}$$

```{r height-width-plot, echo=FALSE, warning=FALSE}
ggplot(data = pp, aes(x = Width_in, y = Height_in)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8E2C90") + 
  labs(
    title = "Height vs. width of paintings",
    subtitle = "Paris auctions, 1764 - 1780",
    x = "Width (inches)",
    y = "Height (inches)"
  )
```


---

```{r out.width="98%", echo=FALSE}
knitr::include_graphics("img/09/tidymodels.png")
```

```{r, eval=FALSE}
# should already be installed for you
library(tidyverse)
```

---

## Step 1: Specify model

```{r}
linear_reg()
```

---

## Step 2: Set model fitting *engine*

```{r}
linear_reg() |>
  set_engine("lm") # lm: linear model
```

---

## Step 3: Fit model & estimate parameters

... using **formula syntax**

```{r fit-model}
linear_reg() |>
  set_engine("lm") |>
  fit(Height_in ~ Width_in, data = pp)
```

---

## A closer look at model output

```{r ref.label="fit-model", echo=FALSE}
```

.large[
$$\widehat{height}_{i} = 3.6214 + 0.7808 \times width_{i}$$
]

---

## A tidy look at model output

```{r}
linear_reg() |>
  set_engine("lm") |>
  fit(Height_in ~ Width_in, data = pp) |>
  tidy()
```

.large[
$$\widehat{height}_{i} = 3.62 + 0.781 \times width_{i}$$
]

---

## Slope and intercept

.large[
$$\widehat{height}_{i} = 3.62 + 0.781 \times width_{i}$$
]

--

- **Slope:** For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.

--
- **Intercept:** Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)

---

## Correlation does not imply causation

Remember this when interpreting model coefficients

```{r echo=FALSE, out.width="90%"}
knitr::include_graphics("img/09/cell_phones.png")
```

.footnote[
Source: XKCD, [Cell phones](https://xkcd.com/925/)
]

---

class: middle

# Parameter estimation

---

## Linear model with a single predictor

- We're interested in $\beta_0$ (population parameter for the intercept) and $\beta_1$ (population parameter for the slope) in the following model:

$$\hat{y}_{i} = \beta_0 + \beta_1~x_{i}$$

--
- Tough luck, you can't have them...

--
- So we use sample statistics to estimate them:

$$\hat{y}_{i} = b_0 + b_1~x_{i}$$

---

## Least squares regression

- The regression line minimizes the sum of squared residuals.

--
- If $e_i = y_i - \hat{y}_i$, then, the regression line minimizes 
$\sum_{i = 1}^n e_i^2$.

---

## Visualizing residuals

```{r vis-res-1, echo=FALSE}
m_ht_wt <- linear_reg() |>
  set_engine("lm") |>
  fit(Height_in ~ Width_in, data = pp)

ht_wt_fit_tidy <- tidy(m_ht_wt$fit) 
ht_wt_fit_aug  <- augment(m_ht_wt$fit) |>
  mutate(res_cat = ifelse(.resid > 0, TRUE, FALSE))

p <- ggplot(data = ht_wt_fit_aug, 
            aes(x = Width_in, y = Height_in)) +
  geom_point(alpha = 0.2) + 
  labs(
    title = "Height vs. width of paintings",
    subtitle = "Just the data",
    x = "Width (inches)",
    y = "Height (inches)"
    ) +
  coord_cartesian(xlim = c(0, 250), ylim = c(0, 200)) +
  theme(plot.subtitle = element_text(colour = "#E48957", face = "bold", size = rel(1.5)))
p
```

---

## Visualizing residuals (cont.)

```{r vis-res-2, echo=FALSE}
p <- p + 
  geom_smooth(method = "lm", color = "#E48957", se = FALSE) +
  geom_point(mapping = aes(y = .fitted), color = "#E48957") +
  labs(subtitle = "Data + least squares line")
p
```

---

## Visualizing residuals (cont.)

```{r vis-res-3, echo = FALSE}
p + 
  geom_segment(mapping = aes(xend = Width_in, yend = .fitted), color = "#E48957", alpha = 0.4) +
  labs(subtitle = "Data + least squares line + residuals")
```

---

## Properties of least squares regression

- The regression line goes through the center of mass point, the coordinates corresponding to average $x$ and average $y$, $(\bar{x}, \bar{y})$:  

$$\bar{y} = b_0 + b_1 \bar{x} ~ \rightarrow ~ b_0 = \bar{y} - b_1 \bar{x}$$

--
- The slope has the same sign as the correlation coefficient: $b_1 = r \frac{s_y}{s_x}$

--
- The sum of the residuals is zero: $\sum_{i = 1}^n e_i = 0$

--
- The residuals and $x$ values are uncorrelated

---

class: middle

# Models with categorical explanatory variables

---

## Categorical predictor with 2 levels

.pull-left[
.small[
```{r echo=FALSE}
pp |> 
  select(name, Height_in, landsALL) |>
  print(n = 20)
```
]
]
.pull-right[
- `landsALL = 0`: No landscape features
- `landsALL = 1`: Some landscape features
]

---

## Height & landscape features

```{r ht-lands-fit}
m_ht_lands <- linear_reg() |>
  set_engine("lm") |>
  fit(Height_in ~ factor(landsALL), data = pp)

m_ht_lands |> tidy()
```

---

## Height & landscape features

$$\widehat{Height_{in}} = 22.7 - 5.645~landsALL$$

- **Slope:** Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features
  - Compares baseline level (`landsALL = 0`) to the other level (`landsALL = 1`)
    
- **Intercept:** Paintings that don't have landscape features are expected, on average, to be 22.7 inches tall

---

## Categorical predictor with >2 levels

.pull-left[
.small[
```{r echo=FALSE}
pp |> 
  select(name, Height_in, school_pntg) |>
  print(n = 20)
```
]
]
.pull-right[
- school from which painting came (details in a few slides)
]

---
## Relationship between height and school

```{r ht-school}
linear_reg() |>
  set_engine("lm") |>
  fit(Height_in ~ school_pntg, data = pp) |>
  tidy()
```

---

## Dummy variables

```{r ref.label="ht-school", echo = FALSE}
```

- When the categorical explanatory variable has many levels, they're encoded to **dummy variables**
- Each coefficient describes the expected difference between heights in that particular school compared to the baseline level

---

## Categorical predictor with 3+ levels

.pull-left-wide[
```{r echo=FALSE}
dummy_df <- pp |> 
  select(school_pntg) |> 
  group_by(school_pntg) |> 
  sample_n(1) |>
  mutate(
    D_FL = as.integer(ifelse(school_pntg == "D/FL", 1L, 0)),
    F    = as.integer(ifelse(school_pntg == "F", 1L, 0)),
    G    = as.integer(ifelse(school_pntg == "G", 1L, 0)),
    I    = as.integer(ifelse(school_pntg == "I", 1L, 0)),
    S    = as.integer(ifelse(school_pntg == "S", 1L, 0)),
    X    = as.integer(ifelse(school_pntg == "X", 1L, 0))
  )

dummy_df |>
  kable(align = "lcccccc") |>
  kable_styling() |>
  column_spec(2, width = "10em", background = spec_color(dummy_df$D_FL[1:7], end = 0.8), color = "white") |>
  column_spec(3, width = "10em", background = spec_color(dummy_df$F[1:7], end = 0.8), color = "white") |>
  column_spec(4, width = "10em", background = spec_color(dummy_df$G[1:7], end = 0.8), color = "white") |>
  column_spec(5, width = "10em", background = spec_color(dummy_df$I[1:7], end = 0.8), color = "white") |>
  column_spec(6, width = "10em", background = spec_color(dummy_df$S[1:7], end = 0.8), color = "white") |>
  column_spec(7, width = "10em", background = spec_color(dummy_df$X[1:7], end = 0.8), color = "white")
```
]
.pull-right-narrow[
.small[
```{r echo=FALSE}
pp |> 
  select(name, Height_in, school_pntg) |>
  print(n = 20)
```
]
]

---

## The linear model with multiple predictors

- Population model:

$$ \hat{y} = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k $$

--

- Sample model that we use to estimate the population model:
  
$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

---
## Relationship b/w height and school

.small[
```{r ref.label="ht-school", echo=FALSE}
```

- **Austrian school (A)** paintings are expected, on average, to be **14 inches** tall.
- **Dutch/Flemish school (D/FL)** paintings are expected, on average, to be **2.33 inches taller** than *Austrian school* paintings.
- **French school (F)** paintings are expected, on average, to be **10.2 inches taller** than *Austrian school* paintings.
- **German school (G)** paintings are expected, on average, to be **1.65 inches taller** than *Austrian school* paintings.
- **Italian school (I)** paintings are expected, on average, to be **10.3 inches taller** than *Austrian school* paintings.
- **Spanish school (S)** paintings are expected, on average, to be **30.4 inches taller** than *Austrian school* paintings.
- Paintings whose school is **unknown (X)** are expected, on average, to be **2.87 inches taller** than *Austrian school* paintings.
]

---


class: center, middle

# Prediction with models

---

## Predict height from width

.question[
On average, how tall are paintings that are 60 inches wide?
$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$
]

--

```{r}
3.62 + 0.78 * 60
```

"On average, we expect paintings that are 60 inches wide to be 50.42 inches high."

**Warning:** We "expect" this to happen, but there will be some variability. (We'll
learn about measuring the variability around the prediction later.)

---

## Prediction vs. extrapolation

.question[
On average, how tall are paintings that are 400 inches wide?
$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$
]

```{r extrapolate, warning = FALSE, echo=FALSE, fig.height=2.75}
newdata <- tibble(Width_in = 400)
newdata <- newdata |>
  mutate(Height_in = predict(m_ht_wt, new_data = newdata))

ggplot(data = pp, aes(x = Width_in, y = Height_in)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", fullrange = TRUE, color = "#8E2C90", se = FALSE) +
  xlim(0, 420) +
  ylim(0, 320) +
  geom_segment(data = newdata, mapping = aes(x = Width_in, y = 0, xend = Width_in, yend = Height_in$.pred), color = color_palette$salmon, lty = 2) +
  geom_segment(data = newdata, mapping = aes(x = Width_in, y = Height_in$.pred, xend = 0, yend = Height_in$.pred), color = color_palette$salmon, lty = 2)
```

---

## Watch out for extrapolation!

> "When those blizzards hit the East Coast this winter, it proved to my satisfaction 
that global warming was a fraud. That snow was freezing cold. But in an alarming 
trend, temperatures this spring have risen. Consider this: On February 6th it was 10 
degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So 
clearly folks the climate debate rages on."<sup>1</sup>  <br>
Stephen Colbert, April 6th, 2010

.footnote[
[1] OpenIntro Statistics. "Extrapolation is treacherous." OpenIntro Statistics.
]

---

class: center, middle

# Measuring model fit

---

## Measuring the strength of the fit

- The strength of the fit of a linear model is most commonly evaluated using $R^2$.

- It tells us what percent of variability in the response variable is explained by 
the model.

- The remainder of the variability is explained by variables not included in the 
model.

- $R^2$ is sometimes called the coefficient of determination.

---

## Obtaining $R^2$ in R

- Height vs. width
.small[
```{r}
glance(m_ht_wt)
glance(m_ht_wt)$r.squared # extract R-squared
```
]

Roughly 68% of the variability in heights of paintings can be explained by their
widths.

- Height vs. landscape features
.small[
```{r}
glance(m_ht_lands)$r.squared
```
]

---


class: center, middle

# Exploring linearity

---

## Data: Paris Paintings

```{r echo=FALSE}
ggplot(data = pp, aes(x = price)) +
  geom_histogram(binwidth = 1000) +
  labs(title = "Prices of paintings")
```

---

## Price vs. width

.question[
Describe the relationship between price and width of painting.
]

```{r echo=FALSE, fig.height=2.75}
ggplot(data = pp, aes(x = Width_in, y = price)) +
  geom_point(alpha = 0.5) +
  labs(x = "Width (in)", y = "Price (livres)")
```

---

## Let's focus on paintings with `Width_in < 100`

```{r}
pp_wt_lt_100 <- pp |> 
  filter(Width_in < 100)
```

---

## Price vs. width

.question[
Which plot shows a more linear relationship?
]

.small[
  
.pull-left[
```{r fig.width=5, fig.height=4, message=FALSE, echo=FALSE}
ggplot(data = pp_wt_lt_100, 
       mapping = aes(x = Width_in, y = price)) +
  geom_point(alpha = 0.5) +
  labs(title = "Price vs. width", subtitle = "For width < 100 in",
       x = "Width (in)", y = "Price (livres)")
```
]

.pull-right[
```{r fig.width=5, fig.height=4, message=FALSE, echo=FALSE}
ggplot(data = pp_wt_lt_100, 
       mapping = aes(x = Width_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  labs(title = "Log(price) vs. width", subtitle = "For width < 100 in",
       x = "Width (in)", y = "Log(price) (log livres)")
```
]

]

---

## Price vs. width, residuals

.question[
Which plot shows a residuals that are uncorrelated with predicted values from the model?
]

.small[
  
.pull-left[
```{r fig.width=5, fig.height=4, message=FALSE, echo=FALSE}
m_wi_pr <- linear_reg() |>
  set_engine("lm") |>
  fit(price ~ Width_in, data = pp)
m_wi_pr_tidy <- tidy(m_wi_pr$fit) 
m_wi_pr_fit_aug  <- augment(m_wi_pr$fit)

ggplot(data = m_wi_pr_fit_aug, 
       mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Price vs. width, residuals", 
    subtitle = "For width < 100 in",
    x = "Predicted price (livres)", 
    y = "Residuals"
    )
```
]

.pull-right[
```{r fig.width=5, fig.height=4, message=FALSE, echo=FALSE}
m_log_wi_pr <- linear_reg() |>
  set_engine("lm") |>
  fit(log(price) ~ Width_in, data = pp_wt_lt_100)


m_log_wi_pr_tidy <- augment(m_log_wi_pr$fit)
ggplot(data = m_log_wi_pr_tidy, 
       mapping = aes(x = .fitted, y = .std.resid)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Log(Price) vs. width, residuals", 
    subtitle = "For width < 100 in",
    x = "Predicted log(price) (log livres)", 
    y = "Residuals"
    )
```
]

]

--

.question[
What's the unit of residuals?
]

---

## Transforming the data

- We saw that `price` has a right-skewed distribution, and the relationship between price and width of painting is non-linear.

--

- In these situations a transformation applied to the response variable may be useful.

--

- In order to decide which transformation to use, we should examine the distribution of the response variable.

--

- The extremely right skewed distribution suggests that a log transformation may 
be useful.
    - log = natural log, $ln$
    - Default base of the `log` function in R is the natural log: <br>
    `log(x, base = exp(1))`
    
---

## Logged price vs. width

.question[
How do we interpret the slope of this model?
]

```{r echo=FALSE}
ggplot(data = pp_wt_lt_100, mapping = aes(x = Width_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "#8E2C90", se = FALSE) +
  labs(x = "Width (in)", y = "Log(price) (log livres)")
```

---

## Interpreting models with log transformation

```{r}
m_lprice_wt <- lm(log(price) ~ Width_in, data = pp_wt_lt_100)
m_lprice_wt |>
  tidy() |>
  select(term, estimate) |>
  mutate(estimate = round(estimate, 3))
```

---

## Linear model with log transformation

$$ \widehat{log(price)} = 4.67 + 0.02 Width $$

--

- For each additional inch the painting is wider, the log price of the
painting is expected to be higher, on average, by 0.02 livres.

--

- which is not a very useful statement...

---

## Working with logs

- Subtraction and logs: $log(a) − log(b) = log(a / b)$

--

- Natural logarithm: $e^{log(x)} = x$

--

- We can use these identities to "undo" the log transformation

---

## Interpreting models with log transformation

The slope coefficient for the log transformed model is 0.02, meaning the log 
price difference between paintings whose widths are one inch apart is predicted 
to be 0.02 log livres.

--

$$ log(\text{price for width x+1}) - log(\text{price for width x}) = 0.02 $$

--

$$ log\left(\frac{\text{price for width x+1}}{\text{price for width x}}\right) = 0.02 $$

--

$$ e^{log\left(\frac{\text{price for width x+1}}{\text{price for width x}}\right)} = e^{0.02} $$

--

$$ \frac{\text{price for width x+1}}{\text{price for width x}} \approx 1.02 $$

--

For each additional inch the painting is wider, the price of the
painting is expected to be higher, on average, by a factor of 1.02.

---

## Shortcuts in R

```{r}
m_lprice_wt |>
  tidy() |>
  select(term, estimate) |>
  mutate(estimate = round(estimate, 3))
```

```{r}
m_lprice_wt |>
  tidy() |>
  select(term, estimate) |>
  mutate(estimate = round(exp(estimate), 3))
```

---

## Recap

- Non-constant variance is one of the most common model violations, however it 
is usually fixable by transforming the response (y) variable.

--

- The most common transformation when the response variable is right skewed is 
the log transform: $log(y)$, especially useful when the response variable is 
(extremely) right skewed.

--

- This transformation is also useful for variance stabilization.

--

- When using a log transformation on the response variable the interpretation of 
the slope changes: *"For each unit increase in x, y is expected on average to be higher/lower <br> by a factor of $e^{b_1}$."*

--

- Another useful transformation is the square root: $\sqrt{y}$, especially 
useful when the response variable is counts.

---

## Aside: when $y = 0$

In some cases the value of the response variable might be 0, and

```{r}
log(0)
```

--

The trick is to add a very small number to the value of the response variable for these cases so that the `log` function can still be applied:

```{r}
log(0 + 0.00001)
```

