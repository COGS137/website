<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Linear models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Shannont Ellis" />
    <meta name="date" content="2021-10-22" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Linear models
### Shannont Ellis
### 2021-10-22

---








## Announcements

- Reading assigned for Mon
- Future HW assignment due dates posted
- Any questions from last time?

---

class: center, middle

# Characterizing relationships with models

---

## Data


```r
pp &lt;- read_csv("data/paris_paintings.csv", 
               na = c("n/a", "", "NA"))
```

---

## Want to follow along?

Go to RStudio Cloud -&gt; make a copy of "Modelling Paris Paintings"

---

## Height &amp; width


```r
(m_ht_wt &lt;- lm(Height_in ~ Width_in, data = pp))
```

```
## 
## Call:
## lm(formula = Height_in ~ Width_in, data = pp)
## 
## Coefficients:
## (Intercept)     Width_in  
##      3.6214       0.7808
```

--

&lt;br&gt;

`$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$`

--

- **Slope:** For each additional inch the painting is wider, the height is expected
to be higher, on average, by 0.78 inches.

--

- **Intercept:** Paintings that are 0 inches wide are expected to be 3.62 inches high,
on average.
    - Does this make sense?

---

## The linear model with a single predictor

- We're interested in the `\(\beta_0\)` (population parameter for the intercept)
and the `\(\beta_1\)` (population parameter for the slope) in the 
following model:

$$ \hat{y} = \beta_0 + \beta_1~x $$

--

- Tough luck, you can't have them...

--

- So we use the sample statistics to estimate them:

$$ \hat{y} = b_0 + b_1~x $$

---

## Least squares regression

The regression line minimizes the sum of squared residuals.

--

If `\(e_i = y - \hat{y}\)`,

then, the regression line minimizes `\(\sum_{i = 1}^n e_i^2\)`.

---

## Visualizing residuals

![](12-linear-models_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

## Visualizing residuals (cont.)


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

---

## Visualizing residuals (cont.)


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;


---

## Properties of the least squares regression line

- The regression line goes through the center of mass point, the coordinates corresponding to average `\(x\)` and average `\(y\)`: `\((\bar{x}, \bar{y})\)`:

`$$\hat{y} = b_0 + b_1 x ~ \rightarrow ~ b_0 = \hat{y} - b_1 x$$`

- The slope has the same sign as the correlation coefficient:

`$$b_1 = r \frac{s_y}{s_x}$$`

- The sum of the residuals is zero: 
`$$\sum_{i = 1}^n e_i = 0$$`

- The residuals and `\(x\)` values are uncorrelated.

---

## Height &amp; landscape features


```r
(m_ht_lands &lt;- lm(Height_in ~ factor(landsALL), data = pp))
```

```
## 
## Call:
## lm(formula = Height_in ~ factor(landsALL), data = pp)
## 
## Coefficients:
##       (Intercept)  factor(landsALL)1  
##            22.680             -5.645
```

--

&lt;br&gt;

`$$\widehat{Height_{in}} = 22.68 - 5.65~landsALL$$`

---

## Height &amp; landscape features (cont.)

- **Slope:** Paintings with landscape features are expected, on average,
to be 5.65 inches shorter than paintings that without landscape features.
    - Compares baseline level (`landsALL = 0`) to other level
    (`landsALL = 1`).

- **Intercept:** Paintings that don't have landscape features are expected, on 
average, to be 22.68 inches tall.

---

## Categorical predictor with 2 levels

<div id="htmlwidget-f58fee09348b35fb8ecb" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-f58fee09348b35fb8ecb">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8"],["L1764-2","L1764-3","L1764-4","L1764-5a","L1764-5b","L1764-6","L1764-7a","L1764-7b"],[360,6,12,6,6,9,12,12],[0,0,1,1,1,0,0,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>name<\/th>\n      <th>price<\/th>\n      <th>landsALL<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>

---

## Relationship between height and school


```r
(m_ht_sch &lt;- lm(Height_in ~ school_pntg, data = pp))
```

```
## 
## Call:
## lm(formula = Height_in ~ school_pntg, data = pp)
## 
## Coefficients:
##     (Intercept)  school_pntgD/FL     school_pntgF     school_pntgG  
##          14.000            2.329           10.197            1.650  
##    school_pntgI     school_pntgS     school_pntgX  
##          10.287           30.429            2.869
```

--

- When the categorical explanatory variable has many levels, they're encoded to
**dummy variables**.

- Each coefficient describes the expected difference between heights in that 
particular school compared to the baseline level.

---

## Categorical predictor with &gt;2 levels

.small[
<div id="htmlwidget-0bf0ad817e4fc59f4b51" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-0bf0ad817e4fc59f4b51">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7"],["A","D/FL","F","G","I","S","X"],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0],[0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>school_pntg<\/th>\n      <th>D_FL<\/th>\n      <th>F<\/th>\n      <th>G<\/th>\n      <th>I<\/th>\n      <th>S<\/th>\n      <th>X<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3,4,5,6,7]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
]

---

## The linear model with multiple predictors

- Population model:

$$ \hat{y} = \beta_0 + \beta_1~x_1 + \beta_2~x_2 + \cdots + \beta_k~x_k $$

--

- Sample model that we use to estimate the population model:
  
$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

---

## Correlation does not imply causation!

Remember this when interpreting model coefficients

---

class: center, middle

# Prediction with models

---

## Predict height from width

.question[
On average, how tall are paintings that are 60 inches wide?
`$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$`
]

--


```r
3.62 + 0.78 * 60
```

```
## [1] 50.42
```

"On average, we expect paintings that are 60 inches wide to be 50.42 inches high."

**Warning:** We "expect" this to happen, but there will be some variability. (We'll
learn about measuring the variability around the prediction later.)

---

## Prediction vs. extrapolation

.question[
On average, how tall are paintings that are 400 inches wide?
`$$\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$`
]


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/extrapolate-1.png)&lt;!-- --&gt;

---

## Watch out for extrapolation!

&gt; "When those blizzards hit the East Coast this winter, it proved to my satisfaction 
that global warming was a fraud. That snow was freezing cold. But in an alarming 
trend, temperatures this spring have risen. Consider this: On February 6th it was 10 
degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So 
clearly folks the climate debate rages on."&lt;sup&gt;1&lt;/sup&gt;  &lt;br&gt;
Stephen Colbert, April 6th, 2010

.footnote[
[1] OpenIntro Statistics. "Extrapolation is treacherous." OpenIntro Statistics.
]

---

class: center, middle

# Measuring model fit

---

## Measuring the strength of the fit

- The strength of the fit of a linear model is most commonly evaluated using `\(R^2\)`.

- It tells us what percent of variability in the response variable is explained by 
the model.

- The remainder of the variability is explained by variables not included in the 
model.

- `\(R^2\)` is sometimes called the coefficient of determination.

---

## Obtaining `\(R^2\)` in R

- Height vs. width
.small[

```r
glance(m_ht_wt)
```

```
## # A tibble: 1 x 12
##   r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.
## # â€¦ with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;
```

```r
glance(m_ht_wt)$r.squared # extract R-squared
```

```
## [1] 0.6829468
```
]

Roughly 68% of the variability in heights of paintings can be explained by their
widths.

- Height vs. lanscape features
.small[

```r
glance(m_ht_lands)$r.squared
```

```
## [1] 0.03456724
```
]

class: center, middle

# Tidy regression output

---

Let's revisit the model predicting heights of paintings from their widths:


```r
pp &lt;- read_csv("data/paris_paintings.csv", 
               na = c("n/a", "", "NA"))
```


```r
m_ht_wt &lt;- lm(Height_in ~ Width_in, data = pp)
```

---

## Not-so-tidy regression output

- You might come accross these in your googling adventures, but we'll try to stay away from them

- Not because they are wrong, but because they don't result in tidy data frames as results.

---

## Not-so-tidy regression output (1)

Option 1:


```r
m_ht_wt
```

```
## 
## Call:
## lm(formula = Height_in ~ Width_in, data = pp)
## 
## Coefficients:
## (Intercept)     Width_in  
##      3.6214       0.7808
```

---

## Not-so-tidy regression output (2)

Option 2:


```r
summary(m_ht_wt)
```

```
## 
## Call:
## lm(formula = Height_in ~ Width_in, data = pp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -86.714  -4.384  -2.422   3.169  85.084 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 3.621406   0.253860   14.27   &lt;2e-16
## Width_in    0.780796   0.009505   82.15   &lt;2e-16
## 
## Residual standard error: 8.304 on 3133 degrees of freedom
##   (258 observations deleted due to missingness)
## Multiple R-squared:  0.6829,	Adjusted R-squared:  0.6828 
## F-statistic:  6749 on 1 and 3133 DF,  p-value: &lt; 2.2e-16
```

---

## Review

.question[
What makes a data frame tidy?
]

--

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

---

## Tidy regression output

Achieved with functions from the broom package:

- `tidy`: Constructs a data frame that summarizes the model's statistical findings: coefficient estimates, *standard errors, test statistics, p-values*.

- `augment`: Adds columns to the original data that was modeled. This includes predictions and residuals.

- `glance`: Constructs a concise one-row summary of the model. This typically contains values such as `\(R^2\)`, adjusted `\(R^2\)`, *and residual standard error that are computed once for the entire model*.

---

## Tidy your model's statistical findings


```r
tidy(m_ht_wt)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    3.62    0.254        14.3 8.82e-45
## 2 Width_in       0.781   0.00950      82.1 0
```


```r
tidy(m_ht_wt) %&gt;%
  select(term, estimate) %&gt;%
  mutate(estimate = round(estimate, 3))
```

```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)    3.62 
## 2 Width_in       0.781
```

---

## Augment data with model results

New variables of note (for now):

- `.fitted`: Predicted value of the response variable
- `.resid`: Residuals

.small[

```r
augment(m_ht_wt) %&gt;%
  slice(1:5)
```

```
## # A tibble: 5 x 9
##   .rownames Height_in Width_in .fitted .resid     .hat .sigma .cooksd .std.resid
##   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1 1                37     29.5    26.7  10.3  0.000399   8.30 3.10e-4      1.25 
## 2 2                18     14      14.6   3.45 0.000396   8.31 3.42e-5      0.415
## 3 3                13     16      16.1  -3.11 0.000361   8.31 2.54e-5     -0.375
## 4 4                14     18      17.7  -3.68 0.000337   8.31 3.30e-5     -0.443
## 5 5                14     18      17.7  -3.68 0.000337   8.31 3.30e-5     -0.443
```
]

--

.question[
Why might we be interested in these new variables?
]

---

## Residuals plot

.small[

```r
m_ht_wt_aug &lt;- augment(m_ht_wt)
ggplot(m_ht_wt_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = 2) +
  labs(x = "Predicted height", y = "Residuals")
```

![](12-linear-models_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;
]

--

.question[
What does this plot tell us about the fit of the linear model?
]

---

## Glance to assess model fit


```r
glance(m_ht_wt)
```

```
## # A tibble: 1 x 12
##   r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.
## # â€¦ with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;
```

--


```r
glance(m_ht_wt)$r.squared
```

```
## [1] 0.6829468
```

--

```
The $R^2$ is 68.29%.
```

The `\(R^2\)` is 68.29%.

--


```r
glance(m_ht_wt)$adj.r.squared
```

```
## [1] 0.6828456
```

---

class: center, middle

# Exploring linearity

---

## Data: Paris Paintings

![](12-linear-models_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;

---

## Price vs. width

.question[
Describe the relationship between price and width of painting.
]


```
## Warning: Removed 256 rows containing missing values (geom_point).
```

![](12-linear-models_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;

---

## Let's focus on paintings with `Width_in &lt; 100`


```r
pp_wt_lt_100 &lt;- pp %&gt;% 
  filter(Width_in &lt; 100)
```

---

## Price vs. width

.question[
Which plot shows a more linear relationship?
]

.small[
  
.pull-left[
![](12-linear-models_files/figure-html/unnamed-chunk-27-1.png)&lt;!-- --&gt;
]

.pull-right[
![](12-linear-models_files/figure-html/unnamed-chunk-28-1.png)&lt;!-- --&gt;
]

]

---

## Price vs. width, residuals

.question[
Which plot shows a residuals that are uncorrelated with predicted values from the model?
]

.small[
  
.pull-left[
![](12-linear-models_files/figure-html/unnamed-chunk-29-1.png)&lt;!-- --&gt;
]

.pull-right[
![](12-linear-models_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;
]

]

--

.question[
What's the unit of residuals?
]

---

## Transforming the data

- We saw that `price` has a right-skewed distribution, and the relationship between price and width of painting is non-linear.

--

- In these situations a transformation applied to the response variable may be useful.

--

- In order to decide which transformation to use, we should examine the distribution of the response variable.

--

- The extremely right skewed distribution suggests that a log transformation may 
be useful.
    - log = natural log, `\(ln\)`
    - Default base of the `log` function in R is the natural log: &lt;br&gt;
    `log(x, base = exp(1))`
    
---

## Logged price vs. width

.question[
How do we interpret the slope of this model?
]


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;

---

## Interpreting models with log transformation


```r
m_lprice_wt &lt;- lm(log(price) ~ Width_in, data = pp_wt_lt_100)
m_lprice_wt %&gt;%
  tidy() %&gt;%
  select(term, estimate) %&gt;%
  mutate(estimate = round(estimate, 3))
```

```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)    4.67 
## 2 Width_in       0.019
```

---

## Linear model with log transformation

$$ \widehat{log(price)} = 4.67 + 0.02 Width $$

--

- For each additional inch the painting is wider, the log price of the
painting is expected to be higher, on average, by 0.02 livres.

--

- which is not a very useful statement...

---

## Working with logs

- Subtraction and logs: `\(log(a) âˆ’ log(b) = log(a / b)\)`

--

- Natural logarithm: `\(e^{log(x)} = x\)`

--

- We can use these identities to "undo" the log transformation

---

## Interpreting models with log transformation

The slope coefficient for the log transformed model is 0.02, meaning the log 
price difference between paintings whose widths are one inch apart is predicted 
to be 0.02 log livres.

--

$$ log(\text{price for width x+1}) - log(\text{price for width x}) = 0.02 $$

--

$$ log\left(\frac{\text{price for width x+1}}{\text{price for width x}}\right) = 0.02 $$

--

$$ e^{log\left(\frac{\text{price for width x+1}}{\text{price for width x}}\right)} = e^{0.02} $$

--

$$ \frac{\text{price for width x+1}}{\text{price for width x}} \approx 1.02 $$

--

For each additional inch the painting is wider, the price of the
painting is expected to be higher, on average, by a factor of 1.02.

---

## Shortcuts in R


```r
m_lprice_wt %&gt;%
  tidy() %&gt;%
  select(term, estimate) %&gt;%
  mutate(estimate = round(estimate, 3))
```

```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)    4.67 
## 2 Width_in       0.019
```


```r
m_lprice_wt %&gt;%
  tidy() %&gt;%
  select(term, estimate) %&gt;%
  mutate(estimate = round(exp(estimate), 3))
```

```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)   107.  
## 2 Width_in        1.02
```

---

## Recap

- Non-constant variance is one of the most common model violations, however it 
is usually fixable by transforming the response (y) variable.

--

- The most common transformation when the response variable is right skewed is 
the log transform: `\(log(y)\)`, especially useful when the response variable is 
(extremely) right skewed.

--

- This transformation is also useful for variance stabilization.

--

- When using a log transformation on the response variable the interpretation of 
the slope changes: *"For each unit increase in x, y is expected on average to be higher/lower &lt;br&gt; by a factor of `\(e^{b_1}\)`."*

--

- Another useful transformation is the square root: `\(\sqrt{y}\)`, especially 
useful when the response variable is counts.

---

## Transform, or learn more?

- Data transformations may also be useful when the relationship is non-linear

- However in those cases a polynomial regression may be more appropriate
  + This is beyond the scope of this course, but youâ€™re welcomed to try it for your final project, and Iâ€™d be happy to provide further guidance

---

## Aside: when `\(y = 0\)`

In some cases the value of the response variable might be 0, and


```r
log(0)
```

```
## [1] -Inf
```

--

The trick is to add a very small number to the value of the response variable for these cases so that the `log` function can still be applied:


```r
log(0 + 0.00001)
```

```
## [1] -11.51293
```

---

class: center, middle

# The linear model with multiple predictors

---

## Price, surface area, and living artist

.question[
What is the typical surface area for paintings?
]

![](12-linear-models_files/figure-html/unnamed-chunk-37-1.png)&lt;!-- --&gt;

--

Less than 1000 square inches (which is roughly a painting that is 31in x 31in). There are very few paintings that have surface area above 5000 square inches.

---

## Price, surface area, and living artist

For simplicity let's focus on the paintings with `Surface &lt; 5000`:

![](12-linear-models_files/figure-html/unnamed-chunk-38-1.png)&lt;!-- --&gt;

---

## Price vs. surface and living artist

.question[
Does the relationship between surface and logged price vary by whether or not
the artist is living?
]

.small[

```r
ggplot(data = pp_Surf_lt_5000,
       mapping = aes(y = log(price), x = Surface, 
                     color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(color = "Living artist")
```

```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-39-1.png)&lt;!-- --&gt;
]

---

## Modelling with main effects 


```r
m_main &lt;- lm(log(price) ~ Surface + factor(artistliving), 
             data = pp_Surf_lt_5000)
m_main %&gt;%
  tidy() %&gt;%
  select(term, estimate) %&gt;%
  mutate(estimate = round(estimate, 5))
```

```
## # A tibble: 3 x 2
##   term                  estimate
##   &lt;chr&gt;                    &lt;dbl&gt;
## 1 (Intercept)            4.88   
## 2 Surface                0.00027
## 3 factor(artistliving)1  0.137
```

--

Linear model:

$$ \widehat{log(price)} = 4.88 + 0.00027~surface + 0.14~artistliving $$

--

- Plug in 0 for `artistliving` to get the linear model for paintings by non-living
artists.

- Plug in 1 for `artistliving` to get the linear model for paintings by living
artists.

---

## Interpretation of main effects

.pull-left[
![](12-linear-models_files/figure-html/unnamed-chunk-41-1.png)&lt;!-- --&gt;
]

.pull-right[
- Non-living artist: 
`\(\widehat{log(price)} = 4.88 + 0.00027~surface + 0.14 \times 0\)`
`\(= 4.88 + 0.00027~surface\)`

- Living artist: 
`\(\widehat{log(price)} = 4.88 + 0.00027~surface + 0.14 \times 1\)`
`\(= 5.02 + 0.00027~surface\)`
]

- Rate of change in price as the surface area of the painting increases does 
not vary between paintings by living and non-living artists (same slope), 

- Paintings by living artists are consistently more expensive than paintings by
non-living artists (different intercept).

class: center, middle

# Inference for regression

---

## Riders in Florence, MA

.small[
The Pioneer Valley Planning Commission collected data in Florence, MA for 90 days from April 5 to November 15, 2005 using a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.

- `hightemp` daily high temperature (in degrees Fahrenheit)
- `volume` estimated number of trail users that day (number of breaks recorded)
]


```r
library(mosaicData)
```

```
## Warning: package 'mosaicData' was built under R version 3.6.2
```

```r
data(RailTrail)
```


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-43-1.png)&lt;!-- --&gt;


---

## Coefficient interpretation

.question[
ðŸ‘¤ Interpret the coefficients of the regression model for predicting volume (estimated number of trail users that day) from hightemp (daily high temperature, in F).
]


```r
m_riders &lt;- lm(volume ~ hightemp, data = RailTrail)
tidy(m_riders) %&gt;%
  select(term, estimate)
```

```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)   -17.1 
## 2 hightemp        5.70
```



---

## Uncertainty around the slope


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-45-1.png)&lt;!-- --&gt;

---


## Bootstrapping the data, once


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-46-1.png)&lt;!-- --&gt;


```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)   -13.4 
## 2 hightemp        5.62
```

---

## Bootstrapping the data, once again


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-48-1.png)&lt;!-- --&gt;


```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)    21.2 
## 2 hightemp        5.11
```

---

## Bootstrapping the data, again


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-50-1.png)&lt;!-- --&gt;


```
## # A tibble: 2 x 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)   102.  
## 2 hightemp        3.91
```

---

## Bootstrapping the regression line


```
## `geom_smooth()` using formula 'y ~ x'
```

![](12-linear-models_files/figure-html/unnamed-chunk-52-1.png)&lt;!-- --&gt;

---

## Bootstrap interval for the slope

.small[

```r
boot_dist &lt;- RailTrail %&gt;%
  specify(response = volume, explanatory = hightemp) %&gt;%
  generate(reps = 1000, type = "bootstrap") %&gt;%
  calculate(stat = "slope")
```
]

![](12-linear-models_files/figure-html/unnamed-chunk-54-1.png)&lt;!-- --&gt;

---

## Bootstrap interval for the slope

.question[
Interpret the bootstrap interval in context of the data.
]


```r
boot_dist %&gt;%
  summarise(l = quantile(stat, 0.025), 
            u = quantile(stat, 0.975))
```

```
## # A tibble: 1 x 2
##       l     u
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  4.27  7.42
```

---

## Hypothesis testing for the slope

`\(H_0\)`: No relationship, `\(\beta_1 = 0\)`  
`\(H_A\)`: There is a relationship, `\(\beta_1 \ne 0\)`

--

.small[

```r
null_dist &lt;- RailTrail %&gt;%
  specify(volume ~ hightemp) %&gt;% 
  hypothesize(null = "independence") %&gt;%
  generate(reps = 1000, type = "permute") %&gt;%
  calculate(stat = "slope")
```
]

--


```
## Warning: Removed 2 rows containing missing values (geom_bar).
```

![](12-linear-models_files/figure-html/unnamed-chunk-57-1.png)&lt;!-- --&gt;

---

## Finding the p-value


```r
null_dist %&gt;%
  filter(stat &gt;= m_riders$coefficients[2]) %&gt;%
  summarise(p_val = 2 * (n()/1000))
```

```
## # A tibble: 1 x 1
##   p_val
##   &lt;dbl&gt;
## 1     0
```

---

## Hypothesis testing for the slope

... the CLT way


```r
tidy(m_riders)
```

```
## # A tibble: 2 x 5
##   term        estimate std.error statistic       p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;
## 1 (Intercept)   -17.1     59.4      -0.288 0.774        
## 2 hightemp        5.70     0.848     6.72  0.00000000171
```

---

## Conditions for inference for regression

Three conditions:

--

1. Observations should be independent

--

2. Residuals should be randomly distributed around 0

--

3. Residuals should be nearly normally distributed, centered at 0

--

4. Residuals should have constant variance


---

## Checking independence

One consideration might be time series structure of the data. We can check whether one residual depends on the previous by plotting the residuals in the order of data collection.


```r
m_riders_aug &lt;- augment(m_riders)
ggplot(data = m_riders_aug, aes(x = 1:nrow(m_riders_aug), y = .resid)) +
  geom_point() +
  labs(x = "Index", y = "Residual")
```

![](12-linear-models_files/figure-html/unnamed-chunk-60-1.png)&lt;!-- --&gt;

---

## Checking distribution of residuals around 0 and constant variance


```r
ggplot(data = m_riders_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, color = "gray") +
  labs(y = "Residuals", x = "Predicted values, y-hat")
```

![](12-linear-models_files/figure-html/unnamed-chunk-61-1.png)&lt;!-- --&gt;

---

## Checking normality of residuals


```r
ggplot(data = m_riders_aug, aes(x = .resid)) +
  geom_histogram(binwidth = 30) +
  labs(x = "Residuals")
```

![](12-linear-models_files/figure-html/unnamed-chunk-62-1.png)&lt;!-- --&gt;

---

## What else can we do with these p-values?

Model selection based on p-value method:

- Backwards elimination: Remove the variable with the highest p-value, re-fit, 
repeat until all variables are significant at the chosen significance level.
- Forward selection: Start with the variable with the lowest p-value, re-fit,
repeat until all no more significant variables left at the chosen significance level.

This is sometimes seen in the literature, but is not recommended!

- Relies on arbitrary significance level cutoff
- Multiple testing!

Instead use adjusted `\(R^2\)`, or AIC, or other criterion based model selection.

---

## My thoughts 

Both sets of p-values are largely useless other than in a few very narrow circumstances

* Coefficient p-value 
    - If you truly want to know if a coefficient is significantly different from zero (taking the other predictors into account) then great
    - If you want to know which predictors are important - use model selection

* Overall model p-value
    - Strawman comparison, I don't really care that my model is better than a mean only model, the latter is almost always going to be terrible

---

class: center, middle

# Testing errors and power

---

## Testing errors

- Type 1: Reject `\(H_0\)` when you shouldn't have
    + P(Type 1 error) = `\(\alpha\)`
    
- Type 2: Fail to reject `\(H_0\)` when you should have
    + P(Type 2 error) is harder to calculate, but increases as `\(\alpha\)` decreases

&lt;div class="question"&gt;
ðŸ‘¤ In a court of law

&lt;ul&gt;
&lt;li&gt; Null hypothesis: Defendant is innocent
&lt;li&gt; Alternative hypothesis: Defendant is guilty
&lt;/ul&gt;

Which is worse: Type 1 or Type 2 error?
&lt;/div&gt;

---

## Probabilities of testing errors

- P(Type 1 error) = `\(\alpha\)`

- P(Type 2 error) = 1 - Power

- Power = P(correctly rejecting the null hypothesis)


&lt;div class="question"&gt;
ðŸ‘¥ Fill in the blanks in terms of correctly/incorrectly rejecting/failing to reject the null hypothesis:

&lt;ul&gt;
&lt;li&gt; `\(alpha\)` is the probability of ___.
&lt;li&gt; 1 - Power is the probability of ___.
&lt;/ul&gt;
&lt;/div&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
