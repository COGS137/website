---
title: "Multiple linear regression + Model selection"
author: "Prof Ellis"
date: "2021-11-01"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/cogs137-logo-hex.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(DT)
# For nonsese...
library(emo)
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
opts_chunk$set(fig.height = 2.5, fig.width = 5, dpi = 300) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

<!-- two different lectures in here  -->

---

class: center, middle

# The linear model with multiple predictors

---

## Price, surface area, and living artist

.question[
What is the typical surface area for paintings?
]

.small[
```{r echo=FALSE,warning=FALSE}
ggplot(data = pp, 
       mapping = aes(y = log(price), x = Surface, color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  labs(color = "Living artist")
```
]
--

Less than 1000 square inches (which is roughly a painting that is 31in x 31in). There are very few paintings that have surface area above 5000 square inches.

---

## Price, surface area, and living artist

For simplicity let's focus on the paintings with `Surface < 5000`:

```{r echo=FALSE, warning=FALSE}
pp_Surf_lt_5000 <- pp %>%
  filter(Surface < 5000)

ggplot(data = pp_Surf_lt_5000, 
       mapping = aes(y = log(price), x = Surface, color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  labs(color = "Living artist")
```

---

## Price vs. surface and living artist

.question[
Does the relationship between surface and logged price vary by whether or not
the artist is living?
]

.small[
```{r fig.height=1.75}
ggplot(data = pp_Surf_lt_5000,
       mapping = aes(y = log(price), x = Surface, 
                     color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(color = "Living artist")
```
]

---

## Modelling with main effects 

```{r}
m_main <- lm(log(price) ~ Surface + factor(artistliving), 
             data = pp_Surf_lt_5000)
m_main %>%
  tidy() %>%
  select(term, estimate) %>%
  mutate(estimate = round(estimate, 5))
```

--

Linear model:

$$ \widehat{log(price)} = 4.88 + 0.00027~surface + 0.14~artistliving $$

--
.small[
- Plug in 0 for `artistliving` to get the linear model for paintings by non-living
artists.

- Plug in 1 for `artistliving` to get the linear model for paintings by living
artists.
]
---

## Interpretation of main effects

.pull-left[
```{r fig.height=4, echo = FALSE}
ggplot(data = pp_Surf_lt_5000,
       aes(y = log(price), x = Surface, color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = 4.88, slope = 0.00027, color = "#F57670", lwd = 1) +
  geom_abline(intercept = 5.02, slope = 0.00027, color = "#1FBEC3", lwd = 1) +
  labs(color = "Living artist")
```
]

.pull-right[
- Non-living artist: 
$\widehat{log(price)} = 4.88 + 0.00027~surface + 0.14 \times 0$
$= 4.88 + 0.00027~surface$

- Living artist: 
$\widehat{log(price)} = 4.88 + 0.00027~surface + 0.14 \times 1$
$= 5.02 + 0.00027~surface$
]

- Rate of change in price as the surface area of the painting increases does 
not vary between paintings by living and non-living artists (same slope), 

- Paintings by living artists are consistently more expensive than paintings by
non-living artists (different intercept).

---

class: center, middle

# Inference for regression

---

## Riders in Florence, MA

.small[
The Pioneer Valley Planning Commission collected data in Florence, MA for 90 days from April 5 to November 15, 2005 using a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.

- `hightemp` daily high temperature (in degrees Fahrenheit)
- `volume` estimated number of trail users that day (number of breaks recorded)
]

```{r}
library(mosaicData)
data(RailTrail)
```

```{r echo=FALSE, fig.height=2.25}
ggplot(data = RailTrail, mapping = aes(x = hightemp, y = volume)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  xlim(40, 100) +
  ylim(120, 750) +
  labs(ylab = "Volume", xlab)
```


---

## Coefficient interpretation

.question[
`r emo::ji("bust_in_silhouette")` Interpret the coefficients of the regression model for predicting volume (estimated number of trail users that day) from hightemp (daily high temperature, in F).
]

```{r}
m_riders <- lm(volume ~ hightemp, data = RailTrail)
tidy(m_riders) %>%
  select(term, estimate)
```



---

## Uncertainty around the slope

```{r echo=FALSE}
ggplot(data = RailTrail, mapping = aes(x = hightemp, y = volume)) +
  geom_point() +
  geom_smooth(method = "lm", color = "black") +
  xlim(40, 100) +
  ylim(120, 750)
```

---


## Bootstrapping the data, once

```{r echo=FALSE}
set.seed(18472)

boot_samples <- RailTrail %>%
  specify(volume ~ hightemp) %>% 
  generate(reps = 50, type = "bootstrap")

first_boot_sample <- boot_samples %>%
  filter(replicate == 1)

ggplot(first_boot_sample, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  scale_color_manual(values = "gray") +
  xlim(40, 100) +
  ylim(120, 750)
```

```{r echo=FALSE}
m_boot <- lm(volume ~ hightemp, data = first_boot_sample)
tidy(m_boot) %>%
  select(term, estimate)
```

---

## Bootstrapping the data, once again

```{r echo=FALSE}
second_boot_sample <- boot_samples %>%
  filter(replicate == 2)

ggplot(second_boot_sample, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  scale_color_manual(values = "gray") +
  xlim(40, 100) +
  ylim(120, 750)
```

```{r echo=FALSE}
m_boot <- lm(volume ~ hightemp, data = second_boot_sample)
tidy(m_boot) %>%
  select(term, estimate)
```

---

## Bootstrapping the data, again

```{r echo=FALSE}
third_boot_sample <- boot_samples %>%
  filter(replicate == 3)

ggplot(third_boot_sample, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  scale_color_manual(values = "gray") +
  xlim(40, 100) +
  ylim(120, 750)
```

```{r echo=FALSE}
m_boot <- lm(volume ~ hightemp, data = third_boot_sample)
tidy(m_boot) %>%
  select(term, estimate)
```

---

## Bootstrapping the regression line

```{r echo=FALSE}
ggplot(boot_samples, aes(x = hightemp, y = volume, color = factor(replicate))) +
  geom_smooth(method = "lm", se = FALSE, lwd = 0.2) +
  geom_abline(slope = m_riders$coefficients[2], intercept = m_riders$coefficients[1], lwd = 1, color = "black") +
  theme(legend.position = "none") +
  scale_color_manual(values = rep("gray", 100)) +
  ylim(100, 750)
```

---

## Bootstrap interval for the slope

.small[
```{r}
boot_dist <- RailTrail %>%
  specify(response = volume, explanatory = hightemp) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "slope")
```
]

```{r echo=FALSE}
ci <- boot_dist %>%
  summarise(l = quantile(stat, 0.025), u = quantile(stat, 0.975))

ggplot(data = boot_dist, mapping = aes(x = stat)) +
  geom_histogram(binwidth = 0.25) +
  geom_vline(xintercept = ci$l, color = "#8E2C90") +
  geom_vline(xintercept = ci$u, color = "#8E2C90")
```

---

## Bootstrap interval for the slope

.question[
Interpret the bootstrap interval in context of the data.
]

```{r}
boot_dist %>%
  summarise(l = quantile(stat, 0.025), 
            u = quantile(stat, 0.975))
```

---

## Hypothesis testing for the slope

$H_0$: No relationship, $\beta_1 = 0$  
$H_A$: There is a relationship, $\beta_1 \ne 0$

--

.small[
```{r}
null_dist <- RailTrail %>%
  specify(volume ~ hightemp) %>% 
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")
```
]

--

```{r echo=FALSE, fig.height = 2.25}
ggplot(data = null_dist, mapping = aes(x = stat)) +
  geom_histogram(binwidth = 0.25) +
  xlim(-6, 6) +
  geom_vline(xintercept = m_riders$coefficients[2], color = color_palette$red) +
  geom_vline(xintercept = -1*m_riders$coefficients[2], color = color_palette$red) 
```

---

## Finding the p-value

```{r}
null_dist %>%
  filter(stat >= m_riders$coefficients[2]) %>%
  summarise(p_val = 2 * (n()/1000))
```

---

## Hypothesis testing for the slope

... the CLT way

```{r}
tidy(m_riders)
```

---

## Conditions for inference for regression

Three conditions:

--

1. Observations should be independent

--

2. Residuals should be randomly distributed around 0

--

3. Residuals should be nearly normally distributed, centered at 0

--

4. Residuals should have constant variance


---

## Checking independence

One consideration might be time series structure of the data. We can check whether one residual depends on the previous by plotting the residuals in the order of data collection.

```{r fig.height = 2.25}
m_riders_aug <- augment(m_riders)
ggplot(data = m_riders_aug, aes(x = 1:nrow(m_riders_aug), y = .resid)) +
  geom_point() +
  labs(x = "Index", y = "Residual")
```

---

## Checking distribution of residuals around 0 and constant variance

```{r}
ggplot(data = m_riders_aug, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, lty = 3, color = "gray") +
  labs(y = "Residuals", x = "Predicted values, y-hat")
```

---

## Checking normality of residuals

```{r}
ggplot(data = m_riders_aug, aes(x = .resid)) +
  geom_histogram(binwidth = 30) +
  labs(x = "Residuals")
```

---

## What else can we do with these p-values?

Model selection based on p-value method:

- Backwards elimination: Remove the variable with the highest p-value, re-fit, 
repeat until all variables are significant at the chosen significance level.
- Forward selection: Start with the variable with the lowest p-value, re-fit,
repeat until all no more significant variables left at the chosen significance level.

This is sometimes seen in the literature, but is not recommended!

- Relies on arbitrary significance level cutoff
- Multiple testing!

Instead use adjusted $R^2$, or AIC, or other criterion based model selection.

---

## My thoughts 

Both sets of p-values are largely useless other than in a few very narrow circumstances

* Coefficient p-value 
    - If you truly want to know if a coefficient is significantly different from zero (taking the other predictors into account) then great
    - If you want to know which predictors are important - use model selection

* Overall model p-value
    - Strawman comparison, I don't really care that my model is better than a mean only model, the latter is almost always going to be terrible

---

class: center, middle

# Testing errors and power

---

## Testing errors

- Type 1: Reject $H_0$ when you shouldn't have
    + P(Type 1 error) = $\alpha$
    
- Type 2: Fail to reject $H_0$ when you should have
    + P(Type 2 error) is harder to calculate, but increases as $\alpha$ decreases

<div class="question">
`r emo::ji("bust_in_silhouette")` In a court of law

<ul>
<li> Null hypothesis: Defendant is innocent
<li> Alternative hypothesis: Defendant is guilty
</ul>

Which is worse: Type 1 or Type 2 error?
</div>

---

## Probabilities of testing errors

- P(Type 1 error) = $\alpha$

- P(Type 2 error) = 1 - Power

- Power = P(correctly rejecting the null hypothesis)


<div class="question">
`r emo::ji("busts_in_silhouette")` Fill in the blanks in terms of correctly/incorrectly rejecting/failing to reject the null hypothesis:

<ul>
<li> $alpha$ is the probability of ___.
<li> 1 - Power is the probability of ___.
</ul>
</div>


## Announcements

- HW 03 due next Monday

---

class: center, middle

## Data cleaning

---

## Load data

```{r message=FALSE}
pp <- read_csv("data/paris_paintings.csv", 
               na = c("n/a", "", "NA"))
```

---

## Shape and material

Collapse levels of `Shape` and `mat`erial variables with `forcats::fct_collapse`:

.small[
```{r}
pp <- pp %>%
  mutate(
    Shape = fct_collapse(Shape, oval = c("oval", "ovale"),
                                round = c("round", "ronde"),
                                squ_rect = "squ_rect",
                                other = c("octogon", "octagon", "miniature")),
    mat = fct_collapse(mat, metal = c("a", "br", "c"),
                            canvas = c("co", "t", "ta"),
                            paper = c("p", "ca"),
                            wood = "b",
                            other = c("e", "g", "h", "mi", "o", "pa", "v", "al", "ar", "m"))
  )
```
]

---

## Review fixes

.pull-left[
```{r}
pp %>%
  count(Shape)
```
]

.pull-right[
```{r}
pp %>%
  count(mat)
```
]

---

class: center, middle

## Review

---

## Main effects, numerical predictors

```{r}
m_main_n <- lm(log(price) ~ Width_in + Height_in, data = pp)
tidy(m_main_n) %>%
  select(term, estimate) %>%
  mutate(estimate = round(estimate, 3))
```

---

## Visualizing the model

```{r echo=FALSE}
library(plotly)
library(widgetframe)
```

```{r echo=FALSE}
p <- plot_ly(pp, x = ~Width_in, y = ~Height_in, z = ~log(price),
        marker = list(size = 3,
                       color = "lightgray",
                       alpha = 0.5,
                       line = list(color = "gray",
                                   width = 2))) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = "Width (in)"),
                      yaxis = list(title = "Height (in)"),
                      zaxis = list(title = "Log(price)"))) %>%
  config(displayModeBar = FALSE)

frameWidget(p, width = '100%', height = '70%')
```


---

## Main effects, numerical and categorical predictors

.small[
```{r}
pp_Surf_lt_5000 <- pp %>% filter(Surface < 5000)
m_main <- lm(log(price) ~ Surface + factor(artistliving), data = pp_Surf_lt_5000)
tidy(m_main) %>%
  select(term, estimate) %>%
  mutate(exp_estimate = round(exp(estimate), 4))
```

```{r include=FALSE}
m_main_coefs <- tidy(m_main) %>% 
  mutate(exp_estimate = round(exp(estimate), 4)) %>%
  select(term, exp_estimate)
```

- All else held constant, for each additional square inch in painting's surface area, the price of the painting is predicted, on average, to be higher by a factor of `r m_main_coefs %>% filter(term == "Surface") %>% pull(exp_estimate)`.

- All else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of `r m_main_coefs %>% filter(term == "factor(artistliving)1") %>% pull(exp_estimate)` compared to paintings by an artist who is no longer alive.

- Paintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be `r m_main_coefs %>% filter(term == "(Intercept)") %>% pull(exp_estimate)` livres.
]

---

## What went wrong?

.question[
Why is our linear regression model different from what we got from `geom_smooth(method = "lm")`?
]

.pull-left[
```{r echo=FALSE, fig.height=4}
ggplot(pp_Surf_lt_5000, aes(x = Surface, y = log(price), color = factor(artistliving))) + 
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm") +
  labs(x = "Surface", y = "Log(price)", color = "Living artist")
```
]
.pull-right[
```{r echo=FALSE, fig.height=4}
m_pr <- lm(log(price) ~ Surface + factor(artistliving), data = pp_Surf_lt_5000)
m_pr_aug <- augment(m_pr)

ggplot(data = m_pr_aug, mapping = aes(y = `log(price)`, x = Surface, color = `factor(artistliving)`)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(y = .fitted))
```
]

---

## What went wrong? (cont.)

- The way we specified our model only lets `artistliving` affect the intercept.

- Model implicitly assumes that paintings with living and deceased artists have the *same slope* and only allows for *different intercepts*.  

- What seems more appropriate in this case? 
    
    * Same slope and same intercept for both colors
    
    * Same slope and different intercept for both colors
    
    * Different slope and different intercept for both colors?

---

## Interacting explanatory variables

- Including an interaction effect in the model allows for different slopes, i.e. 
nonparallel lines.

- This implies that the regression coefficient for an explanatory variable would 
change as another explanatory variable changes.

- This can be accomplished by adding an interaction variable: the product of two 
explanatory variables.

---

## Interaction: surface * artist living

.small[
```{r fig.height=2.5}
ggplot(data = pp_Surf_lt_5000,
       mapping = aes(y = log(price), x = Surface, 
                     color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm") +
  labs(x = "Surface", y = "Log(price)", color = "Living artist")
```
]

---

## Modeling with interaction effects

.small[
```{r}
m_int <- lm(log(price) ~ Surface + factor(artistliving) +
              Surface * factor(artistliving), 
            data = pp_Surf_lt_5000)
tidy(m_int) %>% 
  select(term, estimate)
```
]

$$ \widehat{log(price)} = 4.91 + 0.00021~surface - 0.126~artistliving $$
$$+ ~ 0.00048~surface \times artistliving $$

---

## Interpretation of interaction effects

- Rate of change in price as the surface area of the painting increases does 
vary between paintings by living and non-living artists (different slopes), 

- Some paintings by living artists are more expensive than paintings by
non-living artists, and some are not (different intercept).

.small[
.pull-left[
- Non-living artist: 
$\widehat{log(price)} = 4.91 + 0.00021~surface$
$- 0.126 \times 0 + 0.00048~surface \times 0$
$= 4.91 + 0.00021~surface$

- Living artist: 
$\widehat{log(price)} = 4.91 + 0.00021~surface$
$- 0.126 \times 1 + 0.00048~surface \times 1$
$= 4.91 + 0.00021~surface$
$- 0.126 + 0.00048~surface$
$= 4.784 + 0.00069~surface$
]
.pull-right[
```{r fig.height=4, echo = FALSE}
ggplot(data = pp_Surf_lt_5000,
       aes(y = log(price), x = Surface, color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  stat_smooth(method = "lm", fullrange = TRUE)
```
]
]

---

## Third order interactions

- Can you? Yes

- Should you? Probably not if you want to interpret these interactions in context
of the data.

---

class: center, middle

# Quality of fit in MLR

---

## $R^2$

- $R^2$ is the percentage of variability in the response variable explained by the 
regression model.

```{r}
glance(m_main)$r.squared
glance(m_int)$r.squared
```

--

- Clearly the model with interactions has a higher $R^2$.

--

- However using $R^2$ for model selection in models with multiple explanatory 
variables is not a good idea as $R^2$ increases when **any** variable is added to the 
model.

---

## $R^2$ - first principles

$$ R^2 =  \frac{ SS\_{Reg} }{ SS\_{Total} } = 1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \right) $$

.question[
Calculate $R^2$ based on the output below.
]

```{r digits = 3}
anova(m_main)
```

---

## Adjusted $R^2$

$$ R^2\_{adj} = 1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \times \frac{n - 1}{n - k - 1} \right), $$

where $n$ is the number of cases and $k$ is the number of predictors in the model

--

- Adjusted $R^2$ doesn't increase if the new variable does not provide any new 
informaton or is completely unrelated.

--

- This makes adjusted $R^2$ a preferable metric for model selection in multiple
regression models.

---

## In pursuit of Occam's Razor

- Occam's Razor states that among competing hypotheses that predict equally well, 
the one with the fewest assumptions should be selected.

- Model selection follows this principle.

- We only want to add another variable to the model if the addition of that
variable brings something valuable in terms of predictive power to the model.

- In other words, we prefer the simplest best model, i.e. **parsimonious** model.

---

## Comparing models

It appears that adding the interaction actually increased adjusted $R^2$, so we 
should indeed use the model with the interactions.

```{r}
glance(m_main)$adj.r.squared
glance(m_int)$adj.r.squared
```

---

class: center, middle

# Model selection

---

## Backwards elimination

- Start with **full** model (including all candidate explanatory variables and all
candidate interactions)

- Remove one variable at a time, and select the model with the highest adjusted $R^2$

- Continue until adjusted $R^2$ does not increase

---

## Forward selection

- Start with **empty** model

- Add one variable (or interaction effect) at a time, and select the model with the 
highest adjusted $R^2$

- Continue until adjusted $R^2$ does not increase

---

## Model selection and interaction effects

If an interaction is included in the model, the main effects of both of
those variables must also be in the model

If a main effect is not in the model, then its interaction should not be 
in the model.

---

## Other model selection criteria

- Adjusted $R^2$ is one model selection criterion

- There are others out there (many many others!), we'll discuss some later in the 
course, and some you might see in another courses

---

class: center, middle

# Your turn

---

## Your turn

Work in teams on RStudio Cloud 

**Project:** Model selection for Paris Paintings

Make a copy and get started

---

## Planning

Decide on a subset of variables to consider for your analysis. 

- Think about it as focusing on certain aspects of the price determination, 
as opposed to all aspects. 

- You're allowed a maximum of 10 total variables to consider, including 
interactions. 

- The more variables you consider the longer model selection will take so 
keep that in mind.

---

## Planning (cont.)

Decide among these which variables might make sense to interact. Remember, 
we consider interactions when variables might behave differently for various 
levels of another variable. Ideally, you would get expert guidance for
choosing interactions. Below is a list of interactions compiled by them 
that might be potentially interesting:

.small[
- School of painting & landscape variables: `school_pntg` & `landsALL` / `lands_figs` / `lands_ment`
- Landscapes & paired paintings: `landsALL` / `lands_figs` / `lands_ment` & `paired`
- Other artists & paired paintings: `othartist` & `paired`
- Size & paired paintings: `surface` & `paired`
- Size & figures: `surface` & `figures` / `nfigures`
- Dealer & previous owner: `dealer` & `prevcoll`
- Winning bidder & prevcoll: `endbuyer` & `prevcoll`
- Winning bidder & artist living: `winningbiddertype` & `artistliving`
]

This is not an exhaustive list, so you might come up with others.

---

## Model fitting, selection, and interpretation

- Use backwards elimination to do model selection. Make sure to show 
each step of decision (though you don't have to interpret the models at 
each stage).

- Provide interpretations for the slopes for the final model you arrive at 
and create at least one visualization that supports your narrative.
