[
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "content/labs/03-lab-viz.html",
    "href": "content/labs/03-lab-viz.html",
    "title": "Lab 03 - Data Visualization",
    "section": "",
    "text": "A note on expectations: For each exercise, include any relevant output (tables, summary statistics, plots) in your answer along with text to guide the reader. Place any relevant R code in a code chunk, any relevant text outside of code chunks, and hit Knit HTML.\nSome define statistics as the field that focuses on turning information into knowledge. The first step in that process is to summarize and describe raw information - the data. In this lab we explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While outside the scope of this lab, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads |>\n  arrange(unemployment_rate)\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\nNote that your output here likely has a whole bunch of decimal places in the unemployment variable? You likely don’t want all those values to be displayed.\nThere are two ways we can address this problem. One would be to round the unemployment_rate variable in the dataset or we can change the number of digits displayed, without touching the input data.\nBelow are instructions for how you would do both of these:\n\nRound unemployment_rate: We create a new variable with the mutate function. In this case, we’re overwriting the existing unemployment_rate variable, by rounding it to 4 decimal places.For example, the call to mutate would be: mutate(unemployment_rate = round(unemployment_rate, digits = 4))\nChange displayed number of digits without touching data: We can add an option to our R Markdown document to change the displayed number of digits in the output. To do so, add a new chunk, and set:\n\n\noptions(digits = 2)\n\nNote that the digits in options is scientific digits, and in round they are decimal places. If you’re thinking “Wouldn’t it be nice if they were consistent?”, you’re right…\nYou don’t need to do both of these; that would be redundant. The next exercise asks you to choose one.\n\nExercise 1\nWhich of these options, changing the input data or altering the number of digits displayed without touching the input data, is the better option? Explain your reasoning. Then, implement the option you chose."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\n\nThe desc function specifies that we want unemployment_rate in descending order.\n\ncollege_recent_grads |>\n  arrange(desc(unemployment_rate)) |>\n  select(rank, major, unemployment_rate)\n\n\nExercise 2\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "Lab 03 - Data Visualization",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\n\n\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nExercise 3\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\n\nExercise 4\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\nWe can also calculate summary statistics for this distribution using the summarise function. Note here that you can calculate multiple summary statistics within a single summarise call:\n\ncollege_recent_grads |>\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n\n\nExercise 5\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\n\n\nExercise 6\nNow, plot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\n\nExercise 7\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Also note that we are looking for the highest statistic, so make sure if you arrange to do so in the correct direction.\n\n\nExercise 8\nWhich major category is the least popular in this sample?"
  },
  {
    "objectID": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "href": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "title": "Lab 03 - Data Visualization",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories <- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads <- college_recent_grads |>\n  mutate(major_type = case_when(major_category %in% stem_categories ~ \"stem\",\n                                TRUE ~ \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx < y\nless than\n\n\nx > y\ngreater than\n\n\nx <= y\nless than or equal to\n\n\nx >= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads |>\n  filter(\n    major_type == \"stem\",\n    median < 36000\n  )\n\n\nExercise 9\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "Lab 03 - Data Visualization",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nExercise 10\nCreate a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html",
    "href": "content/labs/04-lab-modelling.html",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "",
    "text": "Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings.\n\n\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. link.\nFor this assignment you will analyze the data from this study in order to learn what goes into a positive professor evaluation.\nThe data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors.\n\n\n\n\n\n\nImportant\n\n\n\nThis lab is maybe longer than what you’ll be able to complete in an hour. We will be looking to see that you minimally ran and interpreted at least a single model (completed Part 3). If you’re able to finish the whole thing, awesome! If not, that’s OK."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#codebook",
    "href": "content/labs/04-lab-modelling.html#codebook",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Codebook",
    "text": "Codebook\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nscore\nAverage professor evaluation score: (1) very unsatisfactory - (5) excellent\n\n\nrank\nRank of professor: teaching, tenure track, tenure\n\n\nethnicity\nEthnicity of professor: not minority, minority\n\n\ngender\nGender of professor: female, male\n\n\nlanguage\nLanguage of school where professor received education: english or non-english\n\n\nage\nAge of professor\n\n\ncls_perc_eval\nPercent of students in class who completed evaluation\n\n\ncls_did_eval\nNumber of students in class who completed evaluation\n\n\ncls_students\nTotal number of students in class\n\n\ncls_level\nClass level: lower, upper\n\n\ncls_profs\nNumber of professors teaching sections in course in sample: single, multiple\n\n\ncls_credits\nNumber of credits of class: one credit (lab, PE, etc.), multi credit\n\n\nbty_f1lower\nBeauty rating of professor from lower level female: (1) lowest - (10) highest\n\n\nbty_f1upper\nBeauty rating of professor from upper level female: (1) lowest - (10) highest\n\n\nbty_f2upper\nBeauty rating of professor from upper level female: (1) lowest - (10) highest\n\n\nbty_m1lower\nBeauty rating of professor from lower level male: (1) lowest - (10) highest\n\n\nbty_m1upper\nBeauty rating of professor from upper level male: (1) lowest - (10) highest\n\n\nbty_m2upper\nBeauty rating of professor from upper level male: (1) lowest - (10) highest"
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-1-data-manipulation",
    "href": "content/labs/04-lab-modelling.html#part-1-data-manipulation",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 1: Data Manipulation",
    "text": "Part 1: Data Manipulation\n\n\nThe rowwise function is useful for applying mathematical operations to each row.\n\nExercise 1\nCreate a new variable called bty_avg that is the average attractiveness score of the six students for each professor (bty_f1lower through bty_m2upper). Add this new variable to the evals data frame. Do this in one pipe, using the rowwise function. Since rowwise is new to you, incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.\n\n___ <- evals |>\n  rowwise() |>\n  ___(bty_avg = mean( c( ___ ) )) |>\n  ungroup()\n\nNote that we end the pipeline with ungroup() to remove the effect of the rowwise function from earlier in the pipeline. The rowwise function works a lot like group_by, except it groups the data frame one row at a time so that any operations applied to the data frame is done once per each row. This is helpful for finding the mean beauty score for each row. However in the remainder of the analysis we don’t want to, say, calculate summary statistics for each row, or fit a model for each row. Hence we need to undo the effect of rowwise, which we can do with ungroup."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-2-exploratory-data-analysis",
    "href": "content/labs/04-lab-modelling.html#part-2-exploratory-data-analysis",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 2: Exploratory Data Analysis",
    "text": "Part 2: Exploratory Data Analysis\n\nExercise 2\nVisualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response.\n\n\nExercise 3\nVisualize and describe the relationship between score and the new variable you created, bty_avg.\n\n\nHint: See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html.\n\n\nExercise 4\nReplot the scatterplot from Exercise 3, but this time use\ngeom_jitter()? What does “jitter” mean? What was misleading about the initial scatterplot?"
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-3-linear-regression-with-a-numerical-predictor",
    "href": "content/labs/04-lab-modelling.html#part-3-linear-regression-with-a-numerical-predictor",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 3: Linear regression with a numerical predictor",
    "text": "Part 3: Linear regression with a numerical predictor\n\n\nLinear model is in the form \\(\\hat{y} = b_0 + b_1 x\\).\n\nExercise 5\nLet’s see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model.\n\n\nExercise 6\nReplot your visualization from Exercise 3, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line.\n\n\nExercise 7\nInterpret the slope of the linear model in context of the data.\n\n\nExercise 8\nInterpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.\n\n\nExercise 9\nDetermine the \\(R^2\\) of the model and interpret it in context of the data."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-4-linear-regression-with-a-categorical-predictor",
    "href": "content/labs/04-lab-modelling.html#part-4-linear-regression-with-a-categorical-predictor",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 4: Linear regression with a categorical predictor",
    "text": "Part 4: Linear regression with a categorical predictor\n\nExercise 10\nFit a new linear model called m_gen to predict average professor evaluation score based on gender of the professor. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data.\n\n\nExercise 11\nWhat is the equation of the line corresponding to male professors? What is it for female professors?\n\n\nExercise 12\nFit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data.\n\n\nSee the course slides on using the forcats package for changing the order of levels.\n\n\nExercise 3\nCreate a new variable called rank_relevel where \"tenure track\" is the baseline level.\n\n\nExercise 14\nFit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in Exercise 13. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model.\n\n\nExercise 15\nCreate another new variable called tenure_eligible that labels \"teaching\" faculty as \"no\" and labels \"tenure track\" and \"tenured\" faculty as \"yes\".\n\n\nExercise 16\nFit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html",
    "href": "content/labs/05-lab-mlr.html",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "",
    "text": "In this lab we revisit the professor evaluations data we modeled in an earlier lab. In the modelling lab we modeled evaluation scores using a single predictor at a time. However, this time we use multiple predictors to model evaluation scores.\nIf you don’t remember the data, review the modelling lab’s introduction before continuing to the exercises.\n\n\n\n\n\n\nImportant\n\n\n\nThis lab is likely longer than what you’ll be able to complete in an hour. We will be looking to see that you minimally ran and interpreted a model using multiple linear regression (completed Part 2). If you’re able to finish the whole thing, awesome! If not, that’s OK."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-1-simple-linear-regression",
    "href": "content/labs/05-lab-mlr.html#part-1-simple-linear-regression",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 1: Simple linear regression",
    "text": "Part 1: Simple linear regression\n\nExercise 2\n[Review from linear regression lab] Fit a linear model (one you have fit before): m_bty, predicting average professor evaluation score based on average beauty rating (bty_avg) only. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\)."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-2-multiple-linear-regression",
    "href": "content/labs/05-lab-mlr.html#part-2-multiple-linear-regression",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 2: Multiple linear regression",
    "text": "Part 2: Multiple linear regression\n\nExercise 3\nFit a linear model: m_bty_gen, predicting average professor evaluation score based on average beauty rating (bty_avg) and gender. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\).\n\n\nExercise 4\nInterpret the slopes and intercept of m_bty_gen in context of the data.\n\n\nExercise 5\nWhat percent of the variability in score is explained by the model m_bty_gen.\n\n\nExercise 6\nWhat is the equation of the line corresponding to just male professors?\n\n\nExercise 7\nFor two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?\n\n\nExercise 8\nHow do the adjusted \\(R^2\\) values of m_bty_gen and m_bty compare? What does this tell us about how useful gender is in explaining the variability in evaluation scores when we already have information on the beauty score of the professor.\n\n\nExercise 9\nCompare the slopes of bty_avg under the two models (m_bty and m_bty_gen). Has the addition of gender to the model changed the parameter estimate (slope) for bty_avg?\n\n\nExercise 10\nCreate a new model called m_bty_rank with gender removed and rank added in. Write the equation of the linear model and interpret the slopes and intercept in context of the data."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-3-the-search-for-the-best-model",
    "href": "content/labs/05-lab-mlr.html#part-3-the-search-for-the-best-model",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 3: The search for the best model",
    "text": "Part 3: The search for the best model\nGoing forward, only consider the following variables as potential predictors: rank, ethnicity, gender, language, age, cls_perc_eval, cls_did_eval, cls_students, cls_level, cls_profs, cls_credits, bty_avg.\n\nExercise 11\nWhich variable, on its own, would you expect to be the worst predictor of evaluation scores? Why? Hint: Think about which variable would you expect to not have any association with the professor’s score.\n\n\nExercise 12\nCheck your suspicions from the previous exercise. Include the model output for that variable in your response.\n\n\nExercise 13\nSuppose you wanted to fit a full model with the variables listed above. If you are already going to include cls_perc_eval and cls_students, which variable should you not include as an additional predictor? Why?\n\n\nExercise 14\nFit a full model with all predictors listed above (except for the one you decided to exclude) in the previous question.\n\n\nExercise 15\nUsing backward-selection (meaning fit all predictors and remove those that are not needed in the model) with adjusted R-squared as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.\n\n\nExercise 16\nInterpret the slopes of one numerical and one categorical predictor based on your final model.\n\n\nExercise 17\nBased on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score.\n\n\nExercise 18\nWould you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not?"
  },
  {
    "objectID": "content/labs/02-lab-wrangling.html",
    "href": "content/labs/02-lab-wrangling.html",
    "title": "Lab 02 - Tooling",
    "section": "",
    "text": "Introduction\nIn the first lab, you got acquainted with RMarkdown documents, knitting, code chunks, and interacting with GitHub; however, most of the required code was provided for you. In this and subsequent labs, there will be less code provided, and it will be up to you to write the requisite code.\nThe goal of this lab is to get you comfortable working with tidy datasets and using dplyr to do so.\n\n\nGetting started\nTo get started, accept the lab02 assignment (link on Canvas), clone the repo (using SSH) into RStudio on datahub. And, then you’re ready to go!\n\n\nPackages\nThe only package required for completion of this lab is tidyverse, as dplyr (which you’ll be using a lot in this lab) is one of the packages in the tidyverse. Be sure to import the tidyverse prior to completing the lab.\n\n\nData\nFor this lab, we’ll be using the storms dataset from the dplyr package, which includes data about a subset of storms from 1975. The description from this dataset states “This data is a subset of the NOAA Atlantic hurricane database best track data, https://www.nhc.noaa.gov/data/#hurdat. The data includes the positions and attributes of 198 tropical storms, measured every six hours during the lifetime of a storm.”\nRemember that you can use ?storms to look up the documentation for the dataset. Be sure to read and understand what information is stored in each variable before proceeding. The instructions below are not very guided and will require that you have read and understood the information in the dataset first.\n\n\nExercises\nFor each of the following, write code using dplyr functions to determine the answers to each of the questions. Your responses should include the code, its output, and a few words that answer the question.\nNote that the final two questions are optional. Definitely give them a try, but it’s OK if you don’t have time to figure them out!\n\nExercise 1\nHow many unique hurricanes are included in this dataset?\n\n\nExercise 2\nWhich tropical storm affected the largest area experiencing tropical storm strength winds? And, what was the maximum sustained wind speed for that storm?\n\n\nExercise 3\nAmong all storms in this dataset, in which month are storms most common? Does this depend on the status of the storm? (In other words, are hurricanes more common in certain months than tropical depressions? or tropical storms?)\n\n\nExercise 4\nYour boss asks for the name, year, and status of all category 5 storms that have happened in the 2000s. Carry out the operations that would deliver what they’re looking for.\n\n\nExercise 5\nFilter these data to only include storms that occurred during your lifetime (your code and results may differ from your classmates!). Among storms that have occurred during your lifetime, what’s the mean and median air pressure across all measurements taken?\n\n\nExercise 6\n(optional challenge) Which decade (of the storms included in the dataset) had the largest number of unique reported storms?\n\n\nExercise 7\n(optional challenge) - Among the subset of storms occurring in your lifetime, which storm lasted the longest? Include your code and explain your answer.\nYay, you’re done! Knit your file, commit all remaining changes to your .Rmd and .html files, use the commit message “Done with Lab 2! 💪”, and push. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo."
  },
  {
    "objectID": "content/labs/06-lab-cs01.html",
    "href": "content/labs/06-lab-cs01.html",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "",
    "text": "This week in class we’ve been discussing data related to Biomarkers of Recent Cannabis Use in Blood, Oral Fluid and Breath. You’ve seen the data in class, but you may not have had a chance to really work with the data yourself yet. This lab will allow you to get more comfortable with the data ahead of the associated case study report coming due. While the case study report will be submitted in groups, this will be submitted individually.The idea is that while you are still encouraged to work together during lab, you and your groupmates may come up with separate ideas. This will allow you to have more ideas when you get together and start working on the case study together."
  },
  {
    "objectID": "content/labs/06-lab-cs01.html#part-1-exploratory-data-analysis-eda",
    "href": "content/labs/06-lab-cs01.html#part-1-exploratory-data-analysis-eda",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "Part 1: Exploratory Data Analysis (EDA)",
    "text": "Part 1: Exploratory Data Analysis (EDA)\nCreate at least two (2) visualizations that help you learn more about these data beyond what was presented in class. (This is intentionally vague. We want you to look at the data and figure out what would be most helpful to visualize from the provided data. These could be different variables than what we looked at in class. Data could be faceted. Something totally different!) These do not have to be fully polished visualizations, but it should be clear from the visualization and accompanying text what’s to be learned from the visualization."
  },
  {
    "objectID": "content/labs/06-lab-cs01.html#part-2-possible-extensions",
    "href": "content/labs/06-lab-cs01.html#part-2-possible-extensions",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "Part 2: Possible extensions?",
    "text": "Part 2: Possible extensions?\nThink about the data you have access to, the EDA/analysis presented in class, and the questions we said we’re going to address. What possible extensions to this analysis would you be interested in carrying out? This is a space for brainstorming. Include any possible thoughts you have here, even if they aren’t “good” or you aren’t sure if they are “possible.” This can be used as a jumping off point for when you start discussing analysis extensions with your group."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html",
    "href": "content/labs/01-lab-intro-r.html",
    "title": "Lab 01 - Tooling",
    "section": "",
    "text": "The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course to learn and practice programming and analyze data.\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\nAn additional goal is to introduce you to git and GitHub, which is the collaboration and version control system that we will be using throughout the course.\n\n\ngit is a version control system (like “Track Changes” features from Microsoft Word on steroids) and GitHub is the home for your Git-based projects on the internet (like DropBox but much, much better).\nAs the labs progress, you are encouraged to explore beyond what the labs say directly; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nYou are encouraged to ask one another questions and work together, but each individual must turn in their own lab each week."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#github-housekeeping",
    "href": "content/labs/01-lab-intro-r.html#github-housekeeping",
    "title": "Lab 01 - Tooling",
    "section": "GitHub Housekeeping",
    "text": "GitHub Housekeeping\n\n\n\n\n\n\nNote\n\n\n\nYour email address is the address tied to your GitHub account and your name should be first and last name.\n\n\nBefore we can get started we need to take care of some required housekeeping (if you didn’t complete it during class). Specifically, we need to configure your git so that RStudio can communicate with GitHub…and so you do not have to type in your username and password every time you want to communicate with GitHub from RStudio. These steps will be demo-ed during lab and you’ll have time to walk through the steps!\n\n\n\n\n\n\nNote\n\n\n\nIf you aren’t able to attend lab and get stuck here, there is a podcast recording from lecture on 9/29 from the COGS 137 Fa21 iteration where you can see these steps demo-ed.\n\n\n\nStep 1: Email and Username\nThe first step requires two pieces of information: your email address and your name.\nTo do so, follow these steps:\n\nGo to the Terminal pane\nType the following two lines of code, replacing the information in the quotation marks with your info:\n\n\ngit config --global user.email \"your email\"\ngit config --global user.name \"your name\"\n\nFor example, for me these are:\n\ngit config --global user.email \"sellis@ucsd.edu\"\ngit config --global user.name \"Shannon Ellis\"\n\nTo confirm that the changes have been implemented, run the following:\n\ngit config --global user.email\ngit config --global user.name\n\n\n\nStep 2: Generate ssh key\nIn the terminal, you’ll want to generate an ssh key by typing:\n\nssh-keygen\n\nAfter hitting enter/return to execute the above, you’ll press return/enter three times to bypass specifying a location and passphrase.\n\n\nStep 3: Copy your ssh key\nFrom the terminal type:\n\ncat ~/.ssh/id_rsa.pub\n\nYou’ll want to highlight and copy the full result of this command. It will start with ssh-rsa and end with something including dsmlp toward the end.\n\n\nStep 4: Let GitHub know your key\n\nIn your browser, navigate to https://github.com/settings/keys\nClick “New SSH Key”\nSet title to DSMLP\nPaste what you copied in step 3 into the “Key” box\nClick “Add SSH Key”\n\n\n\nStep 5: Finalize\nReturn to the terminal in RStudio and run the following command:\n\nssh git@github.com\n\nYou may see a message like `The authenticity of host … can’t be established`. You can type yes and hit return/enter after doing so if you do.\nYou’ll then see a message like You've successfully authenticated, but GitHub does not provide shell access. At this point, you’re all set!\nThis will be the only time you have to do this. From here on out, you’ll be able to “communicate” with GitHub from RStudio without typing your username/password."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#cloning-the-lab",
    "href": "content/labs/01-lab-intro-r.html#cloning-the-lab",
    "title": "Lab 01 - Tooling",
    "section": "Cloning the lab",
    "text": "Cloning the lab\nEach of your assignments will begin with the following steps. You saw these once in class and, they’re outlined in detail here again. Going forward each lab will start with a “Getting started” section but details will be a bit more sparse than this. You can always refer back to this lab for a detailed list of the steps involved for getting started with an assignment.\nClick on the assignment link for this week’s lab on the Canvas homepage. You will have to Accept before proceeding. Refresh the page and follow the link to the repo created for you. This repo contains a template you can build on to complete your lab.\n\n\n\n\n\n\n\nOn this page on GitHub, click the URL provided for you. This will bring you to your copy of the repo on GitHub. Click on the green <> Code button to clone the repo. Be sure that SSH is selected and copy this URL.\n\n\n\n\n\n\nImportant\n\n\n\nBe sure that any time you are copying a link from GitHub under the <>Code button, you select and use the ‘SSH’ URL as this is what will allow you to not have to type your username and password.\n\n\n\n\n\n\n\n\n\nGo to datahub and open RStudio. Go to File > New Project… and select to create a New Project from Version Control. On the following menu, select Git.\nCopy and paste the URL of your assignment repo into the “Repository URL” dialog box:\n\n\n\n\n\n\n\nHit Create Project. Open up lab-01.Rmd and continue through this lab. Your work/answers for this lab will be submitted in that document."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#packages",
    "href": "content/labs/01-lab-intro-r.html#packages",
    "title": "Lab 01 - Tooling",
    "section": "Packages",
    "text": "Packages\nIn this lab we will work with two packages: datasauRus which contains the dataset, and tidyverse which is a collection of packages for doing data analysis in a “tidy” way.\ntidyverse has already been installed for you, so it only needs to be loaded using library. (see below)\nHowever, datasauRus has not yet been installed. Run the following in your console to install this package:\n\ninstall.packages(\"datasauRus\")\n\n\n\nNote that often packages will already be installed for you on datahub in this course, but we want you to know how to install them if you need to install one at any point.\nNote that package installation happens a single time. But, any time you want to use a package (after it’s been installed), it has to be loaded, as we do here:\n\nlibrary(tidyverse) \nlibrary(datasauRus)\n\nYou should be able to Knit your document and see the results.\nNote that the packages are also loaded with the same commands in your R Markdown document."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#yaml",
    "href": "content/labs/01-lab-intro-r.html#yaml",
    "title": "Lab 01 - Tooling",
    "section": "YAML:",
    "text": "YAML:\n\n\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “Yet Another Markup Language”. It is a human-friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\nOpen the R Markdown (Rmd) file in your project, change the author name to your name and knit the document. This will generate an HTML document."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#committing-changes",
    "href": "content/labs/01-lab-intro-r.html#committing-changes",
    "title": "Lab 01 - Tooling",
    "section": "Committing changes:",
    "text": "Committing changes:\nThen Go to the Git pane in your RStudio on Datahub.\nIf you have made changes to your Rmd file, you should see it listed here. Click on it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. Be sure to also select your HTML document. Once you’re happy with these changes, write “Update author name” in the Commit message box and hit Commit.\n\n\n\n\n\n\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the quarter progresses, we will let you make these decisions."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#pushing-changes",
    "href": "content/labs/01-lab-intro-r.html#pushing-changes",
    "title": "Lab 01 - Tooling",
    "section": "Pushing changes:",
    "text": "Pushing changes:\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only).\nIn order to push your changes to GitHub, click on Push. Go check your repo on GitHub - you’ll see your updated documents there!\n\n\n\n\n\n\nThought exercise\n\n\n\nFor which of the above steps (changing project name, making updates to the document, committing, and pushing changes) do you need to have an internet connection? Discuss with your classmates."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#data",
    "href": "content/labs/01-lab-intro-r.html#data",
    "title": "Lab 01 - Tooling",
    "section": "Data",
    "text": "Data\nThe data frame we will be working with today is called datasaurus_dozen2 and it’s in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualization is important and how summary statistics alone can be misleading. The different datasets are marked by the dataset variable.\nTo find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark before the name of an object will always bring up its help file. This command must be ran in the Console.\n\nExercise 1\nBased on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report. When you’re done, commit your changes with the commit message “Added answer for Ex 1”, and push.\nLet’s take a look at what these datasets are. To do so we can make a frequency table of the dataset variable:\n\ndatasaurus_dozen |>\n  count(dataset)\n\n# A tibble: 13 × 2\n   dataset        n\n   <chr>      <int>\n 1 away         142\n 2 bullseye     142\n 3 circle       142\n 4 dino         142\n 5 dots         142\n 6 h_lines      142\n 7 high_lines   142\n 8 slant_down   142\n 9 slant_up     142\n10 star         142\n11 v_lines      142\n12 wide_lines   142\n13 x_shape      142\n\n\nThe original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice.3 In the paper, the authors simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#data-visualization-and-summary",
    "href": "content/labs/01-lab-intro-r.html#data-visualization-and-summary",
    "title": "Lab 01 - Tooling",
    "section": "Data visualization and summary",
    "text": "Data visualization and summary\n\nExercise 2\nPlot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset.\nBelow is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Rmd document and successfully knit it and view the results.\nStart with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data.\n\ndino_data <- datasaurus_dozen |>\n  filter(dataset == \"dino\")\n\nThere is a lot going on here, so let’s slow down and unpack it a bit.\nFirst, the pipe operator: |>, takes what comes before it and sends it as the first argument to what comes after it. So here, we’re saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\".\nSecond, the assignment operator: <-, assigns the name dino_data to the filtered data frame.\nNext, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data you’re visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case we want these to be points,m hence geom_point.\n\nggplot(data = dino_data, mapping = aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\nIf this seems like a lot, it is. And you will learn about the philosophy of building data visualizations in layer in detail next week. For now, follow along with the code that is provided.\nFor the second part of this exercises, we need to calculate a summary statistic: the correlation coefficient. Correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear – it’s dinosaurial!\nBut, for illustrative purposes, let’s calculate correlation coefficient between x and y.\n\n\nStart with dino_data and calculate a summary statistic that we will call r as the correlation between x and y.\n\ndino_data |>\n  summarize(r = cor(x, y))\n\n# A tibble: 1 × 1\n        r\n    <dbl>\n1 -0.0645\n\n\nThis is a good place to pause, commit changes with the commit message “Added answer for Ex 2”, and push.\n\n\nExercise 3\nPlot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nThis is another good place to pause, commit changes with the commit message “Added answer for Ex 3”, and push.\n\n\nExercise 4\nPlot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nYou should pause again, commit changes with the commit message “Added answer for Ex 4”, and push.\n\n\nFacet by the dataset variable, placing the plots in a 3 column grid, and don’t add a legend.\n\n\nExercise 5\nFinally, let’s plot all datasets at once. In order to do this we will make use of facetting.\n\nggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset)) +\n  geom_point() +\n  facet_wrap(~ dataset, ncol = 3) +\n  theme(legend.position = \"none\")\n\nAnd we can use the group_by function to generate all the summary correlation coefficients.\n\ndatasaurus_dozen |>\n  group_by(dataset) |>\n  summarize(r = cor(x, y))\n\nYou’re done with the data analysis exercises, but we’d like you to do two more things:\n\n\n\n\n\nFigure 1: ?(caption)"
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#bonus-exercises",
    "href": "content/labs/01-lab-intro-r.html#bonus-exercises",
    "title": "Lab 01 - Tooling",
    "section": "Bonus Exercises",
    "text": "Bonus Exercises\nComplete these as time permits to further your experience with, comfort in, and understanding of R Markdown documents.\n\nResize your figures\nClick on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the pop up dialogue box go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until you’re happy with the figure sizes. Note that these values get saved in the YAML.\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nYou can also use different figure sizes for different figures. To do so click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well.\n\n\n\n\n\n\n\nChange the look of your report\nOnce again click on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the General tab of the pop up dialogue box try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until you’re happy with the look.\n\n\n\n\n\n\nNote\n\n\n\nNot sure how to use emojis on your computer? Maybe a classmate can help? Or you can ask your TA as well!"
  },
  {
    "objectID": "content/hw/hw-01.html",
    "href": "content/hw/hw-01.html",
    "title": "HW 01 - R Basics",
    "section": "",
    "text": "This assignment is meant to get you comfortable with 1) the format of homework assignments in this course and 2) writing R code in RStudio. The specific questions on this assignment will be simpler and should take you less time than those on future assignments. This assignment focuses on variables, operators, datasets, and dplyr basics.\n\nGetting started\nHere are the steps for getting started:\n\nStart by navigating to the hw01 GitHub URL (found on Canvas)\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit your file and take a look at the document generated\nCommit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\n\nYou can of course push multiple to GitHub times throughout the assignment. Your final push at the deadline will be used for grading. (This means even if you made mistakes before that on GitHub, you wouldn’t be penalized for them, so long as the final state of your work is correct).\n\n\n\n\n\n\nImportant\n\n\n\nYou’ll always want to knit your RMarkdown document to HTML and review that HTML document to ensure it includes all the information you want and looks as you intended, as we grade from the knit HTML. Both your .Rmd and .html files should be on GitHub."
  },
  {
    "objectID": "content/hw/hw-03.html",
    "href": "content/hw/hw-03.html",
    "title": "HW 03 - Bike rentals in DC",
    "section": "",
    "text": "Bike sharing systems take traditional bike rentals but automate the entire process (membership, rental, return, etc.). Through these systems, users are able to easily rent a bike from a particular position and return it at another position. There are hundreds of bike-sharing programs around the world comprising hundreds of thousands of bicycles. Today, there exists great interest in these (and other “alternative” transit) systems due to their important role in traffic, environmental, and health issues.\nApart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.\nSource: UCI Machine Learning Repository - Bike Sharing Dataset"
  },
  {
    "objectID": "content/hw/hw-03.html#getting-started",
    "href": "content/hw/hw-03.html#getting-started",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nStart with an assignment link that creates the GitHub repo with starter documents (link on Canvas).\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically commit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\nThis assignment will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading. (This means even if you made mistakes before that submission on GitHub, you won’t be penalized for them, so long as the final state of your work is correct)."
  },
  {
    "objectID": "content/hw/hw-03.html#data",
    "href": "content/hw/hw-03.html#data",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Data",
    "text": "Data\nThe data include daily bike rental counts (by members and casual users) of Capital Bikeshare in Washington, DC in 2011 and 2012 as well as weather information on these days.\nThe original data sources are http://capitalbikeshare.com/system-data and http://www.freemeteo.com.\nThe codebook is below:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\ninstant\nrecord index\n\n\ndteday\ndate\n\n\nseason\nseason (1:winter, 2:spring, 3:summer, 4:fall)\n\n\nyr\nyear (0: 2011, 1:2012)\n\n\nmnth\nmonth (1 to 12)\n\n\nholiday\nweather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n\n\nweekday\nday of the week\n\n\nworkingday\nif day is neither weekend nor holiday is 1, otherwise is 0.\n\n\nweathersit\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\n\n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\n\n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n\n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n\ntemp\nNormalized temperature in Celsius. The values are divided by 41 (max)\n\n\natemp\nNormalized feeling temperature in Celsius. The values are divided by 50 (max)\n\n\nhum\nNormalized humidity. The values are divided by 100 (max)\n\n\nwindspeed\nNormalized wind speed. The values are divided by 67 (max)\n\n\ncasual\nCount of casual users\n\n\nregistered\nCount of registered users\n\n\ncnt\nCount of total rental bikes including both casual and registered"
  },
  {
    "objectID": "content/hw/hw-03.html#setup",
    "href": "content/hw/hw-03.html#setup",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Setup",
    "text": "Setup\nYou are free to utilize any packages in this homework. We think you’ll likely use tidyverse, tidymodels (…and maybe olsrr)"
  },
  {
    "objectID": "content/hw/hw-03.html#questions",
    "href": "content/hw/hw-03.html#questions",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Questions",
    "text": "Questions\n\nData wrangling\n\nQuestion 1\nRecode the season variable to be a factor with meaningful level names as outlined in the codebook, with spring as the baseline level.\n\n\nQuestion 2\nRecode the binary variables holiday and workingday to be factors with levels no (0) and yes (1), with no as the baseline level.\n\n\nQuestion 3\nRecode the yr variable to be a factor with levels 2011 and 2012, with 2011 as the baseline level.\n\n\nQuestion 4\nRecode the weathersit variable as 1 - clear, 2 - mist, 3 - light precipitation, and 4 - heavy precipitation, with clear as the baseline.\n\n\nQuestion 5\nCalculate raw temperature, feeling temperature, humidity, and windspeed as their values given in the dataset multiplied by the maximum raw values stated in the codebook for each variable. Instead of writing over the existing variables, create new ones with concise but informative names.\n\n\nQuestion 6\nCheck that the sum of casual and registered adds up to cnt for each record.\n\n\n\nExploratory data analysis\n\nQuestion 7\nRecreate the following visualization, and interpret it in context of the data. Hint: You will need to use one of the variables you created above.\n\n\n\nQuestion 8\nCreate a visualization displaying the relationship between bike rentals and season. Interpret the plot in context of the data.\n\n\n\nModelling\n\nQuestion 9\nFit a linear model predicting total daily bike rentals from daily temperature. Write the linear model, interpret the slope and the intercept in context of the data, and determine and interpret the \\(R^2\\).\n\n\nQuestion 10\nFit another linear model predicting total daily bike rentals from daily feeling temperature. Write the linear model, interpret the slope and the intercept in context of the data, and determine and interpret the \\(R^2\\). Is temperature or feeling temperature a better predictor of bike rentals?\n\n\nQuestion 11\nFit a full model predicting total daily bike rentals from season, year, whether the day is holiday or not, whether the day is a workingday or not, the weather category, temperature, feeling temperature, humidity, and windspeed, as well as the interaction between at least one numerical and one categorical variable.\n\n\nQuestion 12\nPerform backward selection using adjusted \\(R^2\\) as the decision criterion to find the “best” model. Provide the model output for the final model.\n\n\nQuestion 13\nInterpret slope coefficients associated with two of the variables in your final model in context of the data. Note: If one of these is categorical with multiple levels, make sure you interpret all of the slope coefficients associated with the levels of the variable.\n\n\nQuestion 14\nBased on the final model you found in the previous question, discuss what makes for a good day to bike in DC (as measured by rental bikes being more in demand)."
  },
  {
    "objectID": "content/hw/hw-03.html#submission",
    "href": "content/hw/hw-03.html#submission",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Submission",
    "text": "Submission\nBe sure to knit your file to HTML, look at the output HTML file to make sure everything looks as you expected, and then commit and push your final changes to GitHub. We will be grading from the HTML file. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo."
  },
  {
    "objectID": "content/hw/hw-02.html",
    "href": "content/hw/hw-02.html",
    "title": "HW 02 - Data Visualization",
    "section": "",
    "text": "This assignment is meant to get you more comfortable with generating and customizing visualizations in R using ggplot2. The first section will guide you toward the visualizations you’re expected to generate, while the final two sections will be more open-ended. There are multiple distinct visualizations that could be totally “correct” for each question. You may make a different decision than your classmate and could both be correct.\nAlso, note that the final two parts of this assignment will take you way longer than you think they will. Definitely do not wait until the last minute to start this assignment."
  },
  {
    "objectID": "content/hw/hw-02.html#getting-started",
    "href": "content/hw/hw-02.html#getting-started",
    "title": "HW 02 - Data Visualization",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nStart with an assignment link that creates a repo on GitHub with starter documents (link on Canvas).\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically commit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\n\nYour final GitHub push prior to the deadline will be used for grading. (This means even if you made mistakes before that submission on GitHub, you won’t be penalized for them, so long as the final state of your work is correct).\n\nImports\nThe following packages must be imported prior to completing this homework: tidyverse and palmerpenguins (Note: If you did not install palmerpenguins during lecture, you’ll have to run install.packages(\"palmerpenguins\") prior to importing it.)\n\n\nGround Rules\nFor this assignment, all visualizations must:\n\nbe completed using ggplot2\nhave an informative title and labeled axes\nfollow good visualization practices (discussed in class)"
  },
  {
    "objectID": "content/hw/hw-02.html#part-i-ggplot2",
    "href": "content/hw/hw-02.html#part-i-ggplot2",
    "title": "HW 02 - Data Visualization",
    "section": "Part I: ggplot2",
    "text": "Part I: ggplot2\nThis first section will continue to use the penguins dataset from the palmerpenguins package that was used during the ggplot2 lecture.\n\nQuestion 1\nGenerate a visualization that will allow readers to determine whether male or female penguins are larger (by mass).\n\n\nQuestion 2\nGenerate a plot that clearly visualizes how many penguins there are from each species on each island. Each island should be a different panel, and each chart should visualize the species count.\n\n\nQuestion 3\nGenerate a plot that will allow the viewer to determine whether flipper length has differed over the study year. Be sure to color the points on this plot by species. And, remember that year is often best handled as a factor."
  },
  {
    "objectID": "content/hw/hw-02.html#part-ii-imitation-is-the-highest-form-of-flattery",
    "href": "content/hw/hw-02.html#part-ii-imitation-is-the-highest-form-of-flattery",
    "title": "HW 02 - Data Visualization",
    "section": "Part II: Imitation is the highest form of flattery",
    "text": "Part II: Imitation is the highest form of flattery\nIn class we learned a handful of ways to customize visualizations. Now, it’s your turn to apply what you learned by recreating someone else’s visualization.\n\nQuestion 4\nFor this question, find a visualization somewhere on the Internet and recreate the visualization as close as you can using ggplot2. To make this easier on yourself, you’ll likely want to find a visualization where the data are readily available. (To get started, FiveThirtyEight makes a lot of the data from their articles available and has many charts in their articles. You are not required to recreate a visualization from FiveThirtyEight; however, if you’re not sure where to start, you have this option.) Your answer should include an image of the original visualization, a reference to the original image (this could simply be a URL), and your code + recreation.\nNotes:\n\nTo insert an image in an RMarkdown document, you can use the syntax ![alt text](path/to/image.png).\nThe R/ggplot2 code to create your visualization cannot already exist on the internet. (For example, choosing to recreate a plot from the R Graph Gallery would not be an option b/c all the code is already there and you wouldn’t learn as much.)\n\n\n\nQuestion 5\nBriefly explain what you learned about ggplot2 in the process of re-creating this visualization.\n\n\nQuestion 6\nExplain how your visualization differs from the original (It’s OK if your’s is not a perfect recreation!)"
  },
  {
    "objectID": "content/hw/hw-02.html#part-iii-take-a-sad-plot-and-make-it-better",
    "href": "content/hw/hw-02.html#part-iii-take-a-sad-plot-and-make-it-better",
    "title": "HW 02 - Data Visualization",
    "section": "Part III: Take a sad plot and make it better",
    "text": "Part III: Take a sad plot and make it better\n\nQuestion 7\nThis question was inspired by Alison Hill’s talk. The idea here is that there is a lot of data all around us and a whole bunch of visualizations. Some of them are really excellent, and some could be improved. Choose a visualization you’ve created in the past OR a visualization you’ve found out in the world that could benefit from a redesign and/or significant visual improvement. (This could be the same visualization you recreated above, but for most it will likely be a totally different visualization.) Your answer should include an image of the original visualization, a reference to the original image (this could simply be a URL), and your code + improved version.\nNote: If you’re unsure where to look for visualizations that would benefit from improvement, check out Flowing Data’s Ugly Charts or Reddit’s Data is ugly. You may need to recreate/approximate the dataset (meaning store the values from the visualization in a tibble) needed to generate the visualization prior to improving the design.\n\n\nQuestion 8\nBriefly explain what you learned about ggplot2 in the process of re-creating this visualization.\n\n\nQuestion 9\nExplain why you made the design and visualization choices you did for your improved version."
  },
  {
    "objectID": "content/hw/hw-02.html#submission",
    "href": "content/hw/hw-02.html#submission",
    "title": "HW 02 - Data Visualization",
    "section": "Submission",
    "text": "Submission\nBe sure to knit your file to HTML, look at the output HTML file to make sure everything looks as you expected, and then commit and push your final changes to GitHub. We will be grading from the HTML file. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo."
  },
  {
    "objectID": "content/cs/cs-example.html",
    "href": "content/cs/cs-example.html",
    "title": "CS: Example",
    "section": "",
    "text": "library(OCSdata)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(pdftools)\nlibrary(tesseract)\nlibrary(magick)\nlibrary(knitr)\nlibrary(Kendall)\nlibrary(broom)\n\n\n\n\n\n\n\nProf Note\n\n\n\nI would recommend packages be imported after the introduction, but this is very specific and not something anyone would lose credit for."
  },
  {
    "objectID": "content/cs/cs-example.html#introduction",
    "href": "content/cs/cs-example.html#introduction",
    "title": "CS: Example",
    "section": "Introduction",
    "text": "Introduction\nIn this case study, we seek to analyze the youth disconnection among minority groups within the United States. The disconnected youth are the young people who are between the ages of 16 and 24 who are neither working nor in school. Following the aftermath of the Great Recession in 2008, we witnessed a peak youth disconnection rate of 14.7 percent in 2010 which has since been trending downwards to as low as a youth disconnection rate of 11.5 percent in 2017. While analyzing racial and ethnic groups is imperative for viewing any evident disparities, we also intend on analyzing how gender may play a role in affecting the rates of disconnection. Seeing as how we can already see existing disparities between genders within the workforce, we also want to see to what extent, if at all, gender plays a role in youth disconnection among groups.\nThis specific period of young adulthood is one of the most critical in developing the necessary skills and capabilities required of them to be successful in adulthood and their professional careers as youth disconnection during this period stunts development and limits their potential. Measure of America, a nonpartisan project of the nonprofit Social Science Research, claims that “people who experience a period of disconnection as young adults go on to earn less and are less likely to be employed, own a home, or report good health by the time they reach their thirties”. Understanding how these trends among groups arise may be critical for finding solutions to preventing youth disconnection, and analyzing what groups may need more resources in order to prevent further prevent youth disconnection.\n\n\n\n\n\n\nProf Note\n\n\n\nThis introduction is missing citations! Statements that are not your own original thoughts must be cited. See Effective Communication lecture for possible ways to cite. We don’t care what format you use. We do care that you cite others’ work. Also, definitely feel free to find information on the topic beyond what was presented in class."
  },
  {
    "objectID": "content/cs/cs-example.html#questions",
    "href": "content/cs/cs-example.html#questions",
    "title": "CS: Example",
    "section": "Questions",
    "text": "Questions\n\nHow have youth disconnection rates in American youth changed since 2008?\nIn particular, how has this changed for different gender and ethnic groups? Are any groups particularly disconnected?\nDoes removing 2008 (pre-Great Recession time period) data points change our answer?\n\n\n\n\n\n\n\nProf Note\n\n\n\nThe third question here was their extension. Note that this is an ok extension all things considered. This was earlier in the quarter, a more straightforward dataset, there wasn’t as much to do as there is in our dataset. Explaining the why this is their focus in introduction would have been great."
  },
  {
    "objectID": "content/cs/cs-example.html#the-data",
    "href": "content/cs/cs-example.html#the-data",
    "title": "CS: Example",
    "section": "The Data",
    "text": "The Data\nThe data set that we will be using for our case study is provided by two reports from the Measure of America project related to youth disconnect. The data provided will be in the form of images.\n\nData Import\nIn order to begin our analysis, we imported our raw data from the OCSdata library on youth disconnection.\n\n# Data is imported so I get overwrite error when not commented - Adrian\n# Download raw data files\n# load_raw_data(\"ocs-bp-youth-disconnection\", outpath = '.')\n\n\n\n\n\n\n\nProf Note\n\n\n\nReminder that you can control chunk behavior for each chunk within the curly braces. This also applies for the warnings that are displayed later on. Best to supress those in final HTML.\n\n\nSince the raw data is in images, we need to convert to a usable R data set. We started with the major racial ethnic groups.\n\n## Import data for major racial/ethnic groups\nmajor_racial_ethnic_groups <- magick::image_read(\"data/Major_ethnic_groups_screenshot.png\")\nmajor_groups <- magick::image_ocr(major_racial_ethnic_groups)\n\nFor Asian subgroups, we had to import the raw data for 2017 and 2018.\n\n## Import data for Asian subgroups\n\n# 2017 data\nasian_sub_2017 <- image_read(\"data/asian_subgroups_2017.png\")\nasian_sub_2017_A <- image_read(\"data/asian_sub_2017_A.png\")\nasian_sub_2017_B <- image_read(\"data/asian_sub_2017_B.png\")\nasian_sub_2017_C <- image_read(\"data/asian_sub_2017_C.png\")\nasian_sub_2017 <- image_ocr(asian_sub_2017)\nasian_sub_2017_A <- image_ocr(asian_sub_2017_A)\nasian_sub_2017_B <- image_ocr(asian_sub_2017_B)\nasian_sub_2017_C <- image_ocr(asian_sub_2017_C)\n\n# 2018 data\nasian_sub_2018_A <- image_read(\"data/asian_sub_2018_A.png\")\nasian_sub_2018_A <- image_ocr(asian_sub_2018_A)\nasian_sub_2018_B <- image_read(\"data/asian_sub_2018_B.png\")\nasian_sub_2018_B <- image_ocr(asian_sub_2018_B)\n\nFor Latinx subgroups, we also imported 2017 and 2018 data.\n\n## Import data for Latinx subgroups\n\n# 2017 data\nlatinx_imageA <- image_read(\"data/latinx_sub_2017_A.png\")\nlatinx_imageB <- image_read(\"data/latinx_sub_2017_B.png\")\nlatinx_imageC <- image_read(\"data/latinx_sub_2017_C.png\")\nlatinx_sub_2017_A <- image_ocr(latinx_imageA)\nlatinx_sub_2017_B <- image_ocr(latinx_imageB)\nlatinx_sub_2017_C <- image_ocr(latinx_imageC)\n\n# 2018 data\nlatinx_sub_2018 <- image_read(\"data/latinx_subgroups_2018.png\")\nlatinx_sub_2018 <- image_ocr(latinx_sub_2018)\n\nOnce the data import is complete, we saved our data.\n\n# Save data\nsave(\n  major_groups,\n  asian_sub_2017,\n  asian_sub_2017_A,\n  asian_sub_2017_B,\n  asian_sub_2017_C,\n  latinx_sub_2017_A,\n  latinx_sub_2017_B,\n  latinx_sub_2017_C,\n  asian_sub_2018_A,\n  asian_sub_2018_B,\n  latinx_sub_2018,\n  file = \"data/imported_data.rda\")\n\n\n\nData Wrangling\nNow that we have our data properly imported, we need to wrangle our data in R so we can begin our analysis.\n\n\n\n\n\n\nProf Note\n\n\n\nThere is text to guide the viewer, clear code, and code comments as needed. I like this.\n\n\n\nMajor Group Data\nTo begin our data wrangling, we took the imported data that is in a single string and separated the data by new line characters and transformed it into a table.\n\n# Separates string by new line and transforms into a table\nmajor_groups <- major_groups |>\n  stringr::str_split(pattern = \"\\n\") |>\n  unlist() |> \n  tibble::as_tibble()\n\nNow that our data has rows, we created columns Group and Year and assigned the corresponding values according to the data as well as normalizing the capitalization throughout the data set.\n\n# Separate into columns Group and Year\nmajor_groups <- \n  major_groups |>\n  tidyr::separate(col = value, \n                  into = c(\"Group\", \"Years\"), # Set column names\n                  sep = \"(?<=[[:alpha:]])\\\\s(?=[0-9])\")  # Separate after letter string; beginning numerics\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [19].\n\n# Make capitalization format the same\nmajor_groups <- major_groups |> \n  mutate(Group = stringr::str_to_title(Group))\n\nWe have our data in two columns but need to separate our Years column into each individual year. We thus separated the column by every space character, assigned appropriate years for its corresponding values, and made the data values numeric.\n\n# Separate `Years` into columns by individual years\nmajor_groups <- major_groups |> \n  tidyr::separate(col = Years, \n                  into = c(\"2008\", \"2010\", \"2012\", \"2014\", \"2016\", \"2017\"),  # Set column names\n                  sep = \" \") # Separate by spaces\n\n# Remove empty rows\nmajor_groups <- major_groups |> \n  tidyr::drop_na()\n \n# Make data numeric\nmajor_groups <- major_groups |>\n  mutate(\n    across(.cols = -Group,\n           ~ str_remove(string = ., pattern = \"\\\\.\")),  # Remove decimal points from string\n    across(.cols = -Group, as.numeric),  # Convert to numeric\n    across(.cols = -Group, ~ . * 0.1)   # Add decimal point back\n  )\n\nWe have a lot of information in our Group column, so we separated that information and created two new columns, Race_Ethnicity and Gender, to make it easier to analyze our data.\n\n# Create Race_Ethnicity column\nmajor_groups  <- major_groups |>\n  # First, create entries for `All_races`\n  mutate(Race_Ethnicity = dplyr::recode(Group, \"United States\" = \"All_races\",\n                                        \"Female\" = \"All_races\",\n                                        \"Male\" = \"All_races\"),\n  # Second, create entries for all other races by importing data from group column and removing gender  \n         Race_Ethnicity = str_remove(string = Race_Ethnicity,\n                                     pattern = \"Female|Male\"))\n# Create Gender column\nmajor_groups  <- major_groups |>\n  # Extract gender from `Group` column\n  mutate(Gender = str_extract(string = Group, \n                              pattern = \"Female|Male\")) |>\n  # Assign `All' if gender was not in `Group`\n  mutate(Gender = replace_na(Gender, replace = \"All\"))\n\nNow that we have all of our data, we moved the columns containing years into rows and added a new column Percent to log the value contained in the, now former, year column.\n\n# Puts years in rows instead of columns with percent values\nmajor_groups_long <- major_groups |>\n  tidyr::pivot_longer(cols = contains(\"20\"), # any column containing 20\n                      names_to = \"Year\", # Assigns column for year\n                      values_to = \"Percent\", # Assigns column for percent\n                      names_prefix = \"Perc_\") |>\n  dplyr::mutate(Year = as.numeric(Year))\n\n\n\nSubgroup Data\n\nData Wrangling Functions\nWe wrangled the major group data but now have to wrangle the subgroup data, which is a little more complicated. We first created a make_rows function to take the imported raw data and convert it into a table with rows separated by new line characters.\n\n# Create function to separate string into rows\nmake_rows <- function(text){\n  text |>\n  str_split(\"\\n\") |>\n  unlist() |>\n  as_tibble()\n}\n\nWe also created two functions clean_table and clean_table_2018 to clean our subgroup data just like the major group data – separating Group and Years columns, converting data to numeric, and creating Race_Ethnicity and Gender columns.\n\n# Create function to clean 2017 subgroup data \nclean_table <- function(table){\n  table |>\n    separate(col = value,\n             into = c(\"Group\", \"Percentage\"), # Create column names\n             sep =  \"(?<=[[:alpha:]])\\\\s(?=[0-9])\") |> # Split into columns after string of letters\n    drop_na() |> # Remove NA rows\n    # Make percentage data numeric\n    mutate(Group = str_to_title(Group)) |>\n    mutate(Percentage = str_remove(string = Percentage,\n                                   pattern = \"\\\\.\")) |> # Remove decimal points from string\n    separate(Percentage, c(\"Percent\"), sep = \" \") |> # Separate by space\n    mutate(Percent = as.numeric(Percent)) |> # Convert to numeric\n    mutate(Percent = Percent * 0.1) |> # Add back decimal point\n    # Create Race_Ethnicity column\n    mutate(Race_Ethnicity = recode(Group, \n                                   # Create entries for All_races\n                                   \"United States\" = \"All_races\",\n                                   \"Female\" = \"All_races\",\n                                   \"Male\" = \"All_races\")) |>\n    # Create entries for all other races by importing data from group column and removing gender  \n    mutate(Race_Ethnicity = str_remove(string = Race_Ethnicity, \n                                       pattern = \" Female| Male\")) |> \n    # Create Gender column\n    mutate(Gender = str_extract(string = Group,\n                                pattern =\"Female|Male\")) |> # Extract gender from `Group` column\n    mutate(Gender = replace_na(Gender, replace = \"All\"))  # Assign `All' if gender was not in `Group`\n}\n\n# Create function to clean 2018 subgroup data \nclean_table_2018 <- function(table){\n  table |>\n    separate(col = value, \n             into = c(\"Group\", \"Percent\"), # Create column names\n             sep =  \"(?<=[[:alpha:]])\\\\s:\\\\s|\\\\s(?=[0-9])\") |>  # Split into columns after colon\n    mutate(Group = str_remove(string = Group, \n                            pattern = \":\")) |> # Remove colon\n    drop_na() |> # Remove NA rows\n    # Make percentage data numeric\n    mutate(Group = str_to_title(string = Group)) |> \n    mutate(Percent = str_remove(string = Percent, \n                               pattern = \"\\\\.\")) |> # Remove decimal points from string\n    mutate(Percent = as.numeric(Percent)) |> # Convert to numeric\n    mutate(Percent = Percent * 0.1) |> # Add back decimal point\n    # Create Race_Ethnicity column\n    mutate(Race_Ethnicity = str_replace(string = Group,\n                                        pattern = \"Men|Women\", \n                                        replacement = \"missing\")) |> # Replace gender with missing\n    mutate(Race_Ethnicity = na_if(Race_Ethnicity, \"missing\")) |> # Make missing values NA\n    fill(Race_Ethnicity, .direction = \"down\") |> # Fill Race/Ethnicity in missing fields\n    # Create Gender column\n    mutate(Gender = str_extract(string = Group, \n                                pattern = \"Men|Women\")) |> # Extract gender from `Group` column\n    mutate(Gender = replace_na(Gender, replace = \"All\")) # Assign `All' if gender was not in `Group`\n}\n\n\n\nAsian Subgroup Data\nWe ran our make_rows function on our raw data and combined the initially wrangled 2017 data.\n\n## Asian subgroup data\n# 2017 data\n# Apply make_rows function to subgroup data\nasian_sub_2017 <- make_rows(asian_sub_2017)\nasian_sub_2017_A <- make_rows(asian_sub_2017_A)\nasian_sub_2017_B <- make_rows(asian_sub_2017_B)\nasian_sub_2017_C <- make_rows(asian_sub_2017_C)\n\n# Combine data\nasian_sub_2017 <- bind_rows(asian_sub_2017_A, \n                            asian_sub_2017_B,\n                            asian_sub_2017_C)\n\nSimilarly, we applied make_rows to 2018 data.\n\n# 2018 data\n# Combine data\nasian_sub_2018 <- str_c(asian_sub_2018_A, asian_sub_2018_B)\n\n# Apply make_rows function\nasian_sub_2018 <- make_rows(asian_sub_2018)\n\nWe next cleaned both the 2017 and 2018 data using our clean_table and clean_table_2018 functions.\n\n# Apply clean table function to both years data\nasian_sub_2017 <- clean_table(asian_sub_2017)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [17, 22,\n28].\n\nasian_sub_2018 <- clean_table_2018(asian_sub_2018)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 6 rows [4, 8, 15, 19, 21,\n23].\n\n\nThe imported data was missing part of the data, so we manually added it back in as well as created a Year column.\n\n# Add missing data\nasian_sub_2018 <- asian_sub_2018 |>\n  add_row(Group = \"Asian\", Percent = 6.2,\n          Race_Ethnicity = \"Asian\", Gender = \"All\") |>\n  add_row(Group = \"Asian\", Percent = 6.4,\n          Race_Ethnicity = \"Asian\", Gender = \"Men\") |>\n  add_row(Group = \"Asian\", Percent = 6.1,\n          Race_Ethnicity = \"Asian\", Gender = \"Women\")\n\n# Add years to data\nasian_sub_2017 <- asian_sub_2017 |>\n  mutate(Year = 2017)\nasian_sub_2018 <- asian_sub_2018 |>\n  mutate(Year = 2018)\n\nSince the naming convention for males and females different across the 2017 and 2018 data, we standardized the Gender column to only contain Male and Female entries and then combined the data into one data set, and once again moved the years into rows and added a Percent column, just like our major group data.\n\n# Make Male/Female the values for gender across both datasets\nasian_sub_2018 <- asian_sub_2018 |> \n  mutate(across(.cols = c(Gender, Group),\n               ~ str_replace(string = ., \n                             pattern = \"Men\", \n                             replacement = \"Male\")),\n         across(.cols = c(Gender, Group),\n               ~ str_replace(string = ., \n                             pattern = \"Women\", \n                             replacement = \"Female\")))\n\n# Combine 2017 and 2018 data\nasian_subgroups <- bind_rows(asian_sub_2017, asian_sub_2018)\n\n# Add missing categories\nasian_subgroups <- asian_subgroups |> \n  select(-Group) |>\n  pivot_wider(names_from = Year, \n              values_from = Percent) |>\n  pivot_longer(cols = -c(Race_Ethnicity, Gender),\n               names_to = \"Year\",\n               values_to= \"Percent\")\n\n\n\nLatinx Subgroup Data\nTo wrangle our Latinx subgroup data, we followed the same steps as our Asian subgroup data – applying make_rows, clean_table, and clean_table_2018 functions, standardizing the Gender naming convention, combining 2017 and 2018 data, and moving the years into rows while adding a Percent column. For the Latinx data, we also appropriately labeled the Puerto Rican, Dominican, Cuban groups and cleaned the names for Latino/Latina group.\n\n## Latinx subgroup data\n# 2017 data\n# Combine data\nlatinx_sub_2017 <- stringr::str_c(latinx_sub_2017_A,\n                                  latinx_sub_2017_B, \n                                  latinx_sub_2017_C)\n\n# Fix typo\nlatinx_sub_2017 <- latinx_sub_2017 |>\n  str_replace(pattern = \"DR, Cuban Female 15.7\\nPR\", # Identify typo\n              replacement = \"DR, Cuban Male 15.7\\nPR\") # Replace gender\n\n# Apply functions to Latinx data\nlatinx_sub_2017 <- make_rows(latinx_sub_2017)\nlatinx_sub_2017 <- clean_table(table = latinx_sub_2017)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [19].\n\n# 2018 data\n# Clean the data string\nlatinx_sub_2018 <- str_replace_all(string = latinx_sub_2018, \n                                  pattern = \"\\\\s:\\n{2}|\\n{2}\", #remove two newline characters\n                                  replacement = \" \")\n\n# Apply make_rows function\nlatinx_sub_2018 <- make_rows(latinx_sub_2018)\n\n# Apply clean table function\nlatinx_sub_2018 <- clean_table_2018(latinx_sub_2018)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [12].\n\n# Create function to fix naming issues\nfix_latinx_naming <- function(table){\n  table |>\n    # Appropriately label Puerto Rican, Dominican, Cuban group\n  mutate(Group = str_replace(string = Group,\n                             pattern = \"Pr, Dr, Cuban\",\n                             replacement = \"Puerto Rican, Dominican, Cuban\"),\n          Race_Ethnicity = str_replace(string = Race_Ethnicity,\n                                       pattern = \"Pr, Dr, Cuban\",\n                                       replacement = \"Puerto Rican, Dominican, Cuban\"))\n}\n\n# Apply function to both data sets\nlatinx_sub_2017 <- fix_latinx_naming(latinx_sub_2017)\nlatinx_sub_2018 <- fix_latinx_naming(latinx_sub_2018)\n\n# Add missing data\nlatinx_sub_2018 <- latinx_sub_2018 |>\n  add_row(Group = \"Latinx\", Percent = 12.8,\n          Race_Ethnicity = \"Latinx\", Gender = \"All\") |>\n  add_row(Group = \"Latinx\", Percent = 12.3,\n          Race_Ethnicity = \"Latinx\", Gender = \"Men\") |>\n  add_row(Group = \"Latinx\", Percent = 13.3,\n          Race_Ethnicity = \"Latinx\", Gender = \"Women\")\n\n# Make Male/Female the values for gender across both datasets\nlatinx_sub_2018 <- latinx_sub_2018 |>\n  mutate(across(.cols = c(Gender, Group),\n                ~ str_replace(string = ., pattern = \"Men\", replacement = \"Male\")),\n         across(.cols = c(Gender, Group),\n                ~ str_replace(string = ., pattern = \"Women\", replacement = \"Female\")))\n\n# Add years to data\nlatinx_sub_2017 <- latinx_sub_2017 |>\n  mutate(Year = 2017)\nlatinx_sub_2018 <- latinx_sub_2018 |>\n  mutate(Year = 2018)\n\n# Combine 2017 and 2018 data\nlatinx_subgroups <- bind_rows(latinx_sub_2017, latinx_sub_2018)\n\n# Add missing categories\nlatinx_subgroups <- latinx_subgroups |>\n  select(-Group) |>\n  pivot_wider(names_from = Year,\n              values_from = Percent) |>\n  pivot_longer(cols = -c(Race_Ethnicity, Gender),\n               names_to =\"Year\" ,\n               values_to=\"Percent\")\n\n# Clean up Latinx group names\nlatinx_subgroups <- latinx_subgroups |>\n  # Convert Latino/Latina to Latinx\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latina\", \"Latinx\")) |>\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latino\", \"Latinx\")) |>\n  drop_na()\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\nProf Note\n\n\n\nNote that plots are clear, have titles, it’s clear what’s plotted, and there’s a corresponding interpretation guiding the reader. This is all good! These could be improved by having more informative titles that make clear to the viewer what the take-home message is and by handling years as factors.\n\n\nTo get a glance of our overall data set, we plotted overall youth disconnection over time in the United States.\n\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |>\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n  labs(\n    title = \"Youth Disconnection over Time\",\n    x = \"Year\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nWe are able to visualize that from 2008 to 2010 there was an increase in youth disconnect; however, after 2010, there seems to be a consistent downtrend in youth disconnect.\nTo account for differences in gender, we plotted overall youth disconnection over time by gender.\n\nmajor_groups_long |>\n  filter(Gender != \"All\", Race_Ethnicity == \"All_races\") |>\n  ggplot(aes(x = Year, y = Percent, color = Gender)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n    scale_color_viridis_d() +\n  labs(\n    title = \"Youth Disconnection over Time by Gender\"\n  )\n\n\n\n\nInitially in 2008, females faced more disconnect than males. However, throughout the next years, males consistently faced greater disconnect that females. Both male and female disconnect trended downwards 2010 and after.\nWe then separated major groups by race/ethnicity and visualized youth disconnect over time.\n\nmajor_groups_long |>\n  filter(Gender == \"All\", Group != \"United States\") |>\n  ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\"\n  )\n\n\n\n\nWe can see that Native Americans faced the most youth disconnect at about 25% and Asian faced the least with about 7.5% throughout all years in our data set. Overall, there is a small downtrend throughout the pass years in all races/ethnicities 2010 and after.\nWe visualized male/female youth disconnection by race/ethnicity across the observed years.\n\nmajor_groups_long |>\n  filter(Gender != \"All\") |> # Filter for only Male and Female\n  # Combine Latino and Latina into Latinx\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latina \", \"Latinx\")) |>\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latino \", \"Latinx\")) |>\n  #renaming all races column\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"All_races\", \"All Races\")) |>\n  # Plot scatter line plot of Percent vs Year by Gender and Ethnicity\n  ggplot(aes(x = Year, y = Percent, color = Gender)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n  facet_wrap(~Race_Ethnicity, nrow = 2) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Youth Disconnection over Time by Race/Ethnicity and Gender\"\n  )\n\n\n\n\nGender does not seem to affect the percentage much except in Black, Latinx, and Native American groups where there is a marginal difference. However, the main indicator in determining youth disconnection seems to be stronger aligned to Race/Ethnicity as that is where we see the largest disparity, while gender has smaller differences within a group.\nTaking a deeper dive into the Latinx group, we visualized youth disconnection of the different subgroups within the Latinx group.\n\nlatinx_subgroups |>\n  filter(Gender == \"All\", Race_Ethnicity != \"Latinx\") |> # Filter by overall not specific gender\n  # Plot Percent vs Year by Ethnicity\n  ggplot(aes(x = as.numeric(Year), y = Percent, color = Race_Ethnicity)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection of Latinx Subgroups over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\",\n    x = \"Year\"\n  )\n\n\n\n\nMost groups have a relatively similar youth disconnection percentage, however, the South American group clearly has the lowest rate of youth disconnection at around 8% while Puerto Rico, Cuba, Dominican Republic have the highest with around 14.5% across 2017 and 2018.\nTaking a deeper dive into the Asian group, we visualized youth disconnection of the different subgroups within the Asian group.\n\nasian_subgroups |>\n  filter(Gender == \"All\", Race_Ethnicity != \"All_races\") |> # Filter by overall not specific gender\n  # Plot Percent vs Year by Ethnicity\n  ggplot(aes(x = as.numeric(Year), y = Percent, color = Race_Ethnicity)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection of Asian Subgroups over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\",\n    x = \"Year\"\n  )\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nMost groups are in the 4-7% range for youth disconnection percentage; however, China has the lowest at about 4% and Hmong and Cambodia have the highest at about 13% across 2017 and 2018.\n\n\nData Analysis\nWe graphed youth disconnection over time with a line of best-fit layered on top of it.\n\nmajor_groups_long |>\n  # Scatterplot with linear model for overall youth disconnection over time\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |>\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\") + \n    labs(\n    title = \"Youth Disconnection over Time\",\n    subtitle = \"Includes linear model and standard error\",\n    x = \"Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe line of best-fit exhibits a slight negative relationship, showing there is some correlation with youth disconnection and time; however, there is a decent amount of standard error throughout, which may be due to the 2008 data point defying the trend of the other data points.\nTo test our hypothesis that youth disconnection does not have a decreasing trend over time, we did a Mann-Kendall test for monotonicity.\n\n# M-K test\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |>\n  pull(Percent) |>\n  MannKendall() |>\n  tidy()\n\n# A tibble: 1 × 5\n  statistic p.value kendall_score denominator var_kendall_score\n      <dbl>   <dbl>         <dbl>       <dbl>             <dbl>\n1    -0.600   0.133            -9        15.0              28.3\n\n\nWe see there the p-value is 13.3% percent, which is not statistically significant at the traditional 5% level. This means our data is likely not monotonic and we would fail to reject our null hypothesis that youth disconnection is decreasing over time.\nKnowing that the Great Recession occurred in 2008 and could provide an anomaly in our data, we ran a linear model on the data for 2010 and after.\n\n# Scatterplot with linear model for overall youth disconnection over time after 2010\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\", Year != 2008) |>\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Youth Disconnection over Time (2010 and after)\",\n    subtitle = \"Includes linear model and standard error\",\n    x = \"Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis graph seems to show a stronger linear relationship between youth disconnection over time as there is a steeper slope and less standard error than our first linear model visualization.\nAgain, to test our hypothesis that youth disconnection does not have a decreasing trend over time for 2010 and after, we did another Mann-Kendall test.\n\n# M-K test for data after 2008\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\", Year != 2008) |>\n  pull(Percent) |>\n  MannKendall() |>\n  tidy()\n\n# A tibble: 1 × 5\n  statistic p.value kendall_score denominator var_kendall_score\n      <dbl>   <dbl>         <dbl>       <dbl>             <dbl>\n1        -1  0.0275           -10          10              16.7\n\n\nThis time, the p-value is 2.75% and significant at the 5% level. Thus, this data set is likely monotonic and we would reject the null hypothesis that there is no decreasing trend between youth disconnection and time.\n\n\n\n\n\n\nProf Note\n\n\n\nModel is not fully quantified nor explained in the context of the data. Be sure that if you’re stating numbers, you’re contextualizing them. Be sure if you’re stating things that are true, that you’re quantifying them when possible."
  },
  {
    "objectID": "content/cs/cs-example.html#resultsdiscussion",
    "href": "content/cs/cs-example.html#resultsdiscussion",
    "title": "CS: Example",
    "section": "Results/Discussion",
    "text": "Results/Discussion\nThrough our exploratory data analysis, we observed that there does seem to be a decreasing trend of youth disconnection in the United States over the time frame of our data with the 2008 data being a slight anomaly. The decreasing trend with the exception of the initial 2010 spike remained generally consistent regardless of separation by gender, race/ethnicity, and both as seen through out first four visualizations. Although the trend is fairly consistent, there are additional takeaways from our visualizations.\nRace/ethnicity is a major determinant in the magnitude of youth disconnection rates while gender is a much weaker determinant. Through our third visualization we see that the Native American group has a youth disconnection percentage of about 25%, which is over three time the lowest group, Asian, at about 7.5%. The Black group, although not as high as Native American, has a youth disconnection rate higher than the overall average across the years at about 20% versus 13%. Additionally, every group’s rate is 2-5% different from the nearest group with no two groups being similar to each other. This leads us to believe that although a decreasing trend is apparent across all groups, groups still have differences in how disconnected their youth are. Gender, on the other hand, does not show the same disparity. Our second visualization shows that males may be more disconnected, but when broken down into race/ethnicity in our fourth visualization we see the effect of gender depends greatly on the race/ethnic group. Females are more disconnected in Latinx and Asian groups, while the youth disconnection rate is fairly similar across gender overall and in the Asian and White group. The Black group has the most interesting data where males are fairly more disconnected than females but even in other groups such as Latinx and Native American where there is a difference in female and male disconnection, it is not a major difference.\nIn taking a deeper dive into the Latinx and Asian subgroups in visualizations five and six, it can be seen that there are not big differences in youth disconnection between subgroups except in two cases. The South American subgroup has a substantially lower rate at about 8% versus the 12-15% range that the other Latinx subgroups are in. Oppositely, Hmong and Cambodia groups have a much higher youth disconnection rate at about 13% versus its Asian peers in the 4-7% range.\nTo quantify our observations in our exploratory data analysis, we created a linear model for our major group data and performed a Mann-Kendall test. The linear model shows a decreasing trend in youth disconnection rate over time but it does not seem to be a very strong relationship and has relatively high standard error. When performing the Mann-Kendall test, the p-value was 13.3%, which is not statistically significant at the 5% level, implying that there is not a monotonic trend in the data. We decided to create a second linear model and Mann-Kendall test without the 2008 data since youth disconnection likely increased in 2010 due to the Great Recession that the United States faced. When doing this, a much stronger decreasing trend can be seen in the linear model and the Mann-Kendall test has a p-value of 2.75%, which is statistically significant at the 5%, implying that there is a monotonic trend in the data. In this case, a decreasing trend over time."
  },
  {
    "objectID": "content/cs/cs-example.html#conclusion",
    "href": "content/cs/cs-example.html#conclusion",
    "title": "CS: Example",
    "section": "Conclusion",
    "text": "Conclusion\nWhen taking account our entire data set, we cannot mathematically state that youth disconnection rates have changed over time, although visually and when excluding 2008 data, we can see that youth disconnection has been trending downwards starting in 2010 and after. This trend remains consistent across gender and race/ethnicity; however, there are still differences between groups in how disconnected are youth. Native American and Black are the most disconnected groups with Native American clearly have the highest youth disconnection rate. Although gender is not a major factor in determining youth disconnection in most cases, Black males are notably more disconnected than Black females. All other Latinx groups are more disconnected than the South American group and Hmong and Combodian groups are notably more disconnected than the other Asian groups. In conclusion, youth disconnection rates are experiencing a downward trend in 2010 and after and its magnitude is most heavily influenced by race/ethnicity in most cases."
  },
  {
    "objectID": "content/cs/cs01.html",
    "href": "content/cs/cs01.html",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "",
    "text": "This is where you get to put together all you’ve learned so far this quarter into a full data science report! This report will include your analysis from top (the background and question) to bottom (your analysis, interpretation, and conclusions.)\nWe’ll be grading to see that you have: 1) all necessary code for each section of the project. 2) explanatory text that guides the reader from start to finish. 3) polished visualizations that allow the reader to both understand the data you’re working with an your conclusions.\nThis will be submitted and graded as a group. One submission per group."
  },
  {
    "objectID": "content/cs/cs01.html#getting-started",
    "href": "content/cs/cs01.html#getting-started",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nThis will be completed in cs01 group repository that has been created for you and your group mates.\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit and commit changes (for example, once per each new part)1\nPush all your changes back to your GitHub repo\nThis case study will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading.\n\nImports\nYou are allowed to import whichever packages you like for this case study report."
  },
  {
    "objectID": "content/cs/cs01.html#case-study-report",
    "href": "content/cs/cs01.html#case-study-report",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "Case Study Report",
    "text": "Case Study Report\nYour case study can be organized however you see best fit, but we’ll be looking for the following general sections:\n\nTitle\nAuthors\nBackground/Introduction\nQuestion(s)\nData\n\nData Explanation\nData Import\nData Wrangling\n\nAnalysis\n\nExploratory Data Analysis\nData Analysis\n\nResults\nDiscussion of results\nConclusion\n\nNow, you may want to combine some of these sections (i.e. include your results and discussion among your analysis code). That’s totally allowed, but we’ll be looking to see that your report includes sufficient information to understand what you did, why you did it, and what your results are.\n\nExtending the Analysis\nIn addition to getting the code presented in class working, adding explanatory text to your report, and generating polished visualizations, you and your group must “extend the analysis” presented in class in a meaningful way. Now “meaningful” is not a very-easily-measured term. A meaningful extension could be carrying out analysis to answer an additional sub-question beyond what was presented in class, or including a really extensive exploratory data analysis, or generating a really superb set of visualizations to convey your groups’ results, or finding a related dataset and incorporating it into your case study. To determine whether your extension is “meaningful,” you and your group should be able to answer “yes” to the question “Does our extension add something important to this report beyond what was presented in class?”\nThis extension should be included/weaved into your report, meaning it should only be “separated out” as its own section if it makes most sense for the story you’re telling."
  },
  {
    "objectID": "content/cs/cs01.html#group-feedback",
    "href": "content/cs/cs01.html#group-feedback",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the case study to provide feedback about working with your group mates. This is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary."
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#agenda",
    "href": "content/labslides/04-lab-deck.html#agenda",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Agenda",
    "text": "Agenda\n\nLab 04: Modelling course evaluations\nGetting started with lab"
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#lab-06-modelling-course-evaluations",
    "href": "content/labslides/04-lab-deck.html#lab-06-modelling-course-evaluations",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Lab 06: Modelling course evaluations",
    "text": "Lab 06: Modelling course evaluations\n\nMany college courses give students the opportunity to evaluate the course and the instructor anonymously\nThe use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, e.g. the physical appearance of the instructor"
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#youve-all-seen-something-like-this",
    "href": "content/labslides/04-lab-deck.html#youve-all-seen-something-like-this",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "You’ve all seen something like this…",
    "text": "You’ve all seen something like this…"
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#and-then-theres-also-this",
    "href": "content/labslides/04-lab-deck.html#and-then-theres-also-this",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "and then there’s also this",
    "text": "and then there’s also this"
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#data-comes-from",
    "href": "content/labslides/04-lab-deck.html#data-comes-from",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Data comes from…",
    "text": "Data comes from…\n“Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity”\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005. http://www.sciencedirect.com/science/article/pii/S0272775704001165"
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#some-new-challenges-in-this-lab",
    "href": "content/labslides/04-lab-deck.html#some-new-challenges-in-this-lab",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Some new challenges in this lab",
    "text": "Some new challenges in this lab\n\nThere isn’t much code on the lab instructions, you might need to refer to course slides to put the pieces together, however most of the time\n\nyou’ll be visualizing with ggplot,\nfitting a model with lm,\nand viewing some model statistics with glance\n\nInterpretation in the context of the data matters\n\nNumbers need context: saying (for example) “the \\(\\beta_1\\) is 5” isn’t enough; what does that mean? In particular, what does that mean for these data and for this question\n\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#agenda",
    "href": "content/labslides/03-lab-deck.html#agenda",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Agenda",
    "text": "Agenda\n\nTips:\n\nBriefly review a question regarding sorting.\nReview numeric vs categorical variable types.\n\nLab introduction:\n\nReview FiveThirtyEight article on college majors."
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#lab-intro",
    "href": "content/labslides/03-lab-deck.html#lab-intro",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Lab Intro",
    "text": "Lab Intro\n\nLab instructions posted on the course website.\nThe Economic Guide To Picking A College Major by Ben Casselman"
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#reminders",
    "href": "content/labslides/03-lab-deck.html#reminders",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Reminders",
    "text": "Reminders\n\nStart with library(tidyverse) (includes tidyr, readr, dplyr, etc.)\nClone using ‘SSH’ link from GitHub\nKnit to .html & push both .Rmd and .html to GitHub"
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#tips",
    "href": "content/labslides/03-lab-deck.html#tips",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Tips",
    "text": "Tips\n\nBe ready to troubleshoot your document, since it will likely fail to knit on multiple occasions throughout the process. Read the error message carefully and take note of which line is preventing a successful knit.\nMake sure to keep track of your various chunks and to keep text and code in the right place.\nRemember that your R Markdown file is not aware of your project’s global environment and can only make use of variables, functions, etc. that you have loaded or defined in the document.\nRemind yourself how the pipe operator (|>) works.\nIf you’re unsure how a function works or what its arguments are, type ? in front of it and hit enter (?read_csv for instance). The “Help” tab will open and provide a summary of the function as well as some examples.\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#reminders",
    "href": "content/labslides/02-lab-deck.html#reminders",
    "title": "Lab 02: Wrangling",
    "section": "Reminders",
    "text": "Reminders\n\nStart with library(tidyverse) (includes tidyr, readr, dplyr, etc.)\nClone using ‘SSH’ link from GitHub\nKnit to .html & push both .Rmd and .html to GitHub"
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#starting-a-new-project",
    "href": "content/labslides/02-lab-deck.html#starting-a-new-project",
    "title": "Lab 02: Wrangling",
    "section": "Starting a new project",
    "text": "Starting a new project\n\nGo to Canvas to find the link for today’s lab: lab03-wi23.\nOn GitHub, click on the green Clone or download button, select use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nGo to RStudio on datahub. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option.\nCopy and paste the URL of your assignment repo into the dialog box and hit OK.\nOpen the .Rmd file with your template in it. Be sure to update the author to your name."
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#agenda",
    "href": "content/labslides/02-lab-deck.html#agenda",
    "title": "Lab 02: Wrangling",
    "section": "Agenda",
    "text": "Agenda\n\nLab 02 intro and demos: Introduce the lab, and work through the first question as a class.\nOn your own: Work on the rest of the lab “on your own”, but feel free to check in with classmates as much as you like."
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#dplyr-review",
    "href": "content/labslides/02-lab-deck.html#dplyr-review",
    "title": "Lab 02: Wrangling",
    "section": "dplyr: Review",
    "text": "dplyr: Review\ndplyr provides a “Grammar of Data Manipulation” and is based on the concepts of functions as verbs that manipulate data frames.\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)"
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#the-data",
    "href": "content/labslides/02-lab-deck.html#the-data",
    "title": "Lab 02: Wrangling",
    "section": "The Data",
    "text": "The Data\n\nstorms |>\n  slice(1:20)\n\n# A tibble: 20 × 13\n   name   year month   day  hour   lat  long status      category  wind pressure\n   <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <fct>          <dbl> <int>    <int>\n 1 Amy    1975     6    27     0  27.5 -79   tropical d…       NA    25     1013\n 2 Amy    1975     6    27     6  28.5 -79   tropical d…       NA    25     1013\n 3 Amy    1975     6    27    12  29.5 -79   tropical d…       NA    25     1013\n 4 Amy    1975     6    27    18  30.5 -79   tropical d…       NA    25     1013\n 5 Amy    1975     6    28     0  31.5 -78.8 tropical d…       NA    25     1012\n 6 Amy    1975     6    28     6  32.4 -78.7 tropical d…       NA    25     1012\n 7 Amy    1975     6    28    12  33.3 -78   tropical d…       NA    25     1011\n 8 Amy    1975     6    28    18  34   -77   tropical d…       NA    30     1006\n 9 Amy    1975     6    29     0  34.4 -75.8 tropical s…       NA    35     1004\n10 Amy    1975     6    29     6  34   -74.8 tropical s…       NA    40     1002\n11 Amy    1975     6    29    12  33.8 -73.8 tropical s…       NA    45     1000\n12 Amy    1975     6    29    18  33.8 -72.8 tropical s…       NA    50      998\n13 Amy    1975     6    30     0  34.3 -71.6 tropical s…       NA    50      998\n14 Amy    1975     6    30     6  35.6 -70.8 tropical s…       NA    55      998\n15 Amy    1975     6    30    12  35.9 -70.5 tropical s…       NA    60      987\n16 Amy    1975     6    30    18  36.2 -70.2 tropical s…       NA    60      987\n17 Amy    1975     7     1     0  36.2 -69.8 tropical s…       NA    60      984\n18 Amy    1975     7     1     6  36.2 -69.4 tropical s…       NA    60      984\n19 Amy    1975     7     1    12  36.2 -68.3 tropical s…       NA    60      984\n20 Amy    1975     7     1    18  36.7 -67.2 tropical s…       NA    60      984\n# ℹ 2 more variables: tropicalstorm_force_diameter <int>,\n#   hurricane_force_diameter <int>"
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#the-data-documentation",
    "href": "content/labslides/02-lab-deck.html#the-data-documentation",
    "title": "Lab 02: Wrangling",
    "section": "The Data: Documentation",
    "text": "The Data: Documentation\nFrom the console…\n\n?storms\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#agenda",
    "href": "content/labslides/05-lab-deck.html#agenda",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Agenda",
    "text": "Agenda\n\nDocumentation\nLab 05: Modelling course evaluations\nGetting started with lab\n\nReminder to complete mid-course survey"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#documentation",
    "href": "content/labslides/05-lab-deck.html#documentation",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Documentation",
    "text": "Documentation\nDemo on how to utilize/read/understand R Documentation"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#reminder-data-come-from",
    "href": "content/labslides/05-lab-deck.html#reminder-data-come-from",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Reminder: Data come from…",
    "text": "Reminder: Data come from…\n“Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity”\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005. http://www.sciencedirect.com/science/article/pii/S0272775704001165"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#some-notes-on-this-lab",
    "href": "content/labslides/05-lab-deck.html#some-notes-on-this-lab",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Some notes on this lab",
    "text": "Some notes on this lab\n\nThis is an extension of lab04. It may be worth briefly looking over that lab and/or the answer key to get that fresh in your mind prior to beginning this lab.\nThere are three parts. Ideally, you’d get through Exercise 12. We’ll be looking to see you’ve fit and interpreted models with multiple predictors.\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#agenda",
    "href": "content/labslides/07-lab-deck.html#agenda",
    "title": "Lab 07: Logistic Regression",
    "section": "Agenda",
    "text": "Agenda\n\nLab 07: Modelling resumes\nGetting started with lab"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#data-come-from",
    "href": "content/labslides/07-lab-deck.html#data-come-from",
    "title": "Lab 07: Logistic Regression",
    "section": "Data come from…",
    "text": "Data come from…\n“Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.”\nBertrand, Marianne, and Sendhil Mullainathan. 2003. https://doi.org/10.3386/w9873."
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#study",
    "href": "content/labslides/07-lab-deck.html#study",
    "title": "Lab 07: Logistic Regression",
    "section": "Study",
    "text": "Study\nGoal: understand the influence of race and gender on job application callback rates\n\nSpecs:\n\nTwo cities: Boston and Chicago\nTime: several months in 2001 and 2002\n\nPlan:\n\nResearchers generated resumes, randomizing years of experience and education details\nThen: randomly assigned a name to the resume that would communicate the applicant’s gender and race\n\n\nthey tested these names and removed those that did not suggest gender and race consistently\ni.e. Lakisha was a name that their survey indicated would be interpreted as a black woman, while Greg was a name that would generally be interpreted to be associated with a white male.”"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#backwards-elimination-logistic-regression",
    "href": "content/labslides/07-lab-deck.html#backwards-elimination-logistic-regression",
    "title": "Lab 07: Logistic Regression",
    "section": "Backwards elimination (Logistic Regression)",
    "text": "Backwards elimination (Logistic Regression)\n\nStart with full model (including all candidate explanatory variables and all candidate interactions)\nRemove one variable at a time, and select the model with the lowest AIC\nContinue until AIC does not decrease\n\n\n\nYou do NOT have to include every model in your lab…just the final one you settle on."
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#a-note-on-this-lab",
    "href": "content/labslides/07-lab-deck.html#a-note-on-this-lab",
    "title": "Lab 07: Logistic Regression",
    "section": "A note on this lab",
    "text": "A note on this lab\n\nThere are three parts. We’ll be grading to see that you’ve done some EDA and have fit and interpreted at least two models (single and multiple predictors model)\nIt’s OK if you don’t get to backwards elimination\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#agenda",
    "href": "content/labslides/01-lab-deck.html#agenda",
    "title": "Lab 01: Tooling",
    "section": "Agenda",
    "text": "Agenda\n\nLab structure: Lab structure overview.\nLab 01 intro and demos: Introduce the lab, and work through the first section as a class.\nOn your own: Work on the rest of the lab “on your own”, but feel free to check in with classmates as much as you like."
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#lab-structure-1",
    "href": "content/labslides/01-lab-deck.html#lab-structure-1",
    "title": "Lab 01: Tooling",
    "section": "Lab structure",
    "text": "Lab structure\n\n5-10 minute introduction (a bit longer today)\nUse the remaining time to work through the lab exercises and fill out your lab report\n\nSubmit: on your own\nWorking: always allowed to work together\n\nLab instructions posted on the course website on the left panel under “Labs”\n\nLet’s go find today’s lab!"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#tips",
    "href": "content/labslides/01-lab-deck.html#tips",
    "title": "Lab 01: Tooling",
    "section": "Tips",
    "text": "Tips\n\nYou do not have to finish the lab in class; you have until midnight to submit. But, you might choose to get through portions that you think will be challenging (which initially might be the coding component) in class when staff can help you on the spot, and leave the narrative writing until later.\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality lab.\nWhen working with others, do not split up lab among classmates, work on it together in its entirety.\nSometimes you may not finish the entire lab…and that’s ok! When this happens or you’re unsure about what you turn in, be sure to go back and check your thoughts/work against the posted answer key."
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#goals",
    "href": "content/labslides/01-lab-deck.html#goals",
    "title": "Lab 01: Tooling",
    "section": "Goals",
    "text": "Goals\n\nIntroduce you to Git and GitHub: collaboration and version control system that we will be using throughout the course\n\nGit is a version control system – like “Track Changes” features from Microsoft Word/Google Docs on steroids\nGitHub is the home for your git-based projects on the internet\nConnect your RStudio on datahub to your GitHub account\n\nIntroduce you to R and RStudio:\n\nR is the name of the programming language itself\nRStudio is a convenient interface"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#getting-started-github-datahub",
    "href": "content/labslides/01-lab-deck.html#getting-started-github-datahub",
    "title": "Lab 01: Tooling",
    "section": "Getting started: GitHub & datahub",
    "text": "Getting started: GitHub & datahub\nFirst, put away computers, and watch me do it:\n\nDemo of the process\nSteps are spelled out in the “GitHub Housekeeping” portion of the lab"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#getting-started-assignment-retrieval",
    "href": "content/labslides/01-lab-deck.html#getting-started-assignment-retrieval",
    "title": "Lab 01: Tooling",
    "section": "Getting started: Assignment Retrieval",
    "text": "Getting started: Assignment Retrieval\nFirst, put away computers, and watch me do it:\n\nClick on the assignment link on Canvas for today’s lab to create your GitHub repository (which we’ll refer to as “repo” going forward) for the lab. This repo contains a template you can build on to complete your lab.\nOn GitHub, accept the assignment. Click on the link to navigate to the repo.\nOn the repo, click the green “<> Code” button, ensure that “SSH” is selected, and then copy the URL.\nGo to datahub. and open RStudio. Go to File > New Project… and select to create a New Project from Version Control. On the following menu, select Git.\nCopy and paste the URL of your assignment repo into the “Repository URL” dialog box.\nHit Create Project.\n\nNow it’s your turn! Place a green sticky on your laptop when you’re done with this part (you can continue if you like). Place a pink sticky if you have questions."
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html",
    "href": "content/lab-ans/lab03-viz-ans.html",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-lowest-unemployment-rate",
    "href": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-lowest-unemployment-rate",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\n\ncollege_recent_grads |>\n  arrange(unemployment_rate) |>\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# ℹ 163 more rows\n\n\nDisplay only 4 decimal places using mutate and round:\n\ncollege_recent_grads |>\n  arrange(unemployment_rate) |>\n  select(rank, major, unemployment_rate) |>\n  mutate(unemployment_rate = round(unemployment_rate, digits = 4))\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1    53 Mathematics And Computer Science                      0     \n 2    74 Military Technologies                                 0     \n 3    84 Botany                                                0     \n 4   113 Soil Science                                          0     \n 5   121 Educational Administration And Supervision            0     \n 6    15 Engineering Mechanics Physics And Science             0.0063\n 7    20 Court Reporting                                       0.0117\n 8   120 Mathematics Teacher Education                         0.0162\n 9     1 Petroleum Engineering                                 0.0184\n10    65 General Agriculture                                   0.0196\n# ℹ 163 more rows\n\n\nDisplay 2 scientific digits using options:\n\noptions(digits = 2)\n\n\nExercise 1\nWhich of these options, changing the input data or altering the number of digits displayed without touching the input data, is the better option? Explain your reasoning. Then, implement the option you chose.\nI prefer the options approach because it will enforce consistency each time we display variables, without having to make the call to round each time (without changing the underlying data)."
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-highest-percentage-of-women",
    "href": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-highest-percentage-of-women",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\n\nExercise 2\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline.\nThe sharewomen column lists the proportion of women in each major. So we can arrange our data in descending order using desc(sharewomen), then select the columns we want: major, total, and sharewomen. We then just display the top 3 majors using head(3).\n\ncollege_recent_grads |>\n  arrange(desc(sharewomen)) |>\n  select(major, total, sharewomen) |>\n  head(3)\n\n# A tibble: 3 × 3\n  major                                         total sharewomen\n  <chr>                                         <dbl>      <dbl>\n1 Early Childhood Education                     37589      0.969\n2 Communication Disorders Sciences And Services 38279      0.968\n3 Medical Assisting Services                    11123      0.928"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "content/lab-ans/lab03-viz-ans.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\n\nExercise 3\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\nThe mean is more affected by the presence of outliers and by the skew of the distribution. Thus, the presence of a single person with very high income can increase the mean substantively, such that it’s no longer a good impression of the overall distribution. In contrast, the median (or the “middle number”) tells us the income exactly in the middle of the distribution.\n\n\nExercise 4\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram(binwidth=5000)\n\n\n\n\nI ended up choosing binwidth=5000. When binwidth=1000, there were too many small differences in income that were thus not grouped together, and it was harder to see the overall shape of the distribution.\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads |>\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl>\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\n\nExercise 5\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nThe underlying distribution of median incomes is somewhat right-skewed, with at least 1-2 majors making a lot of money. Because of this, I’m going to use the median median income.\nIt would be probably be fine to use the mean median income as well—judging by the distribution of median incomes in each major_category, which are relatively normal-ish.\n\n\nExercise 6\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram(binwidth = 5000) +\n  facet_wrap( ~ major_category, ncol = 4)\n\n\n\n\n\n\nExercise 7\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Use the partial code below, filling it in with the appropriate statistic and function. Also note that we are looking for the highest statistic, so make sure to arrange in the correct direction.\n\ncollege_recent_grads |>\n  group_by(major_category) |>\n  summarise(median = median(median)) |>\n  arrange(desc(median))\n\n# A tibble: 16 × 2\n   major_category                      median\n   <chr>                                <dbl>\n 1 Engineering                          57000\n 2 Computers & Mathematics              45000\n 3 Business                             40000\n 4 Physical Sciences                    39500\n 5 Social Science                       38000\n 6 Biology & Life Science               36300\n 7 Law & Public Policy                  36000\n 8 Agriculture & Natural Resources      35000\n 9 Communications & Journalism          35000\n10 Health                               35000\n11 Industrial Arts & Consumer Services  35000\n12 Interdisciplinary                    35000\n13 Education                            32750\n14 Humanities & Liberal Arts            32000\n15 Arts                                 30750\n16 Psychology & Social Work             30000\n\n\nEngineering.\n\n\nExercise 8\nWhich major category is the least popular in this sample? To answer this question we can take advantage of the total column:\n\ncollege_recent_grads |>\n  filter(is.na(total) == FALSE) |>\n  group_by(major_category) |>\n  summarise(popularity = sum(total)) |>\n  arrange(popularity)\n\n# A tibble: 16 × 2\n   major_category                      popularity\n   <chr>                                    <dbl>\n 1 Interdisciplinary                        12296\n 2 Agriculture & Natural Resources          75620\n 3 Law & Public Policy                     179107\n 4 Physical Sciences                       185479\n 5 Industrial Arts & Consumer Services     229792\n 6 Computers & Mathematics                 299008\n 7 Arts                                    357130\n 8 Communications & Journalism             392601\n 9 Biology & Life Science                  453862\n10 Health                                  463230\n11 Psychology & Social Work                481007\n12 Social Science                          529966\n13 Engineering                             537583\n14 Education                               559129\n15 Humanities & Liberal Arts               713468\n16 Business                               1302376"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#all-stem-fields-arent-the-same",
    "href": "content/lab-ans/lab03-viz-ans.html#all-stem-fields-arent-the-same",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories <- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads <- college_recent_grads |>\n  mutate(major_type = case_when(\n    major_category %in% stem_categories ~ \"stem\",\n    TRUE ~ \"not stem\"\n  ))\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads |>\n  filter(\n    major_type == \"stem\",\n    median < median(college_recent_grads$median)\n  )\n\n# A tibble: 10 × 22\n    rank major_code major        major_category  total sample_size    men  women\n   <dbl>      <dbl> <chr>        <chr>           <dbl>       <dbl>  <dbl>  <dbl>\n 1    93       1301 Environment… Biology & Lif…  25965         225  10787  15178\n 2    98       5098 Multi-Disci… Physical Scie…  62052         427  27015  35037\n 3   102       3608 Physiology   Biology & Lif…  22060          99   8422  13638\n 4   106       2001 Communicati… Computers & M…  18035         208  11431   6604\n 5   109       3611 Neuroscience Biology & Lif…  13663          53   4944   8719\n 6   111       5002 Atmospheric… Physical Scie…   4043          32   2744   1299\n 7   123       3699 Miscellaneo… Biology & Lif…  10706          63   4747   5959\n 8   124       3600 Biology      Biology & Lif… 280709        1370 111762 168947\n 9   133       3604 Ecology      Biology & Lif…   9154          86   3878   5276\n10   169       3609 Zoology      Biology & Lif…   8409          47   3050   5359\n# ℹ 14 more variables: sharewomen <dbl>, employed <dbl>,\n#   employed_fulltime <dbl>, employed_parttime <dbl>,\n#   employed_fulltime_yearround <dbl>, unemployed <dbl>,\n#   unemployment_rate <dbl>, p25th <dbl>, median <dbl>, p75th <dbl>,\n#   college_jobs <dbl>, non_college_jobs <dbl>, low_wage_jobs <dbl>,\n#   major_type <chr>\n\n\n\nExercise 9\nWhich STEM majors have median salaries equal to or less than the median for all majors’s median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top.\n\ncollege_recent_grads |>\n  filter(\n    major_type == \"stem\",\n    median < median(college_recent_grads$median)\n  ) |>\n  select(major, median, p25th, p75th) |>\n  arrange(desc(median))\n\n# A tibble: 10 × 4\n   major                                 median p25th p75th\n   <chr>                                  <dbl> <dbl> <dbl>\n 1 Environmental Science                  35600 25000 40200\n 2 Multi-Disciplinary Or General Science  35000 24000 50000\n 3 Physiology                             35000 20000 50000\n 4 Communication Technologies             35000 25000 45000\n 5 Neuroscience                           35000 30000 44000\n 6 Atmospheric Sciences And Meteorology   35000 28000 50000\n 7 Miscellaneous Biology                  33500 23000 48000\n 8 Biology                                33400 24000 45000\n 9 Ecology                                33000 23000 42000\n10 Zoology                                26000 20000 39000"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "content/lab-ans/lab03-viz-ans.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nExercise 10\nCreate a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables.\n\ncollege_recent_grads |>\n  drop_na(sharewomen) |> ## This will drop rows for which sharewomen has NA value\n  ggplot(aes(x = median,\n             y = sharewomen,\n             color = major_type)) +\n  geom_point(alpha = .6) +\n  labs(x = \"Median income for major\",\n       y = \"Share of women in major\",\n       color = \"STEM major?\")\n\n\n\n\nIn general, there appears to be a negative relationship between median income and the proportion of women (sharewomen) in a major. Both variables are also correlated with major_type: stem majors tend to have a lower proportion of women (and lower median income), while not stem majors have a higher proportion (and higher median income).\n(Note that the negative relationship with median income is somewhat clearer if log(median) is used instead.)"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html",
    "href": "content/lab-ans/lab04-modelling-ans.html",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nevals <- read_csv(\"data/evals-mod.csv\")"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-1",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-1",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 1",
    "text": "Part 1\n\nExercise 1\nFirst, we use rowwise to group evals by each row. We then use mutate to compute a new variable (bty_avg) corresponding to the average of the six beauty scores.\n\nevals <- evals |>\n  rowwise() |>\n  mutate(bty_avg = mean( c( bty_f1lower, bty_f1upper,\n                            bty_f2upper, bty_m1lower,\n                            bty_m1upper, bty_m2upper) ))"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-2",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-2",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 2",
    "text": "Part 2\n\nExercise 2\nThe distribution is slightly left-skewed, i.e., many scores are on the higher side (between 4-5) with a longer tail going to the left.\nThis is further corroborrated by the fact that the mean is lower than the median. Given that the mean is more affected by skew than the median, this is what we’d expect with negatively skewed data.\n\nevals |>\n  ggplot(aes(x = score))+\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nevals |>\n  summarise(mean = mean(score),\n            median = median(score),\n            sd = sd(score))\n\n# A tibble: 463 × 3\n    mean median    sd\n   <dbl>  <dbl> <dbl>\n 1   4.7    4.7    NA\n 2   4.1    4.1    NA\n 3   3.9    3.9    NA\n 4   4.8    4.8    NA\n 5   4.6    4.6    NA\n 6   4.3    4.3    NA\n 7   2.8    2.8    NA\n 8   4.1    4.1    NA\n 9   3.4    3.4    NA\n10   4.5    4.5    NA\n# ℹ 453 more rows\n\n\n\n\nExercise 3\nIt seems like there’s a slight positive relationship between bty_avg and score, though it’s hard to tell given that many points overlap and form “bands” along particular values (e.g., many teachers have the same score).\n\nevals |>\n  ggplot(aes(x = bty_avg,\n             y = score))+\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\nExercise 4\nOne way to deal with the above is to add jitter to the plot, which reveals that certain clusters of points were either larger or smaller than initially thought. (Another apprpoach would be to change the alpha of geom_point, allowing us to detect denser clusters.)\n\nevals |>\n  ggplot(aes(x = bty_avg,\n             y = score))+\n  geom_jitter() +\n  theme_bw()"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-3",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-3",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 3",
    "text": "Part 3\n\nExercise 5\nAccording to the model, the linear equation would be as follows:\n\\(\\hat{y} = 3.88 + 0.07*X\\)\nWhere \\(X\\) is bty_avg.\nWe use broom::tidy to transform the model object into a tidy dataframe.\n\nm_bty <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg, data = evals)\n\nm_bty |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.88  \n2 bty_avg       0.0666\n\n\n\n\nExercise 6\n\nevals |>\n  ggplot(aes(x = bty_avg,\n             y = score))+\n  geom_point() +\n  geom_smooth(method = \"lm\", \n              color = \"orange\",\n              se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nExercise 7\nProfessors with higher average beauty ratings also tend to receive higher teaching scores.\nThe slope is approximately 0.07, which means that for every 1-unit increase in beauty rating (i.e., from 4 to 5), we can expect that a professor will receive .07 higher teaching scores (with a ~0.02 standard error).\n\nm_bty |>\n  tidy() |>\n  filter(term == \"bty_avg\") |>\n  select(term, estimate, std.error)\n\n# A tibble: 1 × 3\n  term    estimate std.error\n  <chr>      <dbl>     <dbl>\n1 bty_avg   0.0666    0.0163\n\n\n\n\nExercise 8\nThe intercept is 3.88. This is the estimated score for professors with a beauty score of 0, i.e., it is the value of \\(\\hat{y}\\) when the regression line crosses \\(x = 0\\).\nThis may or may not make sense; given that possible beauty scores ranged from 1-6, it is a little strange to imagine a scenario where the beauty score is 0 (and it would be even stranger to imagine scenarios where the beauty scores is negative).\nOn the other hand, it is not inconsistent with other features of the data: the mean score, for example, is 4.17, so it makes sense that the intercept of a linear model would be slightly lower than the mean, allowing the explanatory variable to account for increases in score throughout the data.\n\nm_bty |>\n  tidy() |>\n  filter(term == \"(Intercept)\") |>\n  select(term, estimate, std.error)\n\n# A tibble: 1 × 3\n  term        estimate std.error\n  <chr>          <dbl>     <dbl>\n1 (Intercept)     3.88    0.0761\n\n\n\n\nExercise 9\nHere, we identify the \\(R^2\\) of the model using glance, which identifies model-level characteristics.\nThe value is 0.0350, which means that our explanatory variable accounts for roughly 3.5% of the variance in our response variable.\n\nm_bty |>\n  glance() |>\n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1    0.0350"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-4",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-4",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 4",
    "text": "Part 4\n\nExercise 10\nThe linear equation would be as follows:\n\\(\\hat{y} = 4.09 + 0.142*X\\)\nWhere \\(X = 1\\) for male professors and \\(X = 0\\) for female professors. In other words, the intercept corresponds to the mean score for female professors, while the slope tells us the difference in mean between the female average and the male average.\n\nm_gen <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit( score ~ gender, data = evals)\n\nm_gen |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    4.09     0.0387    106.   0      \n2 gendermale     0.142    0.0508      2.78 0.00558\n\n\n\n\nExercise 11\nAs specified above, the slope coefficient for the \\(X\\) term signifies the difference in means between male and female professors.\n\n\nExercise 12\nHere, the model equation would look as follows:\n\\(\\hat{y} = 4.28 + -0.13*X_1 + -0.145*X_2\\)\nWhere \\(X_1 = 1\\) for tenure track professors and 0 for all else, and \\(X_2 = 1\\) for tenured professors and 0 for all else.\nIn other words: the intercept tells us the mean for teaching professors, and the two slope coefficients tell us the difference between that mean and the two other levels of rank.\n\ntable(evals$rank)\n\n\n    teaching tenure track      tenured \n         102          108          253 \n\nm_rank <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ rank, data = evals)\n\nm_rank |>\n  tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)         4.28     0.0537     79.9  1.02e-271\n2 ranktenure track   -0.130    0.0748     -1.73 8.37e-  2\n3 ranktenured        -0.145    0.0636     -2.28 2.28e-  2\n\n\n\n\nExercise 13\nHere, we use the factor command (within mutate) to reorder the levels of rank such that tenure track is the first level.\n\nevals <- evals |>\n  mutate(rank_relevel = fct_relevel(factor(rank), c(\"tenure track\",\n                                                    \"teaching\",\n                                                    \"tenured\")))\n\nWarning: There were 463 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `rank_relevel = fct_relevel(factor(rank), c(\"tenure track\",\n  \"teaching\", \"tenured\"))`.\nℹ In row 1.\nCaused by warning:\n! 2 unknown levels in `f`: teaching and tenured\nℹ Run `dplyr::last_dplyr_warnings()` to see the 462 remaining warnings.\n\n\n\n\nExercise 14\nThis model has the same structure as the previous model using rank, but it has reordered the levels of rank (for rank_relevel) such that tenure track is now first.\nThis means that the intercept should now be interpreted as the mean score for tenure track professors, and the coefficients represent the difference in means for teaching and tenured professors, respectively.\n\\(\\hat{y} = 4.15 + 0.13*X_1 + -0.0155*X_2\\)\nWhere \\(X_1 = 1\\) for teaching professors and 0 for all else; and \\(X_2 = 1\\) for tenured professors and 0 for all else.\nWe can compare tehse to the coefficients for the previous model, where teaching was the baseline: in both cases, we see that the difference between teaching and tenure track is 0.13 (just in opposite directions, depending on the baseline).\nThe \\(R^2\\) of the model is 0.0116 in both cases. In other words, the model explains approximately 1.16% of the variance in score.\n\nm_rank_relevel <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ rank_relevel, data = evals)\n\n\nm_rank_relevel |>\n  tidy()\n\n# A tibble: 3 × 5\n  term                 estimate std.error statistic   p.value\n  <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)            4.15      0.0521    79.7   2.58e-271\n2 rank_releveltenured   -0.0155    0.0623    -0.249 8.04e-  1\n3 rank_relevelteaching   0.130     0.0748     1.73  8.37e-  2\n\nm_rank_relevel |>\n  glance() |>\n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1    0.0116\n\n\n\n\nExercise 15\n\nevals <- evals |>\n  mutate(tenure_eligible = case_when(\n    rank == \"teaching\" ~ \"no\",\n    TRUE ~ \"yes\"\n  ))\n\n## Double check\ntable(evals$rank, evals$tenure_eligible)\n\n              \n                no yes\n  teaching     102   0\n  tenure track   0 108\n  tenured        0 253\n\n\n\n\nExercise 16\nThe linear equation is as follows:\n\\(\\hat{y} = 4.28 + -0.141*X_1\\)\nWhere \\(X_1 = 1\\) for tenure eligible professors and \\(X_1 = 0\\) for non-tenure-eligible professors.\nThus, this means that the mean score for non-tenure-eligible professors is 4.28, and professors that are tenure eligible can expect a score that’s -0.141 lower, on average.\n\nm_tenure_eligible <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ tenure_eligible, data = evals)\n\nm_tenure_eligible |>\n  tidy()\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic   p.value\n  <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)           4.28     0.0536     79.9  2.72e-272\n2 tenure_eligibleyes   -0.141    0.0607     -2.32 2.10e-  2\n\n\nWe can also calculate the \\(R^2\\), and we see that it is 0.0115. In other words, the model explains approximately 1.15% of the variance in score.\n\nm_tenure_eligible |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0115       0.00935 0.541      5.36  0.0210     1  -372.  750.  762.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html",
    "href": "content/lab-ans/lab05-mlr-ans.html",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\n\n\nFirst, we read in the data from the appropriate directory.\nWe also recreate the bty_avg column as we did in Lab 4, which is just the mean of beauty ratings from each of the relevant groups.\n\nevals = read_csv(\"data/evals-mod.csv\")\nevals <- evals |>\n  rowwise() |>\n  mutate(bty_avg = mean( c( bty_f1lower, bty_f1upper,\n                            bty_f2upper, bty_m1lower,\n                            bty_m1upper, bty_m2upper) )) |>\n  ungroup()"
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html#part-1",
    "href": "content/lab-ans/lab05-mlr-ans.html#part-1",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "Part 1",
    "text": "Part 1\n\nExercise 2\nAs in Lab 4, we fit a model m_bty predicting score from bty_avg.\nThe linear equation would be written as follows:\n\\(\\hat{y} = 3.88 + 0.07*X\\)\nAdditionally, this model has an \\(R^2\\) of 0.035 and an adjusted \\(R^2\\) of 0.0329.\n\nm_bty <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg, data = evals)\n\nm_bty |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.88  \n2 bty_avg       0.0666\n\nm_bty |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0350        0.0329 0.535      16.7 0.0000508     1  -366.  738.  751.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html#part-2",
    "href": "content/lab-ans/lab05-mlr-ans.html#part-2",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "Part 2",
    "text": "Part 2\n\nExercise 3\n\\(\\hat{y} = 3.75 + 0.07X_1 + 0.17X_2\\)\nWhere \\(X_1\\) is bty_avg and \\(X_2\\) corresponds to a binary variable that takes on the value of 0 for female professors and 1 for male professors.\nThis model has an \\(R^2\\) of 0.059 and an adjusted \\(R^2\\) of 0.055.\n\nm_bty_gen <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + gender, data = evals)\n\nm_bty_gen |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.75  \n2 bty_avg       0.0742\n3 gendermale    0.172 \n\nm_bty_gen |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0591        0.0550 0.529      14.5 0.000000818     2  -360.  729.  745.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\n\nExercise 4\nThe intercept means: female teachers with an average beauty score of 0 are predicted to have a score of 3.75.\nThe slope for bty_avg means that for every 1-unit increase in bty_avg, teachers are predicted to have an increase in their score (relative to the intercept) of approximately 0.07. This is true for both male and female teachers, but of course, the predictions for male teachers are also affected by the slope of gender, which means: holding bty_avg constant, male teachers are predicted to have a higher score (relative to the intercept) by approximately 0.172.\n\nm_bty_gen |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.75  \n2 bty_avg       0.0742\n3 gendermale    0.172 \n\n\n\n\nExercise 5\nThis model has an \\(R^2\\) of 0.059, which means that the model explains about 5.9% of the varaince in score.\n\n\nExercise 6\nFor male professors only, the intercept term can be added to the slope coefficient for \\(X_2\\) (gender), giving us a simplified expression:\n\\(\\hat{y} = 3.92 + 0.07X_1\\)\n\n\nExercise 7\nHolding bty_avg constant, male professors are predicted to have a score that’s about 0.172 higher than female professors. Thus, according to the model, male professors with equivalent beauty ratings have higher scores.\n\n\nExercise 8\nThe adjusted \\(R^2\\) for the larger model (m_bty_gen) is 0.055, whereas it is 0.0329 for the smaller model (m_bty). The difference between these values is 0.0221.\nThis can be interpreted as follows: adding gender to a model explains an additional 2.21% of variance in score beyond the variance already explained by bty_avg.\n\n\nExercise 9\nThe slope for bty_avg in m_bty is 0.067. The slope for bty_avg in m_bty_gen is 0.074.\nThe short answer is yes: adding gender has changed the slope for bty_avg. Specifically, it has increased the steepness of this slope: the predicted increase in score for each 1-unit increase in bty_avg is higher in a model that’s also equipped with gender than in a model without gender.\n\nm_bty |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   3.88      0.0761     51.0  1.56e-191\n2 bty_avg       0.0666    0.0163      4.09 5.08e-  5\n\nm_bty_gen |>\n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   3.75      0.0847     44.3  6.23e-168\n2 bty_avg       0.0742    0.0163      4.56 6.48e-  6\n3 gendermale    0.172     0.0502      3.43 6.52e-  4\n\n\n\n\nExercise 10\n\nm_bty_rank <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + rank, data = evals)\n\nm_bty_rank |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 4 × 2\n  term             estimate\n  <chr>               <dbl>\n1 (Intercept)        3.98  \n2 bty_avg            0.0678\n3 ranktenure track  -0.161 \n4 ranktenured       -0.126 \n\n\n\\(\\hat{y} = 3.98 + 0.068X_1 - 0.16X_2 - 0.13X_3\\)\nWhere \\(X_1\\) is bty_avg, \\(X_2\\) = 1 for tenure track professors (and 0 for all else), and \\(X_3\\) = 1 for tenured professors (and 0 for all else).\nThe intercept can be interpreted as follows: teaching professors with a bty_avg of 0 are predicted to have a score of 3.98.\nFor each 1-unit increase in bty_avg, all professors should have an increase in score by 0.068. The critical difference is simply what else is added to this increase. For teaching professors, this increase is considered relative only to the intercept. However, tenure track professors are subject to a decrease of 0.16 in their predicted score, and tenured professors are also subject to a decrease of 0.13 in their predicted score."
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html#part-3",
    "href": "content/lab-ans/lab05-mlr-ans.html#part-3",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "Part 3",
    "text": "Part 3\n\nExercise 11\nPersonally, I would expect either cls_did_eval or cls_perc_eval to be the worst predictors. All the other variables seem more likely to correlate with score, either because of potential societal biases (like age or gender), or because of how they might correlate with other factors that could make a class a better or worse experience for students, on average (e.g., cls_students).\nOf course, it’s possible that the proportion of students who fill out the survey (cls_perc_eval) is correlated with score: maybe people are more motivated to fill out the evaluations for really great professors. This suggests that cls_did_eval might be the worst predictor, since it won’t accurately reflect the proportion of students who chose to fill out, and it’s confounded with cls_students.\n\n\nExercise 12\nIt turns out I was wrong. The worst \\(R^2\\) values (and adjusted \\(R^2\\) values) are obtained from models with cls_profs (the number of professors teaching the class) or cls_students (the number of students).\nThe absolute worst model is cls_profs (with an \\(R^2\\) of 0.0006). In comparison, the model with cls_did_eval is about 6x better (with an \\(R^2\\) of 0.004). Neither is very good at all, but my prediction/intuition was incorrect.\nThe coefficient for cls_profs:single (i.e., classes with a single professor) is negative, but the standard error is almost 2x as larger as the coefficient, suggesting a lot of uncertainty about that estimate.\n\n## Fit with cls_students\nm_bty_cls_students <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_students, \n      data = evals)\n\nm_bty_cls_students |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1  0.000674      -0.00149 0.544     0.311   0.577     1  -374.  755.  767.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n## Fit with cls_profs\nm_bty_cls_profs <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_profs, \n      data = evals)\n\nm_bty_cls_profs |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1  0.000649      -0.00152 0.544     0.299   0.585     1  -374.  755.  767.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nm_bty_cls_profs |>\n  tidy()\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)       4.18      0.0311   134.      0    \n2 cls_profssingle  -0.0292    0.0534    -0.547   0.585\n\n## Fit model with the variable I predicted would be worst\nm_bty_cls_did_eval <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_did_eval, \n      data = evals)\n\nm_bty_cls_did_eval |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1   0.00395       0.00179 0.543      1.83   0.177     1  -374.  753.  766.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\n\nExercise 13\nIf we’re already going to include cls_students (the number of students in a class) and cls_perc_eval (the percentage who filled out the evaluations), we wouldn’t need cls_did_eval, because that is simply the product of cls_students and cls_perc_eval.\nIncluding this parameter shouldn’t improve the fit of the model, and it’ll only negatively impact metrics like AIC or adjusted R-squared that take into account the number of parameters in the model.\n\n\nExercise 14\nHere, we fit a full model with every parameter but cls_did_eval.\nThe adjusted \\(R^2\\) is 0.141.\n\nm_bty_full <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + rank +\n        ethnicity + gender + language + age +\n        cls_perc_eval + cls_students + cls_level +\n        cls_profs + cls_credits + bty_avg, \n      data = evals)\n\nm_bty_full |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.164         0.141 0.504      7.33 2.40e-12    12  -333.  694.  752.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n\n\n\nExercise 15\nNow, we fit a series of models, taking out one variable at a time.\nFor each model of size \\(k\\), we remove one of the original parameters and ask which parameter’s removal results in the highest adjusted \\(R^2\\).\nIn order of removal from the full model in Exercise 15:\n\ncls_profs (yields adjusted R^2 of 0.143).\n\ncls_level (yields adjusted R^2 of 0.1448).\n\nrank (yields adjusted R^2 of 0.145).\n\nAfter these three, there are no other parameters for which removing them improves model fit.\n\n## First reduced model, no cls_profs\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n\n[1] 0.1430708\n\n## Reduced model 2, no cls_level or cls_profs\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n\n[1] 0.1447586\n\n## Reduced model 3, no cls_level or cls_profs or rank\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        # rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n\n[1] 0.1453683\n\n\nThe final model is below, with the corresponding linear equation:\n\\(\\hat{y} = 3.39 + 0.062X_1 + 0.2X_2 + 0.18X_3 - 0.15X_4 - 0.005X_5 + 0.006X_6 + 0.0004X_7 + 0.52X_8\\)\nWhere:\n\n\\(X_1\\) is bty_avg\n\n\\(X_2\\) is ethnicity (corresponding to 1 for not minority professors)\n\n\\(X_3\\) is gender (corresponding to 1 for male professors)\n\n\\(X_4\\) is language (corresponding to 1 for non-english professors)\n\\(X_5\\) is age\n\n\\(X_6\\) is cls_perc_eval\n\n\\(X_7\\) is cls_students\n\n\\(X_8\\) is cls_credits (corresponding to 1 for one credit professors)\n\n\n## Final model.\nm_final <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        # rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals)\n\nm_final |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.160         0.145 0.503      10.8 5.46e-14     8  -334.  688.  730.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nm_final |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 9 × 2\n  term                   estimate\n  <chr>                     <dbl>\n1 (Intercept)            3.39    \n2 bty_avg                0.0619  \n3 ethnicitynot minority  0.204   \n4 gendermale             0.177   \n5 languagenon-english   -0.151   \n6 age                   -0.00487 \n7 cls_perc_eval          0.00575 \n8 cls_students           0.000407\n9 cls_creditsone credit  0.523   \n\n\nA shorter non-tidy approach…\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n# fit model\nmod <- lm(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        cls_level +\n        cls_profs +\n        cls_credits,\n      data = evals)\n\n# determine which predictors to eliminate\nols_step_backward_p(mod)\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\n\n\n                           Elimination Summary                            \n-------------------------------------------------------------------------\n        Variable                   Adj.                                      \nStep     Removed     R-Square    R-Square     C(p)       AIC        RMSE     \n-------------------------------------------------------------------------\n   1    cls_profs      0.1635      0.1431    9.0279    692.3066    0.5035    \n   2    cls_level      0.1633      0.1448    7.1373    690.4192    0.5030    \n   3    rank           0.1602      0.1454    6.8068    688.1332    0.5028    \n-------------------------------------------------------------------------\n\n\n\n\nExercise 16\nThe variable cls_credits encodes whether a class is worth multiple credits or simply one credit. We see there is a positive coefficient, 0.523, corresponding to the predicted difference for one credit classes relative to multi credit. That is, all else held equal, professors of one credit classes are predicted to have a score 0.523 higher.\nThe variable cls_perc_eval encodes what proportion of students filled out the evaluation. The coefficient is 0.00575, meaning that for every percent increase in how many students filled out the evaluatino, the professor is predicted to have 0.00575 increase in their score.\n\n\nExercise 17\nA high score would be associated with a professor that is:\n\nmale\nnot a minority\nperceived as more attractive\nyounger\n\nreceived education at English-speaking school\n\nAdditionally, scores should be higher for classes for which:\n\na higher proportion of students filled out the evaluation\n\nthere are more students in the class\n\nis one credit (as opposed to multi credit)\n\n\n\nExercise 18\nThe original sample of evaluations was taken from classes/professors at University of Texas, Austin.\nAs always, there are constraints on generalizability. It is possible that the effects observed here are specific to either (or both) the professors or students who happen to attend UT Austin. For example, maybe at other universities, teacher evaluations are less impacted by perceived attractiveness of the professor—or maybe they are more impacted. In both cases, this parameter estimate might be a poor estimate (either over or underestimating the impact of perceived attractiveness on evaluations).\nOn the other hand, I don’t necessarily have a strong reason a priori to think that UT Austin students or professors are somehow outliers in the USA higher-education system. I would feel more comfortable generalizing if I had similar results from another US university, but as it stands, I would tentatively conclude that there’s evidence for the effects we observed, and suggest that the effect should be replicated in other universities."
  },
  {
    "objectID": "content/lab-ans/lab02-wrangling-ans.html",
    "href": "content/lab-ans/lab02-wrangling-ans.html",
    "title": "Lab 02 - Wrangling (Ans)",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse) \n\n\n\n\n\n\n\nWarning\n\n\n\nYour numbers/output may differ based on the version of the storms dataset that you have/used to complete. The logic will follow what you see here. This is expected behavior.\n\n\n\n\nExercise 1\nHow many unique hurricanes are included in this dataset?\n(Note the specific value may differ based on the version of the dataset you’re using, but the code would not change.)\n\nn_unique <- storms |> \n  filter(status == \"hurricane\") |>\n  distinct(name, year, .keep_all = TRUE) |>\n  count() |>\n  pull(n)\n\n# OR\n\nstorms |> \n  filter(status == \"hurricane\") |>\n  group_by(year, name) |> \n  count() |>\n  nrow()\n\n[1] 310\n\n\nThere are 310 unique hurricanes.\n(Note that we need to group by name *and* year, as certain storms have the same name...in different years. There might still be instances where the same storm is double-counted if it extends from December into January, which if you accounted for, excellent work!)\n\n\nExercise 2\nNote: If you used storms on datahub, the ts_diameter column has missing information and were likely unable to complete this question. Otherwise…this would have been the approach…\nWhich tropical storm affected the largest area experiencing tropical storm strength winds? And, what was the maximum sustained wind speed for that storm?\n\nstorms |> \n  filter(status == \"tropical storm\", \n         !is.na(ts_diameter)) |> \n  slice_max(ts_diameter)\n\nOR\n\nstorms |>\n  filter(status == \"tropical storm\",\n         !is.na(ts_diameter)) |> \n  filter(ts_diameter == max(ts_diameter, na.rm=TRUE))\n\nSandy (2012) had the largest area affected.\n\n\nExercise 3\nAmong all storms in this dataset, in which month are storms most common? Does this depend on the status of the storm? (In other words, are hurricanes more common in certain months than tropical depressions? or tropical storms?)\n\n# most common month\nstorms |> \n  distinct(name, year, .keep_all=TRUE) |>\n  group_by(month) |>\n  summarise(n = n()) |> # could alternatively use count() here\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   month     n\n   <dbl> <int>\n 1     9   208\n 2     8   173\n 3    10    99\n 4     7    67\n 5     6    41\n 6    11    29\n 7     5    13\n 8    12     5\n 9     1     2\n10     4     2\n\n\nSeptember is the most common month.\n\n# depend on status?\nstorms |> \n  group_by(status, month) |>\n  summarise(n = n()) |> # could alternatively use count() here\n  slice_max(n)\n\n`summarise()` has grouped output by 'status'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 9 × 3\n# Groups:   status [9]\n  status                 month     n\n  <fct>                  <dbl> <int>\n1 disturbance                7    45\n2 extratropical              9   732\n3 hurricane                  9  2380\n4 other low                  9   446\n5 subtropical depression     8    36\n6 subtropical storm          9    72\n7 tropical depression        9  1315\n8 tropical storm             9  2448\n9 tropical wave              8    55\n\n\nIt does not depend on status. September is the most common for all three storm types.\n\n\nExercise 4\nYour boss asks for the name, year, and status of all category 5 storms that have happened in the 2000s. Carry out the operations that would deliver what they’re looking for.\n\nstorms |>\n  filter(category == 5,\n         between(year, 2000, 2009)) |>\n  select(name, year, status) |>\n  distinct(name, year, .keep_all=TRUE)\n\n# A tibble: 8 × 3\n  name     year status   \n  <chr>   <dbl> <fct>    \n1 Isabel   2003 hurricane\n2 Ivan     2004 hurricane\n3 Emily    2005 hurricane\n4 Katrina  2005 hurricane\n5 Rita     2005 hurricane\n6 Wilma    2005 hurricane\n7 Dean     2007 hurricane\n8 Felix    2007 hurricane\n\n\n\n\nExercise 5\nFilter these data to only include storms that occurred during your lifetime (your code and results may differ from your classmates!). Among storms that have occurred during your lifetime, what’s the mean and median air pressure across all measurements taken?\n\nmy_storms <- storms |>\n  filter(between(year, 1988, 2023)) # alternatively filter(year >= 1988)\n\nmy_storms |>\n  summarise(median_pressure = median(pressure),\n            mean_pressure = mean(pressure))\n\n# A tibble: 1 × 2\n  median_pressure mean_pressure\n            <dbl>         <dbl>\n1            1000          993.\n\n\n\nMedian: 999 millibars\nMean: 991 millibars\n\n\n\nExercise 6\nWhich decade (of the storms included in the dataset) had the largest number of unique reported storms?\n\nstorms |> \n  distinct(name, year) |>\n  mutate(decade = year - year %% 10) |> # there are MANY different ways to approach this!\n  group_by(decade) |>\n  count()\n\n# A tibble: 6 × 2\n# Groups:   decade [6]\n  decade     n\n   <dbl> <int>\n1   1970    40\n2   1980    90\n3   1990   127\n4   2000   169\n5   2010   163\n6   2020    50\n\n\nThe 2000s.\n(Note: we want to be sure to only count each storm once. Could also arrange by desc(n) to have 2000 at top.)\n\n\nExercise 7\nAmong the subset of storms occurring in your lifetime, which storm lasted the longest? Include your code and explain your answer.\n\nmy_storms |>  \n  group_by(name, year) |> \n  count() |> \n  arrange(desc(n))\n\n# A tibble: 532 × 3\n# Groups:   name, year [532]\n   name      year     n\n   <chr>    <dbl> <int>\n 1 Nadine    2012    96\n 2 Ivan      2004    94\n 3 Kyle      2002    90\n 4 Leslie    2018    89\n 5 Paulette  2020    88\n 6 Alberto   2000    87\n 7 Jose      2017    85\n 8 Nicholas  2003    80\n 9 Florence  2018    79\n10 Marilyn   1995    79\n# ℹ 522 more rows\n\n\nNadine lasted the longest (unless you were born after 2012).\n(Note: The logic here is that storms are reported every six hours, per the description of the dataset, so the storm that has the most rows/entries would have lasted the longest)"
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html",
    "href": "content/exams/practice-exam-fa21-ans.html",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal."
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#academic-integrity-statement",
    "href": "content/exams/practice-exam-fa21-ans.html#academic-integrity-statement",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nI, ____________, hereby state that I have not communicated with or gained information in any way from my classmates or anyone during this exam, and that all work is my own.\nA note on sharing / reusing code: We are well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden."
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#getting-help",
    "href": "content/exams/practice-exam-fa21-ans.html#getting-help",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Getting help",
    "text": "Getting help\nBecause we cannot be available to all students across the entire length of the (real) exam, there will be no questions of instructional staff about the exam. If you find wording confusing or are unsure, note that in your answer and explain how you interpreted it. This will be taken into consideration during grading. If you are having technical difficulties or think there is an error on the exam, DM or email Prof Ellis immediately and she’ll work with you as soon as possible."
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#grading-and-feedback",
    "href": "content/exams/practice-exam-fa21-ans.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThe (real) exam is worth 16% of your grade. You will be graded on the correctness of your code, correctness of your answers (often there are multiple “correct” answers, by design), the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization, but we should be able to easily navigate your midterm to find what we’re looking for.)"
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#logistics",
    "href": "content/exams/practice-exam-fa21-ans.html#logistics",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#packages",
    "href": "content/exams/practice-exam-fa21-ans.html#packages",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse package for this (practice) midterm. (For the real deal, you’ll need tidyverse and tidymodels.) If working on datahub, this package has been installed, but you will need to load it. No other packages are required, but if for some reason you want to load in another package, you are permitted to do so."
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#the-data",
    "href": "content/exams/practice-exam-fa21-ans.html#the-data",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "The data",
    "text": "The data\nThe dataset you’ll be working with on this practice midterm is all about beach volleyball. The full dataset is explained in detail here and includes match-level data from 76,756 volleyball matches. You should click on that link to see what information is stored in each column in this dataset and what information is included in each column.\nBriefly, what you’ll use for this midterm is a subset of the full dataset, including only the 11,699 observations (rows) from 2018 and 2019 but all of the original columns. Each row summarizes the results from a single, distinct match played in a volleyball tournament.\nTo briefly describe beach volleyball, it is a sport played 2 on 2, so each match involves only 4 players. These data include matches from two different volleyball circuits, the international FIVB and the US-centric AVP. You will not need to know much at all about this sport to complete this midterm, and anything you need to know will be explained.\nThe data are stored in data/vb_matches.csv. You’ll need to read the dataset in prior to answering any questions on the midterm.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\ndf <- read_csv('data/vb_matches.csv')\n\nRows: 11699 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (17): circuit, tournament, country, gender, w_player1, w_p1_country, w_...\ndbl  (42): year, match_num, w_p1_age, w_p1_hgt, w_p2_age, w_p2_hgt, l_p1_age...\ndate  (5): date, w_p1_birthdate, w_p2_birthdate, l_p1_birthdate, l_p2_birthdate\ntime  (1): duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#questions",
    "href": "content/exams/practice-exam-fa21-ans.html#questions",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Questions",
    "text": "Questions\nQuestion 1 (0.75 points) - How many FIVB and AVP matches are included in this dataset?\n\ndf |> \n  group_by(circuit) |>\n  count()\n\n# A tibble: 2 × 2\n# Groups:   circuit [2]\n  circuit     n\n  <chr>   <int>\n1 AVP      2656\n2 FIVB     9043\n\n\n\n2656 AVP\n9043 FIVB\n\nQuestion 2 (0.75 points) - Find the match with the longest duration. a. Where was this tournament played (City & Country)? b. How long did the match last? c. Who were the two winners?\n\ndf |> \n  slice(which.max(duration))\n\n# A tibble: 1 × 65\n  circuit tournament country        year date       gender match_num w_player1  \n  <chr>   <chr>      <chr>         <dbl> <date>     <chr>      <dbl> <chr>      \n1 AVP     Austin     United States  2018 2018-05-17 M             37 Nathan Yang\n# ℹ 57 more variables: w_p1_birthdate <date>, w_p1_age <dbl>, w_p1_hgt <dbl>,\n#   w_p1_country <chr>, w_player2 <chr>, w_p2_birthdate <date>, w_p2_age <dbl>,\n#   w_p2_hgt <dbl>, w_p2_country <chr>, w_rank <chr>, l_player1 <chr>,\n#   l_p1_birthdate <date>, l_p1_age <dbl>, l_p1_hgt <dbl>, l_p1_country <chr>,\n#   l_player2 <chr>, l_p2_birthdate <date>, l_p2_age <dbl>, l_p2_hgt <dbl>,\n#   l_p2_country <chr>, l_rank <chr>, score <chr>, duration <time>,\n#   bracket <chr>, round <chr>, w_p1_tot_attacks <dbl>, w_p1_tot_kills <dbl>, …\n\n\n\nAustin, USA\n1h 42 min\nNathan Yang & Steven Irvin\n\n\nQuestion 3 (1.5 points) - Across all tournaments included in this dataset, which teams have won the most tournaments? Your response should include both the winning players, their gender, and the number of tournaments they’ve won in descending order. Who has the most wins? How many men’s and how many women’s teams are in the top 10?\nNote: “winning a tournament” is indicated by winning either a “Gold Medal” (FIVB) or “Finals” (AVP) match, specified in the bracket column.\n\ndf |> \n  filter(bracket %in% c(\"Gold Medal\", \"Finals\")) |> \n  group_by(w_player1, w_player2, gender) |>\n  count() |> \n  arrange(desc(n)) |>\n  head(10)\n\n# A tibble: 10 × 4\n# Groups:   w_player1, w_player2, gender [10]\n   w_player1              w_player2                 gender     n\n   <chr>                  <chr>                     <chr>  <int>\n 1 Alix Klineman          \"April Ross\"              W         11\n 2 Anders Mol             \"Christian Sorum\"         M         10\n 3 Melissa Humana-Paredes \"Sarah Pavan\"             W          6\n 4 Nick Lucena            \"Phil Dalhausser\"         M          6\n 5 Jake Gibb              \"Taylor Crabb\"            M          5\n 6 Jingzhe Wang           \"Shuhui Wen\"              W          5\n 7 Agatha Bednarczuk      \"Eduarda \\\"Duda\\\" Lisboa\" W          4\n 8 Aleksandrs Samoilovs   \"Janis Smedins\"           M          4\n 9 Betsi Flint            \"Emily Day\"               W          4\n10 Alexander Brouwer      \"Robert Meeuwsen\"         M          3\n\n\n\nAlix Klineman and April Ross have won the most tournaments\nThere are 5 men’s teams and 5 women’s teams\n\nQuestion 4 (1.5 points) - Of only the AVP tournaments included in this dataset, how many different cities hosted tournaments in 2018 and 2019? And, which cities (if any) hosted a tournament in both 2018 and 2019?\nNote that tournaments are named for the city hosting the tournament.\n\n# distinct locations\ndf |> \n  filter(circuit == 'AVP') |>\n  distinct(tournament)\n\n# A tibble: 9 × 1\n  tournament      \n  <chr>           \n1 Austin          \n2 New York        \n3 Seattle         \n4 San Francisco   \n5 Hermosa Beach   \n6 Manhattan Beach \n7 Chicago         \n8 Waikiki         \n9 Huntington Beach\n\n\n\n9 distinct locations\n\n\nlocations <- df |> \n  filter(circuit == 'AVP') |>\n  group_by(year, date) |> \n    distinct(tournament) \n\ndf |>\n  filter(circuit == \"AVP\") |>\n  group_by(year, date) |>\n  distinct(tournament) |>\n  group_by(tournament) |>\n  count() |>\n  filter(n > 1)\n\n# A tibble: 6 × 2\n# Groups:   tournament [6]\n  tournament          n\n  <chr>           <int>\n1 Austin              2\n2 Chicago             2\n3 Hermosa Beach       2\n4 Manhattan Beach     2\n5 New York            2\n6 Seattle             2\n\n\n\n6 locations were duplicates between 2018 and 2019, including Chicago, Manhattan Beach, Hermosa Beach, Seattle, New York, and Austin\n\nQuestion 5 (2.5 points) - Prof Ellis plays a lot of women’s beach volleyball and is only 5’5” (65 inches). Despite not having the sheer talent or raw athletic ability to make it as a professional volleyball player, she wonders if she ever had a chance at her height. To help her out, answer each of the following: a. Who was the shortest women’s player to compete in a tournament in 2018/2019? b. How tall are they? c. Did they win a tournament in 2018 or 2019?\nReminder: there are 4 players in each match whose height should be considered.\n\n# find shortest in each column\ndf |>\n  filter(gender == \"W\") |>\n  summarize(min_p1 = min(w_p1_hgt, na.rm=TRUE), \n            min_p2  = min(w_p2_hgt, na.rm=TRUE), \n            min_l_p1 = min(l_p1_hgt, na.rm=TRUE),\n            min_l_p2 = min(l_p2_hgt, na.rm=TRUE))\n\n# A tibble: 1 × 4\n  min_p1 min_p2 min_l_p1 min_l_p2\n   <dbl>  <dbl>    <dbl>    <dbl>\n1     63     64       62       61\n\n# determine the shortest player\ndf |> filter(l_p2_hgt == 61)\n\n# A tibble: 2 × 65\n  circuit tournament    country  year date       gender match_num w_player1     \n  <chr>   <chr>         <chr>   <dbl> <date>     <chr>      <dbl> <chr>         \n1 FIVB    Visakhapatnam India    2019 2019-02-28 W              7 Melissa Fuchs…\n2 FIVB    Visakhapatnam India    2019 2019-02-28 W             16 Ekaterina Fil…\n# ℹ 57 more variables: w_p1_birthdate <date>, w_p1_age <dbl>, w_p1_hgt <dbl>,\n#   w_p1_country <chr>, w_player2 <chr>, w_p2_birthdate <date>, w_p2_age <dbl>,\n#   w_p2_hgt <dbl>, w_p2_country <chr>, w_rank <chr>, l_player1 <chr>,\n#   l_p1_birthdate <date>, l_p1_age <dbl>, l_p1_hgt <dbl>, l_p1_country <chr>,\n#   l_player2 <chr>, l_p2_birthdate <date>, l_p2_age <dbl>, l_p2_hgt <dbl>,\n#   l_p2_country <chr>, l_rank <chr>, score <chr>, duration <time>,\n#   bracket <chr>, round <chr>, w_p1_tot_attacks <dbl>, w_p1_tot_kills <dbl>, …\n\n\nPerumal Yogeshwari was the shortest to compete (at 61 inches). By deduction, since this is the l_p2 column, we know that she did not win any tournaments.\nQuestion 6 (3 pts) - Which country has hosted the most FIVB tournaments? Did this differ by year? Generate a visualization that shows how many FIVB tournaments each country hosted. Allow viewer to visualize this by year. And, be sure each tournament is only counted once (regardless of how many games were played).\n\ndf |>\n  filter(circuit == 'FIVB') |>\n  distinct(tournament, year, .keep_all = TRUE) |>\n  ggplot(aes(y=fct_infreq(country))) + \n  geom_bar() + \n  facet_wrap(~year) +\n  labs(y=NULL,\n       x=\"Count\",\n       title = \"Number of FIVB tournaments hosted by each country\") + \n  theme_bw() +\n  theme(plot.title.position = \"plot\")\n\n\n\n\nQuestion 7 (3 pts) - Recreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be.\nHint: The visualization uses the variable avg_team_height, which is not included in the provided data frame. You will have to create avg_team_height yourself, be determining the average (mean) team height for each winning team.\n\n\ndf |>\n  filter(circuit == 'AVP') |>\n  mutate(avg_team_height = (w_p1_hgt + w_p2_hgt)/2) |>\n  ggplot(aes(x = fct_infreq(w_p1_country), y = avg_team_height, fill=w_p1_country)) +\n  geom_boxplot() + \n  facet_wrap(~gender, \n             scales=\"free_y\", \n             nrow=2) +\n  labs(y = 'Average Team Height (in)',\n       x = 'Country',\n       title = 'Average team heights for AVP match winners in 2018 and 2019') +\n  theme(plot.title.position = \"plot\") +\n  guides(fill=\"none\") \n\nWarning: Removed 1288 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nVisualizes which countries have the tallest winning teams on average. For men, we see that players from the Netherlands are quite tall. For Women, it’s the US and Canada.\n\nQuestion 8 (1 pts) - If you were in charge of designing the plot you just recreated in the plot above, what changes would you make to improve its effectiveness as a visualization? (You do not have to write any code for this question, just explain the different design/viz choices you would make.)\n\n\nswap axes\naccurate/more informative labelling labeling\nmeaningful ordering of countries (geography?)\nimproved color choices (flag colors?)\nincrease text size\nremove background\netc."
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html",
    "href": "content/exams/practice-exam-fa21.html",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal.\n\n\n\n\n\n\nNote\n\n\n\nThis is the midterm from when the course was offered in fa21. Linear regression was not covered prior to the midterm the last time this course was offered. There will be a question or two on linear regression and the interpretation of linear models on this year’s midterm."
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#logistics",
    "href": "content/exams/practice-exam-fa21.html#logistics",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam-fa21.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#packages",
    "href": "content/exams/practice-exam-fa21.html#packages",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse package for this (practice) midterm. (For the real deal, you’ll need tidyverse and tidymodels.) If working on datahub, this package has been installed, but you will need to load it. No other packages are required, but if for some reason you want to load in another package, you are permitted to do so."
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#the-data",
    "href": "content/exams/practice-exam-fa21.html#the-data",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "The data",
    "text": "The data\nThe dataset you’ll be working with on this practice midterm is all about beach volleyball. The full dataset is explained in detail here and includes match-level data from 76,756 volleyball matches. You should click on that link to see what information is stored in each column in this dataset and what information is included in each column.\nBriefly, what you’ll use for this midterm is a subset of the full dataset, including only the 11,699 observations (rows) from 2018 and 2019 but all of the original columns. Each row summarizes the results from a single, distinct match played in a volleyball tournament.\nTo briefly describe beach volleyball, it is a sport played 2 on 2, so each match involves only 4 players. These data include matches from two different volleyball circuits, the international FIVB and the US-centric AVP. You will not need to know much at all about this sport to complete this midterm, and anything you need to know will be explained.\nThe data are stored in data/vb_matches.csv. You’ll need to read the dataset in prior to answering any questions on the midterm."
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#questions",
    "href": "content/exams/practice-exam-fa21.html#questions",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "Questions",
    "text": "Questions\nQuestion 1 (0.75 points) - How many FIVB and AVP matches are included in this dataset?\nQuestion 2 (0.75 points) - Find the match with the longest duration.\na.  Where was this tournament played (City & Country)?\nb.  How long did the match last?\nc.  Who were the two winners? </br>\nQuestion 3 (1.5 points) - Across all tournaments included in this dataset, which teams have won the most tournaments? Your response should include both the winning players, their gender, and the number of tournaments they’ve won in descending order. Who has the most wins? How many men’s and how many women’s teams are in the top 10? Note: “winning a tournament” is indicated by winning either a “Gold Medal” (FIVB) or “Finals” (AVP) match, specified in the bracket column.\nQuestion 4 (1.5 points) - Of only the AVP tournaments included in this dataset, how many different cities hosted tournaments in 2018 and 2019? And, which cities (if any) hosted a tournament in both 2018 and 2019? Note that tournaments are named for the city hosting the tournament.\nQuestion 5 (2.5 points) - Prof Ellis plays a lot of women’s beach volleyball and is only 5’5” (65 inches). Despite not having the sheer talent or raw athletic ability to make it as a professional volleyball player, she wonders if she ever had a chance at her height. (Reminder: there are 4 players in each match whose height should be considered.) To help her out, answer each of the following:\na.  Who was the shortest women's player to compete in a tournament in 2018/2019?\nb.  How tall are they?\nc.  Did they *win* a tournament in 2018 or 2019? </br>\nQuestion 6 (3 points) - Which country has hosted the most FIVB tournaments? Did this differ by year? Generate a visualization that shows how many FIVB tournaments each country hosted. Allow viewer to visualize this by year. And, be sure each tournament is only counted once (regardless of how many games were played).\nQuestion 7 (3 points) - Recreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be. (Hint: The visualization uses the variable avg_team_height, which is not included in the provided data frame. You will have to create avg_team_height yourself, by determining the average (mean) team height for each winning team.)\n\n\n\n\n\n\n\nNote\n\n\n\nQ7 had a typo when this course was offered previously leading to students spending wayyyyy longer than intended on this exam. That typo has been fixed for this practice midterm.\n\n\nQuestion 8 (1 pts) - If you were in charge of designing the plot you just recreated in the plot above, what changes would you make to improve its effectiveness as a visualization? (You do not have to write any code for this question, just explain the different design/viz choices you would make.)"
  },
  {
    "objectID": "content/exams/midterm-fa23.html",
    "href": "content/exams/midterm-fa23.html",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-fa23.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Nov 6th to complete this exam and turn it in via your personal Github repo.\nThere will be no Piazza posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please message or email Prof Ellis as soon as possible.\nEach question requires R code to determine the answer and text explaining your answer (except Q10, which just requires a text response). You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4"
  },
  {
    "objectID": "content/exams/midterm-fa23.html#academic-integrity",
    "href": "content/exams/midterm-fa23.html#academic-integrity",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBe sure to complete the AI statement in the exam you submit itself.\nA note on sharing / reusing code: I am well aware that a huge volume of code is available on the web to solve any number of problems and that LLMs have the ability to provide you code to prompts you give it. For this exam you are allowed to make use of any online resources but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden."
  },
  {
    "objectID": "content/exams/midterm-fa23.html#grading-and-feedback",
    "href": "content/exams/midterm-fa23.html#grading-and-feedback",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThis exam is worth 15% of your grade. You will be graded on the correctness of your code, correctness of your answers, the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization but the template gets you started on a well-organized exam. We should be able to easily navigate your midterm to find what we’re looking for.) Organization + Clarity in written communication - 1pt"
  },
  {
    "objectID": "content/exams/midterm-fa23.html#logistics",
    "href": "content/exams/midterm-fa23.html#logistics",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called midterm-fa23.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/midterm-fa23.html#packages",
    "href": "content/exams/midterm-fa23.html#packages",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these packages have been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/exams/midterm-fa23.html#the-data",
    "href": "content/exams/midterm-fa23.html#the-data",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Richmondway R Pacakge and have been provided by the TidyTuesday team.\nThe data are stored in data/richmondway.csv You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, this dataset includes data from three seasons of the TV show Ted Lasso. Each observation is a single episode of the show. The variables, generally, relate to the number of times Roy Kent (a foul-mouthed character on the show) and the entire cast say the F-word (often referred to as dropping the “F bomb”)."
  },
  {
    "objectID": "content/exams/midterm-fa23.html#questions",
    "href": "content/exams/midterm-fa23.html#questions",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nF-bomb summary:\n\nCalculate how many times total Roy Kent said the F-word within each season.\nComment on in which season Roy Kent said the F-word the most overall.\n\n\n\nQuestion 2 (0.5 points)\nDetermine how many episodes had more F bombs by Roy Kent than every other character on the show combined (excluding Roy Kent)?\n\n\nQuestion 3 (1.5 points)\nGenerate an exploratory* visualization that displays the typical range of Roy Kent F-bombs in an episode, broken down by season and explain three things you’ve learned about the data from this plot.\n\n\n*Note: exploratory here means that it does NOT have to be polished. Do NOT worry about title, axis labels, etc. We just care about understanding the data here. (If you do customize, you will NOT be penalized. It’s just not required for this question.)\n\n\nQuestion 4 (1 point)\nGenerate an exploratory* visualization that displays the relationship between Imdb_rating and Roy Kent F-bombs. Describe the relationship you see in this plot.\n\n\nQuestion 5 (1 point)\nBackground: Keeley is a character on Ted Lasso who is dating Roy Kent for some but not all of the episodes.\nGenerate a visualization that enables you to answer the questions below: - Does the median number of Roy Kent F bombs differ when Roy is dating Keeley (vs. when he is not)? - In the episode when Roy Kent dropped the most F bombs, was Roy dating Keeley?\n\n\nQuestion 6 (1.5 points)\nWhat is the effect of dating Keeley on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results.\n\n\nQuestion 7 (1.5 points)\nBackground: In Season 1, Roy Kent is a player. After retiring, he eventually becomes a coach. So, Roy is a coach in some but not all of the episodes.\nWhat is the effect of whether or not Roy Kent is coaching on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results. Then, comment on whether coaching or dating Keeley is a better predictor of Roy Kent F bombs and explain how you came to that conclusion.\n\n\nQuestion 8 (2.5 points)\nGenerate a polished* visualization that allows viewers to compare proportion/percentage of F-bombs broken down by season for Roy Kent vs those by everyone other than Roy Kent.\n\n\n*Note: polished here means we want you to take the time to make a finished visualization that adheres to the design principles discussed in class. There is more than one correct answer here, but we want you to pay attention to details and ensure this visualization effectively communicates what we’re asking.\n\n\nQuestion 9 (3 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, describe at least one change that you would make to improve the design of the plot.\n\n\nNote: the hex values for the colors used in this plot are: “#deebf7” (lightest), “#9ecae1”, and “#3182bd” (darkest)\n\n\n\nQuestion 10 (1 point)\nDescribe at least 1) two things you like about how the plot in Question 9 communicates the data and 2) two things you would do differently to make this a more effective visualization for communication."
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html",
    "href": "content/exams/practice-exam-wi23-ans.html",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-01.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Feb 13th to complete this exam and turn it in via your personal Github repo - late work will not be accepted. Technical difficulties are not an excuse for late work - do not wait until the last minute to knit / commit / push.\nThere will be no Campuswire posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please DM or email Prof Ellis as soon as possible.\nEach question requires a (brief) narrative as well as a (brief) description of your approach. You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4"
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#academic-integrity-statement",
    "href": "content/exams/practice-exam-wi23-ans.html#academic-integrity-statement",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nI, ____________, hereby state that I have not communicated with or gained information in any way from my classmates or anyone during this exam, and that all work is my own.\nA note on sharing / reusing code: I am well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden."
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#grading-and-feedback",
    "href": "content/exams/practice-exam-wi23-ans.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThis exam is worth 14% of your grade. You will be graded on the correctness of your code, correctness of your answers, the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization but the template gets you started on a well-organized exam. We should be able to easily navigate your midterm to find what we’re looking for.) Organization + Clarity in written communication - 1pt"
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#logistics",
    "href": "content/exams/practice-exam-wi23-ans.html#logistics",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called midterm-01.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#packages",
    "href": "content/exams/practice-exam-wi23-ans.html#packages",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these package has been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#the-data",
    "href": "content/exams/practice-exam-wi23-ans.html#the-data",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Axios and Harris Poll and have been provided by the TidyTuesday team.\nThe data are stored in two different files in the data/ folder: poll.csv and reputation.csv. You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, these two files include data about the 100 “most visible” brands in America. Specifically, reputation.csv includes information from the 2022 poll about these 100 stores across different reputation categories. poll.csv includes information about the same 100 stores but includes information about their rankings across multiple years.\n\npoll <- read_csv(\"data/poll.csv\")\nreputation <- read_csv(\"data/reputation.csv\")"
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#questions",
    "href": "content/exams/practice-exam-wi23-ans.html#questions",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nHow many different industries (industry variable) are represented in these data?\n\n# either df can be used here\npoll |> \n  distinct(industry) # |>\n\n# A tibble: 19 × 1\n   industry          \n   <chr>             \n 1 Retail            \n 2 Food & Beverage   \n 3 Groceries         \n 4 Tech              \n 5 Ecommerce         \n 6 Automotive        \n 7 Healthcare        \n 8 Other             \n 9 Logistics         \n10 Financial Services\n11 Industrial        \n12 Consumer Goods    \n13 Pharma            \n14 Telecom           \n15 Insurance         \n16 Media             \n17 Energy            \n18 Airline           \n19 Food Delivery     \n\n  # count() # can optionally include count()\n\n# OR \nn_distinct(poll$industry)\n\n[1] 19\n\n\nThere are 19 different industries represented.\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here.\n\n\n\n\nQuestion 2 (0.5 points)\n\nWhich company had the lowest overall ranking in 2022?\nAnd for which category (from the name variable) did this organization score lowest?\n\n\nreputation |>\n  filter(rank == max(rank)) |> # could also hard code rank == 100 as you know 100 companies included in dataset\n  arrange(score)\n\n# A tibble: 7 × 5\n  company                industry name        score  rank\n  <chr>                  <chr>    <chr>       <dbl> <dbl>\n1 The Trump Organization Other    ETHICS       51.2   100\n2 The Trump Organization Other    TRUST        52.9   100\n3 The Trump Organization Other    CULTURE      53.0   100\n4 The Trump Organization Other    CITIZENSHIP  53.6   100\n5 The Trump Organization Other    GROWTH       55.1   100\n6 The Trump Organization Other    P&S          55.7   100\n7 The Trump Organization Other    VISION       59.4   100\n\n# ChatGPT used a really circuitous approach using baseR indexing/filtering that would not work for this dataset\n\n\nThe Trump Organization\nEthics\n\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here. Most common error was miscalculating the score within category, due to misunderstanding the structure of the data.\n\n\n\n\nQuestion 3 (1 point)\nWhich company in the reputation.csv dataset has the “best” average (mean) rank across all seven categories?\n\nreputation |> \n  group_by(company) |> \n  summarize(avg_rank = mean(rank)) |> \n  arrange(avg_rank)\n\n# A tibble: 100 × 2\n   company                  avg_rank\n   <chr>                       <dbl>\n 1 Trader Joe's                 4.86\n 2 The Hershey Company          5   \n 3 Patagonia                    6.71\n 4 HEB Grocery                  7   \n 5 Samsung                      8.86\n 6 Wegmans                     10.4 \n 7 Amazon.com                  12.1 \n 8 Toyota Motor Corporation    12.4 \n 9 Honda Motor Company         13.1 \n10 Microsoft                   13.1 \n# ℹ 90 more rows\n\n\nTrader Joe’s has the highest average ranking.\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here. Most common error was misreading the question.\n\n\n\n\nQuestion 4 (1 point)\nWhich company had the biggest increase in rank from 2021 to 2022?\n\npoll |> \n  filter(year==2021) |> \n  arrange(desc(change))\n\n# A tibble: 100 × 8\n   company               industry `2022_rank` `2022_rq` change  year  rank    rq\n   <chr>                 <chr>          <dbl>     <dbl>  <dbl> <dbl> <dbl> <dbl>\n 1 The Home Depot        Retail            16      78.9     29  2021    45  75.4\n 2 Google                Tech              31      77.8     29  2021    60  73.3\n 3 IBM                   Tech              11      79.5     28  2021    39  76.3\n 4 Samsung               Tech               6      80.5     25  2021    31  77.5\n 5 Sony                  Tech              10      79.6     24  2021    34  77.3\n 6 Starbucks Corporation Food & …          43      76.6     22  2021    65  72.3\n 7 Microsoft             Tech              15      79       21  2021    36  76.8\n 8 Adidas                Retail            29      77.9     20  2021    49  75.1\n 9 General Motors        Automot…          51      75.4     17  2021    68  72  \n10 Yum! Brands           Food & …          53      75.3     17  2021    70  71.5\n# ℹ 90 more rows\n\n# if \"change\" column not noticed \npoll |> \n  filter(year==2021) |> \n  mutate(rank_diff = rank - `2022_rank`) |> \n  arrange(desc(rank_diff))\n\n# A tibble: 100 × 9\n   company     industry `2022_rank` `2022_rq` change  year  rank    rq rank_diff\n   <chr>       <chr>          <dbl>     <dbl>  <dbl> <dbl> <dbl> <dbl>     <dbl>\n 1 The Home D… Retail            16      78.9     29  2021    45  75.4        29\n 2 Google      Tech              31      77.8     29  2021    60  73.3        29\n 3 IBM         Tech              11      79.5     28  2021    39  76.3        28\n 4 Samsung     Tech               6      80.5     25  2021    31  77.5        25\n 5 Sony        Tech              10      79.6     24  2021    34  77.3        24\n 6 Starbucks … Food & …          43      76.6     22  2021    65  72.3        22\n 7 Microsoft   Tech              15      79       21  2021    36  76.8        21\n 8 Adidas      Retail            29      77.9     20  2021    49  75.1        20\n 9 General Mo… Automot…          51      75.4     17  2021    68  72          17\n10 Yum! Brands Food & …          53      75.3     17  2021    70  71.5        17\n# ℹ 90 more rows\n\n# Chat GPT assumes you need a join/doesn't understand the structure of the data\n\nThe Home Depot and Google saw the biggest jump, each increasing by 29 places.\n\n\n\n\n\n\nGrader Note\n\n\n\nBecause of the wording of the question, suggesting that there was only one company, credit was granted if students said either Home Depot or Google (or both).\n\n\n\n\nQuestion 5 (1.5 points)\nFor the industry with only a single “most visible” company in the dataset, has their RQ score been increasing or decreasing overall since 2017?\n\n# find the industry programmatically (or do a group by and figure out that it's insurance)\nindus <- poll |> \n  group_by(industry) |> \n  count() |> \n  arrange(n) |> \n  ungroup() |> \n  slice(1) |> \n  pull(industry)\n\n# or\n\nindus <- poll |>\n  distinct(company, industry) |>\n  group_by(industry) |>\n  summarise(count = n()) |>\n  filter(count == \"1\") |>\n  pull(industry)\n\n# look at this output\npoll |> \n  filter(industry == indus)\n\n# A tibble: 5 × 8\n  company                industry `2022_rank` `2022_rq` change  year  rank    rq\n  <chr>                  <chr>          <dbl>     <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 Progressive Corporati… Insuran…          57      74.4     NA  2017    NA  72.7\n2 Progressive Corporati… Insuran…          57      74.4     NA  2018    NA  73.2\n3 Progressive Corporati… Insuran…          57      74.4     NA  2019    NA  71.5\n4 Progressive Corporati… Insuran…          57      74.4     NA  2020    NA  74  \n5 Progressive Corporati… Insuran…          57      74.4     NA  2021    NA  NA  \n\n# could also plot to determine but would take more work b/c data are across multiple columns - not tidy!\n\nWhile initially there was some variability, Progressive’s RQ has been increasing overall (74.4 in 2022, lower than that years prior).\n\n\n\n\n\n\nGrader Note\n\n\n\nAvoid hard-coding whenever possible. Porgrammatically determining the industry takes more work here, but avoids possible typos. Credit was granted if hard-coded. Most common error here was likely misreading the question.\n\n\n\n\nQuestion 6 (2 points)\nHow many companies from each industry category are represented in the 2022 ‘100 Most Visible’ companies in America data? Generate a visualization to display the answer to this question. Be sure to follow best visualization practices discussed in class.\n\n# note - should only display 100 companies total\npoll |> distinct(company, .keep_all=TRUE) |>\nggplot(aes(y=fct_rev(fct_infreq(industry)))) +\n  geom_bar() + \n  labs(title = \"Retail Companies Most Common Among 100 'Most Visible' in 2022\",\n       subtitle = \"Industries for the 100 Companies Included in the Axios and Harris Poll\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title.position = \"plot\",\n        axis.title = element_blank())\n\n\n\n# ChatGPT does data manipulation first and then plots from there; does include some \"best practices\" for data viz...but their operations are not correct\n\n\n\n\n\n\n\nGrader Note\n\n\n\nPlot should only display 100 companies. Barplot great here. Remember that categorical variables should be ordered meaningfully. Credit was lost if they were not, unless you directly labeled the values. Titles and axis labels are required. Because feedback for hw02 had not yet been returned, points were not deducted if your title was not as informative as it could have been. Most common errors: 1) not considering how to best display the plot so that labels were most readable (for most, this involved putting them on the y-axis); 2) not ordering axis with categorical data; 3) including more than 100 companies.\n\n\n\n\nQuestion 7 (2 points)\nOf industries that have at least 5 companies in the dataset, which industry has the highest median 2022 rank? Generate a visualization that allows you to answer this question. Be sure to follow best practices.\n\nindustries <- poll |> \n  distinct(company, .keep_all=TRUE) |>\n  group_by(industry) |>\n  count() |> \n  filter(n>5) |>\n  pull(industry)\n\npoll |> \n  filter(industry %in% industries) |>\n  ggplot(aes(x=fct_reorder(industry, `2022_rank`), y=`2022_rank`)) +\n  geom_boxplot() +\n  labs(title = \"Automotive Industry Has the Highest Median Rank in 2022\",\n       subtitle = \"...among industries with at least 5 companies on the 'most visible' list\",\n       x = \"Industries\") +\n  theme_classic(base_size = 12) +\n  theme(plot.title.position = \"plot\",\n        axis.title.y = element_blank())\n\n\n\n# a bit more convoluted answer on Chat GPT\n\n\n\n\n\n\n\nGrader Note\n\n\n\nFull credit was granted whether a boxplot or a barplot was used; however, a boxplot displays more information overall. Remember that categorical variables should be ordered meaningfully. Credit was lost if they were not, unless you directly labeled the values. Titles and axis labels are required. Because feedback for hw02 had not yet been returned, points were not deducted if your title was not as informative as it could have been. Most common errors: 1) not filtering w/ correct logic and 2) not ordering axis with categorical information\n\n\n\n\nQuestion 8 (2 points)\nYour boss is curious about how much rankings change from one year to the next. To answer this question, they ask you to determine how well 2021 rankings explain the following year’s 2022 rankings. Generate a linear model to answer this question. Be sure to include your interpretation of the model (in other words your answer to the question “how well do 2021 rankings explain 2022’s rankings?”)\n\ndf <- poll %>% filter(year == 2021)\n\nmod <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(`2022_rank` ~ rank, data = df)\n\nmod |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    6.63     3.37        1.97 5.27e- 2\n2 rank           0.864    0.0573     15.1  6.08e-25\n\n\n\nmod |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.742         0.739  14.8      227. 6.08e-25     1  -332.  671.  678.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\n# Chat GPT gets the estimate incorrect (0.8 instead of 0.86) and Rsquared states 60% rather than 74%\n\nWe can see here that the 2021 data explain 74% of the variance in the 2022 data, making it a pretty good model for explaining 2022 rankings. Further, by looking at the coefficient, a company who was ranked in both 2021 and 2022 could expect their rank to increase, on average, by 0.86 in 2022.\n\n\n\n\n\n\nGrader Note\n\n\n\nModel had to be interpreted accurately for full credit and answer the question posed. Most common errors were: 1) flipping the predictor and outcome; 2) not answering the question posed\n\n\n\n\nQuestion 9 (2.5 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be.\n\n\nggplot(df, aes(x=rank, y=`2022_rank`)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", col=\"orange\") + \n  theme_classic(base_size = 14) + \n  labs(title = \"2021 Rank Explains 74% of the variance in 2022 rank\",\n       subtitle = \"On average, companies saw their rank increase almost one spot higher in 2022\",\n       x = \"2021 rank\",\n       y = \"2022 rank\") +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nGrader Note\n\n\n\nOn the answer key I have fixed the % variance explained; this original 78% was not meant to trick. It was a mistake by prof. As such, I have graded this question leniently. Most common point deductions here were for: 1) not matching theme; 2) not changing line color; 3) not left-aligning title. No points were lost for not increasing text size as that’s hard to visually see."
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html",
    "href": "content/exams/practice-exam-wi23.html",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the exam. There will also be an academic integrity statement for you to complete. Replace the `____________` with your name on the real deal.\n\n\n\n\n\n\nNote\n\n\n\nThis is the midterm from when the course was offered in wi23."
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#logistics",
    "href": "content/exams/practice-exam-wi23.html#logistics",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam-wi23.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#packages",
    "href": "content/exams/practice-exam-wi23.html#packages",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these packages have been installed, but you will need to load them. You are allowed, but not required, to use additional packages."
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#the-data",
    "href": "content/exams/practice-exam-wi23.html#the-data",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Axios and Harris Poll and have been provided by the TidyTuesday team.\nThe data are stored in two different files in the data/ folder: poll.csv and reputation.csv. You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, these two files include data about the 100 “most visible” brands in America. Specifically, reputation.csv includes information from the 2022 poll about these 100 stores across different reputation categories. poll.csv includes information about the same 100 stores but includes information about their rankings across multiple years."
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#questions",
    "href": "content/exams/practice-exam-wi23.html#questions",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nHow many different industries (industry variable) are represented in these data?\n\n\nQuestion 2 (0.5 points)\n\nWhich company had the lowest overall ranking in 2022?\nAnd for which category (from the name variable) did this organization score lowest?\n\n\n\nQuestion 3 (1 point)\nWhich company in the reputation.csv dataset has the “best” average (mean) rank across all seven categories?\n\n\nQuestion 4 (1 point)\nWhich company had the biggest increase in rank from 2021 to 2022?\n\n\nQuestion 5 (1.5 points)\nFor the industry with only a single “most visible” company in the dataset, has their RQ score been increasing or decreasing overall since 2017?\n\n\nQuestion 6 (2 points)\nHow many companies from each industry category are represented in the 2022 ‘100 Most Visible’ companies in America data? Generate a visualization to display the answer to this question. Be sure to follow best visualization practices discussed in class.\n\n\nQuestion 7 (2 points)\nOf industries that have at least 5 companies in the dataset, which industry has the highest median 2022 rank? Generate a visualization that allows you to answer this question. Be sure to follow best practices.\n\n\nQuestion 8 (2 points)\nYour boss is curious about how much rankings change from one year to the next. To answer this question, they ask you to determine how well 2021 rankings explain the following year’s 2022 rankings. Generate a linear model to answer this question. Be sure to include your interpretation of the model (in other words your answer to the question “how well do 2021 rankings explain 2022’s rankings?”)\n\n\nQuestion 9 (2.5 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be."
  },
  {
    "objectID": "content/exams/midterm-fa23-ans.html",
    "href": "content/exams/midterm-fa23-ans.html",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "",
    "text": "You will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these packages have been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrichmondway <- read_csv(\"data/richmondway.csv\")"
  },
  {
    "objectID": "content/exams/midterm-fa23-ans.html#the-data",
    "href": "content/exams/midterm-fa23-ans.html#the-data",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Richmondway R Pacakge and have been provided by the TidyTuesday team.\nThe data are stored in data/richmondway.csv You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, this dataset includes data from three seasons of the TV show Ted Lasso. Each observation is a single episode of the show. The variables, generally, relate to the number of times Roy Kent (a foul-mouthed character on the show) and the entire cast say the F-word (often referred to as dropping the “F bomb”)."
  },
  {
    "objectID": "content/exams/midterm-fa23-ans.html#questions",
    "href": "content/exams/midterm-fa23-ans.html#questions",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nF-bomb summary:\n\nCalculate how many times total Roy Kent said the F-word within each season.\nComment on in which season Roy Kent said the F-word the most overall.\n\n\nrichmondway |>\n  group_by(Season) |>\n  summarize(count = sum(F_count_RK))\n\n# A tibble: 3 × 2\n  Season count\n   <dbl> <dbl>\n1      1    56\n2      2   106\n3      3   138\n\n\n\nSeason 1: 56; Season 2: 106; Season 3: 138\nSeason 3\n\n\n\nQuestion 2 (0.5 points)\nDetermine how many episodes had more F bombs by Roy Kent than every other character on the show combined (excluding Roy Kent)?\n\n# utilizing F_perc column\nrichmondway |> filter(F_perc > 50)\n\n# A tibble: 9 × 16\n  Character Episode_order Season Episode Season_Episode F_count_RK F_count_total\n  <chr>             <dbl>  <dbl>   <dbl> <chr>               <dbl>         <dbl>\n1 Roy Kent              3      1       3 S1_e3                   7            13\n2 Roy Kent              9      1       9 S1_e9                  14            22\n3 Roy Kent             11      2       1 S2_e1                  11            16\n4 Roy Kent             15      2       5 S2_e5                  23            32\n5 Roy Kent             16      2       6 S2_e6                  12            18\n6 Roy Kent             22      2      12 S2_e12                 23            44\n7 Roy Kent             24      3       2 S3_e2                  16            31\n8 Roy Kent             28      3       6 S3_e6                  13            21\n9 Roy Kent             32      3      10 S3_e10                 10            18\n# ℹ 9 more variables: cum_rk_season <dbl>, cum_total_season <dbl>,\n#   cum_rk_overall <dbl>, cum_total_overall <dbl>, F_score <dbl>, F_perc <dbl>,\n#   Dating_flag <chr>, Coaching_flag <chr>, Imdb_rating <dbl>\n\n# OR \n\n# calculating diff if you didn't see/underestand F_perc column\nrichmondway |> \n  mutate(not_roy = F_count_total - F_count_RK) |>\n  filter(F_count_RK > not_roy)\n\n# A tibble: 9 × 17\n  Character Episode_order Season Episode Season_Episode F_count_RK F_count_total\n  <chr>             <dbl>  <dbl>   <dbl> <chr>               <dbl>         <dbl>\n1 Roy Kent              3      1       3 S1_e3                   7            13\n2 Roy Kent              9      1       9 S1_e9                  14            22\n3 Roy Kent             11      2       1 S2_e1                  11            16\n4 Roy Kent             15      2       5 S2_e5                  23            32\n5 Roy Kent             16      2       6 S2_e6                  12            18\n6 Roy Kent             22      2      12 S2_e12                 23            44\n7 Roy Kent             24      3       2 S3_e2                  16            31\n8 Roy Kent             28      3       6 S3_e6                  13            21\n9 Roy Kent             32      3      10 S3_e10                 10            18\n# ℹ 10 more variables: cum_rk_season <dbl>, cum_total_season <dbl>,\n#   cum_rk_overall <dbl>, cum_total_overall <dbl>, F_score <dbl>, F_perc <dbl>,\n#   Dating_flag <chr>, Coaching_flag <chr>, Imdb_rating <dbl>, not_roy <dbl>\n\n\nThere were nine episodes where Roy Kent had more F bombs than all of the other characters combined.\n\n\nQuestion 3 (1.5 points)\nGenerate an exploratory* visualization that displays the typical range of Roy Kent F-bombs in an episode, broken down by season and explain three things you’ve learned about the data from this plot.\n\n\n*Note: exploratory here means that it does NOT have to be polished. Do NOT worry about title, axis labels, etc. We just care about understanding the data here. (If you do customize, you will NOT be penalized. It’s just not required for this question.)\n\nggplot(richmondway, aes(x=factor(Season), y=F_count_RK)) +\n  geom_boxplot()\n\n\n\n\nHere we see that the median value increases from season 1 to season 3, with a typical season 1 episode having approximately 6 F bombs from Roy Kent. (However, there was an outlier episode with almost 15 in Season 1). By season 3, the median value was more than 10. The episode with the most Roy Kent F bombs was season 2, with more than 25!\nRubric:\n\nhandles season as a factor\ncorrect variables\nboxplot (or other viz that displays range)\nthree things observed are accurate\n\n\n\nQuestion 4 (1 point)\nGenerate an exploratory* visualization that displays the relationship between Imdb_rating and Roy Kent F-bombs. Describe the relationship you see in this plot.\n\n\n*Note: exploratory here means that it does NOT have to be polished. Do NOT worry about title, axis labels, etc. We just care about understanding the data here. (If you do customize, you will NOT be penalized. It’s just not required for this question.)\n\nggplot(richmondway, aes(x=Imdb_rating, y=F_count_RK)) + \n  geom_point()\n\n\n\n\nRubric:\n\nscatterplot most typical\ncorrect variables plotted\ninterpretation correct (little to no relationship)\n\n\n\nQuestion 5 (1 point)\nBackground: Keeley is a character on Ted Lasso who is dating Roy Kent for some but not all of the episodes.\nGenerate a visualization that enables you to answer the questions below: - Does the median number of Roy Kent F bombs differ when Roy is dating Keeley (vs. when he is not)? - In the episode when Roy Kent dropped the most F bombs, was Roy dating Keeley?\n\nggplot(richmondway, aes(x=Dating_flag, y=F_count_RK)) + \n  geom_boxplot()  +\n  geom_jitter(width=0.25)\n\n\n\n\n\nboxplot most typical\nanswers correct (fewer when dating Keeley; most when he is dating Keeley)\n\n\n\nQuestion 6 (1.5 points)\nWhat is the effect of dating Keeley on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results.\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(F_count_RK ~ factor(Dating_flag) , data = richmondway) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term                   estimate std.error statistic     p.value\n  <chr>                     <dbl>     <dbl>     <dbl>       <dbl>\n1 (Intercept)              8.84        1.31    6.74   0.000000128\n2 factor(Dating_flag)Yes  -0.0421      1.97   -0.0213 0.983      \n\n\nRubric:\n\noutcome is F bombs\ndating handled as a factor\ninterpretation is correct. (A typical episode when they’re not dating has 8.84 F bombs. And, on average there are slightly fewer F bombs when Keeley and Roy are dating (-0.04); however, this effect is quite small (in magnitude and significance) ). Must interpret intercept and effect/slope.\n\n\n\nQuestion 7 (1.5 points)\nBackground: In Season 1, Roy Kent is a player. After retiring, he eventually becomes a coach. So, Roy is a coach in some but not all of the episodes.\nWhat is the effect of whether or not Roy Kent is coaching on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results. Then, comment on whether coaching or dating Keeley is a better predictor of Roy Kent F bombs and explain how you came to that conclusion.\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(F_count_RK ~ factor(Coaching_flag) , data = richmondway) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term                     estimate std.error statistic  p.value\n  <chr>                       <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)                  5.79      1.36      4.26 0.000167\n2 factor(Coaching_flag)Yes     5.16      1.77      2.92 0.00639 \n\n\nRubric:\n\nmodel correct\ninterpretation is correct. (When not coaching, 5.79 F bombs; when coaching 5.16 more (on average)). Must interpret intercept and effect/slope.\ncomment states that coaching is better (likely uses R^2 and/or p-value; effect size not appropriate for model comparison)\n\n\n\nQuestion 8 (2.5 points)\nGenerate a polished visualization that allows viewers to compare proportion/percentage of F-bombs broken down by season for Roy Kent vs those by everyone other than Roy Kent. Be sure to consider effective visualization principles discussed in class in this plot.\n\ndf <- richmondway |>\n  mutate(`All Other Characters` = F_count_total - F_count_RK,\n         `Roy Kent` = F_count_RK) |>\n  select(Season, `Roy Kent`, `All Other Characters` ) |>\n  pivot_longer(cols=-Season, names_to=\"character\")\n\nggplot(df, aes(x=value, y=str_wrap(character, width = 10), fill=factor(Season), group=character)) + \n  geom_col(position=\"fill\") +\n  scale_fill_manual(values = c(\"1\" = \"#deebf7\",\n                                \"2\" = \"#9ecae1\",\n                                \"3\" = \"#3182bd\")) +\n  labs(title=\"Third Season of Ted Lasso comprises almost half of the total F bombs\", \n       subtitle=\"Similar proportion of F bombs across seasons for Roy Kent vs. all other characters\",\n       x = \"Proportion of F Bombs\",\n       y = NULL,\n       fill = \"Season\") +\n  guides(color = \"none\") +\n  theme_classic(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\nRubric:\n\ndetermined/calculated number of not-roy\nproportions displayed\ntitle describes take-home\naxes labeled correctly\ngood design principles (colors, labels, etc.)\n\n\n\nQuestion 9 (3 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, describe at least one change that you would make to improve the design of the plot.\nNote: the hex values for the colors used in this plot are: “#deebf7” (lightest), “#9ecae1”, and “#3182bd” (darkest)\n\nCode to reproduce plot:\n\nggplot(richmondway, aes(x=Episode, y=F_count_total, group=Season, color=as.factor(Season))) + \n  geom_line(linewidth=2) +\n  geom_point(size=3) +\n  scale_x_continuous(n.breaks=12) +\n  scale_color_manual(values = c(\"1\" = \"#deebf7\",\n                                \"2\" = \"#9ecae1\",\n                                \"3\" = \"#3182bd\")) + \n  theme_classic() +\n  labs(title = \"Ted Lasso Episodes always have at least a handful of F bombs, but typically there are at least 20\",\n       subtitle = \"Number of F Bombs Dropped by all characters per episode across 3 seasons of Ted Lasso\",\n       y=\"F Count\",\n       color=\"Season\") +\n  theme(plot.title.position = \"plot\")\n\n\n\nQuestion 10 (1 point)\nDescribe at least 1) two things you like about how the plot in Question 9 communicates the data and 2) two things you would do differently to make this a more effective visualization for communication.\nLots of possible answers here. Most common cons: title too wordy!, line color, text size, would add grid lines, woudl visualize differently"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#agenda",
    "href": "content/lectures/15-cs02-data-slides.html#agenda",
    "title": "15-cs02-data",
    "section": "Agenda",
    "text": "Agenda\n\nBackground\nQuestion\nData Intro\nWrangle"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#opencasestudies",
    "href": "content/lectures/15-cs02-data-slides.html#opencasestudies",
    "title": "15-cs02-data",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\nWright, Carrie and Meng, Qier and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com//opencasestudies/ocs-bp-air-pollution. Predicting Annual Air Pollution (Version v1.0.0)."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#air-pollutants",
    "href": "content/lectures/15-cs02-data-slides.html#air-pollutants",
    "title": "15-cs02-data",
    "section": "Air Pollutants",
    "text": "Air Pollutants\nSome sources are natural while others are anthropogenic (human-derived):\n\n\n\n[source]\n\nMajor types of air pollutants\n\nGaseous - Carbon Monoxide (CO), Ozone (O3), Nitrogen Oxides(NO, NO2), Sulfur Dioxide (SO2)\nParticulate - small liquids and solids suspended in the air (includes lead- can include certain types of dust)\nDust - small solids (larger than particulates) that can be suspended in the air for some time but eventually settle\nBiological - pollen, bacteria, viruses, mold spores\n\nSee here for more detail on the types of pollutants in the air."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#particulate-pollution",
    "href": "content/lectures/15-cs02-data-slides.html#particulate-pollution",
    "title": "15-cs02-data",
    "section": "Particulate Pollution",
    "text": "Particulate Pollution\nAir pollution particulates are generally described by their size:\n\nLarge Coarse Particulate Matter - has diameter of >10 micrometers (10 µm)\nCoarse Particulate Matter (called PM10-2.5) - has diameter of between 2.5 µm and 10 µm\nFine Particulate Matter (called PM2.5) - has diameter of < 2.5 µm\n\nPM10 includes any particulate matter <10 µm (both coarse and fine particulate matter)\n\nIn relation to a piece of human hair:\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#common-pollutants-and-their-size",
    "href": "content/lectures/15-cs02-data-slides.html#common-pollutants-and-their-size",
    "title": "15-cs02-data",
    "section": "Common Pollutants and their size",
    "text": "Common Pollutants and their size\n\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#penetration-into-the-human-body",
    "href": "content/lectures/15-cs02-data-slides.html#penetration-into-the-human-body",
    "title": "15-cs02-data",
    "section": "Penetration into the human body",
    "text": "Penetration into the human body\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#negative-health-impacts",
    "href": "content/lectures/15-cs02-data-slides.html#negative-health-impacts",
    "title": "15-cs02-data",
    "section": "Negative Health Impacts",
    "text": "Negative Health Impacts\nExposure to air pollution is:\n\nassociated with higher rates of mortality in older adults\nknown to be a risk factor for many diseases and conditions including (but not limited to):\n\n\n\nAsthma - fine particle exposure (PM2.5) was found to be associated with higher rates of asthma in children\nInflammation in type 1 diabetes - fine particle exposure (PM2.5) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with Type 1 diabetes\nLung function and emphysema - higher concentrations of ozone (O3), nitrogen oxides (NOx), black carbon, and fine particle exposure PM2.5 , at study baseline were significantly associated with greater increases in percent emphysema per 10 years\nLow birthweight - fine particle exposure(PM2.5) was associated with lower birth weight in full-term live births\nViral Infection - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (PM2.5)\n\n\nSee this review article for more information about sources of air pollution and the influence of air pollution on health."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#sparse-monitoring-is-problematic-for-ph",
    "href": "content/lectures/15-cs02-data-slides.html#sparse-monitoring-is-problematic-for-ph",
    "title": "15-cs02-data",
    "section": "Sparse monitoring is problematic for PH",
    "text": "Sparse monitoring is problematic for PH\n\n\nHistorically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country.\nHowever, these monitors are relatively sparse in certain regions of the country and are not necessarily located near pollution sources.\ndramatic differences in pollution rates can be seen even within the same city. (In fact, the term micro-environments describes environments within cities or counties which may vary greatly from one block to another.)\n\n\n\n[source]\n\nLack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#machine-learning-offers-a-solution",
    "href": "content/lectures/15-cs02-data-slides.html#machine-learning-offers-a-solution",
    "title": "15-cs02-data",
    "section": "Machine Learning offers a solution",
    "text": "Machine Learning offers a solution\nAn article published in the Environmental Health journal dealt with this issue by using data, including population density and road density, among other features, to model or predict air pollution levels at a more localized scale using machine learning (ML) methods.\n\n\n\n[source]\n\nThe authors of this article state that:\n\n“Exposure to atmospheric particulate matter (PM) remains an important public health concern, although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or county-specific ambient concentrations.”\n\n\n\nThe article above demonstrates that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems.\n\n\nSo…we’re going to do the same"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#the-state-of-global-air",
    "href": "content/lectures/15-cs02-data-slides.html#the-state-of-global-air",
    "title": "15-cs02-data",
    "section": "The State of Global Air",
    "text": "The State of Global Air\nThe State of Global Air is a report released every year to communicate the impact of air pollution on public health.\n\nThe State of Global Air 2019 report (which uses data from 2017) stated that:\n\nAir pollution is the fifth leading risk factor for mortality worldwide. It is responsible for more deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity. Each year, more people die from air pollution–related disease than from road traffic injuries or malaria.\n\n\n\n\n[source]\n\n\n\nIn 2017, air pollution is estimated to have contributed to close to 5 million deaths globally — nearly 1 in every 10 deaths.\n\n\n[source]\n\n\nThe State of Global Air 2018 report (using data from 2016) separated different types of air pollution & found that particulate pollution was particularly associated with mortality.\n\n[source]\n\n\nThe 2019 report shows that the highest levels of fine particulate pollution occur in Africa and Asia and that:\n\nMore than 90% of people worldwide live in areas exceeding the World Health Organization (WHO) Guideline for healthy air. More than half live in areas that do not even meet WHO’s least-stringent air quality target.\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#overall-improvement",
    "href": "content/lectures/15-cs02-data-slides.html#overall-improvement",
    "title": "15-cs02-data",
    "section": "Overall Improvement",
    "text": "Overall Improvement\nLooking at the US specifically, air pollution levels are generally improving, with declining national air pollutant concentration averages as shown from the 2019 Our Nation’s Air report from the US Environmental Protection Agency (EPA):\n\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#an-issue-nonetheless",
    "href": "content/lectures/15-cs02-data-slides.html#an-issue-nonetheless",
    "title": "15-cs02-data",
    "section": "An Issue Nonetheless",
    "text": "An Issue Nonetheless\n\nair pollution continues to contribute to health risk for Americans, in particular in regions with higher than national average rates of pollution that, at times, exceed the WHO’s recommended level.\nimportant to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.\n\n\nYou can see that current air quality conditions at this website, and you will notice variation across different cities.\nFor example, here are the conditions in San Francisco yesterday:\n\n[source]\n\n\n\nreports particulate values using what is called the Air Quality Index (AQI).\nThis calculator indicates that 138 AQI is equivalent to 50.5 ug/m3 and is considered unhealthy for sensitive individuals.\nThus, some areas exceed the WHO annual exposure guideline (10 ug/m3), and this may adversely affect the health of people living in these locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#adverse-health-effects",
    "href": "content/lectures/15-cs02-data-slides.html#adverse-health-effects",
    "title": "15-cs02-data",
    "section": "Adverse health effects",
    "text": "Adverse health effects\n\nAdverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines.\nit appears that the composition of the particulate matter and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. (For example, see this article for more details.)"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#monitor-data",
    "href": "content/lectures/15-cs02-data-slides.html#monitor-data",
    "title": "15-cs02-data",
    "section": "Monitor Data",
    "text": "Monitor Data\n\n\nMonitor data in this case study come from a system of monitors in which roughly 90% are located within cities.\nThere is an equity issue in terms of capturing the air pollution levels of more rural areas.\nTo get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be useful to estimate air pollution levels in areas with little to no monitoring.\nSpecifically, these methods can be used to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:\n\n\n\n\n\n\n[source]\nThis is what we aim to achieve in this case study."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#limitations",
    "href": "content/lectures/15-cs02-data-slides.html#limitations",
    "title": "15-cs02-data",
    "section": "Limitations",
    "text": "Limitations\n\n\nThe data do not include information about the composition of particulate matter. Different types of particulates may be more benign or deleterious for health outcomes.\nOutdoor pollution levels are not necessarily an indication of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. Researchers are now developing personal monitoring systems to track air pollution levels on the personal level.\nOur analysis will use annual mean estimates of pollution levels, but these can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data; however, we are interested in long term exposures, as these appear to be the most influential for health outcomes.\nThese data are US-focused."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#supervised-ml",
    "href": "content/lectures/15-cs02-data-slides.html#supervised-ml",
    "title": "15-cs02-data",
    "section": "Supervised ML",
    "text": "Supervised ML\nHere, we’ll need:\n\nA continuous outcome variable that we want to predict\nA set of feature(s) (or predictor variables) that we use to predict the outcome variable\n\n\nTo build (or train) our model, we use both the outcome and features.\n\n\nThe goal is to identify informative features that can explain a large amount of variation in our outcome variable.\n\n\nUsing this model, we can then predict the outcome from new observations with the same features where have not observed the outcome.\n(More details here)"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#outcome",
    "href": "content/lectures/15-cs02-data-slides.html#outcome",
    "title": "15-cs02-data",
    "section": "Outcome",
    "text": "Outcome\nThe monitor data that we will be using comes from gravimetric monitors (see picture below) operated by the US Environmental Protection Agency (EPA).\n\n[image courtesy of Kirsten Koehler]\n\nThese monitors use a filtration system to specifically capture fine particulate matter.\n\n[source]\n\n\nThe weight of this particulate matter is manually measured daily or weekly.\nFor the EPA standard operating procedure for PM gravimetric analysis in 2008, we refer the reader to here.\n\n\nIn our data set, the value column indicates the PM2.5 monitor average for 2008 in mass of fine particles/volume of air for 876 gravimetric monitors.\n\n\nThe units are micrograms of fine particulate matter (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m3).\n\n\nRecall the WHO exposure guideline is < 10 ug/m3 on average annually for PM2.5."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-import",
    "href": "content/lectures/15-cs02-data-slides.html#data-import",
    "title": "15-cs02-data",
    "section": "Data Import",
    "text": "Data Import\nAll of our data was previously collected by a researcher at the Johns Hopkins School of Public Health who studies air pollution and climate change. (Roger now works at UT Austin)\n\nWe have one CSV file that contains both our single outcome variable and all of our features (or predictor variables). You can download this file using the OCSdata package:\n\n# install.packages(\"OCSdata\")\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\n\n\n\nhere::here() helps manage file paths; will always locate files relative to your project root\n\n# install.packages(\"here\")\npm <- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#pm-2.5-data",
    "href": "content/lectures/15-cs02-data-slides.html#pm-2.5-data",
    "title": "15-cs02-data",
    "section": "PM 2.5 Data",
    "text": "PM 2.5 Data\n\n876 monitors\n40 columns\n\nvalue | outcome variable\n\n\n\npm |>\n  glimpse()\n\nRows: 876\nColumns: 50\n$ id                          <dbl> 1003.001, 1027.000, 1033.100, 1049.100, 10…\n$ value                       <dbl> 9.597647, 10.800000, 11.212174, 11.659091,…\n$ fips                        <dbl> 1003, 1027, 1033, 1049, 1055, 1069, 1073, …\n$ lat                         <dbl> 30.49800, 33.28126, 34.75878, 34.28763, 33…\n$ lon                         <dbl> -87.88141, -85.80218, -87.65056, -85.96830…\n$ state                       <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\"…\n$ county                      <chr> \"Baldwin\", \"Clay\", \"Colbert\", \"DeKalb\", \"E…\n$ city                        <chr> \"Fairhope\", \"Ashland\", \"Muscle Shoals\", \"C…\n$ CMAQ                        <dbl> 8.098836, 9.766208, 9.402679, 8.534772, 9.…\n$ zcta                        <dbl> 36532, 36251, 35660, 35962, 35901, 36303, …\n$ zcta_area                   <dbl> 190980522, 374132430, 16716984, 203836235,…\n$ zcta_pop                    <dbl> 27829, 5103, 9042, 8300, 20045, 30217, 901…\n$ imp_a500                    <dbl> 0.01730104, 1.96972318, 19.17301038, 5.782…\n$ imp_a1000                   <dbl> 1.4096021, 0.8531574, 11.1448962, 3.867647…\n$ imp_a5000                   <dbl> 3.3360118, 0.9851479, 15.1786154, 1.231141…\n$ imp_a10000                  <dbl> 1.9879187, 0.5208189, 9.7253870, 1.0316469…\n$ imp_a15000                  <dbl> 1.4386207, 0.3359198, 5.2472094, 0.9730444…\n$ county_area                 <dbl> 4117521611, 1564252280, 1534877333, 201266…\n$ county_pop                  <dbl> 182265, 13932, 54428, 71109, 104430, 10154…\n$ log_dist_to_prisec          <dbl> 4.648181, 7.219907, 5.760131, 3.721489, 5.…\n$ log_pri_length_5000         <dbl> 8.517193, 8.517193, 8.517193, 8.517193, 9.…\n$ log_pri_length_10000        <dbl> 9.210340, 9.210340, 9.274303, 10.409411, 1…\n$ log_pri_length_15000        <dbl> 9.630228, 9.615805, 9.658899, 11.173626, 1…\n$ log_pri_length_25000        <dbl> 11.32735, 10.12663, 10.15769, 11.90959, 12…\n$ log_prisec_length_500       <dbl> 7.295356, 6.214608, 8.611945, 7.310155, 8.…\n$ log_prisec_length_1000      <dbl> 8.195119, 7.600902, 9.735569, 8.585843, 9.…\n$ log_prisec_length_5000      <dbl> 10.815042, 10.170878, 11.770407, 10.214200…\n$ log_prisec_length_10000     <dbl> 11.88680, 11.40554, 12.84066, 11.50894, 12…\n$ log_prisec_length_15000     <dbl> 12.205723, 12.042963, 13.282656, 12.353663…\n$ log_prisec_length_25000     <dbl> 13.41395, 12.79980, 13.79973, 13.55979, 13…\n$ log_nei_2008_pm25_sum_10000 <dbl> 0.318035438, 3.218632928, 6.573127301, 0.0…\n$ log_nei_2008_pm25_sum_15000 <dbl> 1.967358961, 3.218632928, 6.581917457, 3.2…\n$ log_nei_2008_pm25_sum_25000 <dbl> 5.067308, 3.218633, 6.875900, 4.887665, 4.…\n$ log_nei_2008_pm10_sum_10000 <dbl> 1.35588511, 3.31111648, 6.69187313, 0.0000…\n$ log_nei_2008_pm10_sum_15000 <dbl> 2.26783411, 3.31111648, 6.70127741, 3.3500…\n$ log_nei_2008_pm10_sum_25000 <dbl> 5.628728, 3.311116, 7.148858, 5.171920, 4.…\n$ popdens_county              <dbl> 44.265706, 8.906492, 35.460814, 35.330814,…\n$ popdens_zcta                <dbl> 145.716431, 13.639555, 540.887040, 40.7189…\n$ nohs                        <dbl> 3.3, 11.6, 7.3, 14.3, 4.3, 5.8, 7.1, 2.7, …\n$ somehs                      <dbl> 4.9, 19.1, 15.8, 16.7, 13.3, 11.6, 17.1, 6…\n$ hs                          <dbl> 25.1, 33.9, 30.6, 35.0, 27.8, 29.8, 37.2, …\n$ somecollege                 <dbl> 19.7, 18.8, 20.9, 14.9, 29.2, 21.4, 23.5, …\n$ associate                   <dbl> 8.2, 8.0, 7.6, 5.5, 10.1, 7.9, 7.3, 8.0, 4…\n$ bachelor                    <dbl> 25.3, 5.5, 12.7, 7.9, 10.0, 13.7, 5.9, 17.…\n$ grad                        <dbl> 13.5, 3.1, 5.1, 5.8, 5.4, 9.8, 2.0, 8.7, 2…\n$ pov                         <dbl> 6.1, 19.5, 19.0, 13.8, 8.8, 15.6, 25.5, 7.…\n$ hs_orless                   <dbl> 33.3, 64.6, 53.7, 66.0, 45.4, 47.2, 61.4, …\n$ urc2013                     <dbl> 4, 6, 4, 6, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ urc2006                     <dbl> 5, 6, 4, 5, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ aod                         <dbl> 37.36364, 34.81818, 36.00000, 33.08333, 43…\n\n\n\nThere are 48 features with values for each of the 876 monitors (observations).\nThe data comes from the US Environmental Protection Agency (EPA), the National Aeronautics and Space Administration (NASA), the US Census, and the National Center for Health Statistics (NCHS)."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html",
    "href": "content/lectures/15-cs02-data.html",
    "title": "15-cs02-data",
    "section": "",
    "text": "Background\nQuestion\nData Intro\nWrangle"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#opencasestudies",
    "href": "content/lectures/15-cs02-data.html#opencasestudies",
    "title": "15-cs02-data",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\nWright, Carrie and Meng, Qier and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com//opencasestudies/ocs-bp-air-pollution. Predicting Annual Air Pollution (Version v1.0.0)."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#air-pollutants",
    "href": "content/lectures/15-cs02-data.html#air-pollutants",
    "title": "15-cs02-data",
    "section": "Air Pollutants",
    "text": "Air Pollutants\nSome sources are natural while others are anthropogenic (human-derived):\n\n\n\n[source]\n\nMajor types of air pollutants\n\nGaseous - Carbon Monoxide (CO), Ozone (O3), Nitrogen Oxides(NO, NO2), Sulfur Dioxide (SO2)\nParticulate - small liquids and solids suspended in the air (includes lead- can include certain types of dust)\nDust - small solids (larger than particulates) that can be suspended in the air for some time but eventually settle\nBiological - pollen, bacteria, viruses, mold spores\n\nSee here for more detail on the types of pollutants in the air."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#particulate-pollution",
    "href": "content/lectures/15-cs02-data.html#particulate-pollution",
    "title": "15-cs02-data",
    "section": "Particulate Pollution",
    "text": "Particulate Pollution\nAir pollution particulates are generally described by their size:\n\nLarge Coarse Particulate Matter - has diameter of >10 micrometers (10 µm)\nCoarse Particulate Matter (called PM10-2.5) - has diameter of between 2.5 µm and 10 µm\nFine Particulate Matter (called PM2.5) - has diameter of < 2.5 µm\n\nPM10 includes any particulate matter <10 µm (both coarse and fine particulate matter)\n\nIn relation to a piece of human hair:\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#common-pollutants-and-their-size",
    "href": "content/lectures/15-cs02-data.html#common-pollutants-and-their-size",
    "title": "15-cs02-data",
    "section": "Common Pollutants and their size",
    "text": "Common Pollutants and their size\n\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#penetration-into-the-human-body",
    "href": "content/lectures/15-cs02-data.html#penetration-into-the-human-body",
    "title": "15-cs02-data",
    "section": "Penetration into the human body",
    "text": "Penetration into the human body\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#negative-health-impacts",
    "href": "content/lectures/15-cs02-data.html#negative-health-impacts",
    "title": "15-cs02-data",
    "section": "Negative Health Impacts",
    "text": "Negative Health Impacts\nExposure to air pollution is:\n\nassociated with higher rates of mortality in older adults\nknown to be a risk factor for many diseases and conditions including (but not limited to):\n\n\n\nAsthma - fine particle exposure (PM2.5) was found to be associated with higher rates of asthma in children\nInflammation in type 1 diabetes - fine particle exposure (PM2.5) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with Type 1 diabetes\nLung function and emphysema - higher concentrations of ozone (O3), nitrogen oxides (NOx), black carbon, and fine particle exposure PM2.5 , at study baseline were significantly associated with greater increases in percent emphysema per 10 years\nLow birthweight - fine particle exposure(PM2.5) was associated with lower birth weight in full-term live births\nViral Infection - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (PM2.5)\n\n\nSee this review article for more information about sources of air pollution and the influence of air pollution on health."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#sparse-monitoring-is-problematic-for-ph",
    "href": "content/lectures/15-cs02-data.html#sparse-monitoring-is-problematic-for-ph",
    "title": "15-cs01-data",
    "section": "Sparse monitoring is problematic for PH",
    "text": "Sparse monitoring is problematic for PH\n\n\nHistorically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country.\nHowever, these monitors are relatively sparse in certain regions of the country and are not necessarily located near pollution sources.\ndramatic differences in pollution rates can be seen even within the same city. (In fact, the term micro-environments describes environments within cities or counties which may vary greatly from one block to another.)\n\n\n\n[source]\n\nLack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#machine-learning-offers-a-solution",
    "href": "content/lectures/15-cs02-data.html#machine-learning-offers-a-solution",
    "title": "15-cs02-data",
    "section": "Machine Learning offers a solution",
    "text": "Machine Learning offers a solution\nAn article published in the Environmental Health journal dealt with this issue by using data, including population density and road density, among other features, to model or predict air pollution levels at a more localized scale using machine learning (ML) methods.\n\n\n\n[source]\n\nThe authors of this article state that:\n\n“Exposure to atmospheric particulate matter (PM) remains an important public health concern, although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or county-specific ambient concentrations.”\n\n\n\nThe article above demonstrates that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems.\n\n\nSo…we’re going to do the same"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#the-state-of-global-air",
    "href": "content/lectures/15-cs02-data.html#the-state-of-global-air",
    "title": "15-cs02-data",
    "section": "The State of Global Air",
    "text": "The State of Global Air\nThe State of Global Air is a report released every year to communicate the impact of air pollution on public health.\n\nThe State of Global Air 2019 report (which uses data from 2017) stated that:\n\nAir pollution is the fifth leading risk factor for mortality worldwide. It is responsible for more deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity. Each year, more people die from air pollution–related disease than from road traffic injuries or malaria.\n\n\n\n\n[source]\n\n\n\nIn 2017, air pollution is estimated to have contributed to close to 5 million deaths globally — nearly 1 in every 10 deaths.\n\n\n[source]\n\n\nThe State of Global Air 2018 report (using data from 2016) separated different types of air pollution & found that particulate pollution was particularly associated with mortality.\n\n[source]\n\n\nThe 2019 report shows that the highest levels of fine particulate pollution occur in Africa and Asia and that:\n\nMore than 90% of people worldwide live in areas exceeding the World Health Organization (WHO) Guideline for healthy air. More than half live in areas that do not even meet WHO’s least-stringent air quality target.\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#overall-improvement",
    "href": "content/lectures/15-cs02-data.html#overall-improvement",
    "title": "15-cs02-data",
    "section": "Overall Improvement",
    "text": "Overall Improvement\nLooking at the US specifically, air pollution levels are generally improving, with declining national air pollutant concentration averages as shown from the 2019 Our Nation’s Air report from the US Environmental Protection Agency (EPA):\n\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#an-issue-nonetheless",
    "href": "content/lectures/15-cs02-data.html#an-issue-nonetheless",
    "title": "15-cs02-data",
    "section": "An Issue Nonetheless",
    "text": "An Issue Nonetheless\n\nair pollution continues to contribute to health risk for Americans, in particular in regions with higher than national average rates of pollution that, at times, exceed the WHO’s recommended level.\nimportant to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.\n\n\nYou can see that current air quality conditions at this website, and you will notice variation across different cities.\nFor example, here are the conditions in San Francisco yesterday:\n\n[source]\n\n\n\nreports particulate values using what is called the Air Quality Index (AQI).\nThis calculator indicates that 138 AQI is equivalent to 50.5 ug/m3 and is considered unhealthy for sensitive individuals.\nThus, some areas exceed the WHO annual exposure guideline (10 ug/m3), and this may adversely affect the health of people living in these locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#adverse-health-effects",
    "href": "content/lectures/15-cs02-data.html#adverse-health-effects",
    "title": "15-cs02-data",
    "section": "Adverse health effects",
    "text": "Adverse health effects\n\nAdverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines.\nit appears that the composition of the particulate matter and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. (For example, see this article for more details.)"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#monitor-data",
    "href": "content/lectures/15-cs02-data.html#monitor-data",
    "title": "15-cs02-data",
    "section": "Monitor Data",
    "text": "Monitor Data\n\n\nMonitor data in this case study come from a system of monitors in which roughly 90% are located within cities.\nThere is an equity issue in terms of capturing the air pollution levels of more rural areas.\nTo get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be useful to estimate air pollution levels in areas with little to no monitoring.\nSpecifically, these methods can be used to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:\n\n\n\n\n\n\n[source]\nThis is what we aim to achieve in this case study."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#limitations",
    "href": "content/lectures/15-cs02-data.html#limitations",
    "title": "15-cs02-data",
    "section": "Limitations",
    "text": "Limitations\n\n\nThe data do not include information about the composition of particulate matter. Different types of particulates may be more benign or deleterious for health outcomes.\nOutdoor pollution levels are not necessarily an indication of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. Researchers are now developing personal monitoring systems to track air pollution levels on the personal level.\nOur analysis will use annual mean estimates of pollution levels, but these can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data; however, we are interested in long term exposures, as these appear to be the most influential for health outcomes.\nThese data are US-focused."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#supervised-ml",
    "href": "content/lectures/15-cs02-data.html#supervised-ml",
    "title": "15-cs02-data",
    "section": "Supervised ML",
    "text": "Supervised ML\nHere, we’ll need:\n\nA continuous outcome variable that we want to predict\nA set of feature(s) (or predictor variables) that we use to predict the outcome variable\n\n\nTo build (or train) our model, we use both the outcome and features.\n\n\nThe goal is to identify informative features that can explain a large amount of variation in our outcome variable.\n\n\nUsing this model, we can then predict the outcome from new observations with the same features where have not observed the outcome.\n(More details here)"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#outcome",
    "href": "content/lectures/15-cs02-data.html#outcome",
    "title": "15-cs02-data",
    "section": "Outcome",
    "text": "Outcome\nThe monitor data that we will be using comes from gravimetric monitors (see picture below) operated by the US Environmental Protection Agency (EPA).\n\n[image courtesy of Kirsten Koehler]\n\nThese monitors use a filtration system to specifically capture fine particulate matter.\n\n[source]\n\n\nThe weight of this particulate matter is manually measured daily or weekly.\nFor the EPA standard operating procedure for PM gravimetric analysis in 2008, we refer the reader to here.\n\n\nIn our data set, the value column indicates the PM2.5 monitor average for 2008 in mass of fine particles/volume of air for 876 gravimetric monitors.\n\n\nThe units are micrograms of fine particulate matter (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m3).\n\n\nRecall the WHO exposure guideline is < 10 ug/m3 on average annually for PM2.5."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-import",
    "href": "content/lectures/15-cs02-data.html#data-import",
    "title": "15-cs02-data",
    "section": "Data Import",
    "text": "Data Import\nAll of our data was previously collected by a researcher at the Johns Hopkins School of Public Health who studies air pollution and climate change. (Roger now works at UT Austin)\n\nWe have one CSV file that contains both our single outcome variable and all of our features (or predictor variables). You can download this file using the OCSdata package:\n\n# install.packages(\"OCSdata\")\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\n\n\n\nhere::here() helps manage file paths; will always locate files relative to your project root\n\n# install.packages(\"here\")\npm <- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#pm-2.5-data",
    "href": "content/lectures/15-cs02-data.html#pm-2.5-data",
    "title": "15-cs02-data",
    "section": "PM 2.5 Data",
    "text": "PM 2.5 Data\n\n876 monitors\n40 columns\n\nvalue | outcome variable\n\n\n\npm |>\n  glimpse()\n\nRows: 876\nColumns: 50\n$ id                          <dbl> 1003.001, 1027.000, 1033.100, 1049.100, 10…\n$ value                       <dbl> 9.597647, 10.800000, 11.212174, 11.659091,…\n$ fips                        <dbl> 1003, 1027, 1033, 1049, 1055, 1069, 1073, …\n$ lat                         <dbl> 30.49800, 33.28126, 34.75878, 34.28763, 33…\n$ lon                         <dbl> -87.88141, -85.80218, -87.65056, -85.96830…\n$ state                       <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\"…\n$ county                      <chr> \"Baldwin\", \"Clay\", \"Colbert\", \"DeKalb\", \"E…\n$ city                        <chr> \"Fairhope\", \"Ashland\", \"Muscle Shoals\", \"C…\n$ CMAQ                        <dbl> 8.098836, 9.766208, 9.402679, 8.534772, 9.…\n$ zcta                        <dbl> 36532, 36251, 35660, 35962, 35901, 36303, …\n$ zcta_area                   <dbl> 190980522, 374132430, 16716984, 203836235,…\n$ zcta_pop                    <dbl> 27829, 5103, 9042, 8300, 20045, 30217, 901…\n$ imp_a500                    <dbl> 0.01730104, 1.96972318, 19.17301038, 5.782…\n$ imp_a1000                   <dbl> 1.4096021, 0.8531574, 11.1448962, 3.867647…\n$ imp_a5000                   <dbl> 3.3360118, 0.9851479, 15.1786154, 1.231141…\n$ imp_a10000                  <dbl> 1.9879187, 0.5208189, 9.7253870, 1.0316469…\n$ imp_a15000                  <dbl> 1.4386207, 0.3359198, 5.2472094, 0.9730444…\n$ county_area                 <dbl> 4117521611, 1564252280, 1534877333, 201266…\n$ county_pop                  <dbl> 182265, 13932, 54428, 71109, 104430, 10154…\n$ log_dist_to_prisec          <dbl> 4.648181, 7.219907, 5.760131, 3.721489, 5.…\n$ log_pri_length_5000         <dbl> 8.517193, 8.517193, 8.517193, 8.517193, 9.…\n$ log_pri_length_10000        <dbl> 9.210340, 9.210340, 9.274303, 10.409411, 1…\n$ log_pri_length_15000        <dbl> 9.630228, 9.615805, 9.658899, 11.173626, 1…\n$ log_pri_length_25000        <dbl> 11.32735, 10.12663, 10.15769, 11.90959, 12…\n$ log_prisec_length_500       <dbl> 7.295356, 6.214608, 8.611945, 7.310155, 8.…\n$ log_prisec_length_1000      <dbl> 8.195119, 7.600902, 9.735569, 8.585843, 9.…\n$ log_prisec_length_5000      <dbl> 10.815042, 10.170878, 11.770407, 10.214200…\n$ log_prisec_length_10000     <dbl> 11.88680, 11.40554, 12.84066, 11.50894, 12…\n$ log_prisec_length_15000     <dbl> 12.205723, 12.042963, 13.282656, 12.353663…\n$ log_prisec_length_25000     <dbl> 13.41395, 12.79980, 13.79973, 13.55979, 13…\n$ log_nei_2008_pm25_sum_10000 <dbl> 0.318035438, 3.218632928, 6.573127301, 0.0…\n$ log_nei_2008_pm25_sum_15000 <dbl> 1.967358961, 3.218632928, 6.581917457, 3.2…\n$ log_nei_2008_pm25_sum_25000 <dbl> 5.067308, 3.218633, 6.875900, 4.887665, 4.…\n$ log_nei_2008_pm10_sum_10000 <dbl> 1.35588511, 3.31111648, 6.69187313, 0.0000…\n$ log_nei_2008_pm10_sum_15000 <dbl> 2.26783411, 3.31111648, 6.70127741, 3.3500…\n$ log_nei_2008_pm10_sum_25000 <dbl> 5.628728, 3.311116, 7.148858, 5.171920, 4.…\n$ popdens_county              <dbl> 44.265706, 8.906492, 35.460814, 35.330814,…\n$ popdens_zcta                <dbl> 145.716431, 13.639555, 540.887040, 40.7189…\n$ nohs                        <dbl> 3.3, 11.6, 7.3, 14.3, 4.3, 5.8, 7.1, 2.7, …\n$ somehs                      <dbl> 4.9, 19.1, 15.8, 16.7, 13.3, 11.6, 17.1, 6…\n$ hs                          <dbl> 25.1, 33.9, 30.6, 35.0, 27.8, 29.8, 37.2, …\n$ somecollege                 <dbl> 19.7, 18.8, 20.9, 14.9, 29.2, 21.4, 23.5, …\n$ associate                   <dbl> 8.2, 8.0, 7.6, 5.5, 10.1, 7.9, 7.3, 8.0, 4…\n$ bachelor                    <dbl> 25.3, 5.5, 12.7, 7.9, 10.0, 13.7, 5.9, 17.…\n$ grad                        <dbl> 13.5, 3.1, 5.1, 5.8, 5.4, 9.8, 2.0, 8.7, 2…\n$ pov                         <dbl> 6.1, 19.5, 19.0, 13.8, 8.8, 15.6, 25.5, 7.…\n$ hs_orless                   <dbl> 33.3, 64.6, 53.7, 66.0, 45.4, 47.2, 61.4, …\n$ urc2013                     <dbl> 4, 6, 4, 6, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ urc2006                     <dbl> 5, 6, 4, 5, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ aod                         <dbl> 37.36364, 34.81818, 36.00000, 33.08333, 43…\n\n\n\nThere are 48 features with values for each of the 876 monitors (observations).\nThe data comes from the US Environmental Protection Agency (EPA), the National Aeronautics and Space Administration (NASA), the US Census, and the National Center for Health Statistics (NCHS)."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#ad-data-science-student-society",
    "href": "content/lectures/03-tidyr-slides.html#ad-data-science-student-society",
    "title": "03-tidyr",
    "section": "[ad] Data Science Student Society",
    "text": "[ad] Data Science Student Society\nJoin DS3 at their Fall General Body Meeting to learn more about the events they’re offering this quarter, open board positions for the year, and free food! It will be happening on Wednesday (10/11) from 6-8pm, at PC Ballroom West"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#qa",
    "href": "content/lectures/03-tidyr-slides.html#qa",
    "title": "03-tidyr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Is it possible to integrate to github using other systems than datahub? Datahub has already been spotty for me in this course and is notorious for slumping at critical pts in the quarter.\nA: Yup. The same steps can be carried out by downloading RStudio onto your computer and connecting it with GitHub.\n\n\nQ: Should we write in the console or in the rmd file first when writing code?\nA: Great question! I’d suggest starting in the Rmd file and editing there. That way you don’t have to copy+paste once you get it right. It’s already there.\n\n\nQ: How do you take notes for coding classes? I know there are lecture notes available, but how would you recommend taking notes for this class?\nA: I would recommend opening a blank Rmd each day for class and saving it with the lecture number. I’d keep notes and things I tried in that file. But, I wouldn’t copy+paste everything, since the other lecture notes are available."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#course-announcements",
    "href": "content/lectures/03-tidyr-slides.html#course-announcements",
    "title": "03-tidyr",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 02 due Friday\nHW01 now available; due Monday (10/16; 11:59 PM)\nLecture Participation survey open until Thursday\n\n\nNotes:\n\nLab01 scores and feedback posted\nDatahub: Launch RStudio (possible solution?)\nStaff office hours updated (see Canvas or website)"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#student-comment",
    "href": "content/lectures/03-tidyr-slides.html#student-comment",
    "title": "03-tidyr",
    "section": "Student Comment",
    "text": "Student Comment\n\nI have been struggling to grasp the material in the course. It feels like we are diving into the content in the labs, but I don’t even feel like I truly understand what I’m doing. It often seems like I’m just copying and pasting code from the website without a clear understanding of the bigger picture. I’m particularly stuck because I feel like I don’t have a solid grasp of the fundamental concepts of coding in R; it feels so new. I understand that the pace of the course may be challenging, but I think a bit more stronger focus on the foundational aspects of coding in R would greatly benefit students like me who are struggling with the content. I’m looking forward to the course and I hope I can grasp the content as we go through the next week. I’m concerned about learning the material and also how that may affect my grade.\n\n\nLet’s see how y’all feel in a week. The first week can be a lot in this course. Often, students feel a lot more comfortable come week 3."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#student-survey",
    "href": "content/lectures/03-tidyr-slides.html#student-survey",
    "title": "03-tidyr",
    "section": "Student Survey",
    "text": "Student Survey\n\n89% know Python; 15% know R; most (but not all!) have programmed before\n64% feel confident about effective data science communication\nReasons for taking course: learn R, add to resume, analyze data, improve data science skills\n\n\nMy favorite boring facts:\n\nI was actually born on my birthday\ni don’t like to eat eggs but my roommate loves them\nI like to have a midday nap.\nI can raise my eyebrows really well\nI eat peanut butter straight from the jar\ni have a jack russell terrier.. named jack (we weren’t feeling creative)"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#suggested-reading",
    "href": "content/lectures/03-tidyr-slides.html#suggested-reading",
    "title": "03-tidyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 12: Tidy Data\nChapter 13: Relational Data"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#tidy-data",
    "href": "content/lectures/03-tidyr-slides.html#tidy-data",
    "title": "03-tidyr",
    "section": "Tidy Data",
    "text": "Tidy Data\nThe opinionated tidyverse is named as such b/c it assumes/necessitates your data be “tidy”.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. —- Hadley Wickham\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\n\nSource: https://r4ds.had.co.nz/tidy-data.html"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#tidy-or-not",
    "href": "content/lectures/03-tidyr-slides.html#tidy-or-not",
    "title": "03-tidyr",
    "section": "Tidy or not?",
    "text": "Tidy or not?\n❓ Given the rules discussed, is the cat_lovers dataset tidy?\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")\n\nRows: 60 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, number_of_cats, handedness\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncat_lovers |> datatable()\n\n\n\n\n\n\n\n❓ Given the rules discussed, is the bike dataset tidy?\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 5716 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (44): AmbulanceR, BikeAge_Gr, Bike_Alc_D, Bike_Dir, Bike_Injur, Bike_Po...\ndbl   (8): FID, OBJECTID, Bike_Age, Crash_Hour, Crash_Ty_1, Crash_Year, Drvr...\ndttm  (1): Crash_Time\ndate  (1): Crash_Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike |> datatable()\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#summary-tables",
    "href": "content/lectures/03-tidyr-slides.html#summary-tables",
    "title": "03-tidyr",
    "section": "Summary tables",
    "text": "Summary tables\n❓ Which is a dataset? Which is a summary table?"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#your-turn",
    "href": "content/lectures/03-tidyr-slides.html#your-turn",
    "title": "03-tidyr",
    "section": "Your Turn",
    "text": "Your Turn\nThere are four representations of the same data/information provided in the tidyr packages: table1, table2, table3, and the combination of table4a and table4b. Given what we’ve discussed, which is the best (tidiest) way to represent these data?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#common-issues",
    "href": "content/lectures/03-tidyr-slides.html#common-issues",
    "title": "03-tidyr",
    "section": "Common issues",
    "text": "Common issues\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\nSolution: pivoting!"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#pivoting",
    "href": "content/lectures/03-tidyr-slides.html#pivoting",
    "title": "03-tidyr",
    "section": "Pivoting",
    "text": "Pivoting\n\npivot_longerpivot_widerlong vs. wide\n\n\nFor when some of the column names are not names of variables, but values of a variable…\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  <chr>       <chr>  <dbl>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n❓ Why are there backticks around the years? (Note: we have not discussed this yet)\n\n\nFor when an observation is scattered across multiple rows…\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable2 |>\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n❓ Why aren’t there quotes around column names here…but there were in pivot_longer? (Note: we have not discussed this yet.)\n\n\n\nwide data contains values that do not repeat in the first column.\nlong format contains values that do repeat in the first column.\n\nBoth are good/helpful! We’ll return to this idea and discuss more during dataviz next week.\nBriefly:\n\nwide data: analysis\nlong data: plotting"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#separating-uniting",
    "href": "content/lectures/03-tidyr-slides.html#separating-uniting",
    "title": "03-tidyr",
    "section": "Separating & Uniting",
    "text": "Separating & Uniting\n\nseparateunite\n\n\nFor when multiple pieces of information are stored in a single column…\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  <chr>       <dbl> <chr>  <chr>     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n…but…but…cases and population should be numeric…\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nUnite is the opposite…it combines data stored across multiple columns.\nThe general syntax is:\n\ndf |>\n  unite(new_col, first_col, second_col)"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#joins",
    "href": "content/lectures/03-tidyr-slides.html#joins",
    "title": "03-tidyr",
    "section": "Joins",
    "text": "Joins\nIf we look at table4a, it’s missing the population information. That’s stored in a separate table…table4b\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  <chr>            <dbl>      <dbl>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n…which is also in the “wide” format\n\n…so we pivot both tables longer\n\ntidy4a <- table4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntidy4b <- table4b |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\ntidy4b\n\n# A tibble: 6 × 3\n  country     year  population\n  <chr>       <chr>      <dbl>\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n\n\n…but how do we get them into a single tidy dataset?\n\n\nA join!\n\nleft_join(tidy4a, tidy4b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  <chr>       <chr>  <dbl>      <dbl>\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\n\nSource: R4DS"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#the-data-nycflights13",
    "href": "content/lectures/03-tidyr-slides.html#the-data-nycflights13",
    "title": "03-tidyr",
    "section": "The Data: nycflights13",
    "text": "The Data: nycflights13\n\nlibrary(nycflights13)\n\n\nairlines : links airline to two letter code\nairports : ID’ed by FAA code\nplanes : ID’ed by tailnum\nairport : weather each hour; id’ed by two letter airport code\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html)\n\n\n\n\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time)."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#mutating-joins",
    "href": "content/lectures/03-tidyr-slides.html#mutating-joins",
    "title": "03-tidyr",
    "section": "Mutating Joins",
    "text": "Mutating Joins\nmutating joins - add new variables to a data frame from matching observations in another\n\nFor simplicity, we’ll work with only a handful of columns…\n\nflights |> \n  select(year:day, hour, tailnum, carrier) |> \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 7\n    year month   day  hour tailnum carrier name                    \n   <int> <int> <int> <dbl> <chr>   <chr>   <chr>                   \n 1  2013     1     1     5 N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 N3ALAA  AA      American Airlines Inc.  \n# ℹ 336,766 more rows\n\n\nThere is now a new column name…coming from the airlines data frame.\n\n\nleft_join:\n\nkeeps all rows in first df (here: flights)\nadds all matching information from second df (here: airlines); adds NAs for any observations not in airlines\n\n\n\nOther joins:\nright_join: keeps all observations in second df full_join: keeps all observations in either df\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html\n\n\n\n\ninner_join:\n\ntakes only rows in both dfs\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html"
  },
  {
    "objectID": "content/lectures/03-tidyr.html",
    "href": "content/lectures/03-tidyr.html",
    "title": "03-tidyr",
    "section": "",
    "text": "Join DS3 at their Fall General Body Meeting to learn more about the events they’re offering this quarter, open board positions for the year, and free food! It will be happening on Wednesday (10/11) from 6-8pm, at PC Ballroom West\n\n\n\n\n\nQ: Is it possible to integrate to github using other systems than datahub? Datahub has already been spotty for me in this course and is notorious for slumping at critical pts in the quarter.\nA: Yup. The same steps can be carried out by downloading RStudio onto your computer and connecting it with GitHub.\n\n\nQ: Should we write in the console or in the rmd file first when writing code?\nA: Great question! I’d suggest starting in the Rmd file and editing there. That way you don’t have to copy+paste once you get it right. It’s already there.\n\n\nQ: How do you take notes for coding classes? I know there are lecture notes available, but how would you recommend taking notes for this class?\nA: I would recommend opening a blank Rmd each day for class and saving it with the lecture number. I’d keep notes and things I tried in that file. But, I wouldn’t copy+paste everything, since the other lecture notes are available.\n\n\n\n\nDue Dates:\n\nLab 02 due Friday\nHW01 now available; due Monday (10/16; 11:59 PM)\nLecture Participation survey open until Thursday\n\n\nNotes:\n\nLab01 scores and feedback posted\nDatahub: Launch RStudio (possible solution?)\nStaff office hours updated (see Canvas or website)\n\n\n\n\n\n\nI have been struggling to grasp the material in the course. It feels like we are diving into the content in the labs, but I don’t even feel like I truly understand what I’m doing. It often seems like I’m just copying and pasting code from the website without a clear understanding of the bigger picture. I’m particularly stuck because I feel like I don’t have a solid grasp of the fundamental concepts of coding in R; it feels so new. I understand that the pace of the course may be challenging, but I think a bit more stronger focus on the foundational aspects of coding in R would greatly benefit students like me who are struggling with the content. I’m looking forward to the course and I hope I can grasp the content as we go through the next week. I’m concerned about learning the material and also how that may affect my grade.\n\n\nLet’s see how y’all feel in a week. The first week can be a lot in this course. Often, students feel a lot more comfortable come week 3.\n\n\n\n\n\n89% know Python; 15% know R; most (but not all!) have programmed before\n64% feel confident about effective data science communication\nReasons for taking course: learn R, add to resume, analyze data, improve data science skills\n\n\nMy favorite boring facts:\n\nI was actually born on my birthday\ni don’t like to eat eggs but my roommate loves them\nI like to have a midday nap.\nI can raise my eyebrows really well\nI eat peanut butter straight from the jar\ni have a jack russell terrier.. named jack (we weren’t feeling creative)\n\n\n\n\n\nR4DS:\n\nChapter 12: Tidy Data\nChapter 13: Relational Data\n\n\n\n\nThe opinionated tidyverse is named as such b/c it assumes/necessitates your data be “tidy”.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. —- Hadley Wickham\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nSource: https://r4ds.had.co.nz/tidy-data.html\n\n\n\n\n❓ Given the rules discussed, is the cat_lovers dataset tidy?\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")\n\nRows: 60 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, number_of_cats, handedness\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncat_lovers |> datatable()\n\n\n\n\n\n\n\n❓ Given the rules discussed, is the bike dataset tidy?\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 5716 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (44): AmbulanceR, BikeAge_Gr, Bike_Alc_D, Bike_Dir, Bike_Injur, Bike_Po...\ndbl   (8): FID, OBJECTID, Bike_Age, Crash_Hour, Crash_Ty_1, Crash_Year, Drvr...\ndttm  (1): Crash_Time\ndate  (1): Crash_Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike |> datatable()\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\n\n\n\n\n\n❓ Which is a dataset? Which is a summary table?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are four representations of the same data/information provided in the tidyr packages: table1, table2, table3, and the combination of table4a and table4b. Given what we’ve discussed, which is the best (tidiest) way to represent these data?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\nSolution: pivoting!\n\n\n\n\n\npivot_longerpivot_widerlong vs. wide\n\n\nFor when some of the column names are not names of variables, but values of a variable…\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  <chr>       <chr>  <dbl>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n❓ Why are there backticks around the years? (Note: we have not discussed this yet)\n\n\nFor when an observation is scattered across multiple rows…\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable2 |>\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n❓ Why aren’t there quotes around column names here…but there were in pivot_longer? (Note: we have not discussed this yet.)\n\n\n\nwide data contains values that do not repeat in the first column.\nlong format contains values that do repeat in the first column.\n\nBoth are good/helpful! We’ll return to this idea and discuss more during dataviz next week.\nBriefly:\n\nwide data: analysis\nlong data: plotting\n\n\n\n\n\n\n\n\nseparateunite\n\n\nFor when multiple pieces of information are stored in a single column…\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  <chr>       <dbl> <chr>  <chr>     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n…but…but…cases and population should be numeric…\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nUnite is the opposite…it combines data stored across multiple columns.\nThe general syntax is:\n\ndf |>\n  unite(new_col, first_col, second_col)\n\n\n\n\n\n\n\nIf we look at table4a, it’s missing the population information. That’s stored in a separate table…table4b\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  <chr>            <dbl>      <dbl>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n…which is also in the “wide” format\n\n…so we pivot both tables longer\n\ntidy4a <- table4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntidy4b <- table4b |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\ntidy4b\n\n# A tibble: 6 × 3\n  country     year  population\n  <chr>       <chr>      <dbl>\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n\n\n…but how do we get them into a single tidy dataset?\n\n\nA join!\n\nleft_join(tidy4a, tidy4b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  <chr>       <chr>  <dbl>      <dbl>\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\nSource: R4DS\n\n\n\n\n\nlibrary(nycflights13)\n\n\nairlines : links airline to two letter code\nairports : ID’ed by FAA code\nplanes : ID’ed by tailnum\nairport : weather each hour; id’ed by two letter airport code\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html)\n\n\n\n\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time).\n\n\n\n\n\nmutating joins - add new variables to a data frame from matching observations in another\n\nFor simplicity, we’ll work with only a handful of columns…\n\nflights |> \n  select(year:day, hour, tailnum, carrier) |> \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 7\n    year month   day  hour tailnum carrier name                    \n   <int> <int> <int> <dbl> <chr>   <chr>   <chr>                   \n 1  2013     1     1     5 N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 N3ALAA  AA      American Airlines Inc.  \n# ℹ 336,766 more rows\n\n\nThere is now a new column name…coming from the airlines data frame.\n\n\nleft_join:\n\nkeeps all rows in first df (here: flights)\nadds all matching information from second df (here: airlines); adds NAs for any observations not in airlines\n\n\n\nOther joins:\nright_join: keeps all observations in second df full_join: keeps all observations in either df\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html\n\n\n\n\ninner_join:\n\ntakes only rows in both dfs\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#qa",
    "href": "content/lectures/09-mlr-slides.html#qa",
    "title": "09-mlr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: What are some ways we can make our visuals accessible? For example, how can we account for color blind people when incorporating graphs and figures in our presentation?\nA: Love this question! One way to do this is to use tools built for just this purpose! For example, for color blindness, ColorBrewer2 allows you to toggle for color palletes that are “colorblind safe.” Similarly, for written content online, ensuring that you’re including alt-text for all images, to enable visual understanding by those with vision impariments/differences. For those with learning differences (i.e. dyslexia, ADHD), I’m sure there’s research out there with respect to viz, and I should make myself more familiar, but ensuring that you’re not using very similar acronyms/initialisms for different context or using consistent icons (Rather than letters) across visualizations, etc. can really help individuals with learning differences and overall make your viz more consistent/understandable. This is just the tip of the iceberg. Additional suggestions here."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#course-announcements",
    "href": "content/lectures/09-mlr-slides.html#course-announcements",
    "title": "09-mlr",
    "section": "Course Announcements",
    "text": "Course Announcements\n\n🎃 Happy Halloween!\n\nNo OH today after class\nMake up: tomorrow 10-11AM (CSB 243)\n\n🧪 There is NO lab to turn in this week\n\nLab will be used for midterm review\nCome to lab with questions!\n\n🏫 Midterm is due next Monday at 11:59PM\n\nwill be released 5PM Friday\ncompleted individually\nopen notes/open Internet\nNOT timed\ntypically takes students ~2.5h to complete (big range: 2-15h)\n\nLab05 & HW03 will be released next Monday"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#suggested-reading",
    "href": "content/lectures/09-mlr-slides.html#suggested-reading",
    "title": "09-mlr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nIntroduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#agenda",
    "href": "content/lectures/09-mlr-slides.html#agenda",
    "title": "09-mlr",
    "section": "Agenda",
    "text": "Agenda\n\nMultiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#packages-data",
    "href": "content/lectures/09-mlr-slides.html#packages-data",
    "title": "09-mlr",
    "section": "Packages & Data",
    "text": "Packages & Data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nData: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |> \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#multiple-predictors",
    "href": "content/lectures/09-mlr-slides.html#multiple-predictors",
    "title": "09-mlr",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nResponse variable: log_price\nExplanatory variables: Width and height\n\n\n\nlin_mod <- linear_reg() |>\n  set_engine(\"lm\")\n\npp_fit <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data = pp)\ntidy(pp_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#linear-model-with-multiple-predictors",
    "href": "content/lectures/09-mlr-slides.html#linear-model-with-multiple-predictors",
    "title": "09-mlr",
    "section": "Linear model with multiple predictors",
    "text": "Linear model with multiple predictors\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4\n\n\n\n\\[\\widehat{log\\_price} = 4.77 + 0.0269 \\times width - 0.0133 \\times height\\]\n\n❓ How do we interpret this model?"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#price-surface-area-and-living-artist",
    "href": "content/lectures/09-mlr-slides.html#price-surface-area-and-living-artist",
    "title": "09-mlr",
    "section": "Price, surface area, and living artist",
    "text": "Price, surface area, and living artist\n\nExplore the relationship between price of paintings and surface area, conditioned on whether or not the artist is still living\nFirst visualize and explore, then model\n\n\n\nBut first, prep the data:\n\n\npp <- pp |>\n  mutate(artistliving = case_when(artistliving == 0 ~ \"Deceased\", \n                                  TRUE ~ \"Living\"))\n\npp |>\n  count(artistliving)\n\n# A tibble: 2 × 2\n  artistliving     n\n  <chr>        <int>\n1 Deceased      2937\n2 Living         456"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#typical-surface-area",
    "href": "content/lectures/09-mlr-slides.html#typical-surface-area",
    "title": "09-mlr",
    "section": "Typical surface area",
    "text": "Typical surface area\n\nPlotCode\n\n\n\n\n\n\n\nTypical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.\n\n\n\nggplot(data = pp, aes(x = Surface, fill = artistliving)) +\n  geom_histogram(binwidth = 500) + \n  facet_grid(artistliving ~ .) +\n  scale_fill_manual(values = c(\"#E48957\", \"#071381\")) +\n  guides(fill = \"none\") +\n  labs(x = \"Surface area\", y = NULL) +\n  geom_vline(xintercept = 1000) +\n  geom_vline(xintercept = 5000, linetype = \"dashed\", color = \"gray\")"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#narrowing-the-scope",
    "href": "content/lectures/09-mlr-slides.html#narrowing-the-scope",
    "title": "09-mlr",
    "section": "Narrowing the scope",
    "text": "Narrowing the scope\n\nPlotCode\n\n\nFor simplicity let’s focus on the paintings with Surface < 5000:\n\n\n\n\n\n\n\n\npp_Surf_lt_5000 <- pp |>\n  filter(Surface < 5000)\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Artist\", shape = \"Artist\") +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\"))"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#facet-to-get-a-better-look",
    "href": "content/lectures/09-mlr-slides.html#facet-to-get-a-better-look",
    "title": "09-mlr",
    "section": "Facet to get a better look",
    "text": "Facet to get a better look\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~artistliving) +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\")) +\n  labs(color = \"Artist\", shape = \"Artist\")"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#two-ways-to-model",
    "href": "content/lectures/09-mlr-slides.html#two-ways-to-model",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living.\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interacting-explanatory-variables",
    "href": "content/lectures/09-mlr-slides.html#interacting-explanatory-variables",
    "title": "09-mlr",
    "section": "Interacting explanatory variables",
    "text": "Interacting explanatory variables\n\nIncluding an interaction effect in the model allows for different slopes, i.e. nonparallel lines.\nThis implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\nThis can be accomplished by adding an interaction variable: the product of two explanatory variables."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#two-ways-to-model-1",
    "href": "content/lectures/09-mlr-slides.html#two-ways-to-model-1",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\n\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living\n\n\n\n\n\n\n\n\n\n\n❓ Which does your intuition/knowledge of the data suggest is more appropriate?\nPut a green sticky if you think main; pink if you think interaction."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#fit-model-with-main-effects",
    "href": "content/lectures/09-mlr-slides.html#fit-model-with-main-effects",
    "title": "09-mlr",
    "section": "Fit model with main effects",
    "text": "Fit model with main effects\n\nResponse variable: log_price\nExplanatory variables: Surface area and artistliving\n\n\npp_main_fit <- lin_mod |>\n  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)\ntidy(pp_main_fit)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        4.88     0.0424       115.   0       \n2 Surface            0.000265 0.0000415      6.39 1.85e-10\n3 artistlivingLiving 0.137    0.0970         1.41 1.57e- 1\n\n\n\n\\[\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times artistliving\\]"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#solving-the-model",
    "href": "content/lectures/09-mlr-slides.html#solving-the-model",
    "title": "09-mlr",
    "section": "Solving the model",
    "text": "Solving the model\n\nNon-living artist: Plug in 0 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 0\\)\n\\(= 4.88 + 0.000265 \\times surface\\)\n\n\nLiving artist: Plug in 1 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 1\\)\n\\(= 5.017 + 0.000265 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#visualizing-main-effects",
    "href": "content/lectures/09-mlr-slides.html#visualizing-main-effects",
    "title": "09-mlr",
    "section": "Visualizing main effects",
    "text": "Visualizing main effects\n\n\n\nSame slope: Rate of change in price as the surface area increases does not vary between paintings by living and non-living artists.\nDifferent intercept: Paintings by living artists are consistently more expensive than paintings by non-living artists."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interpreting-main-effects",
    "href": "content/lectures/09-mlr-slides.html#interpreting-main-effects",
    "title": "09-mlr",
    "section": "Interpreting main effects",
    "text": "Interpreting main effects\n\ntidy(pp_main_fit) |> \n  mutate(exp_estimate = exp(estimate)) |>\n  select(term, estimate, exp_estimate)\n\n# A tibble: 3 × 3\n  term               estimate exp_estimate\n  <chr>                 <dbl>        <dbl>\n1 (Intercept)        4.88           132.  \n2 Surface            0.000265         1.00\n3 artistlivingLiving 0.137            1.15\n\n\n\n\nAll else held constant, for each additional square inch in painting’s surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.\nAll else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.\nPaintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#main-vs.-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#main-vs.-interaction-effects",
    "title": "09-mlr",
    "section": "Main vs. interaction effects",
    "text": "Main vs. interaction effects\n\nThe way we specified our main effects model only lets artistliving affect the intercept.\nModel implicitly assumes that paintings with living and deceased artists have the same slope and only allows for different intercepts.\n\n\n❓ What seems more appropriate in this case?\n\nSame slope and same intercept for both colors\nSame slope and different intercept for both colors\nDifferent slope and different intercept for both colors"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interaction-surface-artistliving",
    "href": "content/lectures/09-mlr-slides.html#interaction-surface-artistliving",
    "title": "09-mlr",
    "section": "Interaction: Surface * artistliving",
    "text": "Interaction: Surface * artistliving"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#fit-model-with-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#fit-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Fit model with interaction effects",
    "text": "Fit model with interaction effects\n\nResponse variable: log_price\nExplanatory variables: Surface area, artistliving, and their interaction\n\n\npp_int_fit <- lin_mod |>\n  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)\ntidy(pp_int_fit)\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#linear-model-with-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#linear-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Linear model with interaction effects",
    "text": "Linear model with interaction effects\n\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139  \n\n\n\\[\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface - 0.126 \\times artistliving\\] \\[+ ~ 0.00048 \\times surface * artistliving\\]"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interpretation-of-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#interpretation-of-interaction-effects",
    "title": "09-mlr",
    "section": "Interpretation of interaction effects",
    "text": "Interpretation of interaction effects\n\n\nRate of change in price as the surface area of the painting increases does vary between paintings by living and non-living artists (different slopes)\nSome paintings by living artists are more expensive than paintings by non-living artists, and some are not (different intercept).\n\n\n\n\n\n\n\nNon-living artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 0 + 0.00048 \\times surface \\times 0\\) \\(= 4.91 + 0.00021 \\times surface\\)\nLiving artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 1 + 0.00048 \\times surface \\times 1\\) \\(= 4.91 + 0.00021 \\times surface\\) \\(- 0.126 + 0.00048 \\times surface\\) \\(= 4.784 + 0.00069 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#r-squared",
    "href": "content/lectures/09-mlr-slides.html#r-squared",
    "title": "09-mlr",
    "section": "R-squared",
    "text": "R-squared\n\n\\(R^2\\) is the percentage of variability in the response variable explained by the regression model.\n\n\nglance(pp_main_fit)$r.squared\n\n[1] 0.01320884\n\nglance(pp_int_fit)$r.squared\n\n[1] 0.0176922\n\n\n\n\nClearly the model with interactions has a higher \\(R^2\\).\n\n\n\n\nHowever using \\(R^2\\) for model selection in models with multiple explanatory variables is not a good idea as \\(R^2\\) increases when any variable is added to the model."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#adjusted-r-squared",
    "href": "content/lectures/09-mlr-slides.html#adjusted-r-squared",
    "title": "09-mlr",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\nIt appears that adding the interaction actually increased adjusted \\(R^2\\), so we should indeed use the model with the interactions.\n\nglance(pp_main_fit)$adj.r.squared\n\n[1] 0.01258977\n\nglance(pp_int_fit)$adj.r.squared\n\n[1] 0.01676753"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#third-order-interactions",
    "href": "content/lectures/09-mlr-slides.html#third-order-interactions",
    "title": "09-mlr",
    "section": "Third order interactions",
    "text": "Third order interactions\n\nCan you? Yes\nShould you? Probably not if you want to interpret these interactions in context of the data."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#in-pursuit-of-occams-razor",
    "href": "content/lectures/09-mlr-slides.html#in-pursuit-of-occams-razor",
    "title": "09-mlr",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model."
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#backward-selection",
    "href": "content/lectures/09-mlr-slides.html#backward-selection",
    "title": "09-mlr",
    "section": "Backward selection",
    "text": "Backward selection\nFor this demo, we’ll ignore interaction effects…and just model main effects to start:\n\npp_full <-  lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp) \n\nglance(pp_full)$adj.r.squared\n\n[1] 0.02570141\n\n\n\n\\(R^2\\) (full): 0.0257014\n\n\nRemove artistliving\n\npp_noartist <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface, data=pp) \n\nglance(pp_noartist)$adj.r.squared\n\n[1] 0.02579859\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\n…Improved variance explained\n\nRemove Surface\n\npp_noartist_nosurface <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data=pp) \n\nglance(pp_noartist_nosurface)$adj.r.squared\n\n[1] 0.02231559\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\\(R^2\\) (no artistliving or Surface): 0.0223\n\n\n\n…no longer gaining improvement, so we stick with: log_price ~ Width_in + Height_in + Surface"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#other-approach",
    "href": "content/lectures/09-mlr-slides.html#other-approach",
    "title": "09-mlr",
    "section": "Other approach:",
    "text": "Other approach:\n\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n\nStep 1: Fit model (w/o tidymodels)\n\n# fit the model (not using tidymodels)\nmod <- lm(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp_Surf_lt_5000)\n\n\n\nStep 2: Determine which variables to remove\n\nols_step_backward_p(mod)\n\n\n                             Elimination Summary                               \n------------------------------------------------------------------------------\n        Variable                      Adj.                                        \nStep      Removed       R-Square    R-Square     C(p)        AIC         RMSE     \n------------------------------------------------------------------------------\n   1    artistliving      0.0261      0.0251    3.8495    12603.7727    1.8315    \n------------------------------------------------------------------------------\n\n\n…specifies that artistliving should be removed\n\n\nStep 2 (alternate): Compare all possible models…\n\nols_step_all_possible(mod) |>\n  arrange(desc(adjr))\n\n   Index N                              Predictors     R-Square Adj. R-Square\n1     11 3              Width_in Height_in Surface 0.0260749939  0.0251349118\n2     15 4 Width_in Height_in Surface artistliving 0.0263412027  0.0250876993\n3      5 2                      Width_in Height_in 0.0256902566  0.0250634893\n4     12 3         Width_in Height_in artistliving 0.0259732581  0.0250330779\n5      6 2                        Width_in Surface 0.0249136264  0.0242863596\n6     13 3           Width_in Surface artistliving 0.0251787948  0.0242378477\n7      7 2                   Width_in artistliving 0.0212864021  0.0206568018\n8      1 1                                Width_in 0.0209415833  0.0206267736\n9      8 2                    Surface artistliving 0.0132088377  0.0125897717\n10     2 1                                 Surface 0.0125899681  0.0122803381\n11    14 3          Height_in Surface artistliving 0.0130836930  0.0121310711\n12     9 2                       Height_in Surface 0.0126782901  0.0120431523\n13    10 2                  Height_in artistliving 0.0062698155  0.0056305552\n14     3 1                               Height_in 0.0058797727  0.0055601199\n15     4 1                            artistliving 0.0005531617  0.0002397573\n   Mallow's Cp\n1     3.849487\n2     5.000000\n3     3.077206\n4     4.174132\n5     5.555476\n6     6.709309\n7    17.130153\n8    16.230489\n9    51.971190\n10   52.001268\n11   45.305459\n12   44.599123\n13   65.048926\n14   64.293574\n15   91.485605"
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#recap",
    "href": "content/lectures/09-mlr-slides.html#recap",
    "title": "09-mlr",
    "section": "Recap",
    "text": "Recap\n\nCan you model and interpret linear models with multiple predictors?\nCan you explain the difference in a model with main effects vs. interaction effects?\nCan you compare different models and determine how to proceed?\nCan you carry out and explain backward selection?\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/09-mlr.html",
    "href": "content/lectures/09-mlr.html",
    "title": "09-mlr",
    "section": "",
    "text": "Q: What are some ways we can make our visuals accessible? For example, how can we account for color blind people when incorporating graphs and figures in our presentation?\nA: Love this question! One way to do this is to use tools built for just this purpose! For example, for color blindness, ColorBrewer2 allows you to toggle for color palletes that are “colorblind safe.” Similarly, for written content online, ensuring that you’re including alt-text for all images, to enable visual understanding by those with vision impariments/differences. For those with learning differences (i.e. dyslexia, ADHD), I’m sure there’s research out there with respect to viz, and I should make myself more familiar, but ensuring that you’re not using very similar acronyms/initialisms for different context or using consistent icons (Rather than letters) across visualizations, etc. can really help individuals with learning differences and overall make your viz more consistent/understandable. This is just the tip of the iceberg. Additional suggestions here.\n\n\n\n\n\n🎃 Happy Halloween!\n\nNo OH today after class\nMake up: tomorrow 10-11AM (CSB 243)\n\n🔬 There is NO lab to turn in this week\n\nLab will be used for midterm review\nCome to lab with questions!\n\n🏫 Midterm is due next Monday at 11:59PM\n\nwill be released 5PM Friday\ncompleted individually\nopen notes/open Internet\nNOT timed\ntypically takes students ~2.5h to complete (big range: 2-15h)\n\nLab05 & HW03 will be released next Monday\n\n\n\n\nIntroduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors\n\n\n\n\nMultiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nData: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |> \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62"
  },
  {
    "objectID": "content/lectures/09-mlr.html#multiple-predictors",
    "href": "content/lectures/09-mlr.html#multiple-predictors",
    "title": "09-mlr",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nResponse variable: log_price\nExplanatory variables: Width and height\n\n\n\nlin_mod <- linear_reg() |>\n  set_engine(\"lm\")\n\npp_fit <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data = pp)\ntidy(pp_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4"
  },
  {
    "objectID": "content/lectures/09-mlr.html#linear-model-with-multiple-predictors",
    "href": "content/lectures/09-mlr.html#linear-model-with-multiple-predictors",
    "title": "09-mlr",
    "section": "Linear model with multiple predictors",
    "text": "Linear model with multiple predictors\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4\n\n\n\n\\[\\widehat{log\\_price} = 4.77 + 0.0269 \\times width - 0.0133 \\times height\\]\n\n❓ How do we interpret this model?"
  },
  {
    "objectID": "content/lectures/09-mlr.html#price-surface-area-and-living-artist",
    "href": "content/lectures/09-mlr.html#price-surface-area-and-living-artist",
    "title": "09-mlr",
    "section": "Price, surface area, and living artist",
    "text": "Price, surface area, and living artist\n\nExplore the relationship between price of paintings and surface area, conditioned on whether or not the artist is still living\nFirst visualize and explore, then model\n\n\n\nBut first, prep the data:\n\n\npp <- pp |>\n  mutate(artistliving = case_when(artistliving == 0 ~ \"Deceased\", \n                                  TRUE ~ \"Living\"))\n\npp |>\n  count(artistliving)\n\n# A tibble: 2 × 2\n  artistliving     n\n  <chr>        <int>\n1 Deceased      2937\n2 Living         456"
  },
  {
    "objectID": "content/lectures/09-mlr.html#typical-surface-area",
    "href": "content/lectures/09-mlr.html#typical-surface-area",
    "title": "09-mlr",
    "section": "Typical surface area",
    "text": "Typical surface area\n\nPlotCode\n\n\n\n\n\n\n\nTypical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.\n\n\n\nggplot(data = pp, aes(x = Surface, fill = artistliving)) +\n  geom_histogram(binwidth = 500) + \n  facet_grid(artistliving ~ .) +\n  scale_fill_manual(values = c(\"#E48957\", \"#071381\")) +\n  guides(fill = \"none\") +\n  labs(x = \"Surface area\", y = NULL) +\n  geom_vline(xintercept = 1000) +\n  geom_vline(xintercept = 5000, linetype = \"dashed\", color = \"gray\")"
  },
  {
    "objectID": "content/lectures/09-mlr.html#narrowing-the-scope",
    "href": "content/lectures/09-mlr.html#narrowing-the-scope",
    "title": "09-mlr",
    "section": "Narrowing the scope",
    "text": "Narrowing the scope\n\nPlotCode\n\n\nFor simplicity let’s focus on the paintings with Surface < 5000:\n\n\n\n\n\n\n\n\npp_Surf_lt_5000 <- pp |>\n  filter(Surface < 5000)\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Artist\", shape = \"Artist\") +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\"))"
  },
  {
    "objectID": "content/lectures/09-mlr.html#facet-to-get-a-better-look",
    "href": "content/lectures/09-mlr.html#facet-to-get-a-better-look",
    "title": "09-mlr",
    "section": "Facet to get a better look",
    "text": "Facet to get a better look\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~artistliving) +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\")) +\n  labs(color = \"Artist\", shape = \"Artist\")"
  },
  {
    "objectID": "content/lectures/09-mlr.html#two-ways-to-model",
    "href": "content/lectures/09-mlr.html#two-ways-to-model",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living.\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living."
  },
  {
    "objectID": "content/lectures/09-mlr.html#interacting-explanatory-variables",
    "href": "content/lectures/09-mlr.html#interacting-explanatory-variables",
    "title": "09-mlr",
    "section": "Interacting explanatory variables",
    "text": "Interacting explanatory variables\n\nIncluding an interaction effect in the model allows for different slopes, i.e. nonparallel lines.\nThis implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\nThis can be accomplished by adding an interaction variable: the product of two explanatory variables."
  },
  {
    "objectID": "content/lectures/09-mlr.html#two-ways-to-model-1",
    "href": "content/lectures/09-mlr.html#two-ways-to-model-1",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\n\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living\n\n\n\n\n\n\n\n\n\n\n❓ Which does your intuition/knowledge of the data suggest is more appropriate?\nPut a green sticky if you think main; pink if you think interaction."
  },
  {
    "objectID": "content/lectures/09-mlr.html#fit-model-with-main-effects",
    "href": "content/lectures/09-mlr.html#fit-model-with-main-effects",
    "title": "09-mlr",
    "section": "Fit model with main effects",
    "text": "Fit model with main effects\n\nResponse variable: log_price\nExplanatory variables: Surface area and artistliving\n\n\npp_main_fit <- lin_mod |>\n  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)\ntidy(pp_main_fit)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        4.88     0.0424       115.   0       \n2 Surface            0.000265 0.0000415      6.39 1.85e-10\n3 artistlivingLiving 0.137    0.0970         1.41 1.57e- 1\n\n\n\n\\[\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times artistliving\\]"
  },
  {
    "objectID": "content/lectures/09-mlr.html#solving-the-model",
    "href": "content/lectures/09-mlr.html#solving-the-model",
    "title": "09-mlr",
    "section": "Solving the model",
    "text": "Solving the model\n\nNon-living artist: Plug in 0 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 0\\)\n\\(= 4.88 + 0.000265 \\times surface\\)\n\n\nLiving artist: Plug in 1 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 1\\)\n\\(= 5.017 + 0.000265 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/09-mlr.html#visualizing-main-effects",
    "href": "content/lectures/09-mlr.html#visualizing-main-effects",
    "title": "09-mlr",
    "section": "Visualizing main effects",
    "text": "Visualizing main effects\n\n\n\nSame slope: Rate of change in price as the surface area increases does not vary between paintings by living and non-living artists.\nDifferent intercept: Paintings by living artists are consistently more expensive than paintings by non-living artists."
  },
  {
    "objectID": "content/lectures/09-mlr.html#interpreting-main-effects",
    "href": "content/lectures/09-mlr.html#interpreting-main-effects",
    "title": "09-mlr",
    "section": "Interpreting main effects",
    "text": "Interpreting main effects\n\ntidy(pp_main_fit) |> \n  mutate(exp_estimate = exp(estimate)) |>\n  select(term, estimate, exp_estimate)\n\n# A tibble: 3 × 3\n  term               estimate exp_estimate\n  <chr>                 <dbl>        <dbl>\n1 (Intercept)        4.88           132.  \n2 Surface            0.000265         1.00\n3 artistlivingLiving 0.137            1.15\n\n\n\n\nAll else held constant, for each additional square inch in painting’s surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.\nAll else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.\nPaintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres."
  },
  {
    "objectID": "content/lectures/09-mlr.html#main-vs.-interaction-effects",
    "href": "content/lectures/09-mlr.html#main-vs.-interaction-effects",
    "title": "09-mlr",
    "section": "Main vs. interaction effects",
    "text": "Main vs. interaction effects\n\nThe way we specified our main effects model only lets artistliving affect the intercept.\nModel implicitly assumes that paintings with living and deceased artists have the same slope and only allows for different intercepts.\n\n\n❓ What seems more appropriate in this case?\n\nSame slope and same intercept for both colors\nSame slope and different intercept for both colors\nDifferent slope and different intercept for both colors"
  },
  {
    "objectID": "content/lectures/09-mlr.html#interaction-surface-artistliving",
    "href": "content/lectures/09-mlr.html#interaction-surface-artistliving",
    "title": "09-mlr",
    "section": "Interaction: Surface * artistliving",
    "text": "Interaction: Surface * artistliving"
  },
  {
    "objectID": "content/lectures/09-mlr.html#fit-model-with-interaction-effects",
    "href": "content/lectures/09-mlr.html#fit-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Fit model with interaction effects",
    "text": "Fit model with interaction effects\n\nResponse variable: log_price\nExplanatory variables: Surface area, artistliving, and their interaction\n\n\npp_int_fit <- lin_mod |>\n  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)\ntidy(pp_int_fit)\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139"
  },
  {
    "objectID": "content/lectures/09-mlr.html#linear-model-with-interaction-effects",
    "href": "content/lectures/09-mlr.html#linear-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Linear model with interaction effects",
    "text": "Linear model with interaction effects\n\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139  \n\n\n\\[\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface - 0.126 \\times artistliving\\] \\[+ ~ 0.00048 \\times surface * artistliving\\]"
  },
  {
    "objectID": "content/lectures/09-mlr.html#interpretation-of-interaction-effects",
    "href": "content/lectures/09-mlr.html#interpretation-of-interaction-effects",
    "title": "09-mlr",
    "section": "Interpretation of interaction effects",
    "text": "Interpretation of interaction effects\n\n\nRate of change in price as the surface area of the painting increases does vary between paintings by living and non-living artists (different slopes)\nSome paintings by living artists are more expensive than paintings by non-living artists, and some are not (different intercept).\n\n\n\n\n\n\n\nNon-living artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 0 + 0.00048 \\times surface \\times 0\\) \\(= 4.91 + 0.00021 \\times surface\\)\nLiving artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 1 + 0.00048 \\times surface \\times 1\\) \\(= 4.91 + 0.00021 \\times surface\\) \\(- 0.126 + 0.00048 \\times surface\\) \\(= 4.784 + 0.00069 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/09-mlr.html#r-squared",
    "href": "content/lectures/09-mlr.html#r-squared",
    "title": "09-mlr",
    "section": "R-squared",
    "text": "R-squared\n\n\\(R^2\\) is the percentage of variability in the response variable explained by the regression model.\n\n\nglance(pp_main_fit)$r.squared\n\n[1] 0.01320884\n\nglance(pp_int_fit)$r.squared\n\n[1] 0.0176922\n\n\n\n\nClearly the model with interactions has a higher \\(R^2\\).\n\n\n\n\nHowever using \\(R^2\\) for model selection in models with multiple explanatory variables is not a good idea as \\(R^2\\) increases when any variable is added to the model."
  },
  {
    "objectID": "content/lectures/09-mlr.html#adjusted-r-squared",
    "href": "content/lectures/09-mlr.html#adjusted-r-squared",
    "title": "09-mlr",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\nIt appears that adding the interaction actually increased adjusted \\(R^2\\), so we should indeed use the model with the interactions.\n\nglance(pp_main_fit)$adj.r.squared\n\n[1] 0.01258977\n\nglance(pp_int_fit)$adj.r.squared\n\n[1] 0.01676753"
  },
  {
    "objectID": "content/lectures/09-mlr.html#third-order-interactions",
    "href": "content/lectures/09-mlr.html#third-order-interactions",
    "title": "09-mlr",
    "section": "Third order interactions",
    "text": "Third order interactions\n\nCan you? Yes\nShould you? Probably not if you want to interpret these interactions in context of the data."
  },
  {
    "objectID": "content/lectures/09-mlr.html#in-pursuit-of-occams-razor",
    "href": "content/lectures/09-mlr.html#in-pursuit-of-occams-razor",
    "title": "09-mlr",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model."
  },
  {
    "objectID": "content/lectures/09-mlr.html#backward-selection",
    "href": "content/lectures/09-mlr.html#backward-selection",
    "title": "09-mlr",
    "section": "Backward selection",
    "text": "Backward selection\nFor this demo, we’ll ignore interaction effects…and just model main effects to start:\n\npp_full <-  lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp) \n\nglance(pp_full)$adj.r.squared\n\n[1] 0.02570141\n\n\n\n\\(R^2\\) (full): 0.0257014\n\n\n\nRemove artistliving\n\npp_noartist <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface, data=pp) \n\nglance(pp_noartist)$adj.r.squared\n\n[1] 0.02579859\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\n…Improved variance explained\n\n\n\nRemove Surface\n\npp_noartist_nosurface <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data=pp) \n\nglance(pp_noartist_nosurface)$adj.r.squared\n\n[1] 0.02231559\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\\(R^2\\) (no artistliving or Surface): 0.0223\n\n\n\n…no longer gaining improvement, so we stick with: log_price ~ Width_in + Height_in + Surface"
  },
  {
    "objectID": "content/lectures/09-mlr.html#other-approach",
    "href": "content/lectures/09-mlr.html#other-approach",
    "title": "09-mlr",
    "section": "Other approach:",
    "text": "Other approach:\n\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n\nStep 1: Fit model (w/o tidymodels)\n\n# fit the model (not using tidymodels)\nmod <- lm(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp_Surf_lt_5000)\n\n\n\nStep 2: Determine which variables to remove\n\nols_step_backward_p(mod)\n\n\n                             Elimination Summary                               \n------------------------------------------------------------------------------\n        Variable                      Adj.                                        \nStep      Removed       R-Square    R-Square     C(p)        AIC         RMSE     \n------------------------------------------------------------------------------\n   1    artistliving      0.0261      0.0251    3.8495    12603.7727    1.8315    \n------------------------------------------------------------------------------\n\n\n…specifies that artistliving should be removed\n\n\nStep 2 (alternate): Compare all possible models…\n\nols_step_all_possible(mod) |>\n  arrange(desc(adjr))\n\n   Index N                              Predictors     R-Square Adj. R-Square\n1     11 3              Width_in Height_in Surface 0.0260749939  0.0251349118\n2     15 4 Width_in Height_in Surface artistliving 0.0263412027  0.0250876993\n3      5 2                      Width_in Height_in 0.0256902566  0.0250634893\n4     12 3         Width_in Height_in artistliving 0.0259732581  0.0250330779\n5      6 2                        Width_in Surface 0.0249136264  0.0242863596\n6     13 3           Width_in Surface artistliving 0.0251787948  0.0242378477\n7      7 2                   Width_in artistliving 0.0212864021  0.0206568018\n8      1 1                                Width_in 0.0209415833  0.0206267736\n9      8 2                    Surface artistliving 0.0132088377  0.0125897717\n10     2 1                                 Surface 0.0125899681  0.0122803381\n11    14 3          Height_in Surface artistliving 0.0130836930  0.0121310711\n12     9 2                       Height_in Surface 0.0126782901  0.0120431523\n13    10 2                  Height_in artistliving 0.0062698155  0.0056305552\n14     3 1                               Height_in 0.0058797727  0.0055601199\n15     4 1                            artistliving 0.0005531617  0.0002397573\n   Mallow's Cp\n1     3.849487\n2     5.000000\n3     3.077206\n4     4.174132\n5     5.555476\n6     6.709309\n7    17.130153\n8    16.230489\n9    51.971190\n10   52.001268\n11   45.305459\n12   44.599123\n13   65.048926\n14   64.293574\n15   91.485605"
  },
  {
    "objectID": "content/lectures/09-mlr.html#recap",
    "href": "content/lectures/09-mlr.html#recap",
    "title": "09-mlr",
    "section": "Recap",
    "text": "Recap\n\nCan you model and interpret linear models with multiple predictors?\nCan you explain the difference in a model with main effects vs. interaction effects?\nCan you compare different models and determine how to proceed?\nCan you carry out and explain backward selection?"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#qa",
    "href": "content/lectures/14-tidymodels-slides.html#qa",
    "title": "14-tidymodels",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I had a question about the presentations for the final projects; since it is due during finals week, is it a live presentation in class or do we submit a video? If it is a live presentation, do we present during our designated final day/time on webreg?\nA: Video submission!\n\n\nQ: I also wanted to mention that the mid/pre-course extra credit surveys doesn’t reflect a change in grade on canvas. (For ex. if i put a 0 or 100 for E.C my grade stays the same).\nA: Correct - I add these in at the end. Canvas can do many things, but it doesn’t handle EC well (from what I can tell).\n\n\nQ: I’m overwhelmed/confused by “the code :’) it’s quite a bit to take in”\nA: Yes! It’s a lot! This is why we have group mates on the case study. I encourage everyone to sit with the code after class and then work through it together as you complete the case study!\n\n\nQ: For oral fluid you mentioned looking more into why there’s that big dip in specificity and that we should look more into that on Friday with eda but would that be slightly guided because I have no idea where to start with that.\nA: I would make some plots that specifically look at the data/numbers there to figure out what could be leading to that drop at that particular time window.\n\n\nQ: Why are specificity graphs so high?  A: Good question - this is generally b/c people who didn’t smoke have values very close to zero across compounds…so they will rarely be above the cutoff, making this very effective at identifying individuals who did not smoke\n\n\nQ: What is the dplyr::select notation, like is it a way to use select from dplyr without librarying first?\nA: Yes!\n\n\nQ: Also separate topic, but do we have information on impairment so we can account for that with recent use?  A: Great question - impairment is very hard to define here. We (the researchers) have data on self-reported high and what the police officers determined, but y’all don’t have that data. So, we’re using knowledge from other studies (see 11-cs01-data notes) to understand what we know on impoairment but only focusing on detecting recent use here.\n\n\nQ: I am unable to locate where to sign up for groups for the final project\nA: This form was just released (sorry for delay). link to survey\n\n\nQ: I think I need more time to digest how the code works together to produce the visuals that we saw.\nA: I agree. I think I could balance and give more time in class…but I will say this is an exercise I want groups to work through together!"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#course-announcements",
    "href": "content/lectures/14-tidymodels-slides.html#course-announcements",
    "title": "14-tidymodels",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nNo class This Th; No Lab this Fri (Happy Thanksgiving!)\nCS01 due Monday 11/27\n\ngroup work survey due Tues 11/28\n\n\n\nNotes:\n\nBe sure you watch the video from last Thursday on Canvas\nAny questions about CS01?"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#agenda",
    "href": "content/lectures/14-tidymodels-slides.html#agenda",
    "title": "14-tidymodels",
    "section": "Agenda",
    "text": "Agenda\n\nmachine learning intro\n(re)introduce tidymodels\nworked example: ML in tidymodels"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#suggested-reading",
    "href": "content/lectures/14-tidymodels-slides.html#suggested-reading",
    "title": "14-tidymodels",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nThe package itself has some worked examples: https://www.tidymodels.org/start/models/\nThere’s a whole book (written by the developer of tidymodels) that covers the tidymodels package: https://www.tmwr.org/\nThis may be a helpful reference for this lecture, although, of course, the book goes into way more detail than we will in a single lecture."
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-philosophy",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-philosophy",
    "title": "14-tidymodels",
    "section": "tidymodels: philosophy",
    "text": "tidymodels: philosophy\n\n“Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: pre-processing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret. The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do.” - Max Kuhn (tidymodels developer)\n\n\nBenefits:\n\nStandardized workflow/format/notation across different types of machine learning algorithms\nCan easily modify pre-processing, algorithm choice, and hyper-parameter tuning making optimization easy"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-ecosystem",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-ecosystem",
    "title": "14-tidymodels",
    "section": "tidymodels: ecosystem",
    "text": "tidymodels: ecosystem\nThe main packages (and their roles):"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#classic-ml",
    "href": "content/lectures/14-tidymodels-slides.html#classic-ml",
    "title": "14-tidymodels",
    "section": "Classic ML",
    "text": "Classic ML\nTypically, you use data where you have both the input and output data to train a machine learning algorithm.\n\nWhat you need:\n\n\nA data set to train from.\nAn algorithm or set of algorithms you can use to try values of \\(f\\).\nA distance metric \\(d\\) for measuring how close \\(Y\\) is to \\(\\hat{Y}\\).\nA definition of what a “good” distance is."
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-for-ml",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-for-ml",
    "title": "14-tidymodels",
    "section": "tidymodels for ML",
    "text": "tidymodels for ML\nHow these packages fit together for carrying out machine learning:"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-steps",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-steps",
    "title": "14-tidymodels",
    "section": "tidymodels: steps",
    "text": "tidymodels: steps"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#model-intro",
    "href": "content/lectures/14-tidymodels-slides.html#model-intro",
    "title": "14-tidymodels",
    "section": "Model Intro",
    "text": "Model Intro\n\nglmnet\n`rf``"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#setup",
    "href": "content/lectures/14-tidymodels-slides.html#setup",
    "title": "14-tidymodels",
    "section": "Setup",
    "text": "Setup\n\n# install.packages(\"glmnet\")\n# install.packages(\"ranger\")\n# install.packages(\"vip\")\n\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#data",
    "href": "content/lectures/14-tidymodels-slides.html#data",
    "title": "14-tidymodels",
    "section": "Data",
    "text": "Data\nHotel Bookings Data from: Antonio, Almeida, and Nunes (2019)\n\nData Dictionary\n\n\n\nhotels <- \n  read_csv(\"https://tidymodels.org/start/case-study/hotels.csv\") |>\n  mutate(across(where(is.character), as.factor))\n\n\n\n\nglimpse(hotels)\n\nRows: 50,000\nColumns: 23\n$ hotel                          <fct> City_Hotel, City_Hotel, Resort_Hotel, R…\n$ lead_time                      <dbl> 217, 2, 95, 143, 136, 67, 47, 56, 80, 6…\n$ stays_in_weekend_nights        <dbl> 1, 0, 2, 2, 1, 2, 0, 0, 0, 2, 1, 0, 1, …\n$ stays_in_week_nights           <dbl> 3, 1, 5, 6, 4, 2, 2, 3, 4, 2, 2, 1, 2, …\n$ adults                         <dbl> 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, …\n$ children                       <fct> none, none, none, none, none, none, chi…\n$ meal                           <fct> BB, BB, BB, HB, HB, SC, BB, BB, BB, BB,…\n$ country                        <fct> DEU, PRT, GBR, ROU, PRT, GBR, ESP, ESP,…\n$ market_segment                 <fct> Offline_TA/TO, Direct, Online_TA, Onlin…\n$ distribution_channel           <fct> TA/TO, Direct, TA/TO, TA/TO, Direct, TA…\n$ is_repeated_guest              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_cancellations         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ previous_bookings_not_canceled <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ reserved_room_type             <fct> A, D, A, A, F, A, C, B, D, A, A, D, A, …\n$ assigned_room_type             <fct> A, K, A, A, F, A, C, A, D, A, D, D, A, …\n$ booking_changes                <dbl> 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ deposit_type                   <fct> No_Deposit, No_Deposit, No_Deposit, No_…\n$ days_in_waiting_list           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ customer_type                  <fct> Transient-Party, Transient, Transient, …\n$ average_daily_rate             <dbl> 80.75, 170.00, 8.00, 81.00, 157.60, 49.…\n$ required_car_parking_spaces    <fct> none, none, none, none, none, none, non…\n$ total_of_special_requests      <dbl> 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 0, 1, 0, …\n$ arrival_date                   <date> 2016-09-01, 2017-08-25, 2016-11-19, 20…\n\n\n\n\nOutcome variable: children (two levels):\n\nhotels %>% \n  count(children) %>% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  children     n   prop\n  <fct>    <int>  <dbl>\n1 children  4038 0.0808\n2 none     45962 0.919 \n\n\n\n\nNotes: - Data are imbalanced - There are methods for combating this issue usingrecipes(search for steps toupsampleordownsample) or other more specialized packages likethemis` - For demo purposes, we’ll model as-is"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#data-splitting",
    "href": "content/lectures/14-tidymodels-slides.html#data-splitting",
    "title": "14-tidymodels",
    "section": "Data Splitting",
    "text": "Data Splitting\nTypically, data are split into a training and testing datasets\n\n\nTraining | data used to build & tune the model; model “learns” from these data\nTesting | data withheld from training to evaluate model performance (can it generalize?)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#data-splitting-hotels-rsample",
    "href": "content/lectures/14-tidymodels-slides.html#data-splitting-hotels-rsample",
    "title": "14-tidymodels",
    "section": "Data Splitting: Hotels (rsample)",
    "text": "Data Splitting: Hotels (rsample)\nReminder: children is pretty imbalanced so we’ll use a stratified random sample (to ensure similar proportion of chilren in training and testing)\n\nset.seed(123)\nsplits <- initial_split(hotels, strata = children)\nsplits\n\n<Training/Testing/Total>\n<37500/12500/50000>\n\n\n\n\nhotel_other <- training(splits)\nhotel_test  <- testing(splits)\n\n\n\n\n# training set proportions by children\nhotel_other %>% \n  count(children) %>% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  children     n   prop\n  <fct>    <int>  <dbl>\n1 children  3027 0.0807\n2 none     34473 0.919 \n\n\n\n\n\n# test set proportions by children\nhotel_test  %>% \n  count(children) %>% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  children     n   prop\n  <fct>    <int>  <dbl>\n1 children  1011 0.0809\n2 none     11489 0.919"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#re-sampling",
    "href": "content/lectures/14-tidymodels-slides.html#re-sampling",
    "title": "14-tidymodels",
    "section": "Re-sampling",
    "text": "Re-sampling\n\nmultiple approaches (k-fold CV)\nhere: validation set\n\n Image Source: https://www.tidymodels.org/start/case-study/"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#resampling-hotels",
    "href": "content/lectures/14-tidymodels-slides.html#resampling-hotels",
    "title": "14-tidymodels",
    "section": "Resampling: Hotels",
    "text": "Resampling: Hotels\n\nTraining: 30,000\nValidation: 7,500\nTesting: 12,500\n\n\n\nset.seed(234)\nval_set <- validation_split(hotel_other, \n                            strata = children, \n                            prop = c(0.8))\n\nval_set\n\n# Validation Set Split (0.8/0.2)  using stratification \n# A tibble: 1 × 2\n  splits               id        \n  <list>               <chr>     \n1 <split [30000/7500]> validation"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#build-a-model-recipes",
    "href": "content/lectures/14-tidymodels-slides.html#build-a-model-recipes",
    "title": "14-tidymodels",
    "section": "Build a Model (`recipes``)",
    "text": "Build a Model (`recipes``)\nList ingredients | specify which variables we will be using as our outcome and predictors (assign roles)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#model-building-hotels",
    "href": "content/lectures/14-tidymodels-slides.html#model-building-hotels",
    "title": "14-tidymodels",
    "section": "Model building: Hotels",
    "text": "Model building: Hotels\nSpecify model (parsnip)\n\nlr_mod <- \n  logistic_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n\n\nBuild recipe:\n\n\nstep_date() creates predictors for the year, month, and day of the week.\nstep_holiday() generates a set of indicator variables for specific holidays. Although we don’t know where these two hotels are located, we do know that the countries for origin for most stays are based in Europe.\nstep_rm() removes variables; here we’ll use it to remove the original date variable since we no longer want it in the model.\nstep_dummy() converts characters or factors (i.e., nominal variables) into one or more numeric binary model terms for the levels of the original data.\nstep_zv() removes indicator variables that only contain a single unique value (e.g. all zeros). This is important because, for penalized models, the predictors should be centered and scaled.\nstep_normalize() centers and scales numeric variables.\n\n\n\nholidays <- c(\"AllSouls\", \"AshWednesday\", \"ChristmasEve\", \"Easter\", \n              \"ChristmasDay\", \"GoodFriday\", \"NewYearsDay\", \"PalmSunday\")\n\nlr_recipe <- \n  recipe(children ~ ., data = hotel_other) %>% \n  step_date(arrival_date) %>% \n  step_holiday(arrival_date, holidays = holidays) %>% \n  step_rm(arrival_date) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors())"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#create-workflow-fit-the-model",
    "href": "content/lectures/14-tidymodels-slides.html#create-workflow-fit-the-model",
    "title": "14-tidymodels",
    "section": "Create Workflow: fit the model",
    "text": "Create Workflow: fit the model\n\nlr_workflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(lr_recipe)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tuning-set-up",
    "href": "content/lectures/14-tidymodels-slides.html#tuning-set-up",
    "title": "14-tidymodels",
    "section": "Tuning (set up)",
    "text": "Tuning (set up)\n\nlr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))\n\n. . . Lowest penalty values:\n\nlr_reg_grid %>% top_n(-5)\n\n# A tibble: 5 × 1\n   penalty\n     <dbl>\n1 0.0001  \n2 0.000127\n3 0.000161\n4 0.000204\n5 0.000259\n\n\nHighest penalty values:\n\nlr_reg_grid %>% top_n(5)\n\n# A tibble: 5 × 1\n  penalty\n    <dbl>\n1  0.0386\n2  0.0489\n3  0.0621\n4  0.0788\n5  0.1"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#train-and-tune",
    "href": "content/lectures/14-tidymodels-slides.html#train-and-tune",
    "title": "14-tidymodels",
    "section": "Train and Tune",
    "text": "Train and Tune\n(Will take a few seconds to run)\n\nlr_res <- \n  lr_workflow %>% \n  tune_grid(val_set,\n            grid = lr_reg_grid,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(roc_auc))"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#results-penalty",
    "href": "content/lectures/14-tidymodels-slides.html#results-penalty",
    "title": "14-tidymodels",
    "section": "Results: penalty",
    "text": "Results: penalty\n\nlr_res %>% \n  collect_metrics() %>% \n  ggplot(aes(x = penalty, y = mean)) + \n  geom_point() + \n  geom_line() + \n  ylab(\"Area under the ROC Curve\") +\n  scale_x_log10(labels = scales::label_number())\n\n\n\n\nModel is generally better at lower penalty values\nMajority of predictors are important to the model\nROC AUC seems like a reasonable metric for assessing model performance"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#best-models",
    "href": "content/lectures/14-tidymodels-slides.html#best-models",
    "title": "14-tidymodels",
    "section": "Best models",
    "text": "Best models\n\ntop_models <-\n  lr_res %>% \n  show_best(\"roc_auc\", n = 15) %>% \n  arrange(penalty) \n\n\n\ntop_models\n\n# A tibble: 15 × 7\n    penalty .metric .estimator  mean     n std_err .config              \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1 0.000127 roc_auc binary     0.872     1      NA Preprocessor1_Model02\n 2 0.000161 roc_auc binary     0.872     1      NA Preprocessor1_Model03\n 3 0.000204 roc_auc binary     0.873     1      NA Preprocessor1_Model04\n 4 0.000259 roc_auc binary     0.873     1      NA Preprocessor1_Model05\n 5 0.000329 roc_auc binary     0.874     1      NA Preprocessor1_Model06\n 6 0.000418 roc_auc binary     0.874     1      NA Preprocessor1_Model07\n 7 0.000530 roc_auc binary     0.875     1      NA Preprocessor1_Model08\n 8 0.000672 roc_auc binary     0.875     1      NA Preprocessor1_Model09\n 9 0.000853 roc_auc binary     0.876     1      NA Preprocessor1_Model10\n10 0.00108  roc_auc binary     0.876     1      NA Preprocessor1_Model11\n11 0.00137  roc_auc binary     0.876     1      NA Preprocessor1_Model12\n12 0.00174  roc_auc binary     0.876     1      NA Preprocessor1_Model13\n13 0.00221  roc_auc binary     0.876     1      NA Preprocessor1_Model14\n14 0.00281  roc_auc binary     0.875     1      NA Preprocessor1_Model15\n15 0.00356  roc_auc binary     0.873     1      NA Preprocessor1_Model16\n\n\n\n\n\nlikely want to pick a penalty before we see the decrease start (fewest predictors; equally-good performance) - 12th model\n\n\nlr_best <- \n  lr_res %>% \n  collect_metrics() %>% \n  arrange(penalty) %>% \n  slice(12)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#check-performance",
    "href": "content/lectures/14-tidymodels-slides.html#check-performance",
    "title": "14-tidymodels",
    "section": "Check Performance",
    "text": "Check Performance\n\nlr_auc <- \n  lr_res %>% \n  collect_predictions(parameters = lr_best) %>% \n  roc_curve(children, .pred_children) %>% \n  mutate(model = \"Logistic Regression\")\n\nautoplot(lr_auc)\n\n\n\nModel is performing reasonably well…but maybe another model/approach would improve accuracy?\n\n\nThis is where tidymodels shines"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#random-forest-specify-model",
    "href": "content/lectures/14-tidymodels-slides.html#random-forest-specify-model",
    "title": "14-tidymodels",
    "section": "Random Forest: specify Model",
    "text": "Random Forest: specify Model\nDetect possible running in parallel:\n\ncores <- parallel::detectCores()\ncores\n\n[1] 10\n\n\n\n\nrf_mod <- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores) %>% \n  set_mode(\"classification\")\n\nrf_mod\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nEngine-Specific Arguments:\n  num.threads = cores\n\nComputational engine: ranger"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#build-model-new-recipe",
    "href": "content/lectures/14-tidymodels-slides.html#build-model-new-recipe",
    "title": "14-tidymodels",
    "section": "Build model: new `recipe``",
    "text": "Build model: new `recipe``\n\nrf_recipe <- \n  recipe(children ~ ., data = hotel_other) %>% \n  step_date(arrival_date) %>% \n  step_holiday(arrival_date) %>% \n  step_rm(arrival_date)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#add-model-to-workflow",
    "href": "content/lectures/14-tidymodels-slides.html#add-model-to-workflow",
    "title": "14-tidymodels",
    "section": "Add model to workflow",
    "text": "Add model to workflow\n\nrf_workflow <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(rf_recipe)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#fit-the-model",
    "href": "content/lectures/14-tidymodels-slides.html#fit-the-model",
    "title": "14-tidymodels",
    "section": "Fit the model",
    "text": "Fit the model\n(will take longer to run -it’s trying out 25 different random forests, each w/ 1000 trees)\n\nset.seed(345)\nrf_res <- \n  rf_workflow %>% \n  tune_grid(val_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(roc_auc))"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#results-table",
    "href": "content/lectures/14-tidymodels-slides.html#results-table",
    "title": "14-tidymodels",
    "section": "Results (table)",
    "text": "Results (table)\n\nrf_res %>% \n  show_best(metric = \"roc_auc\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     8     7 roc_auc binary     0.926     1      NA Preprocessor1_Model13\n2    12     7 roc_auc binary     0.926     1      NA Preprocessor1_Model01\n3    13     4 roc_auc binary     0.925     1      NA Preprocessor1_Model05\n4     9    12 roc_auc binary     0.924     1      NA Preprocessor1_Model19\n5     6    18 roc_auc binary     0.924     1      NA Preprocessor1_Model24"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#results-viz",
    "href": "content/lectures/14-tidymodels-slides.html#results-viz",
    "title": "14-tidymodels",
    "section": "Results (viz)",
    "text": "Results (viz)\n\nautoplot(rf_res)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#select-best-model",
    "href": "content/lectures/14-tidymodels-slides.html#select-best-model",
    "title": "14-tidymodels",
    "section": "Select (best) model",
    "text": "Select (best) model\n\nrf_best <- \n  rf_res %>% \n  select_best(metric = \"roc_auc\")\n\nrf_best\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     8     7 Preprocessor1_Model13"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#make-predictions",
    "href": "content/lectures/14-tidymodels-slides.html#make-predictions",
    "title": "14-tidymodels",
    "section": "Make predictions",
    "text": "Make predictions\n\nrf_res %>% \n  collect_predictions()\n\n# A tibble: 187,500 × 8\n   id         .pred_children .pred_none  .row  mtry min_n children .config      \n   <chr>               <dbl>      <dbl> <int> <int> <int> <fct>    <chr>        \n 1 validation        0.152        0.848    13    12     7 none     Preprocessor…\n 2 validation        0.0302       0.970    20    12     7 none     Preprocessor…\n 3 validation        0.513        0.487    22    12     7 children Preprocessor…\n 4 validation        0.0103       0.990    23    12     7 none     Preprocessor…\n 5 validation        0.0111       0.989    31    12     7 none     Preprocessor…\n 6 validation        0            1        38    12     7 none     Preprocessor…\n 7 validation        0            1        39    12     7 none     Preprocessor…\n 8 validation        0.00325      0.997    50    12     7 none     Preprocessor…\n 9 validation        0.0241       0.976    54    12     7 none     Preprocessor…\n10 validation        0.0441       0.956    57    12     7 children Preprocessor…\n# ℹ 187,490 more rows\n\n\n\nrf_auc <- \n  rf_res %>% \n  collect_predictions(parameters = rf_best) %>% \n  roc_curve(children, .pred_children) %>% \n  mutate(model = \"Random Forest\")"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#model-performance-comparison",
    "href": "content/lectures/14-tidymodels-slides.html#model-performance-comparison",
    "title": "14-tidymodels",
    "section": "Model Performance Comparison",
    "text": "Model Performance Comparison\n\nbind_rows(rf_auc, lr_auc) %>% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6)"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#model-evaluation-testing-set",
    "href": "content/lectures/14-tidymodels-slides.html#model-evaluation-testing-set",
    "title": "14-tidymodels",
    "section": "Model Evaluation: Testing Set",
    "text": "Model Evaluation: Testing Set\n\nlast_rf_mod <- \n  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% \n  set_engine(\"ranger\", num.threads = cores, importance = \"impurity\") %>% # will allow us to see what was most important in the model\n  set_mode(\"classification\")\n\n# the last workflow\nlast_rf_workflow <- \n  rf_workflow %>% \n  update_model(last_rf_mod)\n\n# the last fit\nset.seed(345)\nlast_rf_fit <- \n  last_rf_workflow %>% \n  last_fit(splits)\n\nlast_rf_fit\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                id             .metrics .notes   .predictions .workflow \n  <list>                <chr>          <list>   <list>   <list>       <list>    \n1 <split [37500/12500]> train/test sp… <tibble> <tibble> <tibble>     <workflow>"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#performance-in-test-set",
    "href": "content/lectures/14-tidymodels-slides.html#performance-in-test-set",
    "title": "14-tidymodels",
    "section": "Performance in test set?",
    "text": "Performance in test set?\n\nlast_rf_fit %>% \n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.946 Preprocessor1_Model1\n2 roc_auc  binary         0.923 Preprocessor1_Model1"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#important-features",
    "href": "content/lectures/14-tidymodels-slides.html#important-features",
    "title": "14-tidymodels",
    "section": "Important features?",
    "text": "Important features?\n\nlast_rf_fit %>% \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 20)\n\n\n\n\n\n\nlast_rf_fit %>% \n  collect_predictions() %>% \n  roc_curve(children, .pred_children) %>% \n  autoplot()"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#our-next-case-study",
    "href": "content/lectures/14-tidymodels-slides.html#our-next-case-study",
    "title": "14-tidymodels",
    "section": "Our next Case study",
    "text": "Our next Case study"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#recap",
    "href": "content/lectures/14-tidymodels-slides.html#recap",
    "title": "14-tidymodels",
    "section": "Recap",
    "text": "Recap\n\nCan you describe the basics of machine learning?\nCan you describe the goals of and general steps in tidymodels?\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/14-tidymodels.html",
    "href": "content/lectures/14-tidymodels.html",
    "title": "14-tidymodels",
    "section": "",
    "text": "Q: I had a question about the presentations for the final projects; since it is due during finals week, is it a live presentation in class or do we submit a video? If it is a live presentation, do we present during our designated final day/time on webreg?\nA: Video submission!\n\n\nQ: I also wanted to mention that the mid/pre-course extra credit surveys doesn’t reflect a change in grade on canvas. (For ex. if i put a 0 or 100 for E.C my grade stays the same).\nA: Correct - I add these in at the end. Canvas can do many things, but it doesn’t handle EC well (from what I can tell).\n\n\nQ: I’m overwhelmed/confused by “the code :’) it’s quite a bit to take in”\nA: Yes! It’s a lot! This is why we have group mates on the case study. I encourage everyone to sit with the code after class and then work through it together as you complete the case study!\n\n\nQ: For oral fluid you mentioned looking more into why there’s that big dip in specificity and that we should look more into that on Friday with eda but would that be slightly guided because I have no idea where to start with that.\nA: I would make some plots that specifically look at the data/numbers there to figure out what could be leading to that drop at that particular time window.\n\n\nQ: Why are specificity graphs so high?  A: Good question - this is generally b/c people who didn’t smoke have values very close to zero across compounds…so they will rarely be above the cutoff, making this very effective at identifying individuals who did not smoke\n\n\nQ: What is the dplyr::select notation, like is it a way to use select from dplyr without librarying first?\nA: Yes!\n\n\nQ: Also separate topic, but do we have information on impairment so we can account for that with recent use?  A: Great question - impairment is very hard to define here. We (the researchers) have data on self-reported high and what the police officers determined, but y’all don’t have that data. So, we’re using knowledge from other studies (see 11-cs01-data notes) to understand what we know on impoairment but only focusing on detecting recent use here.\n\n\nQ: I am unable to locate where to sign up for groups for the final project\nA: This form was just released (sorry for delay). link to survey\n\n\nQ: I think I need more time to digest how the code works together to produce the visuals that we saw.\nA: I agree. I think I could balance and give more time in class…but I will say this is an exercise I want groups to work through together!\n\n\n\n\nDue Dates:\n\nNo class This Th; No Lab this Fri (Happy Thanksgiving!)\nCS01 due Monday 11/27\n\ngroup work survey due Tues 11/28\n\n\n\nNotes:\n\nBe sure you watch the video from last Thursday on Canvas\nAny questions about CS01?\n\n\n\n\n\n\nmachine learning intro\n(re)introduce tidymodels\nworked example: ML in tidymodels\n\n\n\n\n\nThe package itself has some worked examples: https://www.tidymodels.org/start/models/\nThere’s a whole book (written by the developer of tidymodels) that covers the tidymodels package: https://www.tmwr.org/\n\n\n\n\n\n“Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: pre-processing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret. The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do.” - Max Kuhn (tidymodels developer)\n\n\nBenefits:\n\nStandardized workflow/format/notation across different types of machine learning algorithms\nCan easily modify pre-processing, algorithm choice, and hyper-parameter tuning making optimization easy\n\n\n\n\n\nThe main packages (and their roles):\n\n\n\n\n\n\nIn intro stats, you should have learned the central dogma of statistics: we sample from a population\n\n\nThe data from the sample are used to make an inference about the population:\n\n\n\nFor prediction, we have a similar sampling problem:\n\n\n\nBut now we are trying to build a rule that can be used to predict a single observation’s value of some characteristic using characteristics of the other observations.\n\n\n\n\n\nThe goal is to:\nbuild a machine learning algorithm\n\nthat uses features as input\n\n\nand predicts an outcome variable\n\n\nin the situation where we do not know the outcome variable.\n\n\n\n\nTypically, you use data where you have both the input and output data to train a machine learning algorithm.\n\nWhat you need:\n\n\nA data set to train from.\nAn algorithm or set of algorithms you can use to try values of \\(f\\).\nA distance metric \\(d\\) for measuring how close \\(Y\\) is to \\(\\hat{Y}\\).\nA definition of what a “good” distance is.\n\n\n\n\n\n\nHow these packages fit together for carrying out machine learning:\n\n\n\n\n\n\n\n\n\nCan you describe the basics of machine learning?\nCan you describe the goals of and general steps in tidymodels?"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#agenda",
    "href": "content/lectures/00-welcome-slides.html#agenda",
    "title": "00-welcome",
    "section": "Agenda",
    "text": "Agenda\n\nDescribe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-r",
    "href": "content/lectures/00-welcome-slides.html#what-is-r",
    "title": "00-welcome",
    "section": "What is R?",
    "text": "What is R?\n : R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-data-science",
    "href": "content/lectures/00-welcome-slides.html#what-is-data-science",
    "title": "00-welcome",
    "section": "What is data science?",
    "text": "What is data science?\n: Data science is the scientific process of using data to answer interesting questions and/or solve important problems."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#practical-data-science-in-r",
    "href": "content/lectures/00-welcome-slides.html#practical-data-science-in-r",
    "title": "00-welcome",
    "section": "Practical Data Science in R",
    "text": "Practical Data Science in R\n\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#who-am-i",
    "href": "content/lectures/00-welcome-slides.html#who-am-i",
    "title": "00-welcome",
    "section": "Who am I?",
    "text": "Who am I?\nShannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com    MOS 0204     Tu/Th 2-3:20PM (Lab: Fri 3-3:50PM)"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#who-all-is-involved",
    "href": "content/lectures/00-welcome-slides.html#who-all-is-involved",
    "title": "00-welcome",
    "section": "Who all is involved?",
    "text": "Who all is involved?\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11A-12P\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTime TBD\nLocation TBD\n\n\nIAs\nShenova Davis\n\nTime TBD\nLocation TBD"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#course-staff",
    "href": "content/lectures/00-welcome-slides.html#course-staff",
    "title": "00-welcome",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\nKunal Rustagi (TA)\nShenova Davis (IA)"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-this-course",
    "href": "content/lectures/00-welcome-slides.html#what-is-this-course",
    "title": "00-welcome",
    "section": "What is this course?",
    "text": "What is this course?\nEverything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#soi-dont-have-to-know-how-to-program-already",
    "href": "content/lectures/00-welcome-slides.html#soi-dont-have-to-know-how-to-program-already",
    "title": "00-welcome",
    "section": "So…I don’t have to know how to program already?",
    "text": "So…I don’t have to know how to program already?\n\n\n\n\nNope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-general-plan",
    "href": "content/lectures/00-welcome-slides.html#the-general-plan",
    "title": "00-welcome",
    "section": "The General Plan",
    "text": "The General Plan\n\nWeeks 1-4: Learn to program in the tidyverse in R\nWeeks 5-10: Communication, Data Analysis, Statistics, & Case Studies (two Case Studies)\n\n\nNote: This course is back-loaded. But, that’s when group work happens."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-nitty-gritty",
    "href": "content/lectures/00-welcome-slides.html#the-nitty-gritty",
    "title": "00-welcome",
    "section": "The Nitty Gritty",
    "text": "The Nitty Gritty\n\nLectureIn-personWaitlistLab & OHMaterials\n\n\nClass Meetings\n\nInteractive\nLectures & lots of learn-by-doing\nBring your laptop to class every day\n\n\n\nIn-person, synchronous learning\n\nI will be teaching (so long as I’m healthy and have child care) in person.\nLectures and lab will be podcast.\nAttendance will be incentivized using a daily participation survey.\nIf you’re not feeling well, please stay home. I will do the same.\nExam will be take-home.\n\n\n\nThe (Dreaded) Waitlist\n\nCourse enrollment is supposed to be 50 for this course\nThere are 72 people currently enrolled\nI don’t control the waitlist (cogsadvising@ucsd.edu does)\nI’d anticipate our staff adding 3-5 people from the waitlist (but cannot guarantee this)\n\n\n\nLab & Office Hours\n\nOffice hours begin week 1\n\nProf: Tu: 3:30-4:30 (drop-in); W 11-12 (10 min slots; appt.)\n\nLab begins week 1 (next Friday)\n\nit’s not in a computer lab, so you’ll need to bring your own\ndetails about labs covered on Tues and in lab\ntypically labs will be released Monday and due Friday\n\nI will hang out after class today for questions/concerns from students\n\n\n\nCourse Materials\n\nTextbooks are free and available online\nCourse platforms:\n\nWebsite : schedule, policies, due dates, etc.\nGitHub : retrieving assignments, labs, exams, etc.\ndatahub : completing assignments, labs, exams etc.\nCanvas : grades, course-specific links\nPiazza : Q&A"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#diversity-inclusion",
    "href": "content/lectures/00-welcome-slides.html#diversity-inclusion",
    "title": "00-welcome",
    "section": "Diversity & Inclusion:",
    "text": "Diversity & Inclusion:\nGoal: every student be well-served by this course\n\nPhilosophy: The diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding.\n\n\nPlan: Present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture)\n\n\nBut… if I ever fall short or if you ever have suggestions for improvement, please do share with me! There is also an anonymous Google Form if you’re more comfortable there."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#a-new-ish-course",
    "href": "content/lectures/00-welcome-slides.html#a-new-ish-course",
    "title": "00-welcome",
    "section": "A new-ish course!",
    "text": "A new-ish course!\n\nOffered twice previously\nIf something doesn’t make sense, tell me!\nIf you’ve got feedback/suggestions, I’m all ears!\n\n\nChanges since last iteration (based on feedback):\n\nspread out second half\nlikely changing the heaviness of a case study\nadd in communication to public portion\none fewer HW assignments"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-to-get-help",
    "href": "content/lectures/00-welcome-slides.html#how-to-get-help",
    "title": "00-welcome",
    "section": "How to get help",
    "text": "How to get help\n\nLab\nOffice Hours\nPiazza\n\n\nA few (Piazza) guidelines:\n1. No duplicates.\n2. Public posts are best.\n3. Posts should include your question, what you've tried so far, & resources used.\n4. Helping others is encouraged.\n5. No assignment code in public posts.\n6. We're not robots."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-r-community",
    "href": "content/lectures/00-welcome-slides.html#the-r-community",
    "title": "00-welcome",
    "section": " The R Community",
    "text": "The R Community\n\n\n\nR Rollercoaster\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#academic-integrity",
    "href": "content/lectures/00-welcome-slides.html#academic-integrity",
    "title": "00-welcome",
    "section": "Academic integrity",
    "text": "Academic integrity\nDon’t cheat.\n\nTeamwork is allowed, but you should be able to answer “Yes” to each of the following:\n\nCan I explain each piece of code and each analysis carried out in what I’m submitting?\nCould I reproduce this code/analysis on my own?\n\n\n\nThe Internet is a great resource. Cite your sources.\n\n\nTeamwork is not allowed on your midterm. It is open-notes and open-Google/ChatGPT. You cannot discuss the questions on the exam with anyone."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#when-to-can-i-use-chatgptllms",
    "href": "content/lectures/00-welcome-slides.html#when-to-can-i-use-chatgptllms",
    "title": "00-welcome",
    "section": "When To (Can I) Use ChatGPT/LLMs?",
    "text": "When To (Can I) Use ChatGPT/LLMs?\nFor anything in this course."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-to-use-chatgptllms",
    "href": "content/lectures/00-welcome-slides.html#how-to-use-chatgptllms",
    "title": "00-welcome",
    "section": "How To Use ChatGPT/LLMs",
    "text": "How To Use ChatGPT/LLMs\nProbably never first or right away.\n\nTo learn: Think first. Try first. Then use external resources.\n\n\nAlways read/think about/understand the output."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#chatgpt-what-to-avoid",
    "href": "content/lectures/00-welcome-slides.html#chatgpt-what-to-avoid",
    "title": "00-welcome",
    "section": "ChatGPT: What to Avoid",
    "text": "ChatGPT: What to Avoid\n\n\nOver-reliance (thwarts learning)\nHaving to look everything up (wastes time)\nLeaving tasks to the last minute (can lead to bad decisions/academic integrity issues)\nTaking the output without thinking (thwarts learning; limits critical thinking practice)\nUsing it right away for brainstorming ideas (limits ideas generated)"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#course-components",
    "href": "content/lectures/00-welcome-slides.html#course-components",
    "title": "00-welcome",
    "section": "Course components:",
    "text": "Course components:\n\n\nLabs (8): Individual submission; graded on effort\nHomework (3): Individual submission; graded on correctness\nExam (1): Individual completion & submission, take-home midterm\nCase Studies (2): Team submission, technical analysis report\nFinal Project (1) : Team submission, due Tues of finals week"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#grading",
    "href": "content/lectures/00-welcome-slides.html#grading",
    "title": "00-welcome",
    "section": "Grading",
    "text": "Grading\nYour final grade will be comprised of the following:\n\n\n\nAssignment (#)\n% of grade\n\n\n\n\nLabs (8)\n16%\n\n\nHomework (3)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal project* (1)\n17%\n\n\n\n* indicates group submission"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#latemissed-work-policy",
    "href": "content/lectures/00-welcome-slides.html#latemissed-work-policy",
    "title": "00-welcome",
    "section": "Late/missed work policy",
    "text": "Late/missed work policy\n\nHomework and case study projects: accepted up to 3 days (72 hours) after the assigned deadline for a 25% deduction\nNo late deadlines for labs, the exam, or the final project\n\n\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#datahub",
    "href": "content/lectures/00-welcome-slides.html#datahub",
    "title": "00-welcome",
    "section": "Datahub",
    "text": "Datahub\nDatahub is a platform hosted by UCSD that gives students access to computational resources.\nThis means that while you’ll be typing on your keyboard, you’ll be using UCSD’s computers in this class.\nWebsite: https://datahub.ucsd.edu/\n\nLaunch Environment\nWhen working on “stuff” for this course, select the COGS 137 environment.\n ## Datahub Usage\nQ: Do I have to use datahub?\nA: Nope. You could download and install all the packages we use and complete the course locally! However, many packages have already been installed for you on datahub, so it will be a tiny bit more work up front…but you won’t be dependent on the internet/datahub!"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#toolkit",
    "href": "content/lectures/00-welcome-slides.html#toolkit",
    "title": "00-welcome",
    "section": "Toolkit",
    "text": "Toolkit\n\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub\nThe Internet (Google/ChatGPT/etc.)"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-and-rstudio",
    "href": "content/lectures/00-welcome-slides.html#r-and-rstudio",
    "title": "00-welcome",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR/RStudioTourTryR packages\n\n\nR & RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integreated development environment, IDE)\n\n\n\n\n[DEMO]\n\nConcepts introduced:\n\nConsole\nUsing R as a calculator\nEnvironment\nLoading and viewing a data frame\nAccessing a variable in a data frame\nR functions\n\n\n\nYour Turn\n\nLogin to datahub\nCarry out a mathematical operation in the console\nView the airquality dataframe\nAccess a column from the airquality dataframe\nCalculate the median for one of the numeric columns\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\nPackages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data 1\nAs of Sept 2023, there are ~19,941 R packages available on CRAN (the Comprehensive R Archive Network)2\nWe’re going to work with a small (but important) subset of these!\n\n\n\n\nWickham and Bryan, R PackagesCRAN contributed packages"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-the-tidyverse",
    "href": "content/lectures/00-welcome-slides.html#what-is-the-tidyverse",
    "title": "00-welcome",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\n\n\n\n\n\ntidyverse.org\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying philosophy and a common syntax."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#rstudio-projects3",
    "href": "content/lectures/00-welcome-slides.html#rstudio-projects3",
    "title": "00-welcome",
    "section": "RStudio Projects1",
    "text": "RStudio Projects1\n\nBuilt-in functionality to keep all files for a single project organized\n\nRStudio Projects Documentation"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-markdown",
    "href": "content/lectures/00-welcome-slides.html#r-markdown",
    "title": "00-welcome",
    "section": "R Markdown",
    "text": "R Markdown\n\nFully reproducible reports – each time you knit, the document is executed from top to bottom\nSimple markdown syntax for text\nCode goes in chunks, defined by three backticks, narrative goes outside of chunks"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-markdown-tips",
    "href": "content/lectures/00-welcome-slides.html#r-markdown-tips",
    "title": "00-welcome",
    "section": "R Markdown tips",
    "text": "R Markdown tips\n\nKeep the R Markdown cheat sheet and Markdown Quick Reference (Help -> Markdown Quick Reference) handy, we’ll refer to it often as the course progresses\nThe workspace of your R Markdown document is separate from the Console\n\n\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-will-we-use-r-markdown",
    "href": "content/lectures/00-welcome-slides.html#how-will-we-use-r-markdown",
    "title": "00-welcome",
    "section": "How will we use R Markdown?",
    "text": "How will we use R Markdown?\n\nEvery lab / midterm / project / homework / notes / etc. is an R Markdown document\nYou’ll always have a template R Markdown document to start with\nThe amount of scaffolding in the template will decrease over the quarter"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#collaboration-git-github",
    "href": "content/lectures/00-welcome-slides.html#collaboration-git-github",
    "title": "00-welcome",
    "section": "Collaboration: Git & GitHub",
    "text": "Collaboration: Git & GitHub\n\nThe statistical programming language we’ll use is R\nThe software we use to interface with R is RStudio\nBut how do I get you the course materials that you can build on for your assignments?\n\nI’m not going to email you documents, that would be a mess!"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#version-control",
    "href": "content/lectures/00-welcome-slides.html#version-control",
    "title": "00-welcome",
    "section": "Version control",
    "text": "Version control\n\nWe introduced GitHub as a platform for collaboration\nBut it’s much more than that…\nIt’s actually designed for version control"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#versioning",
    "href": "content/lectures/00-welcome-slides.html#versioning",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\n\nLego versions"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#versioning-1",
    "href": "content/lectures/00-welcome-slides.html#versioning-1",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\nwith human readable messages\n\nLego versions with commit messages"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#why-do-we-need-version-control",
    "href": "content/lectures/00-welcome-slides.html#why-do-we-need-version-control",
    "title": "00-welcome",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\nPhD Comics"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#git-and-github-tips",
    "href": "content/lectures/00-welcome-slides.html#git-and-github-tips",
    "title": "00-welcome",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\nGit is a version control system – like “Track Changes” feature Google Docs…but optimized for code. GitHub is the home for your Git-based projects on the internet – like Drive with additional features for code.\n\n\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\n\n\nWe will be doing Git things and interfacing with GitHub through RStudio, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\n\n\n\n\nResource: happygitwithr.com: book for working with git in R; Some content is beyond the scope of this course, but it’s a good resource"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#lets-take-a-tour-git-github",
    "href": "content/lectures/00-welcome-slides.html#lets-take-a-tour-git-github",
    "title": "00-welcome",
    "section": "Let’s take a tour – Git / GitHub",
    "text": "Let’s take a tour – Git / GitHub\nWe’ll cover this time permitting, you’ll see it again in lab this week\nConcepts introduced:\n\nConnect an R project to Github repository\nWorking with a local and remote repository\nCommitting, Pushing and Pulling\n\nThere is a bit more of GitHub that we’ll use in this class, but for today this is enough."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#documentation",
    "href": "content/lectures/00-welcome-slides.html#documentation",
    "title": "00-welcome",
    "section": "Documentation",
    "text": "Documentation\nConsider ggplot2 (a package we’ll learn a lot)\n\n\nOfficial documentation (CRAN): https://cran.r-project.org/web/packages/ggplot2/index.html\nCode (Github): https://github.com/tidyverse/ggplot2\nDocumentation: https://ggplot2.tidyverse.org/reference/index.html\nSpecific Function: https://ggplot2.tidyverse.org/reference/geom_point.html"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#chatgpt-what-it-could-look-like",
    "href": "content/lectures/00-welcome-slides.html#chatgpt-what-it-could-look-like",
    "title": "00-welcome",
    "section": "ChatGPT: What it could look like",
    "text": "ChatGPT: What it could look like\nImagine: You’ve been asked to carry out a number of wrangling operations on a dataset and make a plot…\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#additional-help",
    "href": "content/lectures/00-welcome-slides.html#additional-help",
    "title": "00-welcome",
    "section": "Additional help",
    "text": "Additional help\n\nclassmates\ncourse staff (OH, Piazza, class, lab)"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#recap",
    "href": "content/lectures/00-welcome-slides.html#recap",
    "title": "00-welcome",
    "section": "Recap",
    "text": "Recap\nCan you answer these questions?\n\nWhat is R vs RStudio?\nWhat are RStudio Projects?\nWhat is version control, and why do we care?\nWhat is git vs GitHub (and do I need to care)?"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#additional-git-resources",
    "href": "content/lectures/00-welcome-slides.html#additional-git-resources",
    "title": "00-welcome",
    "section": "Additional git Resources",
    "text": "Additional git Resources\nVersion Control (git and GitHub):\n\nGetting Started with git\nGitHub Guide\nGitHub Desktop App Tutorial\nGit Command Line Resource\nUsing git from the command line\n\nInstalling and using git (Part 1), by COGS 108 TA Ganesh (youtube, 22min tutorial)\nmerge conflicts and branching (Part 2), by IA Shubham Kulkarni (youtube, 8min tutorial)\n\nUsing git with GitHub Desktop, by COGS 108 TA Sidharth Suresh (youtube, 13min tutorial)\nGIT & GITHUB TUTORIAL, from edureka!\n\nwith notes from COGS 18/108 TA Holly(Yueying) Dong"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#slides-to-pdf",
    "href": "content/lectures/00-welcome-slides.html#slides-to-pdf",
    "title": "00-welcome",
    "section": "Slides to PDF",
    "text": "Slides to PDF\n\nToggle into Print View using the Esc key (or using the Navigation Menu)\nOpen the in-browser print dialog (CTRL/CMD+P).\nChange the Destination setting to Save as PDF.\nChange the Layout to Landscape.\nChange the Margins to None.\nEnable the Background graphics option.\nClick Save 🎉\n\n\n\nInstructions from quarto documentation"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster <- read_sheet('10kG09t5Uvjy2zLt4sToHvveBRnqYXTwfaAhPpgEFr3s')\n\nggplot(roster, aes(x = College)) +\n  geom_bar() +\n  labs(title = \"COGS 137\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\nNote: This code will not run for you because you don’t have access to the roster for this course."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class-1",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class-1",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  mutate(major = substr(Major, 1, 2)) |>\n  ggplot(aes(fct_infreq(major))) + \n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Major\") +\n  theme_bw(base_size = 12) + \n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class-2",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class-2",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  ggplot(aes(fct_relevel(Level, \"SO\", \"JR\", \"SR\"))) +\n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Level\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\nWarning: 1 unknown level in `f`: SO\n1 unknown level in `f`: SO"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#id-like-to-know-more",
    "href": "content/lectures/00-welcome-slides.html#id-like-to-know-more",
    "title": "00-welcome",
    "section": "I’d like to know more!",
    "text": "I’d like to know more!\n(required)Student Survey - complete by Tuesday at 11:59 PM.\nThis is required and completion will be used for CAA/#finaid. DO complete this even if you’re on the waitlist, please.\n\n(optional) Daily Post-Lecture Feedback\n\nopportunity to reflect on learning\nopportunity to ask questions (I will read and answer these.)\nopportunity for extra credit on final project\n\n\n\n\n\nhttps://cogs137.github.io/website/\n\n\n\nNote: Links to both surveys are also on Canvas. I will try to remind you at the end of lecture, but I’ll probably forget. Feel free to remind me/one another!"
  },
  {
    "objectID": "content/lectures/00-welcome.html",
    "href": "content/lectures/00-welcome.html",
    "title": "00-welcome",
    "section": "",
    "text": "Practical Data Science in R\nPlease take one green sticky and one pink sticky as they come around. If you’re able, try and save these. We’ll use them most classes. (But, I’ll always have extra!)\n\n\n\n\n\n\n\n\nDescribe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub\n\n\n\n\n : R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data.\n\n\n\n: Data science is the scientific process of using data to answer interesting questions and/or solve important problems.\n\n\n\n\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports\n\n\n\n\n\nShannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com    MOS 0204     Tu/Th 2-3:20PM (Lab: Fri 3-3:50PM)\n\n\n\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11A-12P\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTime TBD\nLocation TBD\n\n\nIAs\nShenova Davis\n\nTime TBD\nLocation TBD\n\n\n\n\n\n\n\n\n\nKunal Rustagi (TA)\nShenova Davis (IA)\n\n\n\n\n\n\n\n\n\n\n\n\nEverything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!\n\n\n\n\n\n\n\n\n\nNope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-general-plan",
    "href": "content/lectures/00-welcome.html#the-general-plan",
    "title": "00-welcome",
    "section": "The General Plan",
    "text": "The General Plan\n\nWeeks 1-4: Learn to program in the tidyverse in R\nWeeks 5-10: Communication, Data Analysis, Statistics, & Case Studies (two Case Studies)\n\n\nNote: This course is back-loaded. But, that’s when group work happens."
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-nitty-gritty",
    "href": "content/lectures/00-welcome.html#the-nitty-gritty",
    "title": "00-welcome",
    "section": "The Nitty Gritty",
    "text": "The Nitty Gritty\n\nLectureIn-personWaitlistLab & OHMaterials\n\n\nClass Meetings\n\nInteractive\nLectures & lots of learn-by-doing\nBring your laptop to class every day\n\n\n\nIn-person, synchronous learning\n\nI will be teaching (so long as I’m healthy and have child care) in person.\nLectures and lab will be podcast.\nAttendance will be incentivized using a daily participation survey.\nIf you’re not feeling well, please stay home. I will do the same.\nExam will be take-home.\n\n\n\nThe (Dreaded) Waitlist\n\nCourse enrollment is supposed to be 50 for this course\nThere are 72 people currently enrolled\nI don’t control the waitlist (cogsadvising@ucsd.edu does)\nI’d anticipate our staff adding 3-5 people from the waitlist (but cannot guarantee this)\n\n\n\nLab & Office Hours\n\nOffice hours begin week 1\n\nProf: Tu: 3:30-4:30 (drop-in); W 11-12 (10 min slots; appt.)\n\nLab begins week 1 (next Friday)\n\nit’s not in a computer lab, so you’ll need to bring your own\ndetails about labs covered on Tues and in lab\ntypically labs will be released Monday and due Friday\n\nI will hang out after class today for questions/concerns from students\n\n\n\nCourse Materials\n\nTextbooks are free and available online\nCourse platforms:\n\nWebsite : schedule, policies, due dates, etc.\nGitHub : retrieving assignments, labs, exams, etc.\ndatahub : completing assignments, labs, exams etc.\nCanvas : grades, course-specific links\nPiazza : Q&A"
  },
  {
    "objectID": "content/lectures/00-welcome.html#diversity-inclusion",
    "href": "content/lectures/00-welcome.html#diversity-inclusion",
    "title": "00-welcome",
    "section": "Diversity & Inclusion:",
    "text": "Diversity & Inclusion:\nGoal: every student be well-served by this course\n\nPhilosophy: The diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding.\n\n\nPlan: Present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture)\n\n\nBut… if I ever fall short or if you ever have suggestions for improvement, please do share with me! There is also an anonymous Google Form if you’re more comfortable there."
  },
  {
    "objectID": "content/lectures/00-welcome.html#a-new-ish-course",
    "href": "content/lectures/00-welcome.html#a-new-ish-course",
    "title": "00-welcome",
    "section": "A new-ish course!",
    "text": "A new-ish course!\n\nOffered twice previously\nIf something doesn’t make sense, tell me!\nIf you’ve got feedback/suggestions, I’m all ears!\n\n\nChanges since last iteration (based on feedback):\n\nspread out second half\nlikely changing the heaviness of a case study\nadd in communication to public portion\none fewer HW assignments"
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-to-get-help",
    "href": "content/lectures/00-welcome.html#how-to-get-help",
    "title": "00-welcome",
    "section": "How to get help",
    "text": "How to get help\n\nLab\nOffice Hours\nPiazza\n\n\nA few (Piazza) guidelines:\n1. No duplicates.\n2. Public posts are best.\n3. Posts should include your question, what you've tried so far, & resources used.\n4. Helping others is encouraged.\n5. No assignment code in public posts.\n6. We're not robots."
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-r-community",
    "href": "content/lectures/00-welcome.html#the-r-community",
    "title": "00-welcome",
    "section": " The R Community",
    "text": "The R Community\n\n\n\nR Rollercoaster\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome.html#academic-integrity",
    "href": "content/lectures/00-welcome.html#academic-integrity",
    "title": "00-welcome",
    "section": "Academic integrity",
    "text": "Academic integrity\nDon’t cheat.\n\nTeamwork is allowed, but you should be able to answer “Yes” to each of the following:\n\nCan I explain each piece of code and each analysis carried out in what I’m submitting?\nCould I reproduce this code/analysis on my own?\n\n\n\nThe Internet is a great resource. Cite your sources.\n\n\nTeamwork is not allowed on your midterm. It is open-notes and open-Google/ChatGPT. You cannot discuss the questions on the exam with anyone."
  },
  {
    "objectID": "content/lectures/00-welcome.html#when-to-can-i-use-chatgptllms",
    "href": "content/lectures/00-welcome.html#when-to-can-i-use-chatgptllms",
    "title": "00-welcome",
    "section": "When To (Can I) Use ChatGPT/LLMs?",
    "text": "When To (Can I) Use ChatGPT/LLMs?\nFor anything in this course."
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-to-use-chatgptllms",
    "href": "content/lectures/00-welcome.html#how-to-use-chatgptllms",
    "title": "00-welcome",
    "section": "How To Use ChatGPT/LLMs",
    "text": "How To Use ChatGPT/LLMs\nProbably never first or right away.\n\nTo learn: Think first. Try first. Then use external resources.\n\n\nAlways read/think about/understand the output."
  },
  {
    "objectID": "content/lectures/00-welcome.html#chatgpt-what-to-avoid",
    "href": "content/lectures/00-welcome.html#chatgpt-what-to-avoid",
    "title": "00-welcome",
    "section": "ChatGPT: What to Avoid",
    "text": "ChatGPT: What to Avoid\n\n\nOver-reliance (thwarts learning)\nHaving to look everything up (wastes time)\nLeaving tasks to the last minute (can lead to bad decisions/academic integrity issues)\nTaking the output without thinking (thwarts learning; limits critical thinking practice)\nUsing it right away for brainstorming ideas (limits ideas generated)"
  },
  {
    "objectID": "content/lectures/00-welcome.html#course-components",
    "href": "content/lectures/00-welcome.html#course-components",
    "title": "00-welcome",
    "section": "Course components:",
    "text": "Course components:\n\n\nLabs (8): Individual submission; graded on effort\nHomework (3): Individual submission; graded on correctness\nExam (1): Individual completion & submission, take-home midterm\nCase Studies (2): Team submission, technical analysis report\nFinal Project (1) : Team submission, due Tues of finals week"
  },
  {
    "objectID": "content/lectures/00-welcome.html#grading",
    "href": "content/lectures/00-welcome.html#grading",
    "title": "00-welcome",
    "section": "Grading",
    "text": "Grading\nYour final grade will be comprised of the following:\n\n\n\nAssignment (#)\n% of grade\n\n\n\n\nLabs (8)\n16%\n\n\nHomework (3)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal project* (1)\n17%\n\n\n\n* indicates group submission"
  },
  {
    "objectID": "content/lectures/00-welcome.html#latemissed-work-policy",
    "href": "content/lectures/00-welcome.html#latemissed-work-policy",
    "title": "00-welcome",
    "section": "Late/missed work policy",
    "text": "Late/missed work policy\n\nHomework and case study projects: accepted up to 3 days (72 hours) after the assigned deadline for a 25% deduction\nNo late deadlines for labs, the exam, or the final project\n\n\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter."
  },
  {
    "objectID": "content/lectures/00-welcome.html#datahub",
    "href": "content/lectures/00-welcome.html#datahub",
    "title": "00-welcome",
    "section": "Datahub",
    "text": "Datahub\nDatahub is a platform hosted by UCSD that gives students access to computational resources.\nThis means that while you’ll be typing on your keyboard, you’ll be using UCSD’s computers in this class.\nWebsite: https://datahub.ucsd.edu/\n\nLaunch Environment\nWhen working on “stuff” for this course, select the COGS 137 environment.\n ## Datahub Usage\nQ: Do I have to use datahub?\nA: Nope. You could download and install all the packages we use and complete the course locally! However, many packages have already been installed for you on datahub, so it will be a tiny bit more work up front…but you won’t be dependent on the internet/datahub!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#toolkit",
    "href": "content/lectures/00-welcome.html#toolkit",
    "title": "00-welcome",
    "section": "Toolkit",
    "text": "Toolkit\n\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub\nThe Internet (Google/ChatGPT/etc.)"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-and-rstudio",
    "href": "content/lectures/00-welcome.html#r-and-rstudio",
    "title": "00-welcome",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR/RStudioTourTryR packages\n\n\nR & RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integreated development environment, IDE)\n\n\n\n\n[DEMO]\n\nConcepts introduced:\n\nConsole\nUsing R as a calculator\nEnvironment\nLoading and viewing a data frame\nAccessing a variable in a data frame\nR functions\n\n\n\nYour Turn\n\nLogin to datahub\nCarry out a mathematical operation in the console\nView the airquality dataframe\nAccess a column from the airquality dataframe\nCalculate the median for one of the numeric columns\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\nPackages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data 1\nAs of Sept 2023, there are ~19,941 R packages available on CRAN (the Comprehensive R Archive Network)2\nWe’re going to work with a small (but important) subset of these!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "href": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "title": "00-welcome",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\n\n\n\n\n\ntidyverse.org\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying philosophy and a common syntax."
  },
  {
    "objectID": "content/lectures/00-welcome.html#rstudio-projects3",
    "href": "content/lectures/00-welcome.html#rstudio-projects3",
    "title": "00-welcome",
    "section": "RStudio Projects3",
    "text": "RStudio Projects3\n\nBuilt-in functionality to keep all files for a single project organized"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown",
    "href": "content/lectures/00-welcome.html#r-markdown",
    "title": "00-welcome",
    "section": "R Markdown",
    "text": "R Markdown\n\nFully reproducible reports – each time you knit, the document is executed from top to bottom\nSimple markdown syntax for text\nCode goes in chunks, defined by three backticks, narrative goes outside of chunks"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown-tips",
    "href": "content/lectures/00-welcome.html#r-markdown-tips",
    "title": "00-welcome",
    "section": "R Markdown tips",
    "text": "R Markdown tips\n\nKeep the R Markdown cheat sheet and Markdown Quick Reference (Help -> Markdown Quick Reference) handy, we’ll refer to it often as the course progresses\nThe workspace of your R Markdown document is separate from the Console\n\n\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "href": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "title": "00-welcome",
    "section": "How will we use R Markdown?",
    "text": "How will we use R Markdown?\n\nEvery lab / midterm / project / homework / notes / etc. is an R Markdown document\nYou’ll always have a template R Markdown document to start with\nThe amount of scaffolding in the template will decrease over the quarter"
  },
  {
    "objectID": "content/lectures/00-welcome.html#collaboration-git-github",
    "href": "content/lectures/00-welcome.html#collaboration-git-github",
    "title": "00-welcome",
    "section": "Collaboration: Git & GitHub",
    "text": "Collaboration: Git & GitHub\n\nThe statistical programming language we’ll use is R\nThe software we use to interface with R is RStudio\nBut how do I get you the course materials that you can build on for your assignments?\n\nI’m not going to email you documents, that would be a mess!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#version-control",
    "href": "content/lectures/00-welcome.html#version-control",
    "title": "00-welcome",
    "section": "Version control",
    "text": "Version control\n\nWe introduced GitHub as a platform for collaboration\nBut it’s much more than that…\nIt’s actually designed for version control"
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning",
    "href": "content/lectures/00-welcome.html#versioning",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\n\n\n\nLego versions"
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning-1",
    "href": "content/lectures/00-welcome.html#versioning-1",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\nwith human readable messages\n\n\n\nLego versions with commit messages"
  },
  {
    "objectID": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "href": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "title": "00-welcome",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\nPhD Comics"
  },
  {
    "objectID": "content/lectures/00-welcome.html#git-and-github-tips",
    "href": "content/lectures/00-welcome.html#git-and-github-tips",
    "title": "00-welcome",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\nGit is a version control system – like “Track Changes” feature Google Docs…but optimized for code. GitHub is the home for your Git-based projects on the internet – like Drive with additional features for code.\n\n\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\n\n\nWe will be doing Git things and interfacing with GitHub through RStudio, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\n\n\n\nResource: happygitwithr.com: book for working with git in R; Some content is beyond the scope of this course, but it’s a good resource"
  },
  {
    "objectID": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "href": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "title": "00-welcome",
    "section": "Let’s take a tour – Git / GitHub",
    "text": "Let’s take a tour – Git / GitHub\nWe’ll cover this time permitting, you’ll see it again in lab this week\nConcepts introduced:\n\nConnect an R project to Github repository\nWorking with a local and remote repository\nCommitting, Pushing and Pulling\n\nThere is a bit more of GitHub that we’ll use in this class, but for today this is enough."
  },
  {
    "objectID": "content/lectures/00-welcome.html#documentation",
    "href": "content/lectures/00-welcome.html#documentation",
    "title": "00-welcome",
    "section": "Documentation",
    "text": "Documentation\nConsider ggplot2 (a package we’ll learn a lot)\n\n\nOfficial documentation (CRAN): https://cran.r-project.org/web/packages/ggplot2/index.html\nCode (Github): https://github.com/tidyverse/ggplot2\nDocumentation: https://ggplot2.tidyverse.org/reference/index.html\nSpecific Function: https://ggplot2.tidyverse.org/reference/geom_point.html"
  },
  {
    "objectID": "content/lectures/00-welcome.html#chatgpt-what-it-could-look-like",
    "href": "content/lectures/00-welcome.html#chatgpt-what-it-could-look-like",
    "title": "00-welcome",
    "section": "ChatGPT: What it could look like",
    "text": "ChatGPT: What it could look like\nImagine: You’ve been asked to carry out a number of wrangling operations on a dataset and make a plot…\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome.html#additional-help",
    "href": "content/lectures/00-welcome.html#additional-help",
    "title": "00-welcome",
    "section": "Additional help",
    "text": "Additional help\n\nclassmates\ncourse staff (OH, Piazza, class, lab)"
  },
  {
    "objectID": "content/lectures/00-welcome.html#recap",
    "href": "content/lectures/00-welcome.html#recap",
    "title": "00-welcome",
    "section": "Recap",
    "text": "Recap\nCan you answer these questions?\n\nWhat is R vs RStudio?\nWhat are RStudio Projects?\nWhat is version control, and why do we care?\nWhat is git vs GitHub (and do I need to care)?"
  },
  {
    "objectID": "content/lectures/00-welcome.html#additional-git-resources",
    "href": "content/lectures/00-welcome.html#additional-git-resources",
    "title": "00-welcome",
    "section": "Additional git Resources",
    "text": "Additional git Resources\n\nVersion Control (git and GitHub):\n\nGetting Started with git\nGitHub Guide\nGitHub Desktop App Tutorial\nGit Command Line Resource\nUsing git from the command line\n\nInstalling and using git (Part 1), by COGS 108 TA Ganesh (youtube, 22min tutorial)\nmerge conflicts and branching (Part 2), by IA Shubham Kulkarni (youtube, 8min tutorial)\n\nUsing git with GitHub Desktop, by COGS 108 TA Sidharth Suresh (youtube, 13min tutorial)\nGIT & GITHUB TUTORIAL, from edureka!\n\nwith notes from COGS 18/108 TA Holly(Yueying) Dong"
  },
  {
    "objectID": "content/lectures/00-welcome.html#slides-to-pdf",
    "href": "content/lectures/00-welcome.html#slides-to-pdf",
    "title": "00-welcome",
    "section": "Slides to PDF",
    "text": "Slides to PDF\n\nToggle into Print View using the Esc key (or using the Navigation Menu)\nOpen the in-browser print dialog (CTRL/CMD+P).\nChange the Destination setting to Save as PDF.\nChange the Layout to Landscape.\nChange the Margins to None.\nEnable the Background graphics option.\nClick Save 🎉\n\n\n\nInstructions from quarto documentation"
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class",
    "href": "content/lectures/00-welcome.html#whos-in-this-class",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster <- read_sheet('10kG09t5Uvjy2zLt4sToHvveBRnqYXTwfaAhPpgEFr3s')\n\nggplot(roster, aes(x = College)) +\n  geom_bar() +\n  labs(title = \"COGS 137\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\nNote: This code will not run for you because you don’t have access to the roster for this course."
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  mutate(major = substr(Major, 1, 2)) |>\n  ggplot(aes(fct_infreq(major))) + \n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Major\") +\n  theme_bw(base_size = 12) + \n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  ggplot(aes(fct_relevel(Level, \"SO\", \"JR\", \"SR\"))) +\n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Level\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\nWarning: 1 unknown level in `f`: SO\n1 unknown level in `f`: SO"
  },
  {
    "objectID": "content/lectures/00-welcome.html#id-like-to-know-more",
    "href": "content/lectures/00-welcome.html#id-like-to-know-more",
    "title": "00-welcome",
    "section": "I’d like to know more!",
    "text": "I’d like to know more!\n(required)Student Survey - complete by Tuesday at 11:59 PM.\nThis is required and completion will be used for CAA/#finaid. DO complete this even if you’re on the waitlist, please.\n\n(optional) Daily Post-Lecture Feedback\n\nopportunity to reflect on learning\nopportunity to ask questions (I will read and answer these.)\nopportunity for extra credit on final project\n\n\n\n\n\nNote: Links to both surveys are also on Canvas. I will try to remind you at the end of lecture, but I’ll probably forget. Feel free to remind me/one another!"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#qa",
    "href": "content/lectures/07-linear-models-slides.html#qa",
    "title": "07-linear-models",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I was just wondering if you could still provide those videos you talked about with like syntax stuff so that we can follow along. I would also appreciate if it was really through with like a walk through of how you got to each point\nA: I like this suggestion, and I’ll try to make these. No promises though. Just have to find magical time in the schedule to make them. Note that lectures will be the more detailed walk through with more time on your own to figure it out (rather than detailed walk through outside of class), so you will get the information…the time/location will just differ. But, I’ll try to make some supplemental videos for those interested.\n\n\nQ: Had a question about the modeling and the last bit showing how decreasing the alpha showed greater clustering at the bottom left corner. If the bottom left corner looks like 0 height and 0 width, how does that translate into the dimensions of an actual painting?\nA: Good observation. The follow-up to this is…are there any paintings with zero width or zero height? And, if you dig in the data (i.e. min(pp$Height_in, na.rm=TRUE)), you’ll see that there are some very small paintings, but that none are zero.\n\n\nQ: If we want to built our own model, can we plot them with ggplot2?\nA: Yup! This post starts to get at that. It does so for a linear model, but the logic follows for other models.\n\n\nQ: For the paintings dataset, how could we perform EDA on specific subject matter, like seeing how many portraits include Jesus as part of the subject?\nA: Love this question. There is a whole field of natural language processing that would have sophisticated ways to analyze this. A simple first pass would be to, for example, filter for paintings that include “Jesus” in the subject column.\n\n\nQ: I think the segmented bar plots based on proportion seem difficult to read. I’m not sure why we should be using this instead of the stacked plots?\nA: Grouped bar blots are typically most quickly understood. Proportion stacked plots are then easiest to understand proportion across categories. Stacked plots of raw numbers take longer (for most) to understand and thus are often avoided, but like all viz, it depends on context and audience.\n\n\nQ: In the last lecture, we talked about how segmented bar plots might not be the ideal choice for data visualization, but we still demonstrated them in today’s lecture. So, specifically in what cases should we choose segmented bar plots as a data analysis tool?\nA: They can be helpful when the audience is familiar with them, but typically are most helpful when you want to display relative proportions rather than counts"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#course-announcements",
    "href": "content/lectures/07-linear-models-slides.html#course-announcements",
    "title": "07-linear-models",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 04 due Friday\n\nModel Interpretations\nText, code, & viz all matter\n\nLecture Participation survey “due” after class\nHW02 due Monday (10/30; 11:59 PM)\ndiscuss displaying image in Markdown"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#agenda",
    "href": "content/lectures/07-linear-models-slides.html#agenda",
    "title": "07-linear-models",
    "section": "Agenda",
    "text": "Agenda\n\nLinear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & >2 levels)\nresiduals\ndata transformations"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#suggested-reading",
    "href": "content/lectures/07-linear-models-slides.html#suggested-reading",
    "title": "07-linear-models",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#data-paris-paintings",
    "href": "content/lectures/07-linear-models-slides.html#data-paris-paintings",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nNumber of observations: 3393\nNumber of variables: 61"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#goal-predict-height-from-width",
    "href": "content/lectures/07-linear-models-slides.html#goal-predict-height-from-width",
    "title": "07-linear-models",
    "section": "Goal: Predict height from width",
    "text": "Goal: Predict height from width\n\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#tidymodels-1",
    "href": "content/lectures/07-linear-models-slides.html#tidymodels-1",
    "title": "07-linear-models",
    "section": "tidymodels",
    "text": "tidymodels\n\nNOT a core tidyverse package\nfollows the structure of a tidyverse package\n\n\n\n# should already be installed for you on datahub\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-1-specify-model",
    "href": "content/lectures/07-linear-models-slides.html#step-1-specify-model",
    "title": "07-linear-models",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-2-set-model-fitting-engine",
    "href": "content/lectures/07-linear-models-slides.html#step-2-set-model-fitting-engine",
    "title": "07-linear-models",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |>\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-3-fit-model-estimate-parameters",
    "href": "content/lectures/07-linear-models-slides.html#step-3-fit-model-estimate-parameters",
    "title": "07-linear-models",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\n… using formula syntax\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#a-closer-look-at-model-output",
    "href": "content/lectures/07-linear-models-slides.html#a-closer-look-at-model-output",
    "title": "07-linear-models",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n\n\n\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#a-tidy-look-at-model-output",
    "href": "content/lectures/07-linear-models-slides.html#a-tidy-look-at-model-output",
    "title": "07-linear-models",
    "section": "A tidy look at model output",
    "text": "A tidy look at model output\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n\n\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#slope-and-intercept",
    "href": "content/lectures/07-linear-models-slides.html#slope-and-intercept",
    "title": "07-linear-models",
    "section": "Slope and intercept",
    "text": "Slope and intercept\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]\n\n\nSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n\n\n\nIntercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#correlation-does-not-imply-causation",
    "href": "content/lectures/07-linear-models-slides.html#correlation-does-not-imply-causation",
    "title": "07-linear-models",
    "section": "Correlation does not imply causation",
    "text": "Correlation does not imply causation\nRemember this when interpreting model coefficients\n\n\n\n\n\n\n\nSource: XKCD, Cell phones"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#linear-model-with-a-single-predictor",
    "href": "content/lectures/07-linear-models-slides.html#linear-model-with-a-single-predictor",
    "title": "07-linear-models",
    "section": "Linear model with a single predictor",
    "text": "Linear model with a single predictor\n\nWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\n\n\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n\n\nTough luck, you can’t have them…\n\n\n\n\nSo we use sample statistics to estimate them:\n\n\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#least-squares-regression",
    "href": "content/lectures/07-linear-models-slides.html#least-squares-regression",
    "title": "07-linear-models",
    "section": "Least squares regression",
    "text": "Least squares regression\n\nThe regression line minimizes the sum of squared residuals.\n\n\n\nIf \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\)."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals",
    "title": "07-linear-models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.-1",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.-1",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#properties-of-least-squares-regression",
    "href": "content/lectures/07-linear-models-slides.html#properties-of-least-squares-regression",
    "title": "07-linear-models",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\n\n\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\nThe slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\n\n\n\n\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\n\n\n\n\nThe residuals and \\(x\\) values are uncorrelated"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 2 levels",
    "text": "Categorical predictor with 2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   <chr>         <dbl>    <dbl>\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# ℹ 3,373 more rows\n\n\n\n\nlandsALL = 0: No landscape features\nlandsALL = 1: Some landscape features"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#height-landscape-features",
    "href": "content/lectures/07-linear-models-slides.html#height-landscape-features",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\nm_ht_lands <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |> tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#height-landscape-features-1",
    "href": "content/lectures/07-linear-models-slides.html#height-landscape-features-1",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\nSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n\nCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\n\nIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels-1",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels-1",
    "title": "07-linear-models",
    "section": "Categorical predictor with >2 levels",
    "text": "Categorical predictor with >2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows\n\n\n\n\nschool from which painting came (details in a few slides)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#relationship-between-height-and-school",
    "href": "content/lectures/07-linear-models-slides.html#relationship-between-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship between height and school",
    "text": "Relationship between height and school\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ school_pntg, data = pp) |>\n  tidy()\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#dummy-variables",
    "href": "content/lectures/07-linear-models-slides.html#dummy-variables",
    "title": "07-linear-models",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nEach coefficient describes the expected difference between heights in that particular school compared to the baseline level"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-3-levels",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-3-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 3+ levels",
    "text": "Categorical predictor with 3+ levels\n\n\n\n\n\n\n\nschool_pntg\nD_FL\nF\nG\nI\nS\nX\n\n\n\n\nA\n0\n0\n0\n0\n0\n0\n\n\nD/FL\n1\n0\n0\n0\n0\n0\n\n\nF\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n1\n0\n0\n0\n\n\nI\n0\n0\n0\n1\n0\n0\n\n\nS\n0\n0\n0\n0\n1\n0\n\n\nX\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#the-linear-model-with-multiple-predictors",
    "href": "content/lectures/07-linear-models-slides.html#the-linear-model-with-multiple-predictors",
    "title": "07-linear-models",
    "section": "The linear model with multiple predictors",
    "text": "The linear model with multiple predictors\n\nPopulation model:\n\n\\[ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k \\]\n\n\nSample model that we use to estimate the population model:\n\n\\[ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k \\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#relationship-bw-height-and-school",
    "href": "content/lectures/07-linear-models-slides.html#relationship-bw-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship b/w height and school",
    "text": "Relationship b/w height and school\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nAustrian school (A) paintings are expected, on average, to be 14 inches tall.\nDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\nFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\nGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\nItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\nSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\nPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings. ]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#predict-height-from-width",
    "href": "content/lectures/07-linear-models-slides.html#predict-height-from-width",
    "title": "07-linear-models",
    "section": "Predict height from width",
    "text": "Predict height from width\n❓ On average, how tall are paintings that are 60 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]\n\n\n3.62 + 0.78 * 60\n\n[1] 50.42\n\n\n“On average, we expect paintings that are 60 inches wide to be 50.42 inches high.”\nWarning: We “expect” this to happen, but there will be some variability. (We’ll learn about measuring the variability around the prediction later.)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#prediction-vs.-extrapolation",
    "href": "content/lectures/07-linear-models-slides.html#prediction-vs.-extrapolation",
    "title": "07-linear-models",
    "section": "Prediction vs. extrapolation",
    "text": "Prediction vs. extrapolation\n❓ On average, how tall are paintings that are 400 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#watch-out-for-extrapolation",
    "href": "content/lectures/07-linear-models-slides.html#watch-out-for-extrapolation",
    "title": "07-linear-models",
    "section": "Watch out for extrapolation!",
    "text": "Watch out for extrapolation!\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”1  Stephen Colbert, April 6th, 2010\n\nIntroduction to Modern Statistics. “Extrapolation is treacherous.”"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#measuring-the-strength-of-the-fit",
    "href": "content/lectures/07-linear-models-slides.html#measuring-the-strength-of-the-fit",
    "title": "07-linear-models",
    "section": "Measuring the strength of the fit",
    "text": "Measuring the strength of the fit\n\nThe strength of the fit of a linear model is most commonly evaluated using \\(R^2\\).\nIt tells us what percent of variability in the response variable is explained by the model.\nThe remainder of the variability is explained by variables not included in the model.\n\\(R^2\\) is sometimes called the coefficient of determination."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#obtaining-r2-in-r",
    "href": "content/lectures/07-linear-models-slides.html#obtaining-r2-in-r",
    "title": "07-linear-models",
    "section": "Obtaining \\(R^2\\) in R",
    "text": "Obtaining \\(R^2\\) in R\n\nHeight vs. width\n\n\nglance(m_ht_wt)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>\n1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(m_ht_wt)$r.squared # extract R-squared\n\n[1] 0.6829468\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n\n\nHeight vs. landscape features\n\n\nglance(m_ht_lands)$r.squared\n\n[1] 0.03456724"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#data-paris-paintings-1",
    "href": "content/lectures/07-linear-models-slides.html#data-paris-paintings-1",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Describe the relationship between price and width of paintings whose width is less than 100in."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width-1",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width-1",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Which plot shows a more linear relationship?"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width-residuals",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width-residuals",
    "title": "07-linear-models",
    "section": "Price vs. width, residuals",
    "text": "Price vs. width, residuals\n❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❓What’s the unit of residuals?"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#transforming-the-data",
    "href": "content/lectures/07-linear-models-slides.html#transforming-the-data",
    "title": "07-linear-models",
    "section": "Transforming the data",
    "text": "Transforming the data\n\nWe saw that price has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n\n\nIn these situations a transformation applied to the response variable may be useful.\n\n\n\n\nIn order to decide which transformation to use, we should examine the distribution of the response variable.\n\n\n\n\nThe extremely right skewed distribution suggests that a log transformation may be useful.\n\nlog = natural log, \\(ln\\)\nDefault base of the log function in R is the natural log:  log(x, base = exp(1))"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#logged-price-vs.-width",
    "href": "content/lectures/07-linear-models-slides.html#logged-price-vs.-width",
    "title": "07-linear-models",
    "section": "Logged price vs. width",
    "text": "Logged price vs. width\n❓ How do we interpret the slope of this model?"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation",
    "href": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\n\nm_lprice_wt <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(log(price) ~ Width_in, data = pp_wt_lt_100)\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#linear-model-with-log-transformation",
    "href": "content/lectures/07-linear-models-slides.html#linear-model-with-log-transformation",
    "title": "07-linear-models",
    "section": "Linear model with log transformation",
    "text": "Linear model with log transformation\n\\[ \\widehat{log(price)} = 4.67 + 0.02 Width \\]\n\n\nFor each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n\n\n\nwhich is not a very useful statement…"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#working-with-logs",
    "href": "content/lectures/07-linear-models-slides.html#working-with-logs",
    "title": "07-linear-models",
    "section": "Working with logs",
    "text": "Working with logs\n\nSubtraction and logs: \\(log(a) − log(b) = log(a / b)\\)\n\n\n\nNatural logarithm: \\(e^{log(x)} = x\\)\n\n\n\n\nWe can use these identities to “undo” the log transformation"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation-1",
    "href": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation-1",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n\n\\[ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 \\]\n\n\n\\[ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 \\]\n\n\n\\[ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} \\]\n\n\n\\[ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 \\]\n\n\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#shortcuts-in-r",
    "href": "content/lectures/07-linear-models-slides.html#shortcuts-in-r",
    "title": "07-linear-models",
    "section": "Shortcuts in R",
    "text": "Shortcuts in R\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019\n\n\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(exp(estimate), 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   107.  \n2 Width_in        1.02"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#recap-log-transformations",
    "href": "content/lectures/07-linear-models-slides.html#recap-log-transformations",
    "title": "07-linear-models",
    "section": "Recap: Log Transformations",
    "text": "Recap: Log Transformations\n\nNon-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n\n\nThe most common transformation when the response variable is right skewed is the log transform: \\(log(y)\\), especially useful when the response variable is (extremely) right skewed.\n\n\n\n\nThis transformation is also useful for variance stabilization.\n\n\n\n\nWhen using a log transformation on the response variable the interpretation of the slope changes: *“For each unit increase in x, y is expected on average to be higher/lower*  by a factor of \\(e^{b_1}\\).”\n\n\n\n\nAnother useful transformation is the square root: \\(\\sqrt{y}\\), especially useful when the response variable is counts."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#aside-when-y-0",
    "href": "content/lectures/07-linear-models-slides.html#aside-when-y-0",
    "title": "07-linear-models",
    "section": "Aside: when \\(y = 0\\)",
    "text": "Aside: when \\(y = 0\\)\nIn some cases the value of the response variable might be 0, and\n\nlog(0)\n\n[1] -Inf\n\n\n\nThe trick is to add a very small number to the value of the response variable for these cases so that the log function can still be applied:\n\nlog(0 + 0.00001)\n\n[1] -11.51293"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#recap",
    "href": "content/lectures/07-linear-models-slides.html#recap",
    "title": "07-linear-models",
    "section": "Recap",
    "text": "Recap\n\nCan I carry out linear regression using the tidymodels approach?\nCan I interpret and explain the results from a linear model with a single predictor?\nDo I understand the limitations of modelling data w/ linear regression?\nCan I describe and implement the use of a dummy variable in linear regression?\nCan I determine when logistic transformation may be appropriate? Can I interpret these results?\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/07-linear-models.html",
    "href": "content/lectures/07-linear-models.html",
    "title": "07-linear-models",
    "section": "",
    "text": "Q: I was just wondering if you could still provide those videos you talked about with like syntax stuff so that we can follow along. I would also appreciate if it was really through with like a walk through of how you got to each point\nA: I like this suggestion, and I’ll try to make these. No promises though. Just have to find magical time in the schedule to make them. Note that lectures will be the more detailed walk through with more time on your own to figure it out (rather than detailed walk through outside of class), so you will get the information…the time/location will just differ. But, I’ll try to make some supplemental videos for those interested.\n\n\nQ: Had a question about the modeling and the last bit showing how decreasing the alpha showed greater clustering at the bottom left corner. If the bottom left corner looks like 0 height and 0 width, how does that translate into the dimensions of an actual painting?\nA: Good observation. The follow-up to this is…are there any paintings with zero width or zero height? And, if you dig in the data (i.e. min(pp$Height_in, na.rm=TRUE)), you’ll see that there are some very small paintings, but that none are zero.\n\n\nQ: If we want to built our own model, can we plot them with ggplot2?\nA: Yup! This post starts to get at that. It does so for a linear model, but the logic follows for other models.\n\n\nQ: For the paintings dataset, how could we perform EDA on specific subject matter, like seeing how many portraits include Jesus as part of the subject?\nA: Love this question. There is a whole field of natural language processing that would have sophisticated ways to analyze this. A simple first pass would be to, for example, filter for paintings that include “Jesus” in the subject column.\n\n\nQ: I think the segmented bar plots based on proportion seem difficult to read. I’m not sure why we should be using this instead of the stacked plots?\nA: Grouped bar blots are typically most quickly understood. Proportion stacked plots are then easiest to understand proportion across categories. Stacked plots of raw numbers take longer (for most) to understand and thus are often avoided, but like all viz, it depends on context and audience.\n\n\nQ: In the last lecture, we talked about how segmented bar plots might not be the ideal choice for data visualization, but we still demonstrated them in today’s lecture. So, specifically in what cases should we choose segmented bar plots as a data analysis tool?\nA: They can be helpful when the audience is familiar with them, but typically are most helpful when you want to display relative proportions rather than counts\n\n\n\n\nDue Dates:\n\nLab 04 due Friday\n\nModel Interpretations\nText, code, & viz all matter\n\nLecture Participation survey “due” after class\nHW02 due Monday (10/30; 11:59 PM)\ndiscuss displaying image in Markdown\n\n\n\n\n\nLinear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & >2 levels)\nresiduals\ndata transformations\n\n\n\n\n\n\nR4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nNumber of observations: 3393\nNumber of variables: 61"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "title": "07-linear-models",
    "section": "Goal: Predict height from width",
    "text": "Goal: Predict height from width\n\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#tidymodels-1",
    "href": "content/lectures/07-linear-models.html#tidymodels-1",
    "title": "07-linear-models",
    "section": "tidymodels",
    "text": "tidymodels\n\nNOT a core tidyverse package\nfollows the structure of a tidyverse package\n\n\n\n\n\n\n\n# should already be installed for you on datahub\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-1-specify-model",
    "href": "content/lectures/07-linear-models.html#step-1-specify-model",
    "title": "07-linear-models",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "href": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "title": "07-linear-models",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |>\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "href": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "title": "07-linear-models",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\n… using formula syntax\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "title": "07-linear-models",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n\n\n\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "title": "07-linear-models",
    "section": "A tidy look at model output",
    "text": "A tidy look at model output\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n\n\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#slope-and-intercept",
    "href": "content/lectures/07-linear-models.html#slope-and-intercept",
    "title": "07-linear-models",
    "section": "Slope and intercept",
    "text": "Slope and intercept\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]\n\n\nSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n\n\n\nIntercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "href": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "title": "07-linear-models",
    "section": "Correlation does not imply causation",
    "text": "Correlation does not imply causation\nRemember this when interpreting model coefficients\n\n\n\n\n\n\n\nSource: XKCD, Cell phones"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "href": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "title": "07-linear-models",
    "section": "Linear model with a single predictor",
    "text": "Linear model with a single predictor\n\nWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\n\n\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n\n\nTough luck, you can’t have them…\n\n\n\n\nSo we use sample statistics to estimate them:\n\n\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#least-squares-regression",
    "href": "content/lectures/07-linear-models.html#least-squares-regression",
    "title": "07-linear-models",
    "section": "Least squares regression",
    "text": "Least squares regression\n\nThe regression line minimizes the sum of squared residuals.\n\n\n\nIf \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\)."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals",
    "title": "07-linear-models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "href": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "title": "07-linear-models",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\n\n\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\nThe slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\n\n\n\n\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\n\n\n\n\nThe residuals and \\(x\\) values are uncorrelated"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 2 levels",
    "text": "Categorical predictor with 2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   <chr>         <dbl>    <dbl>\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# ℹ 3,373 more rows\n\n\n\n\nlandsALL = 0: No landscape features\nlandsALL = 1: Some landscape features"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features",
    "href": "content/lectures/07-linear-models.html#height-landscape-features",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\nm_ht_lands <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |> tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "href": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\nSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n\nCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\n\nIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "title": "07-linear-models",
    "section": "Categorical predictor with >2 levels",
    "text": "Categorical predictor with >2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows\n\n\n\n\nschool from which painting came (details in a few slides)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship between height and school",
    "text": "Relationship between height and school\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ school_pntg, data = pp) |>\n  tidy()\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#dummy-variables",
    "href": "content/lectures/07-linear-models.html#dummy-variables",
    "title": "07-linear-models",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nEach coefficient describes the expected difference between heights in that particular school compared to the baseline level"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 3+ levels",
    "text": "Categorical predictor with 3+ levels\n\n\n\n\n\n\n\nschool_pntg\nD_FL\nF\nG\nI\nS\nX\n\n\n\n\nA\n0\n0\n0\n0\n0\n0\n\n\nD/FL\n1\n0\n0\n0\n0\n0\n\n\nF\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n1\n0\n0\n0\n\n\nI\n0\n0\n0\n1\n0\n0\n\n\nS\n0\n0\n0\n0\n1\n0\n\n\nX\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "href": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "title": "07-linear-models",
    "section": "The linear model with multiple predictors",
    "text": "The linear model with multiple predictors\n\nPopulation model:\n\n\\[ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k \\]\n\n\nSample model that we use to estimate the population model:\n\n\\[ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k \\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship b/w height and school",
    "text": "Relationship b/w height and school\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nAustrian school (A) paintings are expected, on average, to be 14 inches tall.\nDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\nFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\nGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\nItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\nSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\nPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings. ]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#predict-height-from-width",
    "title": "07-linear-models",
    "section": "Predict height from width",
    "text": "Predict height from width\n❓ On average, how tall are paintings that are 60 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]\n\n\n3.62 + 0.78 * 60\n\n[1] 50.42\n\n\n“On average, we expect paintings that are 60 inches wide to be 50.42 inches high.”\nWarning: We “expect” this to happen, but there will be some variability. (We’ll learn about measuring the variability around the prediction later.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "href": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "title": "07-linear-models",
    "section": "Prediction vs. extrapolation",
    "text": "Prediction vs. extrapolation\n❓ On average, how tall are paintings that are 400 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "href": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "title": "07-linear-models",
    "section": "Watch out for extrapolation!",
    "text": "Watch out for extrapolation!\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”1  Stephen Colbert, April 6th, 2010"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "href": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "title": "07-linear-models",
    "section": "Measuring the strength of the fit",
    "text": "Measuring the strength of the fit\n\nThe strength of the fit of a linear model is most commonly evaluated using \\(R^2\\).\nIt tells us what percent of variability in the response variable is explained by the model.\nThe remainder of the variability is explained by variables not included in the model.\n\\(R^2\\) is sometimes called the coefficient of determination."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "href": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "title": "07-linear-models",
    "section": "Obtaining \\(R^2\\) in R",
    "text": "Obtaining \\(R^2\\) in R\n\nHeight vs. width\n\n\nglance(m_ht_wt)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>\n1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(m_ht_wt)$r.squared # extract R-squared\n\n[1] 0.6829468\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n\n\nHeight vs. landscape features\n\n\nglance(m_ht_lands)$r.squared\n\n[1] 0.03456724"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width",
    "href": "content/lectures/07-linear-models.html#price-vs.-width",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Describe the relationship between price and width of paintings whose width is less than 100in."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Which plot shows a more linear relationship?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "title": "07-linear-models",
    "section": "Price vs. width, residuals",
    "text": "Price vs. width, residuals\n❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❓What’s the unit of residuals?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#transforming-the-data",
    "href": "content/lectures/07-linear-models.html#transforming-the-data",
    "title": "07-linear-models",
    "section": "Transforming the data",
    "text": "Transforming the data\n\nWe saw that price has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n\n\nIn these situations a transformation applied to the response variable may be useful.\n\n\n\n\nIn order to decide which transformation to use, we should examine the distribution of the response variable.\n\n\n\n\nThe extremely right skewed distribution suggests that a log transformation may be useful.\n\nlog = natural log, \\(ln\\)\nDefault base of the log function in R is the natural log:  log(x, base = exp(1))"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "href": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "title": "07-linear-models",
    "section": "Logged price vs. width",
    "text": "Logged price vs. width\n❓ How do we interpret the slope of this model?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\n\nm_lprice_wt <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(log(price) ~ Width_in, data = pp_wt_lt_100)\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "title": "07-linear-models",
    "section": "Linear model with log transformation",
    "text": "Linear model with log transformation\n\\[ \\widehat{log(price)} = 4.67 + 0.02 Width \\]\n\n\nFor each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n\n\n\nwhich is not a very useful statement…"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#working-with-logs",
    "href": "content/lectures/07-linear-models.html#working-with-logs",
    "title": "07-linear-models",
    "section": "Working with logs",
    "text": "Working with logs\n\nSubtraction and logs: \\(log(a) − log(b) = log(a / b)\\)\n\n\n\nNatural logarithm: \\(e^{log(x)} = x\\)\n\n\n\n\nWe can use these identities to “undo” the log transformation"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n\n\\[ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 \\]\n\n\n\\[ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 \\]\n\n\n\\[ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} \\]\n\n\n\\[ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 \\]\n\n\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "href": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "title": "07-linear-models",
    "section": "Shortcuts in R",
    "text": "Shortcuts in R\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019\n\n\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(exp(estimate), 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   107.  \n2 Width_in        1.02"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap-log-transformations",
    "href": "content/lectures/07-linear-models.html#recap-log-transformations",
    "title": "07-linear-models",
    "section": "Recap: Log Transformations",
    "text": "Recap: Log Transformations\n\nNon-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n\n\nThe most common transformation when the response variable is right skewed is the log transform: \\(log(y)\\), especially useful when the response variable is (extremely) right skewed.\n\n\n\n\nThis transformation is also useful for variance stabilization.\n\n\n\n\nWhen using a log transformation on the response variable the interpretation of the slope changes: *“For each unit increase in x, y is expected on average to be higher/lower*  by a factor of \\(e^{b_1}\\).”\n\n\n\n\nAnother useful transformation is the square root: \\(\\sqrt{y}\\), especially useful when the response variable is counts."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#aside-when-y-0",
    "href": "content/lectures/07-linear-models.html#aside-when-y-0",
    "title": "07-linear-models",
    "section": "Aside: when \\(y = 0\\)",
    "text": "Aside: when \\(y = 0\\)\nIn some cases the value of the response variable might be 0, and\n\nlog(0)\n\n[1] -Inf\n\n\n\nThe trick is to add a very small number to the value of the response variable for these cases so that the log function can still be applied:\n\nlog(0 + 0.00001)\n\n[1] -11.51293"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap",
    "href": "content/lectures/07-linear-models.html#recap",
    "title": "07-linear-models",
    "section": "Recap",
    "text": "Recap\n\nCan I carry out linear regression using the tidymodels approach?\nCan I interpret and explain the results from a linear model with a single predictor?\nDo I understand the limitations of modelling data w/ linear regression?\nCan I describe and implement the use of a dummy variable in linear regression?\nCan I determine when logistic transformation may be appropriate? Can I interpret these results?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#qa",
    "href": "content/lectures/13-cs01-analysis-slides.html#qa",
    "title": "13-cs01-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: How extensive does our extension component need to be?\nA: A bit hard to answer in certain terms. We’ll discuss some examples today to hopefully set expectaions well. To explain in writing here, the most typical extension is students using the data provided to ask and answer a question not directly presented in class. Thus, simply generating a visualization not presented in class would NOT be sufficient. At the other end, finding external data on the topic and analyzing that data, while certainly allowed, would far exceed expectations. In between those extremes is what we expect: add significantly to the analysis, beyond what was presented in class."
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#course-announcements",
    "href": "content/lectures/13-cs01-analysis-slides.html#course-announcements",
    "title": "13-cs01-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nHW03 (MLR) due Mon 11/20\nProject Proposal (it will be a Google Form) due 11/20\nCS01 Deadlines:\n\nLab06 due Friday - cs01-focused\nReport & “General Communication” due 11/27\nsurvey about how working with group went - due 11/28\n\n\n\nNotes:\nMidterm scores & Feedback posted\n\noverall, did very well\n\navg: 13.85/15 (92%)\n6 perfect scores\n\nanswer key on course website\n\nI am behind on emails and Piazza posts."
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#mid-course-survey-summary",
    "href": "content/lectures/13-cs01-analysis-slides.html#mid-course-survey-summary",
    "title": "13-cs01-analysis",
    "section": "Mid-course Survey Summary",
    "text": "Mid-course Survey Summary\n\nN=73 (~75%)\nPacing workload (so far) about right\nCourse notes most helpful in the course overall\nAlso helpful: completing labs, doing homework,\nMany are not checking labs against answer keys; most are not doing suggested readings\nOf those that attend lecture, most find it helpful"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#mid-course-time-spent",
    "href": "content/lectures/13-cs01-analysis-slides.html#mid-course-time-spent",
    "title": "13-cs01-analysis",
    "section": "Mid-course: Time Spent",
    "text": "Mid-course: Time Spent"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#mid-course-what-would-you-change",
    "href": "content/lectures/13-cs01-analysis-slides.html#mid-course-what-would-you-change",
    "title": "13-cs01-analysis",
    "section": "Mid-course: What would you change?",
    "text": "Mid-course: What would you change?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#agenda",
    "href": "content/lectures/13-cs01-analysis-slides.html#agenda",
    "title": "13-cs01-analysis",
    "section": "Agenda",
    "text": "Agenda\n\nDebugging/Understanding Code Strategies\nSensitivity & Specificity\nCross-compound correlations\nExtensions"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#summary-figuring-out-whats-going-on-in-code",
    "href": "content/lectures/13-cs01-analysis-slides.html#summary-figuring-out-whats-going-on-in-code",
    "title": "13-cs01-analysis",
    "section": "Summary: Figuring out what’s going on in code",
    "text": "Summary: Figuring out what’s going on in code\nSuggestions (as discussed in class):\n\n\nLook up documentation (i.e. ?...) / Google the function\nRun it on different input; see how output changing\nRun the code line-by-line, understanding output at each step\nAsk ChatGPT"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#packages",
    "href": "content/lectures/13-cs01-analysis-slides.html#packages",
    "title": "13-cs01-analysis",
    "section": "Packages",
    "text": "Packages\nThree additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)\nlibrary(cowplot)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#the-data",
    "href": "content/lectures/13-cs01-analysis-slides.html#the-data",
    "title": "13-cs01-analysis",
    "section": "The Data",
    "text": "The Data\nReading in the data from the end of data wrangling notes:\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nAnd the functions…\n\nsource(\"src/cs01_functions.R\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#sensitivity-specificity",
    "href": "content/lectures/13-cs01-analysis-slides.html#sensitivity-specificity",
    "title": "13-cs01-analysis",
    "section": "Sensitivity & Specificity",
    "text": "Sensitivity & Specificity\nSensitivity | the ability of a test to correctly identify patients with a disease/trait/condition. \\[TP/(TP + FN)\\]\n\nSpecificity | the ability of a test to correctly identify people without the disease/trait/condition. \\[TN/(TN + FP)\\]\n\n\n❓ For this analysis, do you care more about sensitivity? about specificity? equally about both?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#what-is-a-tp-here-tn-fp-fn",
    "href": "content/lectures/13-cs01-analysis-slides.html#what-is-a-tp-here-tn-fp-fn",
    "title": "13-cs01-analysis",
    "section": "What is a TP here? TN? FP? FN?",
    "text": "What is a TP here? TN? FP? FN?\nPost-smoking (cutoff > 0)\n\n\nTP = THC group, value >= cutoff\nFN = THC group, value < cutoff\nFP = Placebo group, value >= cutoff\nTN = Placebo group, value < cutoff\n\n\n\nPost-smoking (cutoff == 0)\nCannot be a TP or FP if zero…\n\nTP = THC group, value > cutoff),\nFN = THC group, value <= cutoff),\nFP = Placebo group, value > cutoff),\nTN = Placebo group, value < cutoff)\n\n\n\nPre-smoking\nCannot be a TP or FN before consuming…\n\nTP = 0\nFN = 0\nFP = value >= cutoff\nTN = value < cutoff"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#roc",
    "href": "content/lectures/13-cs01-analysis-slides.html#roc",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\nReceiver-Operator Characteristic (ROC) Curve: TPR (Sensitivity) vs FPR (1-Specificity)\n\nImage Credit: By cmglee, MartinThoma - Roc-draft-xkcd-style.svg, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=109730045"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculating-sensitivity-and-specificity",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculating-sensitivity-and-specificity",
    "title": "13-cs01-analysis",
    "section": "Calculating Sensitivity and Specificity",
    "text": "Calculating Sensitivity and Specificity\n\nCalculateRunApplyDo it!\n\n\n\nmake_calculations <- function(dataset, dataset_removedups, split, compound, \n                              start = start, stop = stop, tpt_use = tpt_use){\n  ## remove NAs\n  df <- dataset_removedups %>% \n    dplyr::select(treatment, compound, timepoint_use) %>%\n    rename(compound = 2) %>%\n    filter(complete.cases(.))\n  if(nrow(df)>0){\n    if(stop <= 0){\n      output <- df %>% \n        summarise(TP = 0,\n                  FN = 0,\n                  FP = sum(compound >= split),\n                  TN = sum(compound < split)) \n    }else{\n      if(split == 0){\n        output_pre <- df %>% \n          filter(tpt_use == \"pre-smoking\") %>%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound >= split),\n                    TN = sum(compound < split)) \n        \n        output <- df %>% \n          filter(tpt_use != \"pre-smoking\") %>%\n          summarise(TP = sum(treatment != \"Placebo\" & compound > split),\n                    FN = sum(treatment != \"Placebo\" & compound <= split),\n                    FP = sum(treatment == \"Placebo\" & compound > split),\n                    TN = sum(treatment == \"Placebo\" & compound < split))\n        \n        output <- output + output_pre\n      }else{\n        ## calculate values if pre-smoking\n        output_pre <- df %>% \n          filter(tpt_use == \"pre-smoking\") %>%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound >= split),\n                    TN = sum(compound < split)) \n        \n        output <- df %>% \n          filter(tpt_use != \"pre-smoking\") %>%\n          summarise(TP = sum(treatment != \"Placebo\" & compound >= split),\n                    FN = sum(treatment != \"Placebo\" & compound < split),\n                    FP = sum(treatment == \"Placebo\" & compound >= split),\n                    TN = sum(treatment == \"Placebo\" & compound < split))\n        \n        output <- output + output_pre\n      }\n    }\n  }\n  # clean things up; make calculations on above values\n  output <- output %>%\n    mutate(detection_limit = split,\n           compound = compound,\n           time_start = start,\n           time_stop = stop,\n           time_window = tpt_use,\n           NAs = nrow(dataset) - nrow(df),\n           N = nrow(dataset_removedups),\n           N_removed = nrow(dataset) - nrow(dataset_removedups),\n           Sensitivity = (TP/(TP + FN)), \n           Specificity = (TN /(TN + FP)),\n           PPV = (TP/(TP+FP)),\n           NPV = (TN/(TN + FN)),\n           Efficiency = ((TP + TN)/(TP + TN + FP + FN))*100\n    )\n  \n  return(output)\n}\n\n\n\n\ndetermine what cutoff values to try\ncarry out above function on those cutoffs\n\n\nsens_spec <- function(dataset, compound, start, stop, tpt_use, \n                      lowest_value = 0.5, splits = NULL, ...){\n  # if it's not all NAs...\n  if(sum(is.na(dataset[,compound])) != nrow(dataset)){\n    # specify what splits should be used for calculations\n    if(is.null(splits)){\n      limits <- dataset[is.finite(rowSums(dataset[,compound])),compound]\n      ## define lower and upper limits\n      lower = min(limits, na.rm=TRUE)\n      upper = max(limits, na.rm=TRUE)\n      ## determine splits to use for calculations\n      tosplit = pull(limits[,1])[limits[,1]>0]\n      ## only split if there are detectable limits:\n      if(length(tosplit)>=1){\n        splits = c(lowest_value, quantile(tosplit, probs=seq(0, 1, by = 0.01), na.rm=TRUE))\n        splits = unique(splits)\n      }else{\n        splits = 0\n      }\n    }else{\n      splits = splits\n    }\n    # filter to include timepoint of interest\n    dataset <- dataset %>% \n      filter(time_from_start > start & time_from_start <= stop & !is.na(timepoint_use))\n    dataset_removedups <- dataset %>%\n      filter(!is.na(timepoint_use)) %>% \n      group_by(timepoint_use) %>% \n      distinct(id, .keep_all = TRUE) %>% \n      ungroup()\n\n    ## create empty output variable which we'll fill in\n    ## iterate through each possible dose and calculate\n    output <- map_dfr(as.list(splits), ~make_calculations(dataset, \n                                                          dataset_removedups, \n                                                          split = .x,\n                                                          compound,\n                                                          start = start,\n                                                          stop = stop, \n                                                          tpt_use = tpt_use))\n  }\n  \n  return(output)\n}\n\n\n\nMap the above for each matrix…\n\nsens_spec_cpd <- function(dataset, cpd, timepoints, splits = NULL){\n  args2 <- list(start = timepoints$start, \n                stop = timepoints$stop, \n                tpt_use = timepoints$timepoint)\n  out <- args2 %>% \n    pmap_dfr(sens_spec, dataset, compound = cpd, splits = splits)\n  return(out)\n}\n\n\n\nThis takes a few minutes to run… (reminder: cache=TRUE)\n\noutput_WB <- map_dfr(compounds_WB, \n                     ~sens_spec_cpd(dataset = WB, cpd = all_of(.x), \n                                    timepoints = timepoints_WB)) %>% clean_gluc()\n\noutput_BR <- map_dfr(compounds_BR, \n                     ~sens_spec_cpd(dataset = BR,  cpd = all_of(.x),\n                                    timepoints = timepoints_BR))  %>% clean_gluc()\n\noutput_OF <- map_dfr(compounds_OF, \n                     ~sens_spec_cpd(dataset = OF, cpd = all_of(.x),\n                                    timepoints = timepoints_OF))  %>% clean_gluc()"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#roc-1",
    "href": "content/lectures/13-cs01-analysis-slides.html#roc-1",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\n\nCodeCalculatePlot\n\n\n\nss_plot <- function(output, tpts=8, tissue){\n  to_include = output %>%\n    group_by(compound) %>% \n    summarize(mean_detection = mean(detection_limit)) %>% \n    filter(mean_detection > 0)\n  \n  output <-  output %>% \n    mutate(iszero = ifelse(time_start<0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %>%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %>%\n    clean_gluc() %>% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output <- output %>%  mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#A2DDED', '#86BEDC', '#6C9FCA', \n                  '#547EB9', '#3F5EA8', '#2D4096', '#1E2385',\n                  '#181173', '#180762', '#180051')\n  values = c(blue_colors[1:tpts])\n  \n  print(ggplot(output, aes(x = detection_limit, y = Sensitivity, group = fct_inorder(legend))) + \n          geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n          geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n          facet_grid(~compound, scales = \"free_x\") +\n          labs(x = 'Detection Limit',\n               y = 'Sensitivity') +\n          ylim(0,1) +\n          scale_color_manual(values = values, name = 'Time Window') +\n          theme_classic(base_size = 12) + \n          theme(axis.title = element_text(size=16), \n                panel.grid = element_blank(),\n                strip.background = element_blank(),\n                strip.text.x = element_text(size = 12))  \n  )\n  print(\n    ggplot(output, aes(x = detection_limit, y = Specificity, group = fct_inorder(legend))) + \n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound, scales = \"free_x\") +\n      ylim(0,100) +\n      labs(title = tissue,\n           x = 'Detection Limit',\n           y = 'Specificity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12))\n  )\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12))\n  )\n}\n\nroc_plot <- function(output, tpts=8, tissue){\n  to_include = output %>%\n    group_by(compound) %>% \n    summarize(mean_detection = mean(detection_limit)) %>% \n    filter(mean_detection > 0)\n  \n  output <-  output %>% \n    mutate(iszero = ifelse(time_start<0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %>%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %>%\n    clean_gluc() %>% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output <- output %>% mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#86BEDC', \n                  '#547EB9', '#2D4096',\n                  '#181173', '#180051')\n  values = c(blue_colors[1:tpts])\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12) )\n  )\n}\n\n\n\n\nss1_a <- ss_plot(output_WB, tpts = length(unique(output_WB$time_start)), tissue = \"Blood\")\n\n\n\n\n\n\n\n\n\nss2_a <- ss_plot(output_OF, tpts = length(unique(output_OF$time_start)), tissue = \"Oral Fluid\")\n\n\n\n\n\n\n\n\n\nss3_a <- roc_plot(output_BR, tpts = length(unique(output_BR$time_start)), tissue = \"Breath\")\n\n\n\n\n\n\n\nbottom_row <- plot_grid(ss2_a, ss3_a, labels = c('B', 'C'), label_size = 12, ncol = 2, rel_widths = c(0.66, .33))\nplot_grid(ss1_a, bottom_row, labels = c('A', ''), label_size = 12, ncol = 1)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculate-thc",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculate-thc",
    "title": "13-cs01-analysis",
    "section": "Calculate: THC",
    "text": "Calculate: THC\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOFBR\n\n\n\ncutoffs = c(0.5, 1, 2, 5, 10)\nWB_THC <- sens_spec_cpd(dataset = WB, cpd = 'thc',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %>% clean_gluc()\n\nWB_THC\n\n# A tibble: 50 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int>           <dbl> <chr>         <dbl>     <dbl>\n 1     0     0    81   108             0.5 THC            -400         0\n 2     0     0    61   128             1   THC            -400         0\n 3     0     0    45   144             2   THC            -400         0\n 4     0     0    10   179             5   THC            -400         0\n 5     0     0     1   188            10   THC            -400         0\n 6   124     2    28    33             0.5 THC               0        30\n 7   123     3    22    39             1   THC               0        30\n 8   119     7    15    46             2   THC               0        30\n 9   106    20     4    57             5   THC               0        30\n10   101    25     0    61            10   THC               0        30\n# ℹ 40 more rows\n# ℹ 9 more variables: time_window <chr>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>\n\n\n\n\n\nOF_THC <- sens_spec_cpd(dataset = OF, cpd = 'thc',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %>% clean_gluc()\n\nOF_THC\n\n# A tibble: 40 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int>           <dbl> <chr>         <dbl>     <dbl>\n 1     0     0    35   157             0.5 THC            -400         0\n 2     0     0    20   172             1   THC            -400         0\n 3     0     0     9   183             2   THC            -400         0\n 4     0     0     0   192             5   THC            -400         0\n 5     0     0     0   192            10   THC            -400         0\n 6   129     0    39    24             0.5 THC               0        30\n 7   129     0    30    33             1   THC               0        30\n 8   128     1    19    44             2   THC               0        30\n 9   128     1     3    60             5   THC               0        30\n10   125     4     1    62            10   THC               0        30\n# ℹ 30 more rows\n# ℹ 9 more variables: time_window <chr>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>\n\n\n\n\nWhy is there no calculation for breath?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#cutoffs",
    "href": "content/lectures/13-cs01-analysis-slides.html#cutoffs",
    "title": "13-cs01-analysis",
    "section": "Cutoffs",
    "text": "Cutoffs\n\nCodeWBOF\n\n\n\nplot_cutoffs <- function(dataset, timepoint_use_variable, tissue, labels = c(\"A\", \"B\"), vertline, cpd, x_labels){\n    col_val = c(\"#D9D9D9\", \"#BDBDBD\", \"#969696\", \"#636363\", \"#252525\")\n    lines = rep(\"solid\", 5)\n    \n  df_ss <- dataset %>% \n    mutate(time_window = fct_relevel(as.factor(time_window), \n                                     levels(timepoint_use_variable)),\n           detection_limit = as.factor(detection_limit),\n           Sensitivity =  round(Sensitivity*100,0),\n           Specificity =  round(Specificity*100,0),\n           my_label = paste0(time_window, ' N=', N),\n           my_label =  gsub(\" \", \"\\n\", my_label),\n           my_label = fct_relevel(as.factor(my_label), x_labels)) #%>%          \n    \n    p1 <- df_ss %>% \n    ggplot(aes(x = my_label, y = Sensitivity, \n               colour = detection_limit)) + \n    geom_line(size = 1.2, aes(group = detection_limit, \n                              linetype = detection_limit)) + \n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point(show.legend=FALSE) + \n    ylim(0,100) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values=lines) +\n      scale_color_manual(values = col_val, name = \"Cutoff \\n (ng/mL)\",\n                         guide = guide_legend(override.aes = list(linetype = c(1),\n                                                                  shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme( axis.title = element_text(size=16),\n           axis.text = element_text(size=10),\n           legend.position = c(0.08, 0.4),\n           panel.grid = element_blank(),\n           strip.background = element_blank()\n           ) +\n      guides(linetype = FALSE) +\n    labs(x = \"Time Window\", \n         y = \"Sensitivity\", \n         title = paste0(tissue,\": \", cpd) )\n \n  p2 <- df_ss %>% \n    ggplot(aes(x = my_label, y = Specificity,\n               group = detection_limit, \n               colour = detection_limit, \n               linetype = detection_limit)) + \n    geom_line(size = 1.2) +\n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point() + \n    ylim(0,100) +\n    scale_color_manual(values = col_val) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values = lines, \n                          guide = guide_legend(override.aes = list(linetype = \"solid\",\n                                                                   shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme(axis.title = element_text(size=16),\n          axis.text = element_text(size=10),\n          legend.position = \"none\", \n          panel.grid = element_blank(),\n          strip.background = element_blank()) +\n    labs(x = \"Time Window\", \n         y = \"Specificity\",\n         title = \"\" )\n  \n  title <- ggdraw() + \n    draw_label(\n      tissue,\n      x = 0.05,\n      hjust = 0\n    )\n  \n  plot_row <- plot_grid(p1, p2, labels = labels, label_size = 12)\n  \n  plot_grid(\n    title, plot_row,\n    ncol = 1,\n    # rel_heights values control vertical title margins\n    rel_heights = c(0.1, 1)\n  )\n  \n  return(list(plot_row, df_ss))\n\n}\n\n\n\n\nblood_levels <- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(dataset=WB_THC, \n             timepoint_use_variable=WB$timepoint_use, \n             tissue=\"Blood\", \n             vertline=levels(WB$timepoint_use)[5], \n             cpd=\"THC\", \n             x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0    81   108 0.5             THC            -400         0\n 2     0     0    61   128 1               THC            -400         0\n 3     0     0    45   144 2               THC            -400         0\n 4     0     0    10   179 5               THC            -400         0\n 5     0     0     1   188 10              THC            -400         0\n 6   124     2    28    33 0.5             THC               0        30\n 7   123     3    22    39 1               THC               0        30\n 8   119     7    15    46 2               THC               0        30\n 9   106    20     4    57 5               THC               0        30\n10   101    25     0    61 10              THC               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>\n\n\n\n\n\nof_levels <- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_THC, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"THC\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0    35   157 0.5             THC            -400         0\n 2     0     0    20   172 1               THC            -400         0\n 3     0     0     9   183 2               THC            -400         0\n 4     0     0     0   192 5               THC            -400         0\n 5     0     0     0   192 10              THC            -400         0\n 6   129     0    39    24 0.5             THC               0        30\n 7   129     0    30    33 1               THC               0        30\n 8   128     1    19    44 2               THC               0        30\n 9   128     1     3    60 5               THC               0        30\n10   125     4     1    62 10              THC               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculate-cbn",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculate-cbn",
    "title": "13-cs01-analysis",
    "section": "Calculate: CBN",
    "text": "Calculate: CBN\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOF\n\n\n\nWB_CBN =  sens_spec_cpd(dataset = WB, cpd = 'cbn',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %>% clean_gluc()\n\nblood_levels <- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(WB_CBN, WB$timepoint_use, tissue = \"Blood\", vertline=levels(WB$timepoint_use)[5], cpd=\"CBN\", x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0     1   188 0.5             CBN            -400         0\n 2     0     0     0   189 1               CBN            -400         0\n 3     0     0     0   189 2               CBN            -400         0\n 4     0     0     0   189 5               CBN            -400         0\n 5     0     0     0   189 10              CBN            -400         0\n 6   106    20     7    54 0.5             CBN               0        30\n 7    97    29     0    61 1               CBN               0        30\n 8    82    44     0    61 2               CBN               0        30\n 9    40    86     0    61 5               CBN               0        30\n10     9   117     0    61 10              CBN               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>\n\n\n\n\n\nOF_CBN =  sens_spec_cpd(dataset = OF, cpd = 'cbn',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %>% clean_gluc()\n\nof_levels <- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_CBN, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"CBN\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0     5   187 0.5             CBN            -400         0\n 2     0     0     1   191 1               CBN            -400         0\n 3     0     0     1   191 2               CBN            -400         0\n 4     0     0     1   191 5               CBN            -400         0\n 5     0     0     0   192 10              CBN            -400         0\n 6   127     2    41    22 0.5             CBN               0        30\n 7   125     4    32    31 1               CBN               0        30\n 8   122     7    18    45 2               CBN               0        30\n 9   116    13     7    56 5               CBN               0        30\n10   107    22     3    60 10              CBN               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#compound-correlations",
    "href": "content/lectures/13-cs01-analysis-slides.html#compound-correlations",
    "title": "13-cs01-analysis",
    "section": "Compound Correlations",
    "text": "Compound Correlations\n\nCodePlot\n\n\n\nggplotRegression <- function (x, y, xlab, ylab, x_text, y_text,  y_text2, title) {\n  fit <- lm(y ~ x)\n  if(max(fit$model[,1],na.rm=TRUE)!=0){\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), \n                                               digits=2)),\n               vjust=1, hjust=0, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))),\n        vjust=1, hjust=0, size=4.5) + \n      theme_minimal(base_size=14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n  } else{\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      scale_y_continuous(limits = c(0,3)) +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), digits=2)),vjust=1, hjust=1, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))), vjust=1, hjust=1,size=4.5) + \n      theme_minimal(base_size = 14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n    \n    \n  }\n}\n\n\n\n\nwb_reg <- ggplotRegression(WB$thc, WB$cbn, xlab = 'THC (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 150, y_text = 7, y_text2 = 5, title = \"Blood\")\n\nof_reg <- ggplotRegression(OF$thc, OF$cbn, xlab = 'THC  (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 12500, y_text = 750, y_text2 = 500, title = \"Oral Fluid\")\n\nplot_grid(wb_reg, of_reg, labels = 'AUTO', label_size = 12, ncol = 2, scale = 1)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#possible-extensions",
    "href": "content/lectures/13-cs01-analysis-slides.html#possible-extensions",
    "title": "13-cs01-analysis",
    "section": "Possible Extensions",
    "text": "Possible Extensions\nOur current question asks for a single compound…and you’ll need to decide that.\n\n…but you could imagine a world where more than one compound or more than one matrix could be measured at the roadside.\n\n\nSo:\n\n\ncombination of the oral fluid and blood that would better predict recent use? (For example if an officer stopped a driver and got a high oral fluid, but could not get a blood sample for a couple of hours and got a relatively low result would this predict recent use better than blood (or OF) alone?\nIs there a ratio of OF/blood that predicts recent use?\nMachine learning model to determine optimal combination of measurements/cutoffs to detect recent use?\n\n\n\n\nThings to keep in mind:\n\nsome matrices are easier to get at the roadside\ntime from use matters (trying to detect recent use)\nwe may not care equally about sensitivity and specificity"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#cs01-what-to-do-now",
    "href": "content/lectures/13-cs01-analysis-slides.html#cs01-what-to-do-now",
    "title": "13-cs01-analysis",
    "section": "cs01: what to do now?",
    "text": "cs01: what to do now?\n\nCommunicate with your group!\nDiscuss possible extensions\nMake a plan; figure out who’s doing what; set deadlines\nImplement the plan!"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#what-has-to-be-done",
    "href": "content/lectures/13-cs01-analysis-slides.html#what-has-to-be-done",
    "title": "13-cs01-analysis",
    "section": "What has to be done:",
    "text": "What has to be done:\n\n\nQuestion | include in Rmd; add extension if applicable\nBackground | summarize and add to what was discussed in classed\nData\n\nDescribe data & variables\nData wrangling | likely copy + paste from notes; add explanation as you go\n\nAnalysis\n\nEDA | likely borrowing parts from notes and adding more in; be sure to include interpretations of output & guide the reader\nAnalysis | likely borrowing most/all from class; interpretations/guiding reader/contextualizing is essential\nExtension | must be completed\n\nConclusion | summarize\nProofread | ensure it makes sense from top to bottom\nGeneral Audience communication (submit on Canvas; 1 submission per group)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#collaborating-on-github",
    "href": "content/lectures/13-cs01-analysis-slides.html#collaborating-on-github",
    "title": "13-cs01-analysis",
    "section": "Collaborating on GitHub",
    "text": "Collaborating on GitHub\n\nBe sure to pull changes every time you sit down to work\nAvoid working on the same part of the same file as another teammate OR work in separate files and combine at the end\npush your changes once you’re ready to add them to the group"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#recap",
    "href": "content/lectures/13-cs01-analysis-slides.html#recap",
    "title": "13-cs01-analysis",
    "section": "Recap",
    "text": "Recap\n\nCan you describe sensitivity? Specificity?\nCan you explain how TP, TN, FP, and FN were calculated/defined in this experiment?\nCan you describe the code used to carry out the calculations?\nCan you interpret the results from these data?\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html",
    "href": "content/lectures/13-cs01-analysis.html",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Q: How extensive does our extension component need to be?\nA: A bit hard to answer in certain terms. We’ll discuss some examples today to hopefully set expectaions well. To explain in writing here, the most typical extension is students using the data provided to ask and answer a question not directly presented in class. Thus, simply generating a visualization not presented in class would NOT be sufficient. At the other end, finding external data on the topic and analyzing that data, while certainly allowed, would far exceed expectations. In between those extremes is what we expect: add significantly to the analysis, beyond what was presented in class.\n\n\n\n\nDue Dates:\n\nHW03 (MLR) due Mon 11/20\nProject Proposal (it will be a Google Form) due 11/20\nCS01 Deadlines:\n\nLab06 due Friday - cs01-focused\nReport & “General Communication” due 11/27\nsurvey about how working with group went - due 11/28\n\n\n\nNotes:\nMidterm scores & Feedback posted\n\noverall, did very well\n\navg: 13.85/15 (92%)\n6 perfect scores\n\nanswer key on course website\n\nI am behind on emails and Piazza posts.\n\n\n\n\n\nN=73 (~75%)\nPacing workload (so far) about right\nCourse notes most helpful in the course overall\nAlso helpful: completing labs, doing homework,\nMany are not checking labs against answer keys; most are not doing suggested readings\nOf those that attend lecture, most find it helpful\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging/Understanding Code Strategies\nSensitivity & Specificity\nCross-compound correlations\nExtensions\n\n\n\n\nSuggestions (as discussed in class):\n\n\nLook up documentation (i.e. ?...) / Google the function\nRun it on different input; see how output changing\nRun the code line-by-line, understanding output at each step\nAsk ChatGPT"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#packages",
    "href": "content/lectures/13-cs01-analysis.html#packages",
    "title": "13-cs01-analysis",
    "section": "Packages",
    "text": "Packages\nThree additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)\nlibrary(cowplot)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#the-data",
    "href": "content/lectures/13-cs01-analysis.html#the-data",
    "title": "13-cs01-analysis",
    "section": "The Data",
    "text": "The Data\nReading in the data from the end of data wrangling notes:\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nAnd the functions…\n\nsource(\"src/cs01_functions.R\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#sensitivity-specificity",
    "href": "content/lectures/13-cs01-analysis.html#sensitivity-specificity",
    "title": "13-cs01-analysis",
    "section": "Sensitivity & Specificity",
    "text": "Sensitivity & Specificity\nSensitivity | the ability of a test to correctly identify patients with a disease/trait/condition. \\[TP/(TP + FN)\\]\n\nSpecificity | the ability of a test to correctly identify people without the disease/trait/condition. \\[TN/(TN + FP)\\]\n\n\n❓ For this analysis, do you care more about sensitivity? about specificity? equally about both?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#what-is-a-tp-here-tn-fp-fn",
    "href": "content/lectures/13-cs01-analysis.html#what-is-a-tp-here-tn-fp-fn",
    "title": "13-cs01-analysis",
    "section": "What is a TP here? TN? FP? FN?",
    "text": "What is a TP here? TN? FP? FN?\nPost-smoking (cutoff > 0)\n\n\nTP = THC group, value >= cutoff\nFN = THC group, value < cutoff\nFP = Placebo group, value >= cutoff\nTN = Placebo group, value < cutoff\n\n\n\nPost-smoking (cutoff == 0)\nCannot be a TP or FP if zero…\n\nTP = THC group, value > cutoff),\nFN = THC group, value <= cutoff),\nFP = Placebo group, value > cutoff),\nTN = Placebo group, value < cutoff)\n\n\n\nPre-smoking\nCannot be a TP or FN before consuming…\n\nTP = 0\nFN = 0\nFP = value >= cutoff\nTN = value < cutoff"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#roc",
    "href": "content/lectures/13-cs01-analysis.html#roc",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\nReceiver-Operator Characteristic (ROC) Curve: TPR (Sensitivity) vs FPR (1-Specificity)\n\nImage Credit: By cmglee, MartinThoma - Roc-draft-xkcd-style.svg, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=109730045"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculating-sensitivity-and-specificity",
    "href": "content/lectures/13-cs01-analysis.html#calculating-sensitivity-and-specificity",
    "title": "13-cs01-analysis",
    "section": "Calculating Sensitivity and Specificity",
    "text": "Calculating Sensitivity and Specificity\n\nCalculateRunApplyDo it!\n\n\n\nmake_calculations <- function(dataset, dataset_removedups, split, compound, \n                              start = start, stop = stop, tpt_use = tpt_use){\n  ## remove NAs\n  df <- dataset_removedups %>% \n    dplyr::select(treatment, compound, timepoint_use) %>%\n    rename(compound = 2) %>%\n    filter(complete.cases(.))\n  if(nrow(df)>0){\n    if(stop <= 0){\n      output <- df %>% \n        summarise(TP = 0,\n                  FN = 0,\n                  FP = sum(compound >= split),\n                  TN = sum(compound < split)) \n    }else{\n      if(split == 0){\n        output_pre <- df %>% \n          filter(tpt_use == \"pre-smoking\") %>%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound >= split),\n                    TN = sum(compound < split)) \n        \n        output <- df %>% \n          filter(tpt_use != \"pre-smoking\") %>%\n          summarise(TP = sum(treatment != \"Placebo\" & compound > split),\n                    FN = sum(treatment != \"Placebo\" & compound <= split),\n                    FP = sum(treatment == \"Placebo\" & compound > split),\n                    TN = sum(treatment == \"Placebo\" & compound < split))\n        \n        output <- output + output_pre\n      }else{\n        ## calculate values if pre-smoking\n        output_pre <- df %>% \n          filter(tpt_use == \"pre-smoking\") %>%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound >= split),\n                    TN = sum(compound < split)) \n        \n        output <- df %>% \n          filter(tpt_use != \"pre-smoking\") %>%\n          summarise(TP = sum(treatment != \"Placebo\" & compound >= split),\n                    FN = sum(treatment != \"Placebo\" & compound < split),\n                    FP = sum(treatment == \"Placebo\" & compound >= split),\n                    TN = sum(treatment == \"Placebo\" & compound < split))\n        \n        output <- output + output_pre\n      }\n    }\n  }\n  # clean things up; make calculations on above values\n  output <- output %>%\n    mutate(detection_limit = split,\n           compound = compound,\n           time_start = start,\n           time_stop = stop,\n           time_window = tpt_use,\n           NAs = nrow(dataset) - nrow(df),\n           N = nrow(dataset_removedups),\n           N_removed = nrow(dataset) - nrow(dataset_removedups),\n           Sensitivity = (TP/(TP + FN)), \n           Specificity = (TN /(TN + FP)),\n           PPV = (TP/(TP+FP)),\n           NPV = (TN/(TN + FN)),\n           Efficiency = ((TP + TN)/(TP + TN + FP + FN))*100\n    )\n  \n  return(output)\n}\n\n\n\n\ndetermine what cutoff values to try\ncarry out above function on those cutoffs\n\n\nsens_spec <- function(dataset, compound, start, stop, tpt_use, \n                      lowest_value = 0.5, splits = NULL, ...){\n  # if it's not all NAs...\n  if(sum(is.na(dataset[,compound])) != nrow(dataset)){\n    # specify what splits should be used for calculations\n    if(is.null(splits)){\n      limits <- dataset[is.finite(rowSums(dataset[,compound])),compound]\n      ## define lower and upper limits\n      lower = min(limits, na.rm=TRUE)\n      upper = max(limits, na.rm=TRUE)\n      ## determine splits to use for calculations\n      tosplit = pull(limits[,1])[limits[,1]>0]\n      ## only split if there are detectable limits:\n      if(length(tosplit)>=1){\n        splits = c(lowest_value, quantile(tosplit, probs=seq(0, 1, by = 0.01), na.rm=TRUE))\n        splits = unique(splits)\n      }else{\n        splits = 0\n      }\n    }else{\n      splits = splits\n    }\n    # filter to include timepoint of interest\n    dataset <- dataset %>% \n      filter(time_from_start > start & time_from_start <= stop & !is.na(timepoint_use))\n    dataset_removedups <- dataset %>%\n      filter(!is.na(timepoint_use)) %>% \n      group_by(timepoint_use) %>% \n      distinct(id, .keep_all = TRUE) %>% \n      ungroup()\n\n    ## create empty output variable which we'll fill in\n    ## iterate through each possible dose and calculate\n    output <- map_dfr(as.list(splits), ~make_calculations(dataset, \n                                                          dataset_removedups, \n                                                          split = .x,\n                                                          compound,\n                                                          start = start,\n                                                          stop = stop, \n                                                          tpt_use = tpt_use))\n  }\n  \n  return(output)\n}\n\n\n\nMap the above for each matrix…\n\nsens_spec_cpd <- function(dataset, cpd, timepoints, splits = NULL){\n  args2 <- list(start = timepoints$start, \n                stop = timepoints$stop, \n                tpt_use = timepoints$timepoint)\n  out <- args2 %>% \n    pmap_dfr(sens_spec, dataset, compound = cpd, splits = splits)\n  return(out)\n}\n\n\n\nThis takes a few minutes to run… (reminder: cache=TRUE)\n\noutput_WB <- map_dfr(compounds_WB, \n                     ~sens_spec_cpd(dataset = WB, cpd = all_of(.x), \n                                    timepoints = timepoints_WB)) %>% clean_gluc()\n\noutput_BR <- map_dfr(compounds_BR, \n                     ~sens_spec_cpd(dataset = BR,  cpd = all_of(.x),\n                                    timepoints = timepoints_BR))  %>% clean_gluc()\n\noutput_OF <- map_dfr(compounds_OF, \n                     ~sens_spec_cpd(dataset = OF, cpd = all_of(.x),\n                                    timepoints = timepoints_OF))  %>% clean_gluc()"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#roc-1",
    "href": "content/lectures/13-cs01-analysis.html#roc-1",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\n\nCodeCalculatePlot\n\n\n\nss_plot <- function(output, tpts=8, tissue){\n  to_include = output %>%\n    group_by(compound) %>% \n    summarize(mean_detection = mean(detection_limit)) %>% \n    filter(mean_detection > 0)\n  \n  output <-  output %>% \n    mutate(iszero = ifelse(time_start<0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %>%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %>%\n    clean_gluc() %>% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output <- output %>%  mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#A2DDED', '#86BEDC', '#6C9FCA', \n                  '#547EB9', '#3F5EA8', '#2D4096', '#1E2385',\n                  '#181173', '#180762', '#180051')\n  values = c(blue_colors[1:tpts])\n  \n  print(ggplot(output, aes(x = detection_limit, y = Sensitivity, group = fct_inorder(legend))) + \n          geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n          geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n          facet_grid(~compound, scales = \"free_x\") +\n          labs(x = 'Detection Limit',\n               y = 'Sensitivity') +\n          ylim(0,1) +\n          scale_color_manual(values = values, name = 'Time Window') +\n          theme_classic(base_size = 12) + \n          theme(axis.title = element_text(size=16), \n                panel.grid = element_blank(),\n                strip.background = element_blank(),\n                strip.text.x = element_text(size = 12))  \n  )\n  print(\n    ggplot(output, aes(x = detection_limit, y = Specificity, group = fct_inorder(legend))) + \n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound, scales = \"free_x\") +\n      ylim(0,100) +\n      labs(title = tissue,\n           x = 'Detection Limit',\n           y = 'Specificity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12))\n  )\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12))\n  )\n}\n\nroc_plot <- function(output, tpts=8, tissue){\n  to_include = output %>%\n    group_by(compound) %>% \n    summarize(mean_detection = mean(detection_limit)) %>% \n    filter(mean_detection > 0)\n  \n  output <-  output %>% \n    mutate(iszero = ifelse(time_start<0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %>%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %>%\n    clean_gluc() %>% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output <- output %>% mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#86BEDC', \n                  '#547EB9', '#2D4096',\n                  '#181173', '#180051')\n  values = c(blue_colors[1:tpts])\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12) )\n  )\n}\n\n\n\n\nss1_a <- ss_plot(output_WB, tpts = length(unique(output_WB$time_start)), tissue = \"Blood\")\n\n\n\n\n\n\n\n\n\nss2_a <- ss_plot(output_OF, tpts = length(unique(output_OF$time_start)), tissue = \"Oral Fluid\")\n\n\n\n\n\n\n\n\n\nss3_a <- roc_plot(output_BR, tpts = length(unique(output_BR$time_start)), tissue = \"Breath\")\n\n\n\n\n\n\n\nbottom_row <- plot_grid(ss2_a, ss3_a, labels = c('B', 'C'), label_size = 12, ncol = 2, rel_widths = c(0.66, .33))\nplot_grid(ss1_a, bottom_row, labels = c('A', ''), label_size = 12, ncol = 1)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculate-thc",
    "href": "content/lectures/13-cs01-analysis.html#calculate-thc",
    "title": "13-cs01-analysis",
    "section": "Calculate: THC",
    "text": "Calculate: THC\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOFBR\n\n\n\ncutoffs = c(0.5, 1, 2, 5, 10)\nWB_THC <- sens_spec_cpd(dataset = WB, cpd = 'thc',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %>% clean_gluc()\n\nWB_THC\n\n# A tibble: 50 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int>           <dbl> <chr>         <dbl>     <dbl>\n 1     0     0    81   108             0.5 THC            -400         0\n 2     0     0    61   128             1   THC            -400         0\n 3     0     0    45   144             2   THC            -400         0\n 4     0     0    10   179             5   THC            -400         0\n 5     0     0     1   188            10   THC            -400         0\n 6   124     2    28    33             0.5 THC               0        30\n 7   123     3    22    39             1   THC               0        30\n 8   119     7    15    46             2   THC               0        30\n 9   106    20     4    57             5   THC               0        30\n10   101    25     0    61            10   THC               0        30\n# ℹ 40 more rows\n# ℹ 9 more variables: time_window <chr>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>\n\n\n\n\n\nOF_THC <- sens_spec_cpd(dataset = OF, cpd = 'thc',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %>% clean_gluc()\n\nOF_THC\n\n# A tibble: 40 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int>           <dbl> <chr>         <dbl>     <dbl>\n 1     0     0    35   157             0.5 THC            -400         0\n 2     0     0    20   172             1   THC            -400         0\n 3     0     0     9   183             2   THC            -400         0\n 4     0     0     0   192             5   THC            -400         0\n 5     0     0     0   192            10   THC            -400         0\n 6   129     0    39    24             0.5 THC               0        30\n 7   129     0    30    33             1   THC               0        30\n 8   128     1    19    44             2   THC               0        30\n 9   128     1     3    60             5   THC               0        30\n10   125     4     1    62            10   THC               0        30\n# ℹ 30 more rows\n# ℹ 9 more variables: time_window <chr>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>\n\n\n\n\nWhy is there no calculation for breath?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#cutoffs",
    "href": "content/lectures/13-cs01-analysis.html#cutoffs",
    "title": "13-cs01-analysis",
    "section": "Cutoffs",
    "text": "Cutoffs\n\nCodeWBOF\n\n\n\nplot_cutoffs <- function(dataset, timepoint_use_variable, tissue, labels = c(\"A\", \"B\"), vertline, cpd, x_labels){\n    col_val = c(\"#D9D9D9\", \"#BDBDBD\", \"#969696\", \"#636363\", \"#252525\")\n    lines = rep(\"solid\", 5)\n    \n  df_ss <- dataset %>% \n    mutate(time_window = fct_relevel(as.factor(time_window), \n                                     levels(timepoint_use_variable)),\n           detection_limit = as.factor(detection_limit),\n           Sensitivity =  round(Sensitivity*100,0),\n           Specificity =  round(Specificity*100,0),\n           my_label = paste0(time_window, ' N=', N),\n           my_label =  gsub(\" \", \"\\n\", my_label),\n           my_label = fct_relevel(as.factor(my_label), x_labels)) #%>%          \n    \n    p1 <- df_ss %>% \n    ggplot(aes(x = my_label, y = Sensitivity, \n               colour = detection_limit)) + \n    geom_line(size = 1.2, aes(group = detection_limit, \n                              linetype = detection_limit)) + \n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point(show.legend=FALSE) + \n    ylim(0,100) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values=lines) +\n      scale_color_manual(values = col_val, name = \"Cutoff \\n (ng/mL)\",\n                         guide = guide_legend(override.aes = list(linetype = c(1),\n                                                                  shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme( axis.title = element_text(size=16),\n           axis.text = element_text(size=10),\n           legend.position = c(0.08, 0.4),\n           panel.grid = element_blank(),\n           strip.background = element_blank()\n           ) +\n      guides(linetype = FALSE) +\n    labs(x = \"Time Window\", \n         y = \"Sensitivity\", \n         title = paste0(tissue,\": \", cpd) )\n \n  p2 <- df_ss %>% \n    ggplot(aes(x = my_label, y = Specificity,\n               group = detection_limit, \n               colour = detection_limit, \n               linetype = detection_limit)) + \n    geom_line(size = 1.2) +\n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point() + \n    ylim(0,100) +\n    scale_color_manual(values = col_val) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values = lines, \n                          guide = guide_legend(override.aes = list(linetype = \"solid\",\n                                                                   shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme(axis.title = element_text(size=16),\n          axis.text = element_text(size=10),\n          legend.position = \"none\", \n          panel.grid = element_blank(),\n          strip.background = element_blank()) +\n    labs(x = \"Time Window\", \n         y = \"Specificity\",\n         title = \"\" )\n  \n  title <- ggdraw() + \n    draw_label(\n      tissue,\n      x = 0.05,\n      hjust = 0\n    )\n  \n  plot_row <- plot_grid(p1, p2, labels = labels, label_size = 12)\n  \n  plot_grid(\n    title, plot_row,\n    ncol = 1,\n    # rel_heights values control vertical title margins\n    rel_heights = c(0.1, 1)\n  )\n  \n  return(list(plot_row, df_ss))\n\n}\n\n\n\n\nblood_levels <- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(dataset=WB_THC, \n             timepoint_use_variable=WB$timepoint_use, \n             tissue=\"Blood\", \n             vertline=levels(WB$timepoint_use)[5], \n             cpd=\"THC\", \n             x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0    81   108 0.5             THC            -400         0\n 2     0     0    61   128 1               THC            -400         0\n 3     0     0    45   144 2               THC            -400         0\n 4     0     0    10   179 5               THC            -400         0\n 5     0     0     1   188 10              THC            -400         0\n 6   124     2    28    33 0.5             THC               0        30\n 7   123     3    22    39 1               THC               0        30\n 8   119     7    15    46 2               THC               0        30\n 9   106    20     4    57 5               THC               0        30\n10   101    25     0    61 10              THC               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>\n\n\n\n\n\nof_levels <- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_THC, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"THC\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0    35   157 0.5             THC            -400         0\n 2     0     0    20   172 1               THC            -400         0\n 3     0     0     9   183 2               THC            -400         0\n 4     0     0     0   192 5               THC            -400         0\n 5     0     0     0   192 10              THC            -400         0\n 6   129     0    39    24 0.5             THC               0        30\n 7   129     0    30    33 1               THC               0        30\n 8   128     1    19    44 2               THC               0        30\n 9   128     1     3    60 5               THC               0        30\n10   125     4     1    62 10              THC               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculate-cbn",
    "href": "content/lectures/13-cs01-analysis.html#calculate-cbn",
    "title": "13-cs01-analysis",
    "section": "Calculate: CBN",
    "text": "Calculate: CBN\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOF\n\n\n\nWB_CBN =  sens_spec_cpd(dataset = WB, cpd = 'cbn',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %>% clean_gluc()\n\nblood_levels <- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(WB_CBN, WB$timepoint_use, tissue = \"Blood\", vertline=levels(WB$timepoint_use)[5], cpd=\"CBN\", x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0     1   188 0.5             CBN            -400         0\n 2     0     0     0   189 1               CBN            -400         0\n 3     0     0     0   189 2               CBN            -400         0\n 4     0     0     0   189 5               CBN            -400         0\n 5     0     0     0   189 10              CBN            -400         0\n 6   106    20     7    54 0.5             CBN               0        30\n 7    97    29     0    61 1               CBN               0        30\n 8    82    44     0    61 2               CBN               0        30\n 9    40    86     0    61 5               CBN               0        30\n10     9   117     0    61 10              CBN               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>\n\n\n\n\n\nOF_CBN =  sens_spec_cpd(dataset = OF, cpd = 'cbn',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %>% clean_gluc()\n\nof_levels <- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_CBN, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"CBN\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0     5   187 0.5             CBN            -400         0\n 2     0     0     1   191 1               CBN            -400         0\n 3     0     0     1   191 2               CBN            -400         0\n 4     0     0     1   191 5               CBN            -400         0\n 5     0     0     0   192 10              CBN            -400         0\n 6   127     2    41    22 0.5             CBN               0        30\n 7   125     4    32    31 1               CBN               0        30\n 8   122     7    18    45 2               CBN               0        30\n 9   116    13     7    56 5               CBN               0        30\n10   107    22     3    60 10              CBN               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#compound-correlations",
    "href": "content/lectures/13-cs01-analysis.html#compound-correlations",
    "title": "13-cs01-analysis",
    "section": "Compound Correlations",
    "text": "Compound Correlations\n\nCodePlot\n\n\n\nggplotRegression <- function (x, y, xlab, ylab, x_text, y_text,  y_text2, title) {\n  fit <- lm(y ~ x)\n  if(max(fit$model[,1],na.rm=TRUE)!=0){\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), \n                                               digits=2)),\n               vjust=1, hjust=0, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))),\n        vjust=1, hjust=0, size=4.5) + \n      theme_minimal(base_size=14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n  } else{\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      scale_y_continuous(limits = c(0,3)) +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), digits=2)),vjust=1, hjust=1, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))), vjust=1, hjust=1,size=4.5) + \n      theme_minimal(base_size = 14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n    \n    \n  }\n}\n\n\n\n\nwb_reg <- ggplotRegression(WB$thc, WB$cbn, xlab = 'THC (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 150, y_text = 7, y_text2 = 5, title = \"Blood\")\n\nof_reg <- ggplotRegression(OF$thc, OF$cbn, xlab = 'THC  (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 12500, y_text = 750, y_text2 = 500, title = \"Oral Fluid\")\n\nplot_grid(wb_reg, of_reg, labels = 'AUTO', label_size = 12, ncol = 2, scale = 1)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#possible-extensions",
    "href": "content/lectures/13-cs01-analysis.html#possible-extensions",
    "title": "13-cs01-analysis",
    "section": "Possible Extensions",
    "text": "Possible Extensions\nOur current question asks for a single compound…and you’ll need to decide that.\n\n…but you could imagine a world where more than one compound or more than one matrix could be measured at the roadside.\n\n\nSo:\n\n\ncombination of the oral fluid and blood that would better predict recent use? (For example if an officer stopped a driver and got a high oral fluid, but could not get a blood sample for a couple of hours and got a relatively low result would this predict recent use better than blood (or OF) alone?\nIs there a ratio of OF/blood that predicts recent use?\nMachine learning model to determine optimal combination of measurements/cutoffs to detect recent use?\n\n\n\n\nThings to keep in mind:\n\nsome matrices are easier to get at the roadside\ntime from use matters (trying to detect recent use)\nwe may not care equally about sensitivity and specificity"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#cs01-what-to-do-now",
    "href": "content/lectures/13-cs01-analysis.html#cs01-what-to-do-now",
    "title": "13-cs01-analysis",
    "section": "cs01: what to do now?",
    "text": "cs01: what to do now?\n\nCommunicate with your group!\nDiscuss possible extensions\nMake a plan; figure out who’s doing what; set deadlines\nImplement the plan!"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#what-has-to-be-done",
    "href": "content/lectures/13-cs01-analysis.html#what-has-to-be-done",
    "title": "13-cs01-analysis",
    "section": "What has to be done:",
    "text": "What has to be done:\n\n\nQuestion | include in Rmd; add extension if applicable\nBackground | summarize and add to what was discussed in classed\nData\n\nDescribe data & variables\nData wrangling | likely copy + paste from notes; add explanation as you go\n\nAnalysis\n\nEDA | likely borrowing parts from notes and adding more in; be sure to include interpretations of output & guide the reader\nAnalysis | likely borrowing most/all from class; interpretations/guiding reader/contextualizing is essential\nExtension | must be completed\n\nConclusion | summarize\nProofread | ensure it makes sense from top to bottom\nGeneral Audience communication (submit on Canvas; 1 submission per group)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#collaborating-on-github",
    "href": "content/lectures/13-cs01-analysis.html#collaborating-on-github",
    "title": "13-cs01-analysis",
    "section": "Collaborating on GitHub",
    "text": "Collaborating on GitHub\n\nBe sure to pull changes every time you sit down to work\nAvoid working on the same part of the same file as another teammate OR work in separate files and combine at the end\npush your changes once you’re ready to add them to the group"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#recap",
    "href": "content/lectures/13-cs01-analysis.html#recap",
    "title": "13-cs01-analysis",
    "section": "Recap",
    "text": "Recap\n\nCan you describe sensitivity? Specificity?\nCan you explain how TP, TN, FP, and FN were calculated/defined in this experiment?\nCan you describe the code used to carry out the calculations?\nCan you interpret the results from these data?"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#qa",
    "href": "content/lectures/10-projects-slides.html#qa",
    "title": "10-projects",
    "section": "Q&A",
    "text": "Q&A\nComing Soon"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#course-announcements",
    "href": "content/lectures/10-projects-slides.html#course-announcements",
    "title": "10-projects",
    "section": "Course Announcements",
    "text": "Course Announcements\nComing Soon"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#agenda",
    "href": "content/lectures/10-projects-slides.html#agenda",
    "title": "10-projects",
    "section": "Agenda",
    "text": "Agenda\n\nCase Studies\nFinal Project"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#biomarkers-of-recent-use",
    "href": "content/lectures/10-projects-slides.html#biomarkers-of-recent-use",
    "title": "10-projects",
    "section": "Biomarkers of Recent Use",
    "text": "Biomarkers of Recent Use\n\nWe’ll use data from this paper:\n\n\nHubbard et al. Biomarkers of Recent Cannabis Use in Blood, Oral Fluid and Breath. Journal of Analytical Toxicology. 2021. Link to paper."
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#opencasestudies",
    "href": "content/lectures/10-projects-slides.html#opencasestudies",
    "title": "10-projects",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\n\nOpenCaseStudies\nUses R/the tidyverse\nasks public health-centric questions\ngoal: to teach statistical analysis/data science through case studies"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-well-do",
    "href": "content/lectures/10-projects-slides.html#what-well-do",
    "title": "10-projects",
    "section": "What We’ll Do",
    "text": "What We’ll Do\nFor each case study (2), during lecture:\n\nStats: (1-2d)\nBackground, Data & Wrangling (1-2d)\nEDA & Analysis (1-2d)\n\n\nFor each case study:\n\nyou’ll also work with case study data in lab.\nyou’ll work in assigned groups of ~3 students to complete a data science report\n\n\n\nI will share previous student examples and we’ll discuss pros and cons in coming lectures."
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#data-science-reports",
    "href": "content/lectures/10-projects-slides.html#data-science-reports",
    "title": "10-projects",
    "section": "Data Science Reports",
    "text": "Data Science Reports\nWith your group, you will:\n\ncarry out all steps of the analysis\n\nsome code will be taken directly from lecture\n\nadd text/organize into a report\n\n\n\nhave to extend the case study\n\n\n\nThis should be written at the level of a data science-knowledgeable undergrad."
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#general-communication-submission",
    "href": "content/lectures/10-projects-slides.html#general-communication-submission",
    "title": "10-projects",
    "section": "General Communication Submission",
    "text": "General Communication Submission\nThis is (intentionally) very open-ended.\nYou need to communicate the most important aspect/finding/part(s) of your case study to a general audience (any undergrad).\n\nWhat might this look like?\n\nshort TikTok like video\nbrief Youtube video\nslides for an Instagram post\nX (Twitter) thread\nposter to be displayed next to an elevator\nposter to be put on public bulletin boards\neffective email communication"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-does-extend-the-case-study-mean",
    "href": "content/lectures/10-projects-slides.html#what-does-extend-the-case-study-mean",
    "title": "10-projects",
    "section": "What does extend the case study mean?",
    "text": "What does extend the case study mean?\nYou’ll need to do something more on the topic beyond what is presented in class.\n\nExamples:\n\nAsking an additional question and answering it from the data provided\nFinding an additional dataset and using it to add to the case study\nGenerating a handful of additional and very informative visualizations (beyond what’s presented in class)"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#grading",
    "href": "content/lectures/10-projects-slides.html#grading",
    "title": "10-projects",
    "section": "Grading",
    "text": "Grading\nGraded on:\n\ncontent (code, text, viz)\nreport: effective written communication (clarity/content > grammar/spelling)\n\nextension carried out\n\neffective general communication (effectively conveys message to a general audience)"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#final-project-logistics",
    "href": "content/lectures/10-projects-slides.html#final-project-logistics",
    "title": "10-projects",
    "section": "Final Project Logistics",
    "text": "Final Project Logistics\n\nwill be completed in groups of 3-4 students\nyou get to choose the group\nI will ask Monday week 7 for your final project groups (If you are not in one, I will help)\nYou will submit a proposal week 8.\nFinal projects are due during Finals week"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-is-the-final-project-proposal",
    "href": "content/lectures/10-projects-slides.html#what-is-the-final-project-proposal",
    "title": "10-projects",
    "section": "What is the final project proposal?",
    "text": "What is the final project proposal?\n\nA short Google Form\nyou’ll submit your topic and a few details about that topic (depending upon which option you choose)\nYour idea can change after you submit your proposal\nThis has been added to help you start your project before finals week."
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#final-project-details",
    "href": "content/lectures/10-projects-slides.html#final-project-details",
    "title": "10-projects",
    "section": "Final Project Details",
    "text": "Final Project Details\nTwo possible paths:\n\nCreate a technical presentation on a statistics topic and/or an R package.\nCarry out a data analysis"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#option-1-technical-presentation",
    "href": "content/lectures/10-projects-slides.html#option-1-technical-presentation",
    "title": "10-projects",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\n\n.Rmd document used to make slides\n“Teaches” the details of the R package/statistics topic\nDemonstrates how to use the package and/or carry out the statistical analysis in R\nTopic/Package must go beyond what was taught in this course or what you should have learned in an intro stats course\nPresentation Length: 10-15min"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#option-2-data-analysis",
    "href": "content/lectures/10-projects-slides.html#option-2-data-analysis",
    "title": "10-projects",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\n\n.Rmd document used for data science report\nAsks a question, finds data, analyzes data (basically: a mini case report, but you find the data and formulate the question)\nPresentation Length: 3-5min (brief summary of the full report)"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#wherewhen-for-this-presentation",
    "href": "content/lectures/10-projects-slides.html#wherewhen-for-this-presentation",
    "title": "10-projects",
    "section": "Where/when for this presentation?",
    "text": "Where/when for this presentation?\n\nSubmit by Tues of finals week at 11:59 PM"
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#should-i-be-working-on-my-final-project-now",
    "href": "content/lectures/10-projects-slides.html#should-i-be-working-on-my-final-project-now",
    "title": "10-projects",
    "section": "Should I be working on my final project now?",
    "text": "Should I be working on my final project now?\n…probably not\n\nBut, you should start thinking about/getting a group of 3-4 people together. You’ll need to submit who’s in your final project group Monday of week 7.\n\n\nYou’ll need to have a general plan for your final project around wk 8. You’ll submit a “proposal” Monday of week 8."
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-is-the-general-audience-communication",
    "href": "content/lectures/10-projects-slides.html#what-is-the-general-audience-communication",
    "title": "10-projects",
    "section": "What is the “general audience” communication?",
    "text": "What is the “general audience” communication?\nConsider who the audience would be -> design for them\n\nFor example, if you present on an R package, who would benefit from knowing about this package? How would you reach them? What can you design to inform them of what it is and get them to use it?\n\n\nOr if you do a data analysis on a particular topic, what would you want others to know? How would you communicate that?\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/10-projects.html",
    "href": "content/lectures/10-projects.html",
    "title": "10-projects",
    "section": "",
    "text": "Coming Soon\n\n\n\nComing Soon\n\n\n\n\nCase Studies\nFinal Project"
  },
  {
    "objectID": "content/lectures/10-projects.html#biomarkers-of-recent-use",
    "href": "content/lectures/10-projects.html#biomarkers-of-recent-use",
    "title": "10-projects",
    "section": "Biomarkers of Recent Use",
    "text": "Biomarkers of Recent Use\n\nWe’ll use data from this paper:\n\n\nHubbard et al. Biomarkers of Recent Cannabis Use in Blood, Oral Fluid and Breath. Journal of Analytical Toxicology. 2021. Link to paper."
  },
  {
    "objectID": "content/lectures/10-projects.html#opencasestudies",
    "href": "content/lectures/10-projects.html#opencasestudies",
    "title": "10-projects",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\n\nOpenCaseStudies\nUses R/the tidyverse\nasks public health-centric questions\ngoal: to teach statistical analysis/data science through case studies"
  },
  {
    "objectID": "content/lectures/10-projects.html#what-well-do",
    "href": "content/lectures/10-projects.html#what-well-do",
    "title": "10-projects",
    "section": "What We’ll Do",
    "text": "What We’ll Do\nFor each case study (2), during lecture:\n\nStats: (1-2d)\nBackground, Data & Wrangling (1-2d)\nEDA & Analysis (1-2d)\n\n\nFor each case study:\n\nyou’ll also work with case study data in lab.\nyou’ll work in assigned groups of ~3 students to complete a data science report\n\n\n\nI will share previous student examples and we’ll discuss pros and cons in coming lectures."
  },
  {
    "objectID": "content/lectures/10-projects.html#data-science-reports",
    "href": "content/lectures/10-projects.html#data-science-reports",
    "title": "10-projects",
    "section": "Data Science Reports",
    "text": "Data Science Reports\nWith your group, you will:\n\ncarry out all steps of the analysis\n\nsome code will be taken directly from lecture\n\nadd text/organize into a report\n\n\n\nhave to extend the case study\n\n\n\nThis should be written at the level of a data science-knowledgeable undergrad."
  },
  {
    "objectID": "content/lectures/10-projects.html#general-communication-submission",
    "href": "content/lectures/10-projects.html#general-communication-submission",
    "title": "10-projects",
    "section": "General Communication Submission",
    "text": "General Communication Submission\nThis is (intentionally) very open-ended.\nYou need to communicate the most important aspect/finding/part(s) of your case study to a general audience (any undergrad).\n\nWhat might this look like?\n\nshort TikTok like video\nbrief Youtube video\nslides for an Instagram post\nX (Twitter) thread\nposter to be displayed next to an elevator\nposter to be put on public bulletin boards\neffective email communication"
  },
  {
    "objectID": "content/lectures/10-projects.html#what-does-extend-the-case-study-mean",
    "href": "content/lectures/10-projects.html#what-does-extend-the-case-study-mean",
    "title": "10-projects",
    "section": "What does extend the case study mean?",
    "text": "What does extend the case study mean?\nYou’ll need to do something more on the topic beyond what is presented in class.\n\nExamples:\n\nAsking an additional question and answering it from the data provided\nFinding an additional dataset and using it to add to the case study\nGenerating a handful of additional and very informative visualizations (beyond what’s presented in class)"
  },
  {
    "objectID": "content/lectures/10-projects.html#grading",
    "href": "content/lectures/10-projects.html#grading",
    "title": "10-projects",
    "section": "Grading",
    "text": "Grading\nGraded on:\n\ncontent (code, text, viz)\nreport: effective written communication (clarity/content > grammar/spelling)\n\nextension carried out\n\neffective general communication (effectively conveys message to a general audience)"
  },
  {
    "objectID": "content/lectures/10-projects.html#final-project-logistics",
    "href": "content/lectures/10-projects.html#final-project-logistics",
    "title": "10-projects",
    "section": "Final Project Logistics",
    "text": "Final Project Logistics\n\nwill be completed in groups of 3-4 students\nyou get to choose the group\nI will ask Monday week 7 for your final project groups (If you are not in one, I will help)\nYou will submit a proposal week 8.\nFinal projects are due during Finals week"
  },
  {
    "objectID": "content/lectures/10-projects.html#what-is-the-final-project-proposal",
    "href": "content/lectures/10-projects.html#what-is-the-final-project-proposal",
    "title": "10-projects",
    "section": "What is the final project proposal?",
    "text": "What is the final project proposal?\n\nA short Google Form\nyou’ll submit your topic and a few details about that topic (depending upon which option you choose)\nYour idea can change after you submit your proposal\nThis has been added to help you start your project before finals week."
  },
  {
    "objectID": "content/lectures/10-projects.html#final-project-details",
    "href": "content/lectures/10-projects.html#final-project-details",
    "title": "10-projects",
    "section": "Final Project Details",
    "text": "Final Project Details\nTwo possible paths:\n\nCreate a technical presentation on a statistics topic and/or an R package.\nCarry out a data analysis"
  },
  {
    "objectID": "content/lectures/10-projects.html#option-1-technical-presentation",
    "href": "content/lectures/10-projects.html#option-1-technical-presentation",
    "title": "10-projects",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\n\n.Rmd document used to make slides\n“Teaches” the details of the R package/statistics topic\nDemonstrates how to use the package and/or carry out the statistical analysis in R\nTopic/Package must go beyond what was taught in this course or what you should have learned in an intro stats course\nPresentation Length: 10-15min"
  },
  {
    "objectID": "content/lectures/10-projects.html#option-2-data-analysis",
    "href": "content/lectures/10-projects.html#option-2-data-analysis",
    "title": "10-projects",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\n\n.Rmd document used for data science report\nAsks a question, finds data, analyzes data (basically: a mini case report, but you find the data and formulate the question)\nPresentation Length: 3-5min (brief summary of the full report)"
  },
  {
    "objectID": "content/lectures/10-projects.html#wherewhen-for-this-presentation",
    "href": "content/lectures/10-projects.html#wherewhen-for-this-presentation",
    "title": "10-projects",
    "section": "Where/when for this presentation?",
    "text": "Where/when for this presentation?\n\nSubmit by Tues of finals week at 11:59 PM"
  },
  {
    "objectID": "content/lectures/10-projects.html#should-i-be-working-on-my-final-project-now",
    "href": "content/lectures/10-projects.html#should-i-be-working-on-my-final-project-now",
    "title": "10-projects",
    "section": "Should I be working on my final project now?",
    "text": "Should I be working on my final project now?\n…probably not\n\nBut, you should start thinking about/getting a group of 3-4 people together. You’ll need to submit who’s in your final project group Monday of week 7.\n\n\nYou’ll need to have a general plan for your final project around wk 8. You’ll submit a “proposal” Monday of week 8."
  },
  {
    "objectID": "content/lectures/10-projects.html#what-is-the-general-audience-communication",
    "href": "content/lectures/10-projects.html#what-is-the-general-audience-communication",
    "title": "10-projects",
    "section": "What is the “general audience” communication?",
    "text": "What is the “general audience” communication?\nConsider who the audience would be -> design for them\n\nFor example, if you present on an R package, who would benefit from knowing about this package? How would you reach them? What can you design to inform them of what it is and get them to use it?\n\n\nOr if you do a data analysis on a particular topic, what would you want others to know? How would you communicate that?"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#qa",
    "href": "content/lectures/11-cs01-data-slides.html#qa",
    "title": "11-cs01-data",
    "section": "Q&A",
    "text": "Q&A\n\nQ: How much time are we expected to spend on the case studies?\nA: That’s hard to say. I would recommend spending a bit of time after each lecture ensuring I understand the code presented. It will eventually be included in your final report, so you’ll need to understand/describe/explain it. After the case study has been presented, I would expect a few hours from each group member to complete the extension and write the report. Last year students reported typically spending 4-6h on case studies (with a big range around that median).\n\n\nQ: For the general project plan how much time should we budget towards working on this?\nA: Students report spending ~10h on their final project\n\n\nQ: Are we allowed to work with some of our case study partners for a final project?\nA: Absolutely! My hope is through the case studies students will get to know one another a bit and hopefully want to work together again!"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#course-announcements",
    "href": "content/lectures/11-cs01-data-slides.html#course-announcements",
    "title": "11-cs01-data",
    "section": "Course Announcements",
    "text": "Course Announcements\n\n💻 Midterm is due next Monday at 11:59PM (released Friday 5PM; practice answer keys are on website)\n❓ Mid-course survey will open with midterm - please complete after finishing midterm; will have a week to complete\n🔬 Lab is for midterm review; Lab05 & HW03 will be released next Monday"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#agenda",
    "href": "content/lectures/11-cs01-data-slides.html#agenda",
    "title": "11-cs01-data",
    "section": "Agenda",
    "text": "Agenda\n\nBackground\nData Intro\nPaper Results\nWrangle"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#motor-vehicle-accidents-mvas",
    "href": "content/lectures/11-cs01-data-slides.html#motor-vehicle-accidents-mvas",
    "title": "11-cs01-data",
    "section": "Motor Vehicle Accidents (MVAs)",
    "text": "Motor Vehicle Accidents (MVAs)\n\n2/3 of US trauma center admissions are due to MVAs\n~60% of such patients testing positive for drugs or alcohol\nAlcohol and cannabis are most frequently detected\n\nSource: https://academic.oup.com/clinchem/article/59/3/478/5621997"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#legalization-of-marijuana",
    "href": "content/lectures/11-cs01-data-slides.html#legalization-of-marijuana",
    "title": "11-cs01-data",
    "section": "Legalization of Marijuana",
    "text": "Legalization of Marijuana\n\nFederally illegal in the US\nDecriminalized in many states\nMedically available in 15 states\nLegal for recreational use in 24 states (including CA)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#increased-roadside-surveys",
    "href": "content/lectures/11-cs01-data-slides.html#increased-roadside-surveys",
    "title": "11-cs01-data",
    "section": "Increased roadside surveys",
    "text": "Increased roadside surveys\n\n\n25% increase in use nationwide from 2002 to 2015 (survey)\nTHC detection in drivers increased by 48% from 2007 to 2014\nIncreased prevalence of consumption -> possible intoxication -> possible impaired driving -> public health concern"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#dui-of-alcohol-duia",
    "href": "content/lectures/11-cs01-data-slides.html#dui-of-alcohol-duia",
    "title": "11-cs01-data",
    "section": "DUI of Alcohol (DUIA)",
    "text": "DUI of Alcohol (DUIA)\n\n\nThe science is there. Don’t do it.\nDUIA has decreased since the 1970s\n\n% of nighttime, weekend drivers testing over the legal limit (BAC > 0.08 g/dL) decreased from 7.5% (1973) to 2.2% (2007) link"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#dui-of-cannabis",
    "href": "content/lectures/11-cs01-data-slides.html#dui-of-cannabis",
    "title": "11-cs01-data",
    "section": "DUI of Cannabis",
    "text": "DUI of Cannabis\n\nIn a 2007 survey, 16.3% of nighttime drivers were drug-positive link\n\n8.6% of these tested positive for THC\n\nExperimental and cognitive studies suggest cannabis-induced impairment increases risk of motor vehicle crashes:\n\n\n\nEvidence suggests recent smoking and/or blood THC concentrations 2–5 ng/mL are associated with substantial driving impairment, particularly in occasional smokers.link"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#roadside-detection",
    "href": "content/lectures/11-cs01-data-slides.html#roadside-detection",
    "title": "11-cs01-data",
    "section": "Roadside Detection",
    "text": "Roadside Detection\n\nper se laws: “a driver is deemed to have committed an offense if THC is detected at or above a pre-determined cutoff” link\n\n\n\nDefining cutoffs for safe driving is difficult\nTHC concentration differs by:\n\n“smoking topography” (time to smoke; number of puffs)\nfrequency of use\nroute of ingestion\n\n\n\n\nAs of 2021…link\n\n\n19 states have per se or zero tolerance cannabis laws\nStates with per se laws (Illinois, Montana, Nevada, Ohio, Pennsylvania, Washington and West Virginia), cutoffs range from 1 to 5 ng/mL THC in whole blood.\nIn 3 states, per se limits also apply to THC metabolites\nColorado: “reasonable inference” - blood contained >5 ng/mL THC at the time of the offense\n3 states zero tolerance for THC; 8 states for THC and metabolites"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#metabolism",
    "href": "content/lectures/11-cs01-data-slides.html#metabolism",
    "title": "11-cs01-data",
    "section": "Metabolism",
    "text": "Metabolism\n\n\npeak blood concentrations occur during smoking, then drop rapidly link\nsubjective ‘high’ persists for several hours, varies greatly between individuals\nTHC concentrations remain detectable in frequent users longer than occasional users link\nTHC and certain metabolites can be detected in blood for weeks to months after use and do not necessarily indicate impairment"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#detection",
    "href": "content/lectures/11-cs01-data-slides.html#detection",
    "title": "11-cs01-data",
    "section": "Detection",
    "text": "Detection\nVarious approaches:\n\nDetect impairment (officers detect DUIC)\nDetect recent use (test for compounds)\nCombine recent use + impairment\n\n\nFocus here: Can we identify a biomarker of recent use?\n\nrecent use: defined here as within 3h\ntesting THC and metabolites in blood, oral fluid (OF), and breath"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#aside-case-study-report",
    "href": "content/lectures/11-cs01-data-slides.html#aside-case-study-report",
    "title": "11-cs01-data",
    "section": "Aside: Case Study Report",
    "text": "Aside: Case Study Report\n\nYour Case study will need a background section\nIt can use/summarize/paraphrase the information here (you should cite the source, not me)\nBut, you’re not limited to this information\nYou are allowed/encouraged to dig deeper, include what’s most important, add to, remove, etc.\nThere are a lot of citations in this section - go ahead and peruse them/others/use references in these papers"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#participants",
    "href": "content/lectures/11-cs01-data-slides.html#participants",
    "title": "11-cs01-data",
    "section": "Participants",
    "text": "Participants\n\nplacebo-controlled, double-blinded, randomized study\n\n\n\nrecruited:\n\nvolunteers 21-55y/o\nhad a driver’s license\nself-reported cannabis use >= 4x in the past month\n\n\n\n\n\nParticipants were:\n\ncompensated\nmedically evaluated (for safety)\nasked to refrain from use for 2d prior to participation\nexclusion criteria: OF THC concentration ≥5 ng/mL on day of study (n=7)\n\n\n\n\n\nStudy included 191 participants"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#demographics",
    "href": "content/lectures/11-cs01-data-slides.html#demographics",
    "title": "11-cs01-data",
    "section": "Demographics",
    "text": "Demographics\n\nSource: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#experimental-design",
    "href": "content/lectures/11-cs01-data-slides.html#experimental-design",
    "title": "11-cs01-data",
    "section": "Experimental Design",
    "text": "Experimental Design\nParticipants were:\n\n\nrandomly assigned to receive a cigarette containing placebo (0.02%), or 5.9% or 13.4% THC\nBlood, OF and breath were collected prior to smoking\nsmoked a 700 mg cigarette ad libitum within 10 min, with a minimum of four puffs.\nAfter smoking, 4 additional OF and breath and 8 blood collections were completed at time points up to ∼6h from the start of smoking.\nParticipants ate and drank water between collections, although not within 10 min of OF collection."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#timeline",
    "href": "content/lectures/11-cs01-data-slides.html#timeline",
    "title": "11-cs01-data",
    "section": "Timeline",
    "text": "Timeline\n\nSource: Fitzgerald et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#consumption",
    "href": "content/lectures/11-cs01-data-slides.html#consumption",
    "title": "11-cs01-data",
    "section": "Consumption",
    "text": "Consumption\n Source: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#topography",
    "href": "content/lectures/11-cs01-data-slides.html#topography",
    "title": "11-cs01-data",
    "section": "Topography",
    "text": "Topography\n Source: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#subjective-highness",
    "href": "content/lectures/11-cs01-data-slides.html#subjective-highness",
    "title": "11-cs01-data",
    "section": "Subjective Highness",
    "text": "Subjective Highness\n\nSource: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#our-datasets",
    "href": "content/lectures/11-cs01-data-slides.html#our-datasets",
    "title": "11-cs01-data",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#the-data-1",
    "href": "content/lectures/11-cs01-data-slides.html#the-data-1",
    "title": "11-cs01-data",
    "section": "The Data",
    "text": "The Data\nYou’ll have access once your groups/repos are created…(today I want people to follow along; there will be time to try on your own soon!)\n\nWB <- read_csv(\"data/Blood.csv\")\nBR <- read_csv(\"data/Breath.csv\")\nOF <- read_csv(\"data/OF.csv\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-wb",
    "href": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-wb",
    "title": "11-cs01-data",
    "section": "First Look at the data (WB)",
    "text": "First Look at the data (WB)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-of",
    "href": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-of",
    "title": "11-cs01-data",
    "section": "First Look at the data (OF)",
    "text": "First Look at the data (OF)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-br",
    "href": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-br",
    "title": "11-cs01-data",
    "section": "First Look at the data (BR)",
    "text": "First Look at the data (BR)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-1-pre-smoking",
    "href": "content/lectures/11-cs01-data-slides.html#fig-1-pre-smoking",
    "title": "11-cs01-data",
    "section": "Fig 1: Pre-smoking",
    "text": "Fig 1: Pre-smoking"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-2-sensitivity-and-specificity",
    "href": "content/lectures/11-cs01-data-slides.html#fig-2-sensitivity-and-specificity",
    "title": "11-cs01-data",
    "section": "Fig 2: Sensitivity and Specificity",
    "text": "Fig 2: Sensitivity and Specificity"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-3-cross-compound-relationship",
    "href": "content/lectures/11-cs01-data-slides.html#fig-3-cross-compound-relationship",
    "title": "11-cs01-data",
    "section": "Fig 3: Cross-compound relationship",
    "text": "Fig 3: Cross-compound relationship"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-4-cutoffs",
    "href": "content/lectures/11-cs01-data-slides.html#fig-4-cutoffs",
    "title": "11-cs01-data",
    "section": "Fig 4: Cutoffs",
    "text": "Fig 4: Cutoffs"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-5-youden",
    "href": "content/lectures/11-cs01-data-slides.html#fig-5-youden",
    "title": "11-cs01-data",
    "section": "Fig 5: Youden",
    "text": "Fig 5: Youden\n\n\n…and if there’s time PPV and Accuracy post 3h"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#what-came-after",
    "href": "content/lectures/11-cs01-data-slides.html#what-came-after",
    "title": "11-cs01-data",
    "section": "What Came After",
    "text": "What Came After\n Source: Fiztgerald et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#oral-fluid",
    "href": "content/lectures/11-cs01-data-slides.html#oral-fluid",
    "title": "11-cs01-data",
    "section": "Oral Fluid",
    "text": "Oral Fluid\n\nOF <- OF |>\n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |>  \n  janitor::clean_names() |>\n  rename(thcoh = x11_oh_thc,\n         thcv = thc_v)\n\n❓ What’s this accomplishing?"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#whole-blood",
    "href": "content/lectures/11-cs01-data-slides.html#whole-blood",
    "title": "11-cs01-data",
    "section": "Whole Blood",
    "text": "Whole Blood\n\nWB <- WB |> \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\")) |> \n  janitor::clean_names() |>\n  rename(thcoh = x11_oh_thc,\n         thccooh = thc_cooh,\n         thccooh_gluc = thc_cooh_gluc,\n         thcv = thc_v)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#breath",
    "href": "content/lectures/11-cs01-data-slides.html#breath",
    "title": "11-cs01-data",
    "section": "Breath",
    "text": "Breath\n\nBR <- BR |> \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |> \n  janitor::clean_names() |> \n  rename(thc = thc_pg_pad)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#question-1",
    "href": "content/lectures/11-cs01-data-slides.html#question-1",
    "title": "11-cs01-data",
    "section": "Question",
    "text": "Question\n❓ We’re doing very similar things across three similar (albeit different) datasets. What would be a better approach?"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#storing-compounds",
    "href": "content/lectures/11-cs01-data-slides.html#storing-compounds",
    "title": "11-cs01-data",
    "section": "Storing compounds",
    "text": "Storing compounds\nWe’ll need these later in our functions\n\n# whole blood\ncompounds_WB <-  as.list(colnames(Filter(function(x) !all(is.na(x)), WB[6:13])))\n\n# breath\ncompounds_BR <-  as.list(colnames(Filter(function(x) !all(is.na(x)), BR[6])))\n\n# oral fluid\ncompounds_OF <-  as.list(colnames(Filter(function(x) !all(is.na(x)), OF[6:12])))\n\n\n\n# to get a sense of output\ncompounds_WB\n\n[[1]]\n[1] \"cbn\"\n\n[[2]]\n[1] \"cbd\"\n\n[[3]]\n[1] \"thc\"\n\n[[4]]\n[1] \"thcoh\"\n\n[[5]]\n[1] \"thccooh\"\n\n[[6]]\n[1] \"thccooh_gluc\"\n\n[[7]]\n[1] \"cbg\"\n\n[[8]]\n[1] \"thcv\""
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#storing-timepoints",
    "href": "content/lectures/11-cs01-data-slides.html#storing-timepoints",
    "title": "11-cs01-data",
    "section": "Storing timepoints",
    "text": "Storing timepoints\n\ntimepoints_WB = tibble(start = c(-400, 0, 30, 70, 100, 180, 210, 240, 270, 300), \n                       stop = c(0, 30, 70, 100, 180, 210, 240, 270, 300, max(WB$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-70 min\",\n                                     \"71-100 min\",\"101-180 min\",\"181-210 min\",\n                                     \"211-240 min\",\"241-270 min\",\n                                     \"271-300 min\", \"301+ min\") )\n\n\n\ntimepoints_WB\n\n# A tibble: 10 × 3\n   start  stop timepoint  \n   <dbl> <dbl> <chr>      \n 1  -400     0 pre-smoking\n 2     0    30 0-30 min   \n 3    30    70 31-70 min  \n 4    70   100 71-100 min \n 5   100   180 101-180 min\n 6   180   210 181-210 min\n 7   210   240 211-240 min\n 8   240   270 241-270 min\n 9   270   300 271-300 min\n10   300   382 301+ min   \n\n\n\n\n…and in BR and OF\n\ntimepoints_BR = tibble(start = c(-400, 0, 40, 90, 180, 210, 240, 270), \n                       stop = c(0, 40, 90, 180, 210, 240, 270, \n                                max(BR$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-40 min\",\"41-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\"))\ntimepoints_OF = tibble(start = c(-400, 0, 30, 90, 180, 210, 240, 270), \n                       stop = c(0, 30, 90, 180, 210, 240, 270, \n                                max(OF$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\") )"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-udf-assign_timepoint",
    "href": "content/lectures/11-cs01-data-slides.html#first-udf-assign_timepoint",
    "title": "11-cs01-data",
    "section": "First UDF: assign_timepoint",
    "text": "First UDF: assign_timepoint\n\nassign_timepoint <- function(x, timepoints){\n  if(!is.na(x)){ \n    timepoints$timepoint[x > timepoints$start & x <= timepoints$stop]\n  }else{\n    NA\n  }\n}\n\n🧠 What’s a UDF? What do you think this is doing?"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#timepoints-to-use",
    "href": "content/lectures/11-cs01-data-slides.html#timepoints-to-use",
    "title": "11-cs01-data",
    "section": "Timepoints to use",
    "text": "Timepoints to use\n\n WB <- WB |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_WB),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_WB$timepoint))\n\n# let's get a sense for what this did\nlevels(WB$timepoint_use)\n\n [1] \"pre-smoking\" \"0-30 min\"    \"31-70 min\"   \"71-100 min\"  \"101-180 min\"\n [6] \"181-210 min\" \"211-240 min\" \"241-270 min\" \"271-300 min\" \"301+ min\"   \n\n\nNote: map_* allow you to apply a function across multiple “things” (here: across all rows in a dataframe)\n❓What do you think the above is doing?\n\n\nOF <- OF |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_OF),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_OF$timepoint))\n\nBR <- BR |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_BR),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_BR$timepoint))"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#drop-duplicates",
    "href": "content/lectures/11-cs01-data-slides.html#drop-duplicates",
    "title": "11-cs01-data",
    "section": "Drop Duplicates",
    "text": "Drop Duplicates\n\n drop_dups <- function(dataset){\n  out <- dataset |> \n    filter(!is.na(timepoint_use)) |> \n    group_by(timepoint_use) |> \n    distinct(id, .keep_all = TRUE) |> \n    ungroup()\n  return(out)\n} \n\n❓What do you think the above is doing?\n\n\nWB_dups <- drop_dups(WB)\nOF_dups <- drop_dups(OF)\nBR_dups <- drop_dups(BR)\n\n❓What would you do to try to understand what this has done?"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#saving-intermediate-files",
    "href": "content/lectures/11-cs01-data-slides.html#saving-intermediate-files",
    "title": "11-cs01-data",
    "section": "Saving Intermediate Files",
    "text": "Saving Intermediate Files\nCleaned/wrangled files as CSVs:\n\nwrite_csv(WB, \"data/WB_clean.csv\")\nwrite_csv(BR, \"data/BR_clean.csv\")\nwrite_csv(OF, \"data/OF_clean.csv\")\n\nNote: can lose “type” of object (factor levels)\n\n(Alt) Save as RData:\n\nsave(compounds_WB, compounds_BR, compounds_OF, file=\"data/compounds.RData\")\nsave(timepoints_WB, timepoints_BR, timepoints_OF, file=\"data/timepoints.RData\")\nsave(WB, BR, OF, WB_dups, BR_dups, OF_dups, file=\"data/data_clean.RData\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#recap",
    "href": "content/lectures/11-cs01-data-slides.html#recap",
    "title": "11-cs01-data",
    "section": "Recap",
    "text": "Recap\n\nCould you summarize/explain background presented?\nCould you summarize the experiment that was done?\nCould you describe the datasets? (variables, observations, values, etc.)\nDo you understand/could you explain the wrangling that was done?\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html",
    "href": "content/lectures/11-cs01-data.html",
    "title": "11-cs01-data",
    "section": "",
    "text": "Q: How much time are we expected to spend on the case studies?\nA: That’s hard to say. I would recommend spending a bit of time after each lecture ensuring I understand the code presented. It will eventually be included in your final report, so you’ll need to understand/describe/explain it. After the case study has been presented, I would expect a few hours from each group member to complete the extension and write the report. Last year students reported typically spending 4-6h on case studies (with a big range around that median).\n\n\nQ: For the general project plan how much time should we budget towards working on this?\nA: Students report spending ~10h on their final project\n\n\nQ: Are we allowed to work with some of our case study partners for a final project?\nA: Absolutely! My hope is through the case studies students will get to know one another a bit and hopefully want to work together again!\n\n\n\n\n\n💻 Midterm is due next Monday at 11:59PM (released Friday 5PM; practice answer keys are on website)\n❓ Mid-course survey will open with midterm - please complete after finishing midterm; will have a week to complete\n🔬 Lab is for midterm review; Lab05 & HW03 will be released next Monday\n\n\n\n\n\nBackground\nData Intro\nPaper Results\nWrangle"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#motor-vehicle-accidents-mvas",
    "href": "content/lectures/11-cs01-data.html#motor-vehicle-accidents-mvas",
    "title": "11-cs01-data",
    "section": "Motor Vehicle Accidents (MVAs)",
    "text": "Motor Vehicle Accidents (MVAs)\n\n2/3 of US trauma center admissions are due to MVAs\n~60% of such patients testing positive for drugs or alcohol\nAlcohol and cannabis are most frequently detected\n\nSource: https://academic.oup.com/clinchem/article/59/3/478/5621997"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#legalization-of-marijuana",
    "href": "content/lectures/11-cs01-data.html#legalization-of-marijuana",
    "title": "11-cs01-data",
    "section": "Legalization of Marijuana",
    "text": "Legalization of Marijuana\n\nFederally illegal in the US\nDecriminalized in many states\nMedically available in 15 states\nLegal for recreational use in 24 states (including CA)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#increased-roadside-surveys",
    "href": "content/lectures/11-cs01-data.html#increased-roadside-surveys",
    "title": "11-cs01-data",
    "section": "Increased roadside surveys",
    "text": "Increased roadside surveys\n\n\n25% increase in use nationwide from 2002 to 2015 (survey)\nTHC detection in drivers increased by 48% from 2007 to 2014\nIncreased prevalence of consumption -> possible intoxication -> possible impaired driving -> public health concern"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#dui-of-alcohol-duia",
    "href": "content/lectures/11-cs01-data.html#dui-of-alcohol-duia",
    "title": "11-cs01-data",
    "section": "DUI of Alcohol (DUIA)",
    "text": "DUI of Alcohol (DUIA)\n\n\nThe science is there. Don’t do it.\nDUIA has decreased since the 1970s\n\n% of nighttime, weekend drivers testing over the legal limit (BAC > 0.08 g/dL) decreased from 7.5% (1973) to 2.2% (2007) link"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#dui-of-cannabis",
    "href": "content/lectures/11-cs01-data.html#dui-of-cannabis",
    "title": "11-cs01-data",
    "section": "DUI of Cannabis",
    "text": "DUI of Cannabis\n\nIn a 2007 survey, 16.3% of nighttime drivers were drug-positive link\n\n8.6% of these tested positive for THC\n\nExperimental and cognitive studies suggest cannabis-induced impairment increases risk of motor vehicle crashes:\n\n\n\nEvidence suggests recent smoking and/or blood THC concentrations 2–5 ng/mL are associated with substantial driving impairment, particularly in occasional smokers.link"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#roadside-detection",
    "href": "content/lectures/11-cs01-data.html#roadside-detection",
    "title": "11-cs01-data",
    "section": "Roadside Detection",
    "text": "Roadside Detection\n\nper se laws: “a driver is deemed to have committed an offense if THC is detected at or above a pre-determined cutoff” link\n\n\n\nDefining cutoffs for safe driving is difficult\nTHC concentration differs by:\n\n“smoking topography” (time to smoke; number of puffs)\nfrequency of use\nroute of ingestion\n\n\n\n\nAs of 2021…link\n\n\n19 states have per se or zero tolerance cannabis laws\nStates with per se laws (Illinois, Montana, Nevada, Ohio, Pennsylvania, Washington and West Virginia), cutoffs range from 1 to 5 ng/mL THC in whole blood.\nIn 3 states, per se limits also apply to THC metabolites\nColorado: “reasonable inference” - blood contained >5 ng/mL THC at the time of the offense\n3 states zero tolerance for THC; 8 states for THC and metabolites"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#metabolism",
    "href": "content/lectures/11-cs01-data.html#metabolism",
    "title": "11-cs01-data",
    "section": "Metabolism",
    "text": "Metabolism\n\n\npeak blood concentrations occur during smoking, then drop rapidly link\nsubjective ‘high’ persists for several hours, varies greatly between individuals\nTHC concentrations remain detectable in frequent users longer than occasional users link\nTHC and certain metabolites can be detected in blood for weeks to months after use and do not necessarily indicate impairment"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#detection",
    "href": "content/lectures/11-cs01-data.html#detection",
    "title": "11-cs01-data",
    "section": "Detection",
    "text": "Detection\nVarious approaches:\n\nDetect impairment (officers detect DUIC)\nDetect recent use (test for compounds)\nCombine recent use + impairment\n\n\nFocus here: Can we identify a biomarker of recent use?\n\nrecent use: defined here as within 3h\ntesting THC and metabolites in blood, oral fluid (OF), and breath"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#aside-case-study-report",
    "href": "content/lectures/11-cs01-data.html#aside-case-study-report",
    "title": "11-cs01-data",
    "section": "Aside: Case Study Report",
    "text": "Aside: Case Study Report\n\nYour Case study will need a background section\nIt can use/summarize/paraphrase the information here (you should cite the source, not me)\nBut, you’re not limited to this information\nYou are allowed/encouraged to dig deeper, include what’s most important, add to, remove, etc.\nThere are a lot of citations in this section - go ahead and peruse them/others/use references in these papers"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#participants",
    "href": "content/lectures/11-cs01-data.html#participants",
    "title": "11-cs01-data",
    "section": "Participants",
    "text": "Participants\n\nplacebo-controlled, double-blinded, randomized study\n\n\n\nrecruited:\n\nvolunteers 21-55y/o\nhad a driver’s license\nself-reported cannabis use >= 4x in the past month\n\n\n\n\n\nParticipants were:\n\ncompensated\nmedically evaluated (for safety)\nasked to refrain from use for 2d prior to participation\nexclusion criteria: OF THC concentration ≥5 ng/mL on day of study (n=7)\n\n\n\n\n\nStudy included 191 participants"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#demographics",
    "href": "content/lectures/11-cs01-data.html#demographics",
    "title": "11-cs01-data",
    "section": "Demographics",
    "text": "Demographics\n\nSource: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#experimental-design",
    "href": "content/lectures/11-cs01-data.html#experimental-design",
    "title": "11-cs01-data",
    "section": "Experimental Design",
    "text": "Experimental Design\nParticipants were:\n\n\nrandomly assigned to receive a cigarette containing placebo (0.02%), or 5.9% or 13.4% THC\nBlood, OF and breath were collected prior to smoking\nsmoked a 700 mg cigarette ad libitum within 10 min, with a minimum of four puffs.\nAfter smoking, 4 additional OF and breath and 8 blood collections were completed at time points up to ∼6h from the start of smoking.\nParticipants ate and drank water between collections, although not within 10 min of OF collection."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#timeline",
    "href": "content/lectures/11-cs01-data.html#timeline",
    "title": "11-cs01-data",
    "section": "Timeline",
    "text": "Timeline\n\nSource: Fitzgerald et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#consumption",
    "href": "content/lectures/11-cs01-data.html#consumption",
    "title": "11-cs01-data",
    "section": "Consumption",
    "text": "Consumption\n Source: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#topography",
    "href": "content/lectures/11-cs01-data.html#topography",
    "title": "11-cs01-data",
    "section": "Topography",
    "text": "Topography\n Source: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#subjective-highness",
    "href": "content/lectures/11-cs01-data.html#subjective-highness",
    "title": "11-cs01-data",
    "section": "Subjective Highness",
    "text": "Subjective Highness\n\nSource: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#our-datasets",
    "href": "content/lectures/11-cs01-data.html#our-datasets",
    "title": "11-cs01-data",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#the-data-1",
    "href": "content/lectures/11-cs01-data.html#the-data-1",
    "title": "11-cs01-data",
    "section": "The Data",
    "text": "The Data\nYou’ll have access once your groups/repos are created…(today I want people to follow along; there will be time to try on your own soon!)\n\nWB <- read_csv(\"data/Blood.csv\")\nBR <- read_csv(\"data/Breath.csv\")\nOF <- read_csv(\"data/OF.csv\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-wb",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-wb",
    "title": "11-cs01-data",
    "section": "First Look at the data (WB)",
    "text": "First Look at the data (WB)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-of",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-of",
    "title": "11-cs01-data",
    "section": "First Look at the data (OF)",
    "text": "First Look at the data (OF)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-br",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-br",
    "title": "11-cs01-data",
    "section": "First Look at the data (BR)",
    "text": "First Look at the data (BR)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-1-pre-smoking",
    "href": "content/lectures/11-cs01-data.html#fig-1-pre-smoking",
    "title": "11-cs01-data",
    "section": "Fig 1: Pre-smoking",
    "text": "Fig 1: Pre-smoking"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-2-sensitivity-and-specificity",
    "href": "content/lectures/11-cs01-data.html#fig-2-sensitivity-and-specificity",
    "title": "11-cs01-data",
    "section": "Fig 2: Sensitivity and Specificity",
    "text": "Fig 2: Sensitivity and Specificity"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-3-cross-compound-relationship",
    "href": "content/lectures/11-cs01-data.html#fig-3-cross-compound-relationship",
    "title": "11-cs01-data",
    "section": "Fig 3: Cross-compound relationship",
    "text": "Fig 3: Cross-compound relationship"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-4-cutoffs",
    "href": "content/lectures/11-cs01-data.html#fig-4-cutoffs",
    "title": "11-cs01-data",
    "section": "Fig 4: Cutoffs",
    "text": "Fig 4: Cutoffs"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-5-youden",
    "href": "content/lectures/11-cs01-data.html#fig-5-youden",
    "title": "11-cs01-data",
    "section": "Fig 5: Youden",
    "text": "Fig 5: Youden\n\n\n…and if there’s time PPV and Accuracy post 3h"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#what-came-after",
    "href": "content/lectures/11-cs01-data.html#what-came-after",
    "title": "11-cs01-data",
    "section": "What Came After",
    "text": "What Came After\n Source: Fiztgerald et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#oral-fluid",
    "href": "content/lectures/11-cs01-data.html#oral-fluid",
    "title": "11-cs01-data",
    "section": "Oral Fluid",
    "text": "Oral Fluid\n\nOF <- OF |>\n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |>  \n  janitor::clean_names() |>\n  rename(thcoh = x11_oh_thc,\n         thcv = thc_v)\n\n❓ What’s this accomplishing?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#whole-blood",
    "href": "content/lectures/11-cs01-data.html#whole-blood",
    "title": "11-cs01-data",
    "section": "Whole Blood",
    "text": "Whole Blood\n\nWB <- WB |> \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\")) |> \n  janitor::clean_names() |>\n  rename(thcoh = x11_oh_thc,\n         thccooh = thc_cooh,\n         thccooh_gluc = thc_cooh_gluc,\n         thcv = thc_v)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#breath",
    "href": "content/lectures/11-cs01-data.html#breath",
    "title": "11-cs01-data",
    "section": "Breath",
    "text": "Breath\n\nBR <- BR |> \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |> \n  janitor::clean_names() |> \n  rename(thc = thc_pg_pad)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#question-1",
    "href": "content/lectures/11-cs01-data.html#question-1",
    "title": "11-cs01-data",
    "section": "Question",
    "text": "Question\n❓ We’re doing very similar things across three similar (albeit different) datasets. What would be a better approach?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#storing-compounds",
    "href": "content/lectures/11-cs01-data.html#storing-compounds",
    "title": "11-cs01-data",
    "section": "Storing compounds",
    "text": "Storing compounds\nWe’ll need these later in our functions\n\n# whole blood\ncompounds_WB <-  as.list(colnames(Filter(function(x) !all(is.na(x)), WB[6:13])))\n\n# breath\ncompounds_BR <-  as.list(colnames(Filter(function(x) !all(is.na(x)), BR[6])))\n\n# oral fluid\ncompounds_OF <-  as.list(colnames(Filter(function(x) !all(is.na(x)), OF[6:12])))\n\n\n\n# to get a sense of output\ncompounds_WB\n\n[[1]]\n[1] \"cbn\"\n\n[[2]]\n[1] \"cbd\"\n\n[[3]]\n[1] \"thc\"\n\n[[4]]\n[1] \"thcoh\"\n\n[[5]]\n[1] \"thccooh\"\n\n[[6]]\n[1] \"thccooh_gluc\"\n\n[[7]]\n[1] \"cbg\"\n\n[[8]]\n[1] \"thcv\""
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#storing-timepoints",
    "href": "content/lectures/11-cs01-data.html#storing-timepoints",
    "title": "11-cs01-data",
    "section": "Storing timepoints",
    "text": "Storing timepoints\n\ntimepoints_WB = tibble(start = c(-400, 0, 30, 70, 100, 180, 210, 240, 270, 300), \n                       stop = c(0, 30, 70, 100, 180, 210, 240, 270, 300, max(WB$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-70 min\",\n                                     \"71-100 min\",\"101-180 min\",\"181-210 min\",\n                                     \"211-240 min\",\"241-270 min\",\n                                     \"271-300 min\", \"301+ min\") )\n\n\n\ntimepoints_WB\n\n# A tibble: 10 × 3\n   start  stop timepoint  \n   <dbl> <dbl> <chr>      \n 1  -400     0 pre-smoking\n 2     0    30 0-30 min   \n 3    30    70 31-70 min  \n 4    70   100 71-100 min \n 5   100   180 101-180 min\n 6   180   210 181-210 min\n 7   210   240 211-240 min\n 8   240   270 241-270 min\n 9   270   300 271-300 min\n10   300   382 301+ min   \n\n\n\n\n…and in BR and OF\n\ntimepoints_BR = tibble(start = c(-400, 0, 40, 90, 180, 210, 240, 270), \n                       stop = c(0, 40, 90, 180, 210, 240, 270, \n                                max(BR$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-40 min\",\"41-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\"))\ntimepoints_OF = tibble(start = c(-400, 0, 30, 90, 180, 210, 240, 270), \n                       stop = c(0, 30, 90, 180, 210, 240, 270, \n                                max(OF$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\") )"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-udf-assign_timepoint",
    "href": "content/lectures/11-cs01-data.html#first-udf-assign_timepoint",
    "title": "11-cs01-data",
    "section": "First UDF: assign_timepoint",
    "text": "First UDF: assign_timepoint\n\nassign_timepoint <- function(x, timepoints){\n  if(!is.na(x)){ \n    timepoints$timepoint[x > timepoints$start & x <= timepoints$stop]\n  }else{\n    NA\n  }\n}\n\n🧠 What’s a UDF? What do you think this is doing?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#timepoints-to-use",
    "href": "content/lectures/11-cs01-data.html#timepoints-to-use",
    "title": "11-cs01-data",
    "section": "Timepoints to use",
    "text": "Timepoints to use\n\n WB <- WB |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_WB),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_WB$timepoint))\n\n# let's get a sense for what this did\nlevels(WB$timepoint_use)\n\n [1] \"pre-smoking\" \"0-30 min\"    \"31-70 min\"   \"71-100 min\"  \"101-180 min\"\n [6] \"181-210 min\" \"211-240 min\" \"241-270 min\" \"271-300 min\" \"301+ min\"   \n\n\nNote: map_* allow you to apply a function across multiple “things” (here: across all rows in a dataframe)\n❓What do you think the above is doing?\n\n\nOF <- OF |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_OF),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_OF$timepoint))\n\nBR <- BR |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_BR),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_BR$timepoint))"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#drop-duplicates",
    "href": "content/lectures/11-cs01-data.html#drop-duplicates",
    "title": "11-cs01-data",
    "section": "Drop Duplicates",
    "text": "Drop Duplicates\n\n drop_dups <- function(dataset){\n  out <- dataset |> \n    filter(!is.na(timepoint_use)) |> \n    group_by(timepoint_use) |> \n    distinct(id, .keep_all = TRUE) |> \n    ungroup()\n  return(out)\n} \n\n❓What do you think the above is doing?\n\n\nWB_dups <- drop_dups(WB)\nOF_dups <- drop_dups(OF)\nBR_dups <- drop_dups(BR)\n\n❓What would you do to try to understand what this has done?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#saving-intermediate-files",
    "href": "content/lectures/11-cs01-data.html#saving-intermediate-files",
    "title": "11-cs01-data",
    "section": "Saving Intermediate Files",
    "text": "Saving Intermediate Files\nCleaned/wrangled files as CSVs:\n\nwrite_csv(WB, \"data/WB_clean.csv\")\nwrite_csv(BR, \"data/BR_clean.csv\")\nwrite_csv(OF, \"data/OF_clean.csv\")\n\nNote: can lose “type” of object (factor levels)\n\n(Alt) Save as RData:\n\nsave(compounds_WB, compounds_BR, compounds_OF, file=\"data/compounds.RData\")\nsave(timepoints_WB, timepoints_BR, timepoints_OF, file=\"data/timepoints.RData\")\nsave(WB, BR, OF, WB_dups, BR_dups, OF_dups, file=\"data/data_clean.RData\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#recap",
    "href": "content/lectures/11-cs01-data.html#recap",
    "title": "11-cs01-data",
    "section": "Recap",
    "text": "Recap\n\nCould you summarize/explain background presented?\nCould you summarize the experiment that was done?\nCould you describe the datasets? (variables, observations, values, etc.)\nDo you understand/could you explain the wrangling that was done?"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#qa",
    "href": "content/lectures/08-effective-communication-slides.html#qa",
    "title": "08-effective-communication",
    "section": "Q&A",
    "text": "Q&A\n\nQ: for the last part of lecture 07, I tried glance(m_ht_wt) but it didn’t work because m_ht_wt doesn’t exist. Is “m_ht_wt” supposed be a model?\nA: Yup, this model was the height by weight model:\n\n\nm_ht_wt <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\n\nQ: I was not too sure what was going on when talking about the relationship between painting height and school.\nA: I don’t think you were the only one confused! Briefly here (and I’m happy to chat more before/after class and in OH), we were looking to determine/quantify the relationship between the size (height) of a painting and the school from which the painting originated. This was an example of having more than two categories for a categorical (factor) predictor. The important points were undersatnding that each level is compared to the baseline and the linear model that results from multiple categories. Part 4 of the lab gets into this a bit more too. Definitely follow up if you’re unsure after doing that part of the lab!\n\n\nQ: How do you calculate the linear regression model when you have non-numeric values? For example, on lab 04, when it asks to calculate the linear regression model by gender, the gender appears only as male and female. Suppose male is 1 and female is 0 (interpreted by the function), then male linear regression model is y =ax + 1?\nA: Close! the “1” would be plugged in as the value of x (in what you suggested)m not for the intercept. So the function would be \\(y=\\beta_1*1 + \\beta_0\\)"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#course-announcements",
    "href": "content/lectures/08-effective-communication-slides.html#course-announcements",
    "title": "08-effective-communication",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nLab04 due Friday\nHW02 due Monday\n\n\n\nPractice Midterms Now Available\n\nanswers posted next week\n\nMidterm Exam\n\nwill cover material through “Multiple Linear Regression”\nwill be released/posted next Friday after lab\nwill be due Monday Nov 6th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be completed individually (open Notes; open Internet)\n\n\n\n\n\nLink for Later"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#agenda",
    "href": "content/lectures/08-effective-communication-slides.html#agenda",
    "title": "08-effective-communication",
    "section": "Agenda",
    "text": "Agenda\n\nCommunicating for your audience\nOral Communication\nWritten Communication\nVisual Communication"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#suggested-reading",
    "href": "content/lectures/08-effective-communication-slides.html#suggested-reading",
    "title": "08-effective-communication",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nBookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#what-does-this-mean",
    "href": "content/lectures/08-effective-communication-slides.html#what-does-this-mean",
    "title": "08-effective-communication",
    "section": "What does this mean?",
    "text": "What does this mean?\n❓ What does it mean to “consider your audience?”\n\nSimply: You do the work so they don’t have to.\n\n\n…also the aesthetic-usability effect exists."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#whats-the-right-level",
    "href": "content/lectures/08-effective-communication-slides.html#whats-the-right-level",
    "title": "08-effective-communication",
    "section": "What’s the right level?",
    "text": "What’s the right level?\n\n\nGeneral Audience\n✔ background\n🚫 limit technical details\n🎉 emphasize take-home\n\n\n\nTechnical Audience\n⬇ limit background\n💻 all-the-details\n🎉 emphasize take-home"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#considerations",
    "href": "content/lectures/08-effective-communication-slides.html#considerations",
    "title": "08-effective-communication",
    "section": "Considerations",
    "text": "Considerations\n\nPlatform: written? oral?\n\n\n\nSetting: informal? formal?\n\n\n\n\nTiming: never go over your time limit!"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#storytelling",
    "href": "content/lectures/08-effective-communication-slides.html#storytelling",
    "title": "08-effective-communication",
    "section": "Storytelling",
    "text": "Storytelling\n\nStories have a beginning, a middle, and an end.\n\n\n\nStories do not need every detail of what you’ve tried\n\n\n\n\nReports and presentations should tell a story\nPlanning out your report/presentation can help\n\n\n\n\nHold the audience’s attention with what needs to be said; do so effectively\nTell your audience why they should care; why it matters\nYou should explain your choices and the “why”"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#choose-informative-titles",
    "href": "content/lectures/08-effective-communication-slides.html#choose-informative-titles",
    "title": "08-effective-communication",
    "section": "Choose informative titles",
    "text": "Choose informative titles\nOn presentations: Balance b/w short and informative (goal: concise)\n\n\nAvoid: “Analyzing NHANES”\n\nBetter: “Data from the NHANES study shows that diet is related to overall health”\n\n\nOn visualizations: emphasize the take-home! (what’s learned or what action to take)\n\n\n\nAvoid: “Boxplot of gender”\n\nBetter: “Twice as many females as males included for analysis”\n\n\n\nAvoid: “Tickets vs. Time”\n\nBetter: “Staff unable to respond to incoming tickets; need to hire 2 FTEs”"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nconsider your audience\n\n\nspeak organizedly and logically\n\n\nA narrative format is preferable to an enumeration or a nonlinear presentation such as what would arise from reading off an infographic, for example.\n\n\nBe clear and direct!\n\n\nDon't say filler words like \"uhm\" \"like\". Take a pause instead\n\n\nConsciously speak slowly than you normally do (for fast talkers)\n\n\nspeak confidently and know material well enough to sound natural/not just memorize material\n\n\ninteract with the audience\n\n\nTalk slowly and clearly\n\n\nPut yourself in the shoes of your audience\n\n\nSpeak clearly at a good pace (not too fast or slow), make eye contact and engage with your audience\n\n\nEnunciation, proper volume, etc.\n\n\nI tend to speak in long sentences which can confuse the audience.\n\n\nTalk slower and clearer. Enunciation. Eye contact while talking. Avoid filler words.\n\n\nspeak clearly, slow down if you need to, don't just read off slides when presenting\n\n\nbe sure to point out areas of interest on your plots and explain them\n\n\nUse simply words if possible\n\n\nSpeaking slowly at someone\n\n\nDon't assume the audience know the same thing (like the research background or the research design) as you do. Another thing is: try to make sentence as simple as possible.\n\n\nTake moments to pause in between you sentences if you get lost.\n\n\nSpeak slowly and clearly\n\n\nKeep it engaging, involve audience participation, make eye contact, be confident\n\n\nTalk clearly and stick with the theme\n\n\nCater to your audience. Be conscious of what they know and don't know.\n\n\nUse appropriate font.\n\n\nSpeak clearly and slow down when you're picking up a fast pace."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#presentations-are-for-listening",
    "href": "content/lectures/08-effective-communication-slides.html#presentations-are-for-listening",
    "title": "08-effective-communication",
    "section": "Presentations are for listening",
    "text": "Presentations are for listening\n\nAdvantage: words to explain out loud what you’re showing\n\n\n\nYou are presenting for the person in the back of the room.\n\n\n\nTo accomplish:\n\ndon’t read directly off slides\nrepetition is ok: tell what you’re going to tell them, tell them, tell them what you told them\nuse animation to build your story (not to distract)\nintroduce your axes\ntext/labels larger\nwatch your speech speed\npractice!"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "href": "content/lectures/08-effective-communication-slides.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "title": "08-effective-communication",
    "section": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood",
    "text": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood\n\nRed Riding Hood (RRH) has to walk 0.54 mi from Point A (home) to Point B (Grandma’s)\nRRH meets Wolf who (1) runs ahead to Grandma’s, (2) eats her, and (3) dresses in her clothes\nRRH arrives at Grandmas at 2PM, asks her three questions\nIdentified problem: after third question, Wolf eats RRH\nSolution: vendor (Woodsman) employs tool (ax)\nExpected outcome: Grandma and RRH alive, wolf is not"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses-1",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses-1",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nWhen sending a status update on a project to people in my team, I often had a habit of over explaining things such as specific terms or what a specific p-value indicates, etc. and it was redundant to my team, who all knew what these terms mean and how they are defined. On the flip side, during a meeting with non-technical people, a lot of my team's work didn't make sense to some people in the meeting and they requested info in \"layman terms\". My mentor advised me to not over-explain terms in depth to technical people, but keep things simple, clear and concise to those without a technical background.\n\n\nNA\n\n\nBe as concise as possible while getting your point across\n\n\nno need to write full sentences for bullet points\n\n\nwrite in a concise manner, don’t use big words unless it’s relevant\n\n\nkeep things succinct and write in a neutral tone\n\n\nBold or italicize important ideas/ key words in long writing\n\n\nMain idea sentence in the beginning of your text (report, essay, email).\n\n\nRefrain from using the first person. Talk in the past-tense\n\n\nUse grammarly\n\n\nWatch repetition of certain words. Occasionally change the structure of sentences. Know your audience.\n\n\nDont be too repetitive and Don’t have run on long sentences and get caught up in the details too much - I do that a lot ://\n\n\nAvoid ambiguity, have someone else proof read to double check what you've written, try not to make your sentences too wordy.\n\n\nWrite for your audience, avoid overuse of jargon and if necessary be sure to define the terms in a way appropriate for how you’re actually using them.\n\n\nBe clear and concise with the points you're trying to make and don't lose them with sentences that run on for too long\n\n\nBe concise and use as few words to effectively get point across. Don't go off on tangents.\n\n\nOrganize using subheadings, highlight main points using bold or colors if appropriate, vary sentence structure\n\n\nmake your sentences simpler to understand\n\n\nWhen giving a status report to a technical team, no need to over-explain terms. It is a lot of times effective to make a concise bullet point list such as p value= x, correlation coefficinet = y, instead of overexplaining what each value means b/c a technical team probably would know the signifance anyways\n\n\nWrite in words that the readers will understand, and do not assume that the readers will know what you mean.\n\n\nuse an outline to help organize the order of your paper. it helps you figure out where to place images, plots, and text\n\n\nOrganize arguments, don't be overly repetitive"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#benefits-of-written-communciation",
    "href": "content/lectures/08-effective-communication-slides.html#benefits-of-written-communciation",
    "title": "08-effective-communication",
    "section": "Benefits of written communciation",
    "text": "Benefits of written communciation\nYour audience has time to process…but the explanation has to be there!\n\nVisually: more on a single visualization\n\n\nYes, often there are different visualizations for reports/papers than for presentations/lectures."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#when-you-have-time-to-digest-read",
    "href": "content/lectures/08-effective-communication-slides.html#when-you-have-time-to-digest-read",
    "title": "08-effective-communication",
    "section": "When you have time to digest (read)",
    "text": "When you have time to digest (read)\n\n\n❓ What makes this an effective visualization for a written communication?”\nSource: Storytelling wtih data by cole nussbaumer knaflic"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#written-explanations",
    "href": "content/lectures/08-effective-communication-slides.html#written-explanations",
    "title": "08-effective-communication",
    "section": "Written Explanations",
    "text": "Written Explanations\n\nVisualizations should be explained/interpreted\nModels should be explained\n\nshould be clear what question is being answered\nwhat conclusions is being drawn\nand what numbers were used to draw that conclusion"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#data-science-reports-in-.rmd",
    "href": "content/lectures/08-effective-communication-slides.html#data-science-reports-in-.rmd",
    "title": "08-effective-communication",
    "section": "Data Science Reports in .Rmd",
    "text": "Data Science Reports in .Rmd\n\nAs concise as possible\nNecessary details (for your audience); nothing more\n\nBe sure that the knit output contains what you intended (plots displayed; headers etc.)\n…and does NOT display stuff that doesn’t need to be there (messages/warnings suppressed, brainstorming, etc.)\n\nTypical Sections: Introduction/Background, Setup, Data, Analysis, Conclusion, References"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#controlling-html-document-settings",
    "href": "content/lectures/08-effective-communication-slides.html#controlling-html-document-settings",
    "title": "08-effective-communication",
    "section": "Controlling HTML document settings",
    "text": "Controlling HTML document settings\n\nTable of Contents\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\nTheme\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    theme: united\n    highlight: tango\n---\n\n\n\nFigure Options\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    fig_width: 7\n    fig_height: 6\n    fig_caption: true\n---\n\n\n\nCode Folding\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    code_folding: hide\n---"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#controlling-code-chunk-output",
    "href": "content/lectures/08-effective-communication-slides.html#controlling-code-chunk-output",
    "title": "08-effective-communication",
    "section": "Controlling code chunk output",
    "text": "Controlling code chunk output\n\nSpecified in the curly braces, separated by commas\n\n\n\neval: whether to execute the code chunk\necho: whether to include the code in the output\nwarning, message, and error: whether to show warnings, messages, or errors in the knit document\nfig.width and fig.height: control the width/height of plots\n\n\n\n\nControlling for the whole document:\n\nknitr::opts_chunk$set(fig.width = 8, collapse = TRUE)"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#editing-proofreading",
    "href": "content/lectures/08-effective-communication-slides.html#editing-proofreading",
    "title": "08-effective-communication",
    "section": "Editing & Proofreading",
    "text": "Editing & Proofreading\n\nDid you end up telling a story?\n\nThings missing?\nThings to delete?\n\n\n\n\nDo not fall in love with your words/code/plots\n\n\n\n\nDo spell check\nDo read it over before sending/presenting/submitting"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#aside-citing-sources",
    "href": "content/lectures/08-effective-communication-slides.html#aside-citing-sources",
    "title": "08-effective-communication",
    "section": "Aside: Citing Sources",
    "text": "Aside: Citing Sources\nWhen are citations needed?\n\n\n\n“We will be doing our analysis using two different data sets created by two different groups: Donohue and Mustard + Lott, or simply Lott”\n\n\n\n\n\n\n“What turned from the idea of carrying firearms to protect oneself from enemies such as the British monarchy and the unknown frontier of North America has now become a nationwide issue.”\n\n\n\n\n\n\n“Right to Carry Laws refer to laws that specify how citizens are allowed to carry concealed handguns when they’re away from home without a permit”\n\n\n\n\n\n\n“In this case study, we are examining the relationship between unemployment rate, poverty rate, police staffing, and violent crime rate.”\n\n\n\n\n\n\n“In the United States, the second amendment permits the right to bear arms, and this law has not been changed since its creation in 1791.”\n\n\n\n\n\n\n“The Right to Carry Laws (RTC) is defined as”a law that specifies if and how citizens are allowed to have a firearm on their person or nearby in public.””\n\n\n\n\nReminder: You do NOT get docked points for citing others’ work. You can be at risk of AI Violation if you don’t. When in doubt, give credit."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#footnotes-in-.rmd",
    "href": "content/lectures/08-effective-communication-slides.html#footnotes-in-.rmd",
    "title": "08-effective-communication",
    "section": "Footnotes in .Rmd",
    "text": "Footnotes in .Rmd\nHow to specify a footnote in text:\nHere is some body text.[^1]\nHow to include the footnote’s reference:\n[^1]: This footnote will appear at the bottom of the page.\n\n\nNote: .bib files can be included with BibTeX references using the bibliography parameter in your YAML"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses-2",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses-2",
    "title": "08-effective-communication",
    "section": "Student Responses",
    "text": "Student Responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nDon’t make slides overly colorful\n\n\nno borders on plots, graphs.don't write the whole info on one slide. take advantage of white space\n\n\nuse images to help audience understand\n\n\nHighlight important points in visualization.\n\n\ndon’t write everything on slides, just main points, try to use pictures that model/reflect/support talking points\n\n\nuse a legend for graphs\n\n\nmake sure your plot is relevant to the point you are trying to make\n\n\nreduce the number of words on the slide\n\n\nIt should be easy to understand/digest relatively quickly, only put absolutely necessary/relevant things\n\n\nPick a font and size for body+headings and commit to it\n\n\nTry to keep the design minimalistic and aesthetic, no cognitive overload that way.\n\n\nTitle your plots & graphs\n\n\nImages/visuals should help strengthen your presentation/story, not distracting from it\n\n\nUse lots of pictures!\n\n\nIt's better to have meaningful and intuitive color selection.\n\n\nSpecific graphs are more beneficial to a technical audience, while others are better for a non-technical one. My coworkers like graphs such as boxplots, but when presenting to partners, I have found that they prefer more intuitive/popular graphs like histograms or line plots.\n\n\nComplementary colors, appropriate graphs for the type of information you have and want to get across, neat and not cluttered\n\n\nConcise and clear, use colors and space effectively\n\n\nUse color responsibly in graphs/tables, make text large enough for everyone in the room to see, don't overload slides with information\n\n\ndont put too much words\n\n\nDon’t put too many animations (if any)\n\n\nLess is more. Too much can distract and detract from the main point\n\n\ngood contrast color between background and text\n\n\nmake clear visual guide, don’t make it too complicated\n\n\nAvoid neon colors\n\n\nKeep accessibility in mind when presenting visuals. (e.g. using texture instead of color, image descriptions, etc.)\n\n\nMake presentations look cleaner. Seems like you know what youre talking about."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#the-glamour-of-graphics",
    "href": "content/lectures/08-effective-communication-slides.html#the-glamour-of-graphics",
    "title": "08-effective-communication",
    "section": "The Glamour of Graphics",
    "text": "The Glamour of Graphics\n\nbuilds on top of the grammar (components) of a graphic\nconsiderations for the design of a graphic\ncolor, typography, layout\ngoing from accurate to 😍effective\n\n\n\nThese ideas and slides are all modified from Will Chase’s rstudio::conf2020 slides/talk"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#left-align-titles-at-top-left",
    "href": "content/lectures/08-effective-communication-slides.html#left-align-titles-at-top-left",
    "title": "08-effective-communication",
    "section": "Left-align titles at top-left",
    "text": "Left-align titles at top-left\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#avoid-head-tilting",
    "href": "content/lectures/08-effective-communication-slides.html#avoid-head-tilting",
    "title": "08-effective-communication",
    "section": "Avoid head-tilting",
    "text": "Avoid head-tilting\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#borders-backgrounds",
    "href": "content/lectures/08-effective-communication-slides.html#borders-backgrounds",
    "title": "08-effective-communication",
    "section": "Borders & Backgrounds: 👎",
    "text": "Borders & Backgrounds: 👎\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_bw() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#organize-removelighten-as-much-as-possible",
    "href": "content/lectures/08-effective-communication-slides.html#organize-removelighten-as-much-as-possible",
    "title": "08-effective-communication",
    "section": "Organize & Remove/Lighten as much as possible",
    "text": "Organize & Remove/Lighten as much as possible\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#legends-suck",
    "href": "content/lectures/08-effective-communication-slides.html#legends-suck",
    "title": "08-effective-communication",
    "section": "Legends suck",
    "text": "Legends suck\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 7) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 20) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#additional-guidance",
    "href": "content/lectures/08-effective-communication-slides.html#additional-guidance",
    "title": "08-effective-communication",
    "section": "Additional Guidance",
    "text": "Additional Guidance\n\nWhite space is like garlic - take the amount you need and triple it\nFonts Matter\nUse Color Effectively\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html",
    "href": "content/lectures/08-effective-communication.html",
    "title": "08-effective-communication",
    "section": "",
    "text": "Q: for the last part of lecture 07, I tried glance(m_ht_wt) but it didn’t work because m_ht_wt doesn’t exist. Is “m_ht_wt” supposed be a model?\nA: Yup, this model was the height by weight model:\n\n\nm_ht_wt <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\n\nQ: I was not too sure what was going on when talking about the relationship between painting height and school.\nA: I don’t think you were the only one confused! Briefly here (and I’m happy to chat more before/after class and in OH), we were looking to determine/quantify the relationship between the size (height) of a painting and the school from which the painting originated. This was an example of having more than two categories for a categorical (factor) predictor. The important points were undersatnding that each level is compared to the baseline and the linear model that results from multiple categories. Part 4 of the lab gets into this a bit more too. Definitely follow up if you’re unsure after doing that part of the lab!\n\n\nQ: How do you calculate the linear regression model when you have non-numeric values? For example, on lab 04, when it asks to calculate the linear regression model by gender, the gender appears only as male and female. Suppose male is 1 and female is 0 (interpreted by the function), then male linear regression model is y =ax + 1?\nA: Close! the “1” would be plugged in as the value of x (in what you suggested)m not for the intercept. So the function would be \\(y=\\beta_1*1 + \\beta_0\\)\n\n\n\n\n\nLab04 due Friday\nHW02 due Monday\n\n\n\nPractice Midterms Now Available\n\nanswers posted next week\n\nMidterm Exam\n\nwill cover material through “Multiple Linear Regression”\nwill be released/posted next Friday after lab\nwill be due Monday Nov 6th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be completed individually (open Notes; open Internet)\n\n\n\n\n\nLink for Later\n\n\n\n\n\n\n\n\n\nCommunicating for your audience\nOral Communication\nWritten Communication\nVisual Communication\n\n\n\n\n\nBookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "href": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "title": "08-effective-communication",
    "section": "What does this mean?",
    "text": "What does this mean?\n❓ What does it mean to “consider your audience?”\n\nSimply: You do the work so they don’t have to.\n\n\n…also the aesthetic-usability effect exists."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "href": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "title": "08-effective-communication",
    "section": "What’s the right level?",
    "text": "What’s the right level?\n\n\nGeneral Audience\n✔ background\n🚫 limit technical details\n🎉 emphasize take-home\n\n\n\nTechnical Audience\n⬇ limit background\n💻 all-the-details\n🎉 emphasize take-home"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#considerations",
    "href": "content/lectures/08-effective-communication.html#considerations",
    "title": "08-effective-communication",
    "section": "Considerations",
    "text": "Considerations\n\nPlatform: written? oral?\n\n\n\nSetting: informal? formal?\n\n\n\n\nTiming: never go over your time limit!"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#storytelling",
    "href": "content/lectures/08-effective-communication.html#storytelling",
    "title": "08-effective-communication",
    "section": "Storytelling",
    "text": "Storytelling\n\nStories have a beginning, a middle, and an end.\n\n\n\nStories do not need every detail of what you’ve tried\n\n\n\n\nReports and presentations should tell a story\nPlanning out your report/presentation can help\n\n\n\n\nHold the audience’s attention with what needs to be said; do so effectively\nTell your audience why they should care; why it matters\nYou should explain your choices and the “why”"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "href": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "title": "08-effective-communication",
    "section": "Choose informative titles",
    "text": "Choose informative titles\nOn presentations: Balance b/w short and informative (goal: concise)\n\n\nAvoid: “Analyzing NHANES”\n\nBetter: “Data from the NHANES study shows that diet is related to overall health”\n\n\nOn visualizations: emphasize the take-home! (what’s learned or what action to take)\n\n\n\nAvoid: “Boxplot of gender”\n\nBetter: “Twice as many females as males included for analysis”\n\n\n\nAvoid: “Tickets vs. Time”\n\nBetter: “Staff unable to respond to incoming tickets; need to hire 2 FTEs”"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses",
    "href": "content/lectures/08-effective-communication.html#student-responses",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nconsider your audience\n\n\nspeak organizedly and logically\n\n\nA narrative format is preferable to an enumeration or a nonlinear presentation such as what would arise from reading off an infographic, for example.\n\n\nBe clear and direct!\n\n\nDon't say filler words like \"uhm\" \"like\". Take a pause instead\n\n\nConsciously speak slowly than you normally do (for fast talkers)\n\n\nspeak confidently and know material well enough to sound natural/not just memorize material\n\n\ninteract with the audience\n\n\nTalk slowly and clearly\n\n\nPut yourself in the shoes of your audience\n\n\nSpeak clearly at a good pace (not too fast or slow), make eye contact and engage with your audience\n\n\nEnunciation, proper volume, etc.\n\n\nI tend to speak in long sentences which can confuse the audience.\n\n\nTalk slower and clearer. Enunciation. Eye contact while talking. Avoid filler words.\n\n\nspeak clearly, slow down if you need to, don't just read off slides when presenting\n\n\nbe sure to point out areas of interest on your plots and explain them\n\n\nUse simply words if possible\n\n\nSpeaking slowly at someone\n\n\nDon't assume the audience know the same thing (like the research background or the research design) as you do. Another thing is: try to make sentence as simple as possible.\n\n\nTake moments to pause in between you sentences if you get lost.\n\n\nSpeak slowly and clearly\n\n\nKeep it engaging, involve audience participation, make eye contact, be confident\n\n\nTalk clearly and stick with the theme\n\n\nCater to your audience. Be conscious of what they know and don't know.\n\n\nUse appropriate font.\n\n\nSpeak clearly and slow down when you're picking up a fast pace."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "href": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "title": "08-effective-communication",
    "section": "Presentations are for listening",
    "text": "Presentations are for listening\n\nAdvantage: words to explain out loud what you’re showing\n\n\n\nYou are presenting for the person in the back of the room.\n\n\n\nTo accomplish:\n\ndon’t read directly off slides\nrepetition is ok: tell what you’re going to tell them, tell them, tell them what you told them\nuse animation to build your story (not to distract)\nintroduce your axes\ntext/labels larger\nwatch your speech speed\npractice!"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "href": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "title": "08-effective-communication",
    "section": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood",
    "text": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood\n\nRed Riding Hood (RRH) has to walk 0.54 mi from Point A (home) to Point B (Grandma’s)\nRRH meets Wolf who (1) runs ahead to Grandma’s, (2) eats her, and (3) dresses in her clothes\nRRH arrives at Grandmas at 2PM, asks her three questions\nIdentified problem: after third question, Wolf eats RRH\nSolution: vendor (Woodsman) employs tool (ax)\nExpected outcome: Grandma and RRH alive, wolf is not"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-1",
    "href": "content/lectures/08-effective-communication.html#student-responses-1",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nWhen sending a status update on a project to people in my team, I often had a habit of over explaining things such as specific terms or what a specific p-value indicates, etc. and it was redundant to my team, who all knew what these terms mean and how they are defined. On the flip side, during a meeting with non-technical people, a lot of my team's work didn't make sense to some people in the meeting and they requested info in \"layman terms\". My mentor advised me to not over-explain terms in depth to technical people, but keep things simple, clear and concise to those without a technical background.\n\n\nNA\n\n\nBe as concise as possible while getting your point across\n\n\nno need to write full sentences for bullet points\n\n\nwrite in a concise manner, don’t use big words unless it’s relevant\n\n\nkeep things succinct and write in a neutral tone\n\n\nBold or italicize important ideas/ key words in long writing\n\n\nMain idea sentence in the beginning of your text (report, essay, email).\n\n\nRefrain from using the first person. Talk in the past-tense\n\n\nUse grammarly\n\n\nWatch repetition of certain words. Occasionally change the structure of sentences. Know your audience.\n\n\nDont be too repetitive and Don’t have run on long sentences and get caught up in the details too much - I do that a lot ://\n\n\nAvoid ambiguity, have someone else proof read to double check what you've written, try not to make your sentences too wordy.\n\n\nWrite for your audience, avoid overuse of jargon and if necessary be sure to define the terms in a way appropriate for how you’re actually using them.\n\n\nBe clear and concise with the points you're trying to make and don't lose them with sentences that run on for too long\n\n\nBe concise and use as few words to effectively get point across. Don't go off on tangents.\n\n\nOrganize using subheadings, highlight main points using bold or colors if appropriate, vary sentence structure\n\n\nmake your sentences simpler to understand\n\n\nWhen giving a status report to a technical team, no need to over-explain terms. It is a lot of times effective to make a concise bullet point list such as p value= x, correlation coefficinet = y, instead of overexplaining what each value means b/c a technical team probably would know the signifance anyways\n\n\nWrite in words that the readers will understand, and do not assume that the readers will know what you mean.\n\n\nuse an outline to help organize the order of your paper. it helps you figure out where to place images, plots, and text\n\n\nOrganize arguments, don't be overly repetitive"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "href": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "title": "08-effective-communication",
    "section": "Benefits of written communciation",
    "text": "Benefits of written communciation\nYour audience has time to process…but the explanation has to be there!\n\nVisually: more on a single visualization\n\n\nYes, often there are different visualizations for reports/papers than for presentations/lectures."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "href": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "title": "08-effective-communication",
    "section": "When you have time to digest (read)",
    "text": "When you have time to digest (read)\n\n\n❓ What makes this an effective visualization for a written communication?”\nSource: Storytelling wtih data by cole nussbaumer knaflic"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#written-explanations",
    "href": "content/lectures/08-effective-communication.html#written-explanations",
    "title": "08-effective-communication",
    "section": "Written Explanations",
    "text": "Written Explanations\n\nVisualizations should be explained/interpreted\nModels should be explained\n\nshould be clear what question is being answered\nwhat conclusions is being drawn\nand what numbers were used to draw that conclusion"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "title": "08-effective-communication",
    "section": "Data Science Reports in .Rmd",
    "text": "Data Science Reports in .Rmd\n\nAs concise as possible\nNecessary details (for your audience); nothing more\n\nBe sure that the knit output contains what you intended (plots displayed; headers etc.)\n…and does NOT display stuff that doesn’t need to be there (messages/warnings suppressed, brainstorming, etc.)\n\nTypical Sections: Introduction/Background, Setup, Data, Analysis, Conclusion, References"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "href": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "title": "08-effective-communication",
    "section": "Controlling HTML document settings",
    "text": "Controlling HTML document settings\n\nTable of Contents\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\nTheme\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    theme: united\n    highlight: tango\n---\n\n\n\nFigure Options\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    fig_width: 7\n    fig_height: 6\n    fig_caption: true\n---\n\n\n\nCode Folding\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    code_folding: hide\n---"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "href": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "title": "08-effective-communication",
    "section": "Controlling code chunk output",
    "text": "Controlling code chunk output\n\nSpecified in the curly braces, separated by commas\n\n\n\neval: whether to execute the code chunk\necho: whether to include the code in the output\nwarning, message, and error: whether to show warnings, messages, or errors in the knit document\nfig.width and fig.height: control the width/height of plots\n\n\n\n\nControlling for the whole document:\n\nknitr::opts_chunk$set(fig.width = 8, collapse = TRUE)"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#editing-proofreading",
    "href": "content/lectures/08-effective-communication.html#editing-proofreading",
    "title": "08-effective-communication",
    "section": "Editing & Proofreading",
    "text": "Editing & Proofreading\n\nDid you end up telling a story?\n\nThings missing?\nThings to delete?\n\n\n\n\nDo not fall in love with your words/code/plots\n\n\n\n\nDo spell check\nDo read it over before sending/presenting/submitting"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "href": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "title": "08-effective-communication",
    "section": "Aside: Citing Sources",
    "text": "Aside: Citing Sources\nWhen are citations needed?\n\n\n\n“We will be doing our analysis using two different data sets created by two different groups: Donohue and Mustard + Lott, or simply Lott”\n\n\n\n\n\n\n“What turned from the idea of carrying firearms to protect oneself from enemies such as the British monarchy and the unknown frontier of North America has now become a nationwide issue.”\n\n\n\n\n\n\n“Right to Carry Laws refer to laws that specify how citizens are allowed to carry concealed handguns when they’re away from home without a permit”\n\n\n\n\n\n\n“In this case study, we are examining the relationship between unemployment rate, poverty rate, police staffing, and violent crime rate.”\n\n\n\n\n\n\n“In the United States, the second amendment permits the right to bear arms, and this law has not been changed since its creation in 1791.”\n\n\n\n\n\n\n“The Right to Carry Laws (RTC) is defined as”a law that specifies if and how citizens are allowed to have a firearm on their person or nearby in public.””\n\n\n\n\nReminder: You do NOT get docked points for citing others’ work. You can be at risk of AI Violation if you don’t. When in doubt, give credit."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "title": "08-effective-communication",
    "section": "Footnotes in .Rmd",
    "text": "Footnotes in .Rmd\nHow to specify a footnote in text:\nHere is some body text.[^1]\nHow to include the footnote’s reference:\n[^1]: This footnote will appear at the bottom of the page.\n\n\nNote: .bib files can be included with BibTeX references using the bibliography parameter in your YAML"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-2",
    "href": "content/lectures/08-effective-communication.html#student-responses-2",
    "title": "08-effective-communication",
    "section": "Student Responses",
    "text": "Student Responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nDon’t make slides overly colorful\n\n\nno borders on plots, graphs.don't write the whole info on one slide. take advantage of white space\n\n\nuse images to help audience understand\n\n\nHighlight important points in visualization.\n\n\ndon’t write everything on slides, just main points, try to use pictures that model/reflect/support talking points\n\n\nuse a legend for graphs\n\n\nmake sure your plot is relevant to the point you are trying to make\n\n\nreduce the number of words on the slide\n\n\nIt should be easy to understand/digest relatively quickly, only put absolutely necessary/relevant things\n\n\nPick a font and size for body+headings and commit to it\n\n\nTry to keep the design minimalistic and aesthetic, no cognitive overload that way.\n\n\nTitle your plots & graphs\n\n\nImages/visuals should help strengthen your presentation/story, not distracting from it\n\n\nUse lots of pictures!\n\n\nIt's better to have meaningful and intuitive color selection.\n\n\nSpecific graphs are more beneficial to a technical audience, while others are better for a non-technical one. My coworkers like graphs such as boxplots, but when presenting to partners, I have found that they prefer more intuitive/popular graphs like histograms or line plots.\n\n\nComplementary colors, appropriate graphs for the type of information you have and want to get across, neat and not cluttered\n\n\nConcise and clear, use colors and space effectively\n\n\nUse color responsibly in graphs/tables, make text large enough for everyone in the room to see, don't overload slides with information\n\n\ndont put too much words\n\n\nDon’t put too many animations (if any)\n\n\nLess is more. Too much can distract and detract from the main point\n\n\ngood contrast color between background and text\n\n\nmake clear visual guide, don’t make it too complicated\n\n\nAvoid neon colors\n\n\nKeep accessibility in mind when presenting visuals. (e.g. using texture instead of color, image descriptions, etc.)\n\n\nMake presentations look cleaner. Seems like you know what youre talking about."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "href": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "title": "08-effective-communication",
    "section": "The Glamour of Graphics",
    "text": "The Glamour of Graphics\n\nbuilds on top of the grammar (components) of a graphic\nconsiderations for the design of a graphic\ncolor, typography, layout\ngoing from accurate to 😍effective\n\n\n\nThese ideas and slides are all modified from Will Chase’s rstudio::conf2020 slides/talk"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "href": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "title": "08-effective-communication",
    "section": "Left-align titles at top-left",
    "text": "Left-align titles at top-left\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "href": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "title": "08-effective-communication",
    "section": "Avoid head-tilting",
    "text": "Avoid head-tilting\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "href": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "title": "08-effective-communication",
    "section": "Borders & Backgrounds: 👎",
    "text": "Borders & Backgrounds: 👎\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_bw() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "href": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "title": "08-effective-communication",
    "section": "Organize & Remove/Lighten as much as possible",
    "text": "Organize & Remove/Lighten as much as possible\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#legends-suck",
    "href": "content/lectures/08-effective-communication.html#legends-suck",
    "title": "08-effective-communication",
    "section": "Legends suck",
    "text": "Legends suck\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 7) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 20) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#additional-guidance",
    "href": "content/lectures/08-effective-communication.html#additional-guidance",
    "title": "08-effective-communication",
    "section": "Additional Guidance",
    "text": "Additional Guidance\n\nWhite space is like garlic - take the amount you need and triple it\nFonts Matter\nUse Color Effectively"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#qa",
    "href": "content/lectures/06-analysis-slides.html#qa",
    "title": "06-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I was most confused about the differences between facet_wrap and facet_grid. I didn’t understand what made them so different\nA: You’re right that they’re very similar and can even at times be used/coerced to do what the other does. Basically, if you want a 2D comparison (variable1 x variable2), facet_grid. If you just want to break down the plots by a single variable, facet_wrap.\n\n\nQ: Will future HW assignments also include things that were not explicitly covered in class? For example, HW1 which was due last night, included concepts and materials that were just covered today.\nA: Nope. That was my fault. We were behind where I intended to be. This will not be an issue going forward.\n\n\nQ: can we plot data from 2 separate data frames somehow?\nA: Not typically in ggplot2. Instead, data would have to be wrangled into the tidy format (single dataset) first.\n\n\nQ: does filling= mean seperate and color by the value passed in, and we don’t need to assign color again? What if we don’t want color?\nA: Fill indicates how to fill the plot with color. If you don’t want color, you wouldn’t use fill\n\n\nQ: How we would do the second suggestion in how to visualize the brexit data differently?\nA: We’ll do this together today!\n\n\nQ: for datasets that are included in a library (like penguins one), could we read it into a df to see it in the environment? or is that redundant and we should only view it, use functions to see var names, etc\nA: That would be ok!\n\n\nQ: I was wondering where I can find all available themes.\nA: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n\nQ: I have heard about plotly before. I was wondering what will be its difference between this and ggplot?\nA: There is a ggplotly package! Plotly enables interaction on top of ggplot2 packages"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#course-announcements",
    "href": "content/lectures/06-analysis-slides.html#course-announcements",
    "title": "06-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 03 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\n\nNotes:\n\nHow To: Chunk output in console\n\n\n\n\nCase Studies Discussion\n\nBiomarkers of recent marijuana usage (Linear Regression)\nVaping in American Youth (Logistic Regression)\nRight-to-Carry Laws Effect on Violent Crime (Multiple Linear Regression)\nPredicting Annual Air Pollution (Machine Learning)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#ad-css-masters",
    "href": "content/lectures/06-analysis-slides.html#ad-css-masters",
    "title": "06-analysis",
    "section": "[ad] CSS Masters",
    "text": "[ad] CSS Masters"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#agenda",
    "href": "content/lectures/06-analysis-slides.html#agenda",
    "title": "06-analysis",
    "section": "Agenda",
    "text": "Agenda\n\nDiscuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#suggested-reading",
    "href": "content/lectures/06-analysis-slides.html#suggested-reading",
    "title": "06-analysis",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nIntroduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#what-is-eda",
    "href": "content/lectures/06-analysis-slides.html#what-is-eda",
    "title": "06-analysis",
    "section": "What is EDA?",
    "text": "What is EDA?\n\nExploratory data analysis (EDA) is an aproach to analyzing data sets to summarize and understand its main characteristics.\nOften, this is visual….but the visuals do not have to be perfect. (Save that for communication)\nCalculating summary statistics is also part of EDA.\n\n\n\nData tidying/wrangling/manipulation/transformation typically happens before this stage of the analysis.\n\n\n\nThe Goal: KNOW YOUR DATA!"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#consider-no.-of-variables-involved",
    "href": "content/lectures/06-analysis-slides.html#consider-no.-of-variables-involved",
    "title": "06-analysis",
    "section": "Consider: No. of variables involved",
    "text": "Consider: No. of variables involved\n\nUnivariate data analysis - distribution of single variable\nBivariate data analysis - relationship between two variables\nMultivariate data analysis - relationship between many variables at once, usually focusing on the relationship between two while conditioning for others"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#consider-types-of-variables",
    "href": "content/lectures/06-analysis-slides.html#consider-types-of-variables",
    "title": "06-analysis",
    "section": "Consider: Types of variables",
    "text": "Consider: Types of variables\n\nNumerical variables can be classified as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.\nIf the variable is categorical, we can determine if it is ordinal based on whether or not the levels have a natural ordering."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#data-visualization",
    "href": "content/lectures/06-analysis-slides.html#data-visualization",
    "title": "06-analysis",
    "section": "Data visualization",
    "text": "Data visualization\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\n\nData visualization is the creation and study of the visual representation of data.\nThere are many tools for visualizing data (R is one of them), and many approaches/systems within R for making data visualizations (ggplot2 is what we’ll continue to use).\nEDA will involve making plots/visualizing your data"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#paris-paintings",
    "href": "content/lectures/06-analysis-slides.html#paris-paintings",
    "title": "06-analysis",
    "section": "Paris Paintings",
    "text": "Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nSource: Printed catalogs of 28 auction sales in Paris, 1764 - 1780\nData curators Sandra van Ginhoven and Hilary Coe Cronheim (who were PhD students in the Duke Art, Law, and Markets Initiative at the time of putting together this dataset) translated and tabulated the catalogues\n3393 paintings, their prices, and descriptive details from sales catalogues over 60 variables"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auctions-today",
    "href": "content/lectures/06-analysis-slides.html#auctions-today",
    "title": "06-analysis",
    "section": "Auctions today",
    "text": "Auctions today\n\n\n\n\n\n\n\nSource: Sothebys"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auctions-back-in-the-day",
    "href": "content/lectures/06-analysis-slides.html#auctions-back-in-the-day",
    "title": "06-analysis",
    "section": "Auctions back in the day",
    "text": "Auctions back in the day\n\nSource: Pierre-Antoine de Machy, Public Sale at the Hôtel Bullion, Musée Carnavalet, Paris (18th century)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#paris-auction-market",
    "href": "content/lectures/06-analysis-slides.html#paris-auction-market",
    "title": "06-analysis",
    "section": "Paris auction market",
    "text": "Paris auction market"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse",
    "href": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse",
    "title": "06-analysis",
    "section": "Depart pour la chasse",
    "text": "Depart pour la chasse"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auction-catalogue-text",
    "href": "content/lectures/06-analysis-slides.html#auction-catalogue-text",
    "title": "06-analysis",
    "section": "Auction catalogue text",
    "text": "Auction catalogue text\n\n\n\n\nTwo paintings very rich in composition, of a beautiful execution, and whose merit is very remarkable, each 17 inches 3 lines high, 23 inches wide; the first, painted on wood, comes from the Cabinet of Madame la Comtesse de Verrue; it represents a departure for the hunt: it shows in the front a child on a white horse, a man who gives the horn to gather the dogs, a falconer and other figures nicely distributed across the width of the painting; two horses drinking from a fountain; on the right in the corner a lovely country house topped by a terrace, on which people are at the table, others who play instruments; trees and fabriques pleasantly enrich the background."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse-as-data",
    "href": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse-as-data",
    "title": "06-analysis",
    "section": "Depart pour la chasse as Data",
    "text": "Depart pour la chasse as Data"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#section",
    "href": "content/lectures/06-analysis-slides.html#section",
    "title": "06-analysis",
    "section": "",
    "text": "pp |>\n  filter(name == \"R1777-89a\") |>\n  glimpse()\n\nRows: 1\nColumns: 61\n$ name              <chr> \"R1777-89a\"\n$ sale              <chr> \"R1777\"\n$ lot               <chr> \"89\"\n$ position          <dbl> 0.3755274\n$ dealer            <chr> \"R\"\n$ year              <dbl> 1777\n$ origin_author     <chr> \"D/FL\"\n$ origin_cat        <chr> \"D/FL\"\n$ school_pntg       <chr> \"D/FL\"\n$ diff_origin       <dbl> 0\n$ logprice          <dbl> 8.575462\n$ price             <dbl> 5300\n$ count             <dbl> 1\n$ subject           <chr> \"D\\x8epart pour la chasse\"\n$ authorstandard    <chr> \"Wouwerman, Philips\"\n$ artistliving      <dbl> 0\n$ authorstyle       <chr> NA\n$ author            <chr> \"Philippe Wouwermans\"\n$ winningbidder     <chr> \"Langlier, Jacques for Poullain, Antoine\"\n$ winningbiddertype <chr> \"DC\"\n$ endbuyer          <chr> \"C\"\n$ Interm            <dbl> 1\n$ type_intermed     <chr> \"D\"\n$ Height_in         <dbl> 17.25\n$ Width_in          <dbl> 23\n$ Surface_Rect      <dbl> 396.75\n$ Diam_in           <dbl> NA\n$ Surface_Rnd       <dbl> NA\n$ Shape             <chr> \"squ_rect\"\n$ Surface           <dbl> 396.75\n$ material          <chr> \"bois\"\n$ mat               <chr> \"b\"\n$ materialCat       <chr> \"wood\"\n$ quantity          <dbl> 1\n$ nfigures          <dbl> 0\n$ engraved          <dbl> 0\n$ original          <dbl> 0\n$ prevcoll          <dbl> 1\n$ othartist         <dbl> 0\n$ paired            <dbl> 1\n$ figures           <dbl> 0\n$ finished          <dbl> 0\n$ lrgfont           <dbl> 0\n$ relig             <dbl> 0\n$ landsALL          <dbl> 1\n$ lands_sc          <dbl> 0\n$ lands_elem        <dbl> 1\n$ lands_figs        <dbl> 1\n$ lands_ment        <dbl> 0\n$ arch              <dbl> 1\n$ mytho             <dbl> 0\n$ peasant           <dbl> 0\n$ othgenre          <dbl> 0\n$ singlefig         <dbl> 0\n$ portrait          <dbl> 0\n$ still_life        <dbl> 0\n$ discauth          <dbl> 0\n$ history           <dbl> 0\n$ allegory          <dbl> 0\n$ pastorale         <dbl> 0\n$ other             <dbl> 0"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-numerical-data",
    "href": "content/lectures/06-analysis-slides.html#visualizing-numerical-data",
    "title": "06-analysis",
    "section": "Visualizing numerical data",
    "text": "Visualizing numerical data\nDescribing shapes of numerical distributions\n\nshape:\n\nskewness: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail)\nmodality: unimodal, bimodal, multimodal, uniform\n\ncenter: mean (mean), median (median), mode (not always useful)\nspread: range (range), standard deviation (sd), inter-quartile range (IQR)\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#histograms",
    "href": "content/lectures/06-analysis-slides.html#histograms",
    "title": "06-analysis",
    "section": "Histograms",
    "text": "Histograms\n\nHeightsWidthsPrices\n\n\n\nggplot(data = pp, aes(x = Height_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Height, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = Width_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Width, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 100) +\n  labs(x = \"Price\", y = NULL)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#density-plots",
    "href": "content/lectures/06-analysis-slides.html#density-plots",
    "title": "06-analysis",
    "section": "Density plots",
    "text": "Density plots\n\nggplot(data = pp, mapping = aes(x = Height_in)) +\n  geom_density()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#side-by-side-box-plots",
    "href": "content/lectures/06-analysis-slides.html#side-by-side-box-plots",
    "title": "06-analysis",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = pp, mapping = aes(y = Height_in, x = as.factor(landsALL))) +\n  geom_boxplot()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-categorical-data",
    "href": "content/lectures/06-analysis-slides.html#visualizing-categorical-data",
    "title": "06-analysis",
    "section": "Visualizing categorical data",
    "text": "Visualizing categorical data\n\ncount/proportion of values\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#bar-plots",
    "href": "content/lectures/06-analysis-slides.html#bar-plots",
    "title": "06-analysis",
    "section": "Bar plots",
    "text": "Bar plots\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL))) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#segmented-bar-plots-counts",
    "href": "content/lectures/06-analysis-slides.html#segmented-bar-plots-counts",
    "title": "06-analysis",
    "section": "Segmented bar plots, counts",
    "text": "Segmented bar plots, counts\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#segmented-bar-plots-proportions",
    "href": "content/lectures/06-analysis-slides.html#segmented-bar-plots-proportions",
    "title": "06-analysis",
    "section": "Segmented bar plots, proportions",
    "text": "Segmented bar plots, proportions\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\")"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#your-turn",
    "href": "content/lectures/06-analysis-slides.html#your-turn",
    "title": "06-analysis",
    "section": "Your Turn",
    "text": "Your Turn\n❓ Which of the previous two bar plots is a more useful representation for visualizing the relationship between landscape and painting material?\n\n❓ What else would you want to do/know to complete EDA?\n\n\n🧠 Try to answer at least one thing you’d want to know from the dataset.\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#modelling-1",
    "href": "content/lectures/06-analysis-slides.html#modelling-1",
    "title": "06-analysis",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we focus on linear models (but remember there are other types of models too!)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#packages",
    "href": "content/lectures/06-analysis-slides.html#packages",
    "title": "06-analysis",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nYou’re familiar with the tidyverse:\n\n\nlibrary(tidyverse)\n\n\nThe broom package takes the messy output of built-in functions in R, such as lm, and turns them into tidy data frames.\n\n\nlibrary(broom)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#modelling-the-relationship-between-variables",
    "href": "content/lectures/06-analysis-slides.html#modelling-the-relationship-between-variables",
    "title": "06-analysis",
    "section": "Modelling the relationship between variables",
    "text": "Modelling the relationship between variables\nEDA: Prices\n❗ Describe the distribution of prices of paintings.\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 1000)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#models-as-functions",
    "href": "content/lectures/06-analysis-slides.html#models-as-functions",
    "title": "06-analysis",
    "section": "Models as functions",
    "text": "Models as functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs.\n\nPlug in the inputs and receive back the output\nExample: the formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\), when \\(x\\) is \\(5\\), the output \\(y\\) is \\(22\\)\n\n\ny = 3 * 5 + 7 = 22"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#height-as-a-function-of-width",
    "href": "content/lectures/06-analysis-slides.html#height-as-a-function-of-width",
    "title": "06-analysis",
    "section": "Height as a function of width",
    "text": "Height as a function of width\n❗ Describe the relationship between height and width of paintings."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-1",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-1",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… without the measure of uncertainty around the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-2",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-2",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… with different cosmetic choices for the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, \n              col = \"pink\",      # color\n              lty = 2,           # line type\n              linewidth = 3)     # line weight"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#vocabulary",
    "href": "content/lectures/06-analysis-slides.html#vocabulary",
    "title": "06-analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis (dependent variable)\n\n\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis (independent variables)\n\n\n\n\nPredicted value: Output of the function model function\n\nThe model function gives the typical value of the response variable conditioning on the explanatory variables\n\n\n\n\n\nResiduals: Show how far each case is from its model value\n\nResidual = Observed value - Predicted value\nTells how far above/below the model function each case is"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#residuals",
    "href": "content/lectures/06-analysis-slides.html#residuals",
    "title": "06-analysis",
    "section": "Residuals",
    "text": "Residuals\n❓ What does a negative residual mean? Which paintings on the plot have have negative residuals?"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#section-1",
    "href": "content/lectures/06-analysis-slides.html#section-1",
    "title": "06-analysis",
    "section": "",
    "text": "The plot below displays the relationship between height and width of paintings. It uses a lower alpha level for the points than the previous plots we looked at.\n\n❓ What feature is apparent in this plot that was not (as) apparent in the previous plots? What might be the reason for this feature?"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#landscape-paintings",
    "href": "content/lectures/06-analysis-slides.html#landscape-paintings",
    "title": "06-analysis",
    "section": "Landscape paintings",
    "text": "Landscape paintings\n\nLandscape painting is the depiction in art of landscapes – natural scenery such as mountains, valleys, trees, rivers, and forests, especially where the main subject is a wide view – with its elements arranged into a coherent composition.1\n\nLandscape paintings tend to be wider than longer.\n\nPortrait painting is a genre in painting, where the intent is to depict a human subject.2\n\nPortrait paintings tend to be longer than wider.\n\n\nSource: Wikipedia, Landscape paintingSource: Wikipedia, Portait painting"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#multiple-explanatory-variables",
    "href": "content/lectures/06-analysis-slides.html#multiple-explanatory-variables",
    "title": "06-analysis",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\n❓ How, if at all, does the relationship between width and height of paintings vary by whether or not they have any landscape elements?\n\nggplot(data = pp, aes(x = Width_in, y = Height_in, \n                      color = factor(landsALL))) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"landscape\")"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#models---upsides-and-downsides",
    "href": "content/lectures/06-analysis-slides.html#models---upsides-and-downsides",
    "title": "06-analysis",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modelling over simple visual inspection of data.\n\n\n\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#variation-around-the-model",
    "href": "content/lectures/06-analysis-slides.html#variation-around-the-model",
    "title": "06-analysis",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\n\nStatistics is the explanation of variation in the context of what remains unexplained.\n\n\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#how-do-we-use-models",
    "href": "content/lectures/06-analysis-slides.html#how-do-we-use-models",
    "title": "06-analysis",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nExplanation: Characterize the relationship between \\(y\\) and \\(x\\) via slopes for numerical explanatory variables or differences for categorical explanatory variables (Inference)\nPrediction: Plug in \\(x\\), get the predicted \\(y\\) (Machine Learning)"
  },
  {
    "objectID": "content/lectures/06-analysis.html",
    "href": "content/lectures/06-analysis.html",
    "title": "06-analysis",
    "section": "",
    "text": "Q: I was most confused about the differences between facet_wrap and facet_grid. I didn’t understand what made them so different\nA: You’re right that they’re very similar and can even at times be used/coerced to do what the other does. Basically, if you want a 2D comparison (variable1 x variable2), facet_grid. If you just want to break down the plots by a single variable, facet_wrap.\n\n\nQ: Will future HW assignments also include things that were not explicitly covered in class? For example, HW1 which was due last night, included concepts and materials that were just covered today.\nA: Nope. That was my fault. We were behind where I intended to be. This will not be an issue going forward.\n\n\nQ: can we plot data from 2 separate data frames somehow?\nA: Not typically in ggplot2. Instead, data would have to be wrangled into the tidy format (single dataset) first.\n\n\nQ: does filling= mean seperate and color by the value passed in, and we don’t need to assign color again? What if we don’t want color?\nA: Fill indicates how to fill the plot with color. If you don’t want color, you wouldn’t use fill\n\n\nQ: How we would do the second suggestion in how to visualize the brexit data differently?\nA: We’ll do this together today!\n\n\nQ: for datasets that are included in a library (like penguins one), could we read it into a df to see it in the environment? or is that redundant and we should only view it, use functions to see var names, etc\nA: That would be ok!\n\n\nQ: I was wondering where I can find all available themes.\nA: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n\nQ: I have heard about plotly before. I was wondering what will be its difference between this and ggplot?\nA: There is a ggplotly package! Plotly enables interaction on top of ggplot2 packages\n\n\n\n\nDue Dates:\n\nLab 03 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\n\nNotes:\n\nHow To: Chunk output in console\n\n\n\n\nCase Studies Discussion\n\nBiomarkers of recent marijuana usage (Linear Regression)\nVaping in American Youth (Logistic Regression)\nRight-to-Carry Laws Effect on Violent Crime (Multiple Linear Regression)\nPredicting Annual Air Pollution (Machine Learning)\n\n\n\n\n\n\n\n\n\n\n\nDiscuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)\n\n\n\n\n\n\nIntroduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building"
  },
  {
    "objectID": "content/lectures/06-analysis.html#what-is-eda",
    "href": "content/lectures/06-analysis.html#what-is-eda",
    "title": "06-analysis",
    "section": "What is EDA?",
    "text": "What is EDA?\n\nExploratory data analysis (EDA) is an aproach to analyzing data sets to summarize and understand its main characteristics.\nOften, this is visual….but the visuals do not have to be perfect. (Save that for communication)\nCalculating summary statistics is also part of EDA.\n\n\n\nData tidying/wrangling/manipulation/transformation typically happens before this stage of the analysis.\n\n\n\nThe Goal: KNOW YOUR DATA!"
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "href": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "title": "06-analysis",
    "section": "Consider: No. of variables involved",
    "text": "Consider: No. of variables involved\n\nUnivariate data analysis - distribution of single variable\nBivariate data analysis - relationship between two variables\nMultivariate data analysis - relationship between many variables at once, usually focusing on the relationship between two while conditioning for others"
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-types-of-variables",
    "href": "content/lectures/06-analysis.html#consider-types-of-variables",
    "title": "06-analysis",
    "section": "Consider: Types of variables",
    "text": "Consider: Types of variables\n\nNumerical variables can be classified as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.\nIf the variable is categorical, we can determine if it is ordinal based on whether or not the levels have a natural ordering."
  },
  {
    "objectID": "content/lectures/06-analysis.html#data-visualization",
    "href": "content/lectures/06-analysis.html#data-visualization",
    "title": "06-analysis",
    "section": "Data visualization",
    "text": "Data visualization\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\n\nData visualization is the creation and study of the visual representation of data.\nThere are many tools for visualizing data (R is one of them), and many approaches/systems within R for making data visualizations (ggplot2 is what we’ll continue to use).\nEDA will involve making plots/visualizing your data"
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-paintings",
    "href": "content/lectures/06-analysis.html#paris-paintings",
    "title": "06-analysis",
    "section": "Paris Paintings",
    "text": "Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nSource: Printed catalogs of 28 auction sales in Paris, 1764 - 1780\nData curators Sandra van Ginhoven and Hilary Coe Cronheim (who were PhD students in the Duke Art, Law, and Markets Initiative at the time of putting together this dataset) translated and tabulated the catalogues\n3393 paintings, their prices, and descriptive details from sales catalogues over 60 variables"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-today",
    "href": "content/lectures/06-analysis.html#auctions-today",
    "title": "06-analysis",
    "section": "Auctions today",
    "text": "Auctions today\n\n\n\n\n\n\n\nSource: Sothebys"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "href": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "title": "06-analysis",
    "section": "Auctions back in the day",
    "text": "Auctions back in the day\n\n\n\n\n\nSource: Pierre-Antoine de Machy, Public Sale at the Hôtel Bullion, Musée Carnavalet, Paris (18th century)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-auction-market",
    "href": "content/lectures/06-analysis.html#paris-auction-market",
    "title": "06-analysis",
    "section": "Paris auction market",
    "text": "Paris auction market"
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "title": "06-analysis",
    "section": "Depart pour la chasse",
    "text": "Depart pour la chasse"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auction-catalogue-text",
    "href": "content/lectures/06-analysis.html#auction-catalogue-text",
    "title": "06-analysis",
    "section": "Auction catalogue text",
    "text": "Auction catalogue text\n\n\n\n\nTwo paintings very rich in composition, of a beautiful execution, and whose merit is very remarkable, each 17 inches 3 lines high, 23 inches wide; the first, painted on wood, comes from the Cabinet of Madame la Comtesse de Verrue; it represents a departure for the hunt: it shows in the front a child on a white horse, a man who gives the horn to gather the dogs, a falconer and other figures nicely distributed across the width of the painting; two horses drinking from a fountain; on the right in the corner a lovely country house topped by a terrace, on which people are at the table, others who play instruments; trees and fabriques pleasantly enrich the background."
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "title": "06-analysis",
    "section": "Depart pour la chasse as Data",
    "text": "Depart pour la chasse as Data"
  },
  {
    "objectID": "content/lectures/06-analysis.html#section",
    "href": "content/lectures/06-analysis.html#section",
    "title": "06-analysis",
    "section": "",
    "text": "pp |>\n  filter(name == \"R1777-89a\") |>\n  glimpse()\n\nRows: 1\nColumns: 61\n$ name              <chr> \"R1777-89a\"\n$ sale              <chr> \"R1777\"\n$ lot               <chr> \"89\"\n$ position          <dbl> 0.3755274\n$ dealer            <chr> \"R\"\n$ year              <dbl> 1777\n$ origin_author     <chr> \"D/FL\"\n$ origin_cat        <chr> \"D/FL\"\n$ school_pntg       <chr> \"D/FL\"\n$ diff_origin       <dbl> 0\n$ logprice          <dbl> 8.575462\n$ price             <dbl> 5300\n$ count             <dbl> 1\n$ subject           <chr> \"D\\x8epart pour la chasse\"\n$ authorstandard    <chr> \"Wouwerman, Philips\"\n$ artistliving      <dbl> 0\n$ authorstyle       <chr> NA\n$ author            <chr> \"Philippe Wouwermans\"\n$ winningbidder     <chr> \"Langlier, Jacques for Poullain, Antoine\"\n$ winningbiddertype <chr> \"DC\"\n$ endbuyer          <chr> \"C\"\n$ Interm            <dbl> 1\n$ type_intermed     <chr> \"D\"\n$ Height_in         <dbl> 17.25\n$ Width_in          <dbl> 23\n$ Surface_Rect      <dbl> 396.75\n$ Diam_in           <dbl> NA\n$ Surface_Rnd       <dbl> NA\n$ Shape             <chr> \"squ_rect\"\n$ Surface           <dbl> 396.75\n$ material          <chr> \"bois\"\n$ mat               <chr> \"b\"\n$ materialCat       <chr> \"wood\"\n$ quantity          <dbl> 1\n$ nfigures          <dbl> 0\n$ engraved          <dbl> 0\n$ original          <dbl> 0\n$ prevcoll          <dbl> 1\n$ othartist         <dbl> 0\n$ paired            <dbl> 1\n$ figures           <dbl> 0\n$ finished          <dbl> 0\n$ lrgfont           <dbl> 0\n$ relig             <dbl> 0\n$ landsALL          <dbl> 1\n$ lands_sc          <dbl> 0\n$ lands_elem        <dbl> 1\n$ lands_figs        <dbl> 1\n$ lands_ment        <dbl> 0\n$ arch              <dbl> 1\n$ mytho             <dbl> 0\n$ peasant           <dbl> 0\n$ othgenre          <dbl> 0\n$ singlefig         <dbl> 0\n$ portrait          <dbl> 0\n$ still_life        <dbl> 0\n$ discauth          <dbl> 0\n$ history           <dbl> 0\n$ allegory          <dbl> 0\n$ pastorale         <dbl> 0\n$ other             <dbl> 0"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "href": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "title": "06-analysis",
    "section": "Visualizing numerical data",
    "text": "Visualizing numerical data\nDescribing shapes of numerical distributions\n\nshape:\n\nskewness: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail)\nmodality: unimodal, bimodal, multimodal, uniform\n\ncenter: mean (mean), median (median), mode (not always useful)\nspread: range (range), standard deviation (sd), inter-quartile range (IQR)\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis.html#histograms",
    "href": "content/lectures/06-analysis.html#histograms",
    "title": "06-analysis",
    "section": "Histograms",
    "text": "Histograms\n\nHeightsWidthsPrices\n\n\n\nggplot(data = pp, aes(x = Height_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Height, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = Width_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Width, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 100) +\n  labs(x = \"Price\", y = NULL)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#density-plots",
    "href": "content/lectures/06-analysis.html#density-plots",
    "title": "06-analysis",
    "section": "Density plots",
    "text": "Density plots\n\nggplot(data = pp, mapping = aes(x = Height_in)) +\n  geom_density()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "href": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "title": "06-analysis",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = pp, mapping = aes(y = Height_in, x = as.factor(landsALL))) +\n  geom_boxplot()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "href": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "title": "06-analysis",
    "section": "Visualizing categorical data",
    "text": "Visualizing categorical data\n\ncount/proportion of values\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis.html#bar-plots",
    "href": "content/lectures/06-analysis.html#bar-plots",
    "title": "06-analysis",
    "section": "Bar plots",
    "text": "Bar plots\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL))) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "title": "06-analysis",
    "section": "Segmented bar plots, counts",
    "text": "Segmented bar plots, counts\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "title": "06-analysis",
    "section": "Segmented bar plots, proportions",
    "text": "Segmented bar plots, proportions\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\")"
  },
  {
    "objectID": "content/lectures/06-analysis.html#your-turn",
    "href": "content/lectures/06-analysis.html#your-turn",
    "title": "06-analysis",
    "section": "Your Turn",
    "text": "Your Turn\n❓ Which of the previous two bar plots is a more useful representation for visualizing the relationship between landscape and painting material?\n\n❓ What else would you want to do/know to complete EDA?\n\n\n🧠 Try to answer at least one thing you’d want to know from the dataset.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-1",
    "href": "content/lectures/06-analysis.html#modelling-1",
    "title": "06-analysis",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we focus on linear models (but remember there are other types of models too!)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#packages",
    "href": "content/lectures/06-analysis.html#packages",
    "title": "06-analysis",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nYou’re familiar with the tidyverse:\n\n\nlibrary(tidyverse)\n\n\nThe broom package takes the messy output of built-in functions in R, such as lm, and turns them into tidy data frames.\n\n\nlibrary(broom)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "href": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "title": "06-analysis",
    "section": "Modelling the relationship between variables",
    "text": "Modelling the relationship between variables\nEDA: Prices\n❗ Describe the distribution of prices of paintings.\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 1000)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#models-as-functions",
    "href": "content/lectures/06-analysis.html#models-as-functions",
    "title": "06-analysis",
    "section": "Models as functions",
    "text": "Models as functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs.\n\nPlug in the inputs and receive back the output\nExample: the formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\), when \\(x\\) is \\(5\\), the output \\(y\\) is \\(22\\)\n\n\ny = 3 * 5 + 7 = 22"
  },
  {
    "objectID": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "href": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "title": "06-analysis",
    "section": "Height as a function of width",
    "text": "Height as a function of width\n❗ Describe the relationship between height and width of paintings."
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… without the measure of uncertainty around the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… with different cosmetic choices for the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, \n              col = \"pink\",      # color\n              lty = 2,           # line type\n              linewidth = 3)     # line weight"
  },
  {
    "objectID": "content/lectures/06-analysis.html#vocabulary",
    "href": "content/lectures/06-analysis.html#vocabulary",
    "title": "06-analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis (dependent variable)\n\n\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis (independent variables)\n\n\n\n\nPredicted value: Output of the function model function\n\nThe model function gives the typical value of the response variable conditioning on the explanatory variables\n\n\n\n\n\nResiduals: Show how far each case is from its model value\n\nResidual = Observed value - Predicted value\nTells how far above/below the model function each case is"
  },
  {
    "objectID": "content/lectures/06-analysis.html#residuals",
    "href": "content/lectures/06-analysis.html#residuals",
    "title": "06-analysis",
    "section": "Residuals",
    "text": "Residuals\n❓ What does a negative residual mean? Which paintings on the plot have have negative residuals?"
  },
  {
    "objectID": "content/lectures/06-analysis.html#section-1",
    "href": "content/lectures/06-analysis.html#section-1",
    "title": "06-analysis",
    "section": "",
    "text": "The plot below displays the relationship between height and width of paintings. It uses a lower alpha level for the points than the previous plots we looked at.\n\n\n\n\n\n❓ What feature is apparent in this plot that was not (as) apparent in the previous plots? What might be the reason for this feature?"
  },
  {
    "objectID": "content/lectures/06-analysis.html#landscape-paintings",
    "href": "content/lectures/06-analysis.html#landscape-paintings",
    "title": "06-analysis",
    "section": "Landscape paintings",
    "text": "Landscape paintings\n\nLandscape painting is the depiction in art of landscapes – natural scenery such as mountains, valleys, trees, rivers, and forests, especially where the main subject is a wide view – with its elements arranged into a coherent composition.1\n\nLandscape paintings tend to be wider than longer.\n\nPortrait painting is a genre in painting, where the intent is to depict a human subject.2\n\nPortrait paintings tend to be longer than wider."
  },
  {
    "objectID": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "href": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "title": "06-analysis",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\n❓ How, if at all, does the relationship between width and height of paintings vary by whether or not they have any landscape elements?\n\nggplot(data = pp, aes(x = Width_in, y = Height_in, \n                      color = factor(landsALL))) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"landscape\")"
  },
  {
    "objectID": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "href": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "title": "06-analysis",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modelling over simple visual inspection of data.\n\n\n\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "content/lectures/06-analysis.html#variation-around-the-model",
    "href": "content/lectures/06-analysis.html#variation-around-the-model",
    "title": "06-analysis",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\n\nStatistics is the explanation of variation in the context of what remains unexplained.\n\n\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#how-do-we-use-models",
    "href": "content/lectures/06-analysis.html#how-do-we-use-models",
    "title": "06-analysis",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nExplanation: Characterize the relationship between \\(y\\) and \\(x\\) via slopes for numerical explanatory variables or differences for categorical explanatory variables (Inference)\nPrediction: Plug in \\(x\\), get the predicted \\(y\\) (Machine Learning)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#qa",
    "href": "content/lectures/05-viz-slides.html#qa",
    "title": "05-viz",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I just think it’s a bit fast and I would appreciate if you made sure to show like how to import and stuff rather then jumping to the final product\nA: That’s really valuable feedback. I don’t think you’re alone. I’m going to work to better get everyone on the same page going forward.\n\nNote: a handful of comments on needing more time on joins & pivots…which makes sense! They’re new :)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#course-announcements",
    "href": "content/lectures/05-viz-slides.html#course-announcements",
    "title": "05-viz",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHW01 can be submitted by tonight for full credit (datahub issues)\nLab02 Ans Posted\nHW02 Now Available\nMy OH will now be at the tables along the back of this building"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#suggested-reading",
    "href": "content/lectures/05-viz-slides.html#suggested-reading",
    "title": "05-viz",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#keep-it-simple",
    "href": "content/lectures/05-viz-slides.html#keep-it-simple",
    "title": "05-viz",
    "section": "Keep it simple",
    "text": "Keep it simple"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-color-to-draw-attention",
    "href": "content/lectures/05-viz-slides.html#use-color-to-draw-attention",
    "title": "05-viz",
    "section": "Use color to draw attention",
    "text": "Use color to draw attention"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#tell-a-story",
    "href": "content/lectures/05-viz-slides.html#tell-a-story",
    "title": "05-viz",
    "section": "Tell a story",
    "text": "Tell a story\n\n\n\n\n\n\n\nCredit: Angela Zoss and Eric Monson, Duke DVS"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#principles-for-effective-visualizations-1",
    "href": "content/lectures/05-viz-slides.html#principles-for-effective-visualizations-1",
    "title": "05-viz",
    "section": "Principles for effective visualizations",
    "text": "Principles for effective visualizations\n\nOrder matters\nPut long categories on the y-axis\nKeep scales consistent\nSelect meaningful colors\nUse meaningful and nonredundant labels"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#data",
    "href": "content/lectures/05-viz-slides.html#data",
    "title": "05-viz",
    "section": "Data",
    "text": "Data\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\n\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\n\n\n\n\n\n\n\n\n\n\nSource: YouGov Survey Results, retrieved Oct 7, 2019"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#the-data-code",
    "href": "content/lectures/05-viz-slides.html#the-data-code",
    "title": "05-viz",
    "section": "The Data: Code",
    "text": "The Data: Code\n\nbrexit <- tibble(\n  opinion = c(\n    rep(\"Right\", 664), rep(\"Wrong\", 787), rep(\"Don't know\", 188)\n  ),\n  region = c(\n    rep(\"london\", 63), rep(\"rest_of_south\", 241), rep(\"midlands_wales\", 145), rep(\"north\", 176), rep(\"scot\", 39),\n    rep(\"london\", 110), rep(\"rest_of_south\", 257), rep(\"midlands_wales\", 152), rep(\"north\", 176), rep(\"scot\", 92),\n    rep(\"london\", 24), rep(\"rest_of_south\", 49), rep(\"midlands_wales\", 57), rep(\"north\", 48), rep(\"scot\", 10)\n  )\n)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#order-matters",
    "href": "content/lectures/05-viz-slides.html#order-matters",
    "title": "05-viz",
    "section": "Order matters",
    "text": "Order matters\nAlphabetical is rarely ideal\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#order-by-frequency",
    "href": "content/lectures/05-viz-slides.html#order-by-frequency",
    "title": "05-viz",
    "section": "Order by frequency",
    "text": "Order by frequency\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_infreq: Reorder factors’ levels by frequency\n\nggplot(brexit, aes(x = fct_infreq(opinion))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar() +\n  labs( \n    x = \"Opinion\", \n    y = \"Count\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#avoiding-alphabetical-order",
    "href": "content/lectures/05-viz-slides.html#avoiding-alphabetical-order",
    "title": "05-viz",
    "section": "Avoiding Alphabetical Order",
    "text": "Avoiding Alphabetical Order\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = region)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-inherent-level-order",
    "href": "content/lectures/05-viz-slides.html#use-inherent-level-order",
    "title": "05-viz",
    "section": "Use inherent level order",
    "text": "Use inherent level order\n\nRelevelPlot\n\n\nfct_relevel: Reorder factor levels using a custom order\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_relevel( \n      region,\n      \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels-1",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels-1",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nRecodePlot\n\n\nfct_recode: Change factor levels by hand\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_recode( \n      region,\n      London = \"london\",\n      `Rest of South` = \"rest_of_south\",\n      `Midlands / Wales` = \"midlands_wales\",\n      North = \"north\",\n      Scotland = \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#put-long-categories-on-the-y-axis",
    "href": "content/lectures/05-viz-slides.html#put-long-categories-on-the-y-axis",
    "title": "05-viz",
    "section": "Put long categories on the y-axis",
    "text": "Put long categories on the y-axis\nLong categories can be hard to read"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#move-them-to-the-y-axis",
    "href": "content/lectures/05-viz-slides.html#move-them-to-the-y-axis",
    "title": "05-viz",
    "section": "Move them to the y-axis",
    "text": "Move them to the y-axis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#and-reverse-the-order-of-levels",
    "href": "content/lectures/05-viz-slides.html#and-reverse-the-order-of-levels",
    "title": "05-viz",
    "section": "And reverse the order of levels",
    "text": "And reverse the order of levels\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_rev: Reverse order of factor levels\n\nggplot(brexit, aes(y = fct_rev(region))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels-2",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels-2",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = fct_rev(region))) +\n  geom_bar() +\n  labs( \n    x = \"Count\", \n    y = \"Region\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#segmented-bar-plots-can-be-hard-to-read",
    "href": "content/lectures/05-viz-slides.html#segmented-bar-plots-can-be-hard-to-read",
    "title": "05-viz",
    "section": "Segmented bar plots can be hard to read",
    "text": "Segmented bar plots can be hard to read\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region, fill = opinion)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-facets",
    "href": "content/lectures/05-viz-slides.html#use-facets",
    "title": "05-viz",
    "section": "Use facets",
    "text": "Use facets\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = region)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#avoid-redundancy",
    "href": "content/lectures/05-viz-slides.html#avoid-redundancy",
    "title": "05-viz",
    "section": "Avoid redundancy?",
    "text": "Avoid redundancy?"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#redundancy-can-help-tell-a-story",
    "href": "content/lectures/05-viz-slides.html#redundancy-can-help-tell-a-story",
    "title": "05-viz",
    "section": "Redundancy can help tell a story",
    "text": "Redundancy can help tell a story\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#be-selective-with-redundancy",
    "href": "content/lectures/05-viz-slides.html#be-selective-with-redundancy",
    "title": "05-viz",
    "section": "Be selective with redundancy",
    "text": "Be selective with redundancy\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-informative-labels",
    "href": "content/lectures/05-viz-slides.html#use-informative-labels",
    "title": "05-viz",
    "section": "Use informative labels",
    "text": "Use informative labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#a-bit-more-info",
    "href": "content/lectures/05-viz-slides.html#a-bit-more-info",
    "title": "05-viz",
    "section": "A bit more info",
    "text": "A bit more info\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\", \n    caption = \"Source: https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#lets-do-better",
    "href": "content/lectures/05-viz-slides.html#lets-do-better",
    "title": "05-viz",
    "section": "Let’s do better",
    "text": "Let’s do better\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#fix-up-facet-labels",
    "href": "content/lectures/05-viz-slides.html#fix-up-facet-labels",
    "title": "05-viz",
    "section": "Fix up facet labels",
    "text": "Fix up facet labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1,\n    labeller = label_wrap_gen(width = 12) \n  ) + \n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#rainbow-colors-not-always-best",
    "href": "content/lectures/05-viz-slides.html#rainbow-colors-not-always-best",
    "title": "05-viz",
    "section": "Rainbow colors not always best",
    "text": "Rainbow colors not always best"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#manually-choose-colors-when-needed",
    "href": "content/lectures/05-viz-slides.html#manually-choose-colors-when-needed",
    "title": "05-viz",
    "section": "Manually choose colors when needed",
    "text": "Manually choose colors when needed\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c( \n    \"Wrong\" = \"red\", \n    \"Right\" = \"green\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#choosing-better-colors",
    "href": "content/lectures/05-viz-slides.html#choosing-better-colors",
    "title": "05-viz",
    "section": "Choosing better colors",
    "text": "Choosing better colors\ncolorbrewer2.org"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-better-colors",
    "href": "content/lectures/05-viz-slides.html#use-better-colors",
    "title": "05-viz",
    "section": "Use better colors",
    "text": "Use better colors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\", \n    \"Right\" = \"#67a9cf\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#select-theme",
    "href": "content/lectures/05-viz-slides.html#select-theme",
    "title": "05-viz",
    "section": "Select theme",
    "text": "Select theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal() \n\n\n\n\n\n\nggthemes described here"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#customize-theme",
    "href": "content/lectures/05-viz-slides.html#customize-theme",
    "title": "05-viz",
    "section": "Customize theme",
    "text": "Customize theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal(base_size = 16) + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#your-turn",
    "href": "content/lectures/05-viz-slides.html#your-turn",
    "title": "05-viz",
    "section": "Your Turn",
    "text": "Your Turn\n\nRead in the data (Data slide)\nThink of at least three different ways to tell slightly different stories with these data\nTry to implement at least one of these ideas!"
  },
  {
    "objectID": "content/lectures/05-viz.html",
    "href": "content/lectures/05-viz.html",
    "title": "05-viz",
    "section": "",
    "text": "Slides modified from datascienceinabox.org\n\n\n\nQ: I just think it’s a bit fast and I would appreciate if you made sure to show like how to import and stuff rather then jumping to the final product\nA: That’s really valuable feedback. I don’t think you’re alone. I’m going to work to better get everyone on the same page going forward.\n\nNote: a handful of comments on needing more time on joins & pivots…which makes sense! They’re new :)\n\n\n\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHW01 can be submitted by tonight for full credit (datahub issues)\nLab02 Ans Posted\nHW02 Now Available\nMy OH will now be at the tables along the back of this building\n\n\n\n\n\nR4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit: Angela Zoss and Eric Monson, Duke DVS"
  },
  {
    "objectID": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "href": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "title": "05-viz",
    "section": "Principles for effective visualizations",
    "text": "Principles for effective visualizations\n\nOrder matters\nPut long categories on the y-axis\nKeep scales consistent\nSelect meaningful colors\nUse meaningful and nonredundant labels"
  },
  {
    "objectID": "content/lectures/05-viz.html#data",
    "href": "content/lectures/05-viz.html#data",
    "title": "05-viz",
    "section": "Data",
    "text": "Data\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\n\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\n\n\n\n\n\n\n\n\n\n\nSource: YouGov Survey Results, retrieved Oct 7, 2019"
  },
  {
    "objectID": "content/lectures/05-viz.html#the-data-code",
    "href": "content/lectures/05-viz.html#the-data-code",
    "title": "05-viz",
    "section": "The Data: Code",
    "text": "The Data: Code\n\nbrexit <- tibble(\n  opinion = c(\n    rep(\"Right\", 664), rep(\"Wrong\", 787), rep(\"Don't know\", 188)\n  ),\n  region = c(\n    rep(\"london\", 63), rep(\"rest_of_south\", 241), rep(\"midlands_wales\", 145), rep(\"north\", 176), rep(\"scot\", 39),\n    rep(\"london\", 110), rep(\"rest_of_south\", 257), rep(\"midlands_wales\", 152), rep(\"north\", 176), rep(\"scot\", 92),\n    rep(\"london\", 24), rep(\"rest_of_south\", 49), rep(\"midlands_wales\", 57), rep(\"north\", 48), rep(\"scot\", 10)\n  )\n)"
  },
  {
    "objectID": "content/lectures/05-viz.html#order-matters",
    "href": "content/lectures/05-viz.html#order-matters",
    "title": "05-viz",
    "section": "Order matters",
    "text": "Order matters\nAlphabetical is rarely ideal\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#order-by-frequency",
    "href": "content/lectures/05-viz.html#order-by-frequency",
    "title": "05-viz",
    "section": "Order by frequency",
    "text": "Order by frequency\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_infreq: Reorder factors’ levels by frequency\n\nggplot(brexit, aes(x = fct_infreq(opinion))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels",
    "href": "content/lectures/05-viz.html#clean-up-labels",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar() +\n  labs( \n    x = \"Opinion\", \n    y = \"Count\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "href": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "title": "05-viz",
    "section": "Avoiding Alphabetical Order",
    "text": "Avoiding Alphabetical Order\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = region)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-inherent-level-order",
    "href": "content/lectures/05-viz.html#use-inherent-level-order",
    "title": "05-viz",
    "section": "Use inherent level order",
    "text": "Use inherent level order\n\nRelevelPlot\n\n\nfct_relevel: Reorder factor levels using a custom order\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_relevel( \n      region,\n      \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-1",
    "href": "content/lectures/05-viz.html#clean-up-labels-1",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nRecodePlot\n\n\nfct_recode: Change factor levels by hand\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_recode( \n      region,\n      London = \"london\",\n      `Rest of South` = \"rest_of_south\",\n      `Midlands / Wales` = \"midlands_wales\",\n      North = \"north\",\n      Scotland = \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "href": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "title": "05-viz",
    "section": "Put long categories on the y-axis",
    "text": "Put long categories on the y-axis\nLong categories can be hard to read"
  },
  {
    "objectID": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "href": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "title": "05-viz",
    "section": "Move them to the y-axis",
    "text": "Move them to the y-axis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "href": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "title": "05-viz",
    "section": "And reverse the order of levels",
    "text": "And reverse the order of levels\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_rev: Reverse order of factor levels\n\nggplot(brexit, aes(y = fct_rev(region))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-2",
    "href": "content/lectures/05-viz.html#clean-up-labels-2",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = fct_rev(region))) +\n  geom_bar() +\n  labs( \n    x = \"Count\", \n    y = \"Region\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "href": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "title": "05-viz",
    "section": "Segmented bar plots can be hard to read",
    "text": "Segmented bar plots can be hard to read\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region, fill = opinion)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-facets",
    "href": "content/lectures/05-viz.html#use-facets",
    "title": "05-viz",
    "section": "Use facets",
    "text": "Use facets\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = region)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz.html#avoid-redundancy",
    "href": "content/lectures/05-viz.html#avoid-redundancy",
    "title": "05-viz",
    "section": "Avoid redundancy?",
    "text": "Avoid redundancy?"
  },
  {
    "objectID": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "href": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "title": "05-viz",
    "section": "Redundancy can help tell a story",
    "text": "Redundancy can help tell a story\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "href": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "title": "05-viz",
    "section": "Be selective with redundancy",
    "text": "Be selective with redundancy\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-informative-labels",
    "href": "content/lectures/05-viz.html#use-informative-labels",
    "title": "05-viz",
    "section": "Use informative labels",
    "text": "Use informative labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#a-bit-more-info",
    "href": "content/lectures/05-viz.html#a-bit-more-info",
    "title": "05-viz",
    "section": "A bit more info",
    "text": "A bit more info\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\", \n    caption = \"Source: https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#lets-do-better",
    "href": "content/lectures/05-viz.html#lets-do-better",
    "title": "05-viz",
    "section": "Let’s do better",
    "text": "Let’s do better\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#fix-up-facet-labels",
    "href": "content/lectures/05-viz.html#fix-up-facet-labels",
    "title": "05-viz",
    "section": "Fix up facet labels",
    "text": "Fix up facet labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1,\n    labeller = label_wrap_gen(width = 12) \n  ) + \n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "href": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "title": "05-viz",
    "section": "Rainbow colors not always best",
    "text": "Rainbow colors not always best"
  },
  {
    "objectID": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "href": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "title": "05-viz",
    "section": "Manually choose colors when needed",
    "text": "Manually choose colors when needed\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c( \n    \"Wrong\" = \"red\", \n    \"Right\" = \"green\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz.html#choosing-better-colors",
    "href": "content/lectures/05-viz.html#choosing-better-colors",
    "title": "05-viz",
    "section": "Choosing better colors",
    "text": "Choosing better colors\ncolorbrewer2.org"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-better-colors",
    "href": "content/lectures/05-viz.html#use-better-colors",
    "title": "05-viz",
    "section": "Use better colors",
    "text": "Use better colors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\", \n    \"Right\" = \"#67a9cf\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz.html#select-theme",
    "href": "content/lectures/05-viz.html#select-theme",
    "title": "05-viz",
    "section": "Select theme",
    "text": "Select theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal() \n\n\n\n\n\n\nggthemes described here"
  },
  {
    "objectID": "content/lectures/05-viz.html#customize-theme",
    "href": "content/lectures/05-viz.html#customize-theme",
    "title": "05-viz",
    "section": "Customize theme",
    "text": "Customize theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal(base_size = 16) + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "content/lectures/05-viz.html#your-turn",
    "href": "content/lectures/05-viz.html#your-turn",
    "title": "05-viz",
    "section": "Your Turn",
    "text": "Your Turn\n\nRead in the data (Data slide)\nThink of at least three different ways to tell slightly different stories with these data\nTry to implement at least one of these ideas!"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#qa-tu-117",
    "href": "content/lectures/12-cs01-eda-slides.html#qa-tu-117",
    "title": "12-cs01-eda",
    "section": "Q&A (Tu 11/7)",
    "text": "Q&A (Tu 11/7)\n\nQ: was curious about the sensitivity/cut-offs/specificity, but we mainly discussed it in class already; was still slightly confused about it?\nA: We’ll discuss this in detail later this week and early next week!\n\n\nQ: Are Youden’s indices related to ROC curves?\nA: Related, yes! We’ll discuss both soon!\n\n\nQ: I wasn’t sure how to interpret some of the visuals towards the end of lecture.\nA: That’s OK! We’ll be recreating these and discussing them more as we do this case study in class.\n\n\nQ: Did they actually use intravenous blood draws or just thumb pricks because multiple intravenous would not be fun.\nA: It was venous blood from the arm. This unfunness is one of the reasons participants were compensated.\n\n\nQ: Did this study (or other studies on THC) impairment end up influencing any legislation at the local or state level?\nA: Great question! The state is currently reviewing these and other study’s data. The state was definitely aware of this study and waited (im)patiently while we analyzed and worked to publish.\n\n\nQ: How long should our reports be?\nA: It’s hard to say. We’ll discuss an example today so you have a sense!\n\n\nQ: Regarding the final project, will there be a Google form that we can fill out that will help us form groups if we can’t form one ourselves?  A: Yup - I’d say try to find a group using Piazza, in class, or during lab. However, if you’re unable, when you fill out the form to indicate your group next week, you’ll select that you’d like to be placed into a group."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#course-announcements-tu-117",
    "href": "content/lectures/12-cs01-eda-slides.html#course-announcements-tu-117",
    "title": "12-cs01-eda",
    "section": "Course Announcements (Tu 11/7)",
    "text": "Course Announcements (Tu 11/7)\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n\nNotes:\n\nCS01 Groups have been sent out\n\nemail for contact\nGitHub repo <- please accept and open; make sure you have access\ngroup mate feedback is required\nif you made changes to repo yesterday, be sure to pull to get data in your repo\n\nFinal Project: can use Piazza to help find group mates\n\n\n\n\n\n\n\n\nImportant\n\n\nThe CS01 data are data for you only. My collaborator is excited that y’all will be working on this…but these are still research data, so please do not share with others or post publicly."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#qa-th-119",
    "href": "content/lectures/12-cs01-eda-slides.html#qa-th-119",
    "title": "12-cs01-eda",
    "section": "Q&A (Th 11/9)",
    "text": "Q&A (Th 11/9)\n\nQ: Are you allowed to share the average midterm scores for the past few quarters?\nA: IIRC, they were in the mid-high 80%s\n\n\nQ: When describing the dataset in our CSs, would it be okay to format it as a data card rather than a paragraph explanation of the variables structure? Additionally, would it be okay to store this as its own read.me in the repo or should it be a part of the main report?\nA: Yes - like the data card idea. And a detailed README in the repo is great. A short description should still be included in the report and can point to the readme.\n\n\nQ: When should we cite in our case study? It is just whenever we look up and use code from the internet?\nA: There AND any time you get information elsewhere that’s not general knowledge. For example, in your background section, you’ll likely cite a bunch of sources."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#course-announcements-th-119",
    "href": "content/lectures/12-cs01-eda-slides.html#course-announcements-th-119",
    "title": "12-cs01-eda",
    "section": "Course Announcements (Th 11/9)",
    "text": "Course Announcements (Th 11/9)\nDue Dates:\n\n🧪 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n✅ HW02 Scores/Feedback Posted"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#functions-in-r",
    "href": "content/lectures/12-cs01-eda-slides.html#functions-in-r",
    "title": "12-cs01-eda",
    "section": "Functions in R",
    "text": "Functions in R\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice” -Hadley\n\n\nfunction_name <- function(input){\n  # operations using input\n}\n\n\nFor example…\n\ndouble_value <- function(val){\n  val * 2\n}\n\n\n\nTo use/execute:\n\ndouble_value(3)\n\n[1] 6\n\n\n\n\nIn what we’ve done so far, we’ve seen functions that operate on and return the whole dataframe (DF in DF out) (drop_dups) and those that carry out operations on each row of a dataframe with a number of inputs (i.e. assign_timepoint; these require the function to be map-ed)\nAdditional resource: https://r4ds.had.co.nz/functions.html"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#agenda",
    "href": "content/lectures/12-cs01-eda-slides.html#agenda",
    "title": "12-cs01-eda",
    "section": "Agenda",
    "text": "Agenda\n\nPrevious Projects\nExploring the Data"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#example-case-study",
    "href": "content/lectures/12-cs01-eda-slides.html#example-case-study",
    "title": "12-cs01-eda",
    "section": "Example Case Study",
    "text": "Example Case Study\nSee & Discuss: https://cogs137.github.io/website/content/cs/cs-example.html"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#feedback-scores",
    "href": "content/lectures/12-cs01-eda-slides.html#feedback-scores",
    "title": "12-cs01-eda",
    "section": "Feedback & Scores",
    "text": "Feedback & Scores\nFeedback to other students here\n\n\nCommon comments:\n\ncontext/explanation/guidance/lacking\nmissing citations\nfailure to introduce/describe the data\nmaking statements without evidence\nneed to edit for cohesiveness, story, clarity\n\n\n\nYou cannot see the projects, but can read all of the comments and see the associated score. Also, note that the same row is not the same group."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#an-example-rubric",
    "href": "content/lectures/12-cs01-eda-slides.html#an-example-rubric",
    "title": "12-cs01-eda",
    "section": "An (Example) Rubric",
    "text": "An (Example) Rubric\nThis is NOT the rubric for your case study, but it will be similar:"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#notes",
    "href": "content/lectures/12-cs01-eda-slides.html#notes",
    "title": "12-cs01-eda",
    "section": "Notes",
    "text": "Notes\n\n\nLots of code/plots will be provided here\nYou are free to include any of it in your own case study (no attribution needed)\nYou probably should NOT include all of them in your final report\nFor any of the “basic” plots you include in your report, you’ll want to clean them up/improve their design.\nYour final report should be polished from start to finish"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#packages",
    "href": "content/lectures/12-cs01-eda-slides.html#packages",
    "title": "12-cs01-eda",
    "section": "Packages",
    "text": "Packages\nTwo additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#our-datasets",
    "href": "content/lectures/12-cs01-eda-slides.html#our-datasets",
    "title": "12-cs01-eda",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#the-data",
    "href": "content/lectures/12-cs01-eda-slides.html#the-data",
    "title": "12-cs01-eda",
    "section": "The Data",
    "text": "The Data\nReading in the .RData we wrote at the end of the last set of notes…(using load)\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nThis reads the objects stored in these files into your Environment for use."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#what-to-do-with-all-of-these-functions",
    "href": "content/lectures/12-cs01-eda-slides.html#what-to-do-with-all-of-these-functions",
    "title": "12-cs01-eda",
    "section": "What to do with all of these functions?",
    "text": "What to do with all of these functions?\nFor example…we discussed this function in the last set of notes.\n\n drop_dups <- function(dataset){\n  out <- dataset |> \n    filter(!is.na(timepoint_use)) |> \n    group_by(timepoint_use) |> \n    distinct(id, .keep_all=TRUE) |> \n    ungroup()\n  return(out)\n } \n\n\nWe’re going to have a lot of functions throughout…like this helper function to clean up names\n\n# helper function to clean up name of two compounds\nclean_gluc <- function(df){\n  df <- df |> \n    mutate(compound=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))),\n           compound=gsub('THCOH', '11-OH-THC', compound))\n  return(df)\n}\n\n\n\nFunctions can/should be stored in a separate .R file, probably in a src/ directory.\n\n\nTo have access to the functions in that file…\n\nsource(\"path/to/file\")\n\n\n\n\nsource(\"src/cs01_functions.R\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#single-variable-basic-plots",
    "href": "content/lectures/12-cs01-eda-slides.html#single-variable-basic-plots",
    "title": "12-cs01-eda",
    "section": "Single Variable (basic) plots",
    "text": "Single Variable (basic) plots\nFor a single compound…\n\nggplot(WB, aes(x=thc)) + geom_histogram()\n\n\n\n\n\nBut, with wide data, that’s not easy to do for all compounds, so you may want to pivot those data….\n\nWB_long <- WB |> \n  pivot_longer(6:13) |>\n  rename(\"fluid\"=\"fluid_type\")\n\n\n\nDistribtions across all compounds (WB):\n\nggplot(WB_long, aes(x=value)) + \n  geom_histogram() +\n  facet_wrap(~name)\n\n\n\n\n\n\nNow the same for OF and BR:\n\nOF_long <- OF |> pivot_longer(6:12)\nBR_long <- BR |> pivot_longer(6)\n\n\n\nCombining long datasets:\n\ndf_full <- bind_rows(WB_long, OF_long, BR_long)\n\n\n\nPlotting some of these data…\n\ndf_full |>\n  mutate(group_compound=paste0(fluid,\": \", name)) |>\nggplot(aes(x=value)) + \n  geom_histogram() + \n  facet_wrap(~group_compound, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#two-variables-at-a-time",
    "href": "content/lectures/12-cs01-eda-slides.html#two-variables-at-a-time",
    "title": "12-cs01-eda",
    "section": "Two variables at a time",
    "text": "Two variables at a time"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#thc-frequency",
    "href": "content/lectures/12-cs01-eda-slides.html#thc-frequency",
    "title": "12-cs01-eda",
    "section": "THC & Frequency",
    "text": "THC & Frequency\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=group, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#thc-treatment-group",
    "href": "content/lectures/12-cs01-eda-slides.html#thc-treatment-group",
    "title": "12-cs01-eda",
    "section": "THC & Treatment Group",
    "text": "THC & Treatment Group\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#focus-on-a-specific-timepoint",
    "href": "content/lectures/12-cs01-eda-slides.html#focus-on-a-specific-timepoint",
    "title": "12-cs01-eda",
    "section": "Focus on a specific timepoint…",
    "text": "Focus on a specific timepoint…\n\ndf_full |> \n  filter(name==\"thc\", timepoint==\"T2A\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#at-this-point",
    "href": "content/lectures/12-cs01-eda-slides.html#at-this-point",
    "title": "12-cs01-eda",
    "section": "At this point…",
    "text": "At this point…\nWe start to get a sense of the data with these quick and dirty plots, but we’re really only scratching the surface of what’s going on in these data.\n\nThese data require a lot of exploration due to the number of compounds, multiple matrices, and data over time aspects."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#compounds-across-time",
    "href": "content/lectures/12-cs01-eda-slides.html#compounds-across-time",
    "title": "12-cs01-eda",
    "section": "Compounds across time",
    "text": "Compounds across time\n\nCodeExecute (WB)Plots (WB)OFBR\n\n\n\ncompound_scatterplot_group <- function(dataset, compound, timepoints){\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      dataset |> \n        filter(!is.na(time_from_start)) |>\n        ggplot(aes_string(x=\"time_from_start\", \n                          y=compound,\n                          color=\"group\")) + \n        geom_point() +\n        geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                   linetype=\"dashed\", \n                   color=\"gray28\") +\n        scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_y_continuous(limits=c(0,3)) +\n        theme_classic() +\n        theme(legend.position=\"bottom\",\n              legend.title=element_blank()) +\n        labs(x='Time From Start (min)',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        dataset |> \n          filter(!is.na(time_from_start)) |>\n          ggplot(aes_string(x=\"time_from_start\", \n                            y=compound,\n                            color=\"group\")) + \n          geom_point() +\n          geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                     linetype=\"dashed\", \n                     color=\"gray28\")  +\n          scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          theme_classic() +\n          theme(legend.position=\"bottom\",\n                legend.title=element_blank()) +\n          labs(x='Time From Start (min)',\n               y=gsub('GLUC', 'gluc', gsub(\"_\", \"-\", toupper(compound))))\n      )\n    }\n}\n\n\n\n\nscatter_wb <- map(compounds_WB, ~ compound_scatterplot_group( \n    dataset=WB_dups, \n    compound=.x, \n    timepoints=timepoints_WB))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_of <- map(compounds_OF, ~ compound_scatterplot_group( \n    dataset=OF_dups, \n    compound=.x, \n    timepoints=timepoints_OF))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_br <- map(compounds_BR, ~ compound_scatterplot_group( \n    dataset=BR_dups, \n    compound=.x, \n    timepoints=timepoints_BR))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#pairplots",
    "href": "content/lectures/12-cs01-eda-slides.html#pairplots",
    "title": "12-cs01-eda",
    "section": "Pairplots",
    "text": "Pairplots\n\nWBOFBR\n\n\n\npairs(WB[,unlist(compounds_WB)], \n      pch=19, \n      cex=0.3, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(WB[,unlist(compounds_WB)])))))\n\n\n\n\n\n\n\npairs(OF[,unlist(compounds_OF)], \n      pch=19, \n      cex=0.4, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(OF[,unlist(compounds_OF)])))))\n\n\n\n\n\n\n❓ Why is there no pairplot for Breath?"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#group-differences-frequency-of-use",
    "href": "content/lectures/12-cs01-eda-slides.html#group-differences-frequency-of-use",
    "title": "12-cs01-eda",
    "section": "Group Differences: Frequency of Use",
    "text": "Group Differences: Frequency of Use\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_group_only <- function(dataset, compounds, tissue, legend=TRUE, y_lab=TRUE){\n  timepoint_to_use=levels(dataset$timepoint_use)[1]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(group, all_of(compounds)) \n  df <- df |> \n    gather(compound, value, -group) |> \n    clean_gluc() |> \n    group_by(compound) |> \n    mutate(y_max=1.2*max(value)) |> \n    group_by(compound, group) |> \n    mutate(n=n(),\n           my_label=paste0(group, ' N=', n),\n           my_label= gsub(\" \", \"\\n\", my_label))\n  \n  if(tissue == \"Blood\"){\n    df$compound=factor(df$compound, levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  y_pos <- df |> \n    group_by(compound) |> \n    summarize(y.position=mean(y_max))\n  \n  stat.test <- df |>\n    group_by(compound) |>\n    t_test(value ~ my_label) |>\n    adjust_pvalue(method=\"bonferroni\") |>\n    add_significance()\n  test <- stat.test |>\n    left_join(y_pos) |>\n    mutate(p.adj.signif=ifelse(p.adj.signif=='?', 'ns', p.adj.signif),\n           p.adj=ifelse(p.adj < 0.001, \"<0.001\", p.adj))\n\n  if(legend){\n    leg_position='right'\n  }else{\n    leg_position='none'\n  }\n  \n  if(y_lab){\n    y_text=\"Concentration (ng/mL)\"\n  }else{\n    y_text=''\n  }\n  \n  medianFunction <- function(x){\n    return(data.frame(y=round(median(x),1),label=round(median(x,na.rm=T),1)))}\n  \n  p2 <- ggplot(df, aes(x=my_label, y=value, fill=my_label)) + \n    geom_jitter(position=position_jitter(width=.3, height=0), size=0.8, color=\"gray65\")  +\n    geom_boxplot(outlier.shape=NA, alpha=0.6) +\n    stat_summary(fun=\"median\", geom=\"point\", shape=19, size=3, fill=\"black\") + \n    stat_summary(fun.data=medianFunction, geom =\"text\", color=\"black\", size=3.5, vjust=-0.65) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    geom_blank(aes(y=y_max)) + \n    scale_y_continuous(limits=c(0, NA), expand=expansion(mult=c(0, 0.1))) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    theme_classic() +\n    theme(text=element_text(size=14),\n          legend.position=leg_position,\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x='',\n         y=y_text) \n  \n  ann_text <- test |> \n    select(compound, p.adj, value=y.position, my_label=group1) |>\n    filter(p.adj < 0.05) |> \n    mutate(x1=1, x2=2)\n  \n  if(tissue == \"Whole Blood\"){\n    ann_text$compound=factor(ann_text$compound, \n                               levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  p2 + geom_text(data=ann_text, label=ann_text$p.adj, nudge_x=0.5) +\n    geom_segment(data=ann_text, aes(x=x1, xend=x2,\n                                    y=value - (0.04 * value), \n                                    yend=value - (0.04*value)))\n  \n}\n\n\n\n\ncompound_boxplot_group_only(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#group-differences-treatment",
    "href": "content/lectures/12-cs01-eda-slides.html#group-differences-treatment",
    "title": "12-cs01-eda",
    "section": "Group Differences: Treatment",
    "text": "Group Differences: Treatment\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_treatment <- function(dataset, compounds, tissue){\n  timepoint_to_use=levels(dataset$timepoint_use)[2]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(treatment, group, compounds)\n  df <- df |> \n    gather(compound, value, -treatment, -group) |> \n    clean_gluc()\n  \n  df |> \n    ggplot(aes(x=treatment, y=value, fill=group)) + \n    # geom_jitter(color=\"gray36\") +\n    geom_boxplot(outlier.size=0.1) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    scale_x_discrete(labels=function(x) str_wrap(x, width=11)) +\n    theme_classic(base_size=10) +\n    theme(legend.position=\"bottom\",\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x=\"Treatment\",\n         y=\"Measurement (ng/mL)\")\n  \n}\n\n\n\n\ncompound_boxplot_treatment(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#compound-summaries",
    "href": "content/lectures/12-cs01-eda-slides.html#compound-summaries",
    "title": "12-cs01-eda",
    "section": "Compound Summaries",
    "text": "Compound Summaries\n\nCodeWBOFBR\n\n\n\nT2A_plot <- function(dataset, compound, timepoint_use=2){\n  timepoint_to_use=levels(factor(dataset$timepoint_use))[timepoint_use]\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n             aes_string(x=\"group\", \n                        y=compound, \n                        fill=\"group\")) + \n        geom_boxplot(outlier.size=0.1) +\n        scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n        scale_y_continuous(limits=c(0,3)) +\n        facet_grid(~treatment) + \n        theme_classic() +\n        theme(legend.position=\"none\",\n              panel.grid=element_blank(),\n              strip.background=element_blank(),\n              plot.title.position=\"plot\") +\n        labs(title=paste0('Timepoint: ',\n                          levels(dataset$timepoint_use)[timepoint_use],\n                          ' post-smoking'),\n             x='Group',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n               aes_string(x=\"group\", \n                          y=compound, \n                          fill=\"group\")) + \n          geom_boxplot(outlier.shape=NA, alpha=0.8) +\n          scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n          facet_grid(~treatment) + \n          theme_classic() +\n          theme(legend.position=\"none\",\n                panel.grid=element_blank(),\n                strip.background=element_blank(),\n                plot.title.position=\"plot\") +\n          labs(title=paste0('Timepoint: ',\n                            levels(dataset$timepoint_use)[timepoint_use],\n                            ' post-smoking'),\n               x='Group',\n               y=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))))\n      )\n    }\n}\n\n\n\n\npost_wb <- map(compounds_WB, ~ T2A_plot( \n    dataset=WB_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_of <- map(compounds_OF, ~ T2A_plot( \n    dataset=OF_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_br <- map(compounds_BR, ~ T2A_plot( \n    dataset=BR_dups, \n    compound=.x))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#hw02-part-ii",
    "href": "content/lectures/12-cs01-eda-slides.html#hw02-part-ii",
    "title": "12-cs01-eda",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#chess-players",
    "href": "content/lectures/12-cs01-eda-slides.html#chess-players",
    "title": "12-cs01-eda",
    "section": "Chess Players",
    "text": "Chess Players\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n\nchess <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/chess-transfers/transfers.csv\")\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nggplot(chess_trans, aes(y = reorder(Federation, n), x = n)) +\n  geom_bar(stat = \"identity\",fill = \"#1c9099\") +\n  geom_text(aes(x = -3, y = Federation, label = n), size = 4) + #add count at the left side of the bars\n  labs(title = bquote(bold(\"More players transfer to the U.S. than to any other country\")),\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       x = \"NUMBER OF TRANSFERS\",\n       y = \"COUNTRY\") +\n  scale_fill_identity() +\n  theme_minimal() + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank()) + \n  theme(axis.text.y = element_text(size = 10, angle = 0, hjust = 0), #align to the left\n        plot.title = element_text(size=18)) #change font size"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#kushi-fandango",
    "href": "content/lectures/12-cs01-eda-slides.html#kushi-fandango",
    "title": "12-cs01-eda",
    "section": "Kushi: Fandango",
    "text": "Kushi: Fandango\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Kushi also got font working and stars on x-axis that I'd have to spend more time to get working \nfandango_score_comparison <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv\")\n\nall_scores_comp <- fandango_score_comparison |>\n  select(FILM, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars) |>\n  pivot_longer(c(RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars), names_to = \"Site\", values_to = \"Score\")\n\ncounts_of_scores <- all_scores_comp |>\n  group_by(Site, Score) |>\n  summarize(Count = n())\n\ncounts_of_scores <- counts_of_scores |>\n  group_by(Site) |>\n  mutate(Total = sum(Count))\n\nscores_with_percents <- counts_of_scores |>\n  mutate(Percent = Count / Total * 100)\n\n# making percents 0 for any scores that don't have any values\n\nsites <- tibble(Site = c(\"RT_norm_round\", \"RT_user_norm_round\", \"Metacritic_norm_round\", \"Metacritic_user_norm_round\", \"IMDB_norm_round\", \"Fandango_Stars\"))\n\nratings <- tibble(Score = seq(0, 5, by = 0.5))\n\nall_scores <- expand.grid(Site = sites$Site, Score = ratings$Score)\n\nall_scores <- all_scores |>\n  full_join(scores_with_percents, by = c(\"Site\", \"Score\")) |>\n  mutate(Percent = case_when(is.na(Count) ~ 0,\n                             TRUE ~ Percent))\n\nscores <- ggplot(all_scores, aes(x = Score, y = Percent, color = Site)) +\n  geom_line()  +\n  geom_hline(yintercept = 0, size = 0.7, color = \"black\") +\n  labs(x = NULL, y = NULL,\n       title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distrubution of 146 films in theaters in 2015 that\\n had 30+ reviews on Fandango.com\") +\n  scale_y_continuous(labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")) +\n  scale_x_continuous(labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +\n  scale_color_manual(values = c(\"Fandango_Stars\" = \"#fa6d54\",\n                                \"IMDB_norm_round\" = \"#e5c66a\",\n                                \"Metacritic_user_norm_round\" = \"#aeca91\",\n                                \"RT_user_norm_round\" = \"#76bde0\",\n                                \"Metacritic_norm_round\" = \"#b87eb5\",\n                                \"RT_norm_round\" = \"#a3a3a3\")) +\n  geom_ribbon(data = filter(all_scores, Site != \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent), alpha = 0.1) +\n  geom_ribbon(data = filter(all_scores, Site == \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent, fill = \"#fa6d54\", alpha = 0.24)) +\n  guides(color = \"none\", fill = \"none\", alpha = \"none\")\n\nscores + \n  annotate(\"text\", x = 4.9, y = 35, label = \"Fandango\", size = 5, color = \"#fa6d54\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.9, y = 37, label = \"IMDb users\", size = 5, color = \"#e5c66a\") +\n  annotate(\"text\", x = 2.7, y = 27, label = \"Metacritic\\nusers\", size = 5, color = \"#aeca91\") +\n  annotate(\"text\", x = 2.2, y = 20, label = \"Rotten\\nTomatoes\\nusers\", size = 5, color = \"#76bde0\") +\n  annotate(\"text\", x = 1.5, y = 16, label = \"Metacritic\", size = 5, color = \"#b87eb5\") +\n  annotate(\"text\", x = 0.7, y = 13, label = \"Rotten\\nTomatoes\", size = 5, color = \"#a3a3a3\") +\n  theme(#text = element_text(family = \"NimbusSan\"), \n        plot.title = element_text(face = \"bold\", size = 25, hjust = -0.1), \n        plot.subtitle = element_text(size = 15, hjust = -0.1),\n        plot.background = element_rect(fill = \"#f0f0f0\"),\n        panel.background = element_rect(fill = \"#f0f0f0\"),\n        panel.grid.major = element_line(color = \"gray75\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text = element_text(size = 14)\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#markus-congress",
    "href": "content/lectures/12-cs01-eda-slides.html#markus-congress",
    "title": "12-cs01-eda",
    "section": "Markus: Congress",
    "text": "Markus: Congress\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\ncongress_data <- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\")\n\ncongress_ages =\n  congress_data |>\n  select(congress, chamber, age_years) |>\n  mutate(congress_year = case_when(TRUE ~ 1787 + (2 * as.integer(congress)))) |>\n  group_by(congress_year, chamber) |>\n  mutate(mean_age = case_when(TRUE ~ mean(age_years))) |>\n  mutate(chamber = fct_recode(chamber,\n                              SENATE = \"Senate\",\n                              HOUSE = \"House\",))\n\nggplot(data = congress_ages,\n       mapping = aes(y = mean_age,\n                     x = congress_year,\n                     color = fct_rev(chamber),\n                     )) +\n  geom_step(size = 1) + \n  guides() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       caption = \"Data is based on all members who served in either the Senate or House in each Congress, which is notated\\nby the year in which it was seated. Any member who served in bothchambers in the same Congress was\\nassigned to the chamber in which they cast more votes.\\n FiveThirtyEight\\nSOURCES: BIOGRAPHICAL DIRECTORY OF THE U.S. CONGRESS, U.S. HOUSE OF REPRESENTATIVES,\\nU.S. SENATE, UNITEDSTATES GITHUB, VOTEVIEW.COM\",\n       y = NULL,\n       x = NULL\n  ) +\n  scale_color_manual(values=c(\"#6b4ddd\",\"#29ae53\")) +\n  theme_minimal(base_size = 13) + \n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(face = \"bold\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.title=element_blank(),\n        legend.position = \"top\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#voter-demographics",
    "href": "content/lectures/12-cs01-eda-slides.html#voter-demographics",
    "title": "12-cs01-eda",
    "section": "Voter Demographics",
    "text": "Voter Demographics\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggpubr)\n\nvoters = read.csv(\"data/nonvoters_data.csv\")\n\n\n#creating subplots\nrace = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Race\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())+\n    guides(fill = guide_legend(override.aes = list(shape = 16, key_width = 1, key_height = 1))) #trying to change the shape of the legend key\n\nincome = ggplot(data = voters, mapping = aes(y = factor(income_cat), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Income\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\nage = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Age\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\neducation = ggplot(data = voters, mapping = aes(y = factor(educ), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Education\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\npartyID = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"party ID\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\n#combining subplots\nria = ggarrange(race, income, age, ncol = 3, common.legend = TRUE, legend = \"top\") +\n  theme(plot.margin = margin(0.5, 0,-2, -1,\"cm\")) \n\nep = ggarrange(education, partyID) +\n  theme(plot.margin = margin(2, 4, -2.5, 1, \"cm\"))\n\nvoterplot = ggarrange(ria, ep, nrow = 3) \n\n\n#titles\nvoterplot = annotate_figure(annotate_figure(voterplot, \n  top = text_grob(\"Demographic information of survey respondants, by voting history\")),\n  top = text_grob(\"Those who always vote and those who sometimes vote aren't that different\", face = \"bold\")\n)\n\n\nvoterplot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#fouls",
    "href": "content/lectures/12-cs01-eda-slides.html#fouls",
    "title": "12-cs01-eda",
    "section": "Fouls",
    "text": "Fouls\n\nOriginalPlot ICode IPlot (Banso)Code (Banso)\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_csv_file <- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/foul-balls/foul-balls.csv\"\nfoulballs <- read.csv(url(raw_csv_file))\n\nfoulballs <- foulballs |>\n  mutate(over90 = case_when(\n    exit_velocity < 90 ~ \"no\",\n    exit_velocity >= 90 ~ \"yes\"\n  ))\n\nggplot(foulballs,\n       aes (y = used_zone)) +\n  geom_bar(aes(fill = over90),\n           position = position_stack(reverse = TRUE),\n           show.legend = FALSE) +\n  scale_fill_manual(labels = c(\"< 90 mph\", \"≥ 90 mph\", \"Unknown exit velocity\"),\n                    values = c(\"#97c16d\", \"#63abb0\"), na.value = \"#d3d3d3\") +\n  scale_y_discrete(limits = rev(unique(foulballs$used_zone)))+\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"#ececec\"),\n        panel.background = element_rect(fill = \"white\"),\n        axis.text.x = element_text(color = \"#9e9e9e\"),\n        plot.caption = element_text(color = \"#b5b5b5\"),\n        axis.line.y = element_line(colour = \"#343434\"),\n        axis.title.y = element_text(angle = 0, vjust = 0.12, color = \"#343434\", size = 9),\n        axis.ticks = element_blank(),\n        legend.title = element_blank()) +\n  labs(title = \"The hardest-hit fouls seem to land in unprotected areas\",\n       subtitle = str_wrap(\"Foul balls by the stadium zone they landed in and their exit velocity, among 906 fouls hit this season in the most foul-heavy day at the 10 MLB stadiums that produced the most fouls as of June 5\", 85),\n       x = \"\", y = \"Zone\",\n       caption = \"SOURCE: BASEBALL SAVANT\") +\n  annotate(\"text\", x = 75, y = 1, label = \"< 90 mph\", col = \"white\") +\n  annotate(\"text\", x = 140, y = 3.3, label = \"≥ 90 mph\", col = \"#63abb0\") +\n  annotate(\"text\", x = 215, y = 1, label = \"Unknown exit velocity\", col = \"white\")\n\n\n\n\n\n\n\n\n\n\n\nball_data <- foulballs |>\n  mutate(category = case_when(\n    exit_velocity < 90 ~ \"<90\",\n    exit_velocity >= 90 ~ \">=90\",\n    is.na(exit_velocity) ~ \"Unknown\"\n                           ))\nmy_plot <- ggplot(ball_data, aes(y = fct_rev(as.character(used_zone)), fill = category)) +\n  geom_bar(position = position_stack(reverse = TRUE)) +\n  labs(title = \"The hardest-hit fouls seem \\nto land in unprotected areas\",\n       subtitle = \"The 906 foul balls hit this season from \\nthe most foul-heavy day at each of the \\n10 MLB stadiums that produced the \\nthe most fouls as of June 5, by zone where \\nthe balls landed and their exit velocities\",\n       y = \"Zone\", x = NULL) +\n  scale_fill_manual(values = c(\"Unknown\" = \"#DEDEDE\", \"<90\" = \"#9ECE88\", \">=90\" = \"#17AFAD\")) +\n  scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250),position = \"top\") +\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major.x = element_line(size = 0.5, linetype=\"solid\", color=\"#CECECE\"),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(color = \"black\", face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(size = 10, margin = margin(b = 20))\n)\n\nmy_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#colin-nba-raptor-ratings",
    "href": "content/lectures/12-cs01-eda-slides.html#colin-nba-raptor-ratings",
    "title": "12-cs01-eda",
    "section": "Colin: NBA RAPTOR ratings",
    "text": "Colin: NBA RAPTOR ratings\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\nraptor <- read.csv(\"data/latest_RAPTOR_by_player.csv\")\n\nraptor_rounded <- raptor |> \n  filter(mp >= 1137) |>\n  mutate(across(where(is.numeric), round, 1))\n\nraptor_plot <- raptor_rounded |>\n  ggplot(aes(x=raptor_offense, y = raptor_defense)) + \n  \n  # Annotations\n    # The colored rectangles for the 1 & 3 quadrants\n    annotate(\"rect\", xmin=0, xmax=10, ymin=0, ymax=10, fill = '#c5ecee', alpha = .85) + \n    annotate(\"rect\", xmin=-10, xmax=0, ymin=-10, ymax=0, fill = '#fecada', alpha = .85) +\n    \n    # The 3rd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -7.8, ymax = -6.8, fill = '#fd97b6') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = -7.4, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = -9, label = \" -  defense\") +\n    \n    # The 1st quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 8.4, ymax = 9.4, fill = '#8cdadf') +\n    annotate(\"text\", x = 7.5, y = 8.8, label = \" +  offense\") +\n    annotate(\"text\", x = 7.55, y = 7.1, label = \" +  defense\") +\n  \n    # The 2nd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 8.4, ymax = 9.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = 8.8, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = 7.1, label = \" +  defense\") +\n  \n    # The 4th quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -7.8, ymax = -6.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = 7.55, y = -7.4, label = \" +  offense\") +\n    annotate(\"text\", x = 7.5, y = -9, label = \" -  defense\") +\n  \n  geom_point(shape= 21, colour = \"black\", fill = \"white\", size = 4) + \n  \n  labs(\n    x = \"Offensive RAPTOR rating\", \n    y = \"Defensive RAPTOR rating\",\n    title = paste0('Nikola Jokic is the Best NBA Player Based on Overall RAPTOR Rating',\n            '<br>',\n            '<sup>',\n            'An Analytical Approach to the 2022 - 2023 NBA Season','</sup>')\n    ) + \n    \n  # Theme settings\n  theme_light() + \n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(), \n    panel.grid.major = element_line(color = \"#cdcdcd\", linewidth = 0.5), \n    plot.margin = margin(l = 100, r = 100, b = 20, t = 10),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(color = \"#cdcdcd\", size = 12),\n    axis.title.x = element_text(margin = margin(t=3)),\n    axis.title.y = element_text(margin = margin(r=3)),\n    ) + \n  coord_fixed(ratio = 1) \n\n# interactive\n# ggplotly(raptor_plot, text=player_name, hoverinfo='text') \n\nraptor_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#biden-approval",
    "href": "content/lectures/12-cs01-eda-slides.html#biden-approval",
    "title": "12-cs01-eda",
    "section": "Biden Approval",
    "text": "Biden Approval\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n#getting only data from beginning of COVID to Jan 19, 2021 and getting rid of the \"all\" category in the party column \nyear_20 <- \n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year <= 2020)|>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n\nyear_21 <-\n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year == 2021 & end_month == 1 & end_day <= 19) |>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n       #  if (end_year == 2021 & end_month == 1 & end_day <= 19))\n\nselect_COVID_approval <-\n  full_join(year_20, year_21)\n\nggplot(select_COVID_approval, aes(x = as.Date(end_date),\n                           y = approve,\n                           color = party)) +\n  geom_smooth(aes(group = party), span = 0.05, se = FALSE) + \n  scale_color_manual(values = c(\"D\" = \"#008fd5\", \n                               \"R\" = \"#ff2700\",\n                               \"I\" = \"#a55330\")) +\n  \n  #Important Dates\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-02-29\"), y = 95, label=\"First U.S. \\n Death \\n Reported\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-05-20\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-05-20\"), y = 95, label=\"U.S. Deaths \\n surpass \\n 100,000\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype=3) + #Trump Diagnosed with COVID-19\n    annotate(\"text\", x= as.Date(\"2020-10-02\"), y = 95, label=\"Trump \\n Diagnosed \\n with \\n COVID-19\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-11-07\"), y = 95, label=\"Biden \\n declared \\n election \\n winner\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2021-01-19\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2021-01-19\"), y = 95, label=\"Biden \\n sworn \\n into \\n office\", size=3, color=\"black\") +\n  \n  labs(title = \"Approval of Trump’s response varies widely by party\",\n       subtitle = \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s \\n handling of the coronavirus outbreak\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5 ), \n        plot.subtitle = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.minor = element_line(colour=\"gray\")\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#recap",
    "href": "content/lectures/12-cs01-eda-slides.html#recap",
    "title": "12-cs01-eda",
    "section": "Recap",
    "text": "Recap\n\nCan you explain/describe the plots generated in the context of these data?\nCan you generate EDA plots of your own for these data\nCan you understand/work through the more complicated code provided (even if you couldn’t have come up with it on your own)\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html",
    "href": "content/lectures/12-cs01-eda.html",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Q: was curious about the sensitivity/cut-offs/specificity, but we mainly discussed it in class already; was still slightly confused about it?\nA: We’ll discuss this in detail later this week and early next week!\n\n\nQ: Are Youden’s indices related to ROC curves?\nA: Related, yes! We’ll discuss both soon!\n\n\nQ: I wasn’t sure how to interpret some of the visuals towards the end of lecture.\nA: That’s OK! We’ll be recreating these and discussing them more as we do this case study in class.\n\n\nQ: Did they actually use intravenous blood draws or just thumb pricks because multiple intravenous would not be fun.\nA: It was venous blood from the arm. This unfunness is one of the reasons participants were compensated.\n\n\nQ: Did this study (or other studies on THC) impairment end up influencing any legislation at the local or state level?\nA: Great question! The state is currently reviewing these and other study’s data. The state was definitely aware of this study and waited (im)patiently while we analyzed and worked to publish.\n\n\nQ: How long should our reports be?\nA: It’s hard to say. We’ll discuss an example today so you have a sense!\n\n\nQ: Regarding the final project, will there be a Google form that we can fill out that will help us form groups if we can’t form one ourselves?  A: Yup - I’d say try to find a group using Piazza, in class, or during lab. However, if you’re unable, when you fill out the form to indicate your group next week, you’ll select that you’d like to be placed into a group.\n\n\n\n\nDue Dates:\n\n🔭 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n\nNotes:\n\nCS01 Groups have been sent out\n\nemail for contact\nGitHub repo <- please accept and open; make sure you have access\ngroup mate feedback is required\nif you made changes to repo yesterday, be sure to pull to get data in your repo\n\nFinal Project: can use Piazza to help find group mates\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe CS01 data are data for you only. My collaborator is excited that y’all will be working on this…but these are still research data, so please do not share with others or post publicly.\n\n\n\n\n\n\n\nQ: Are you allowed to share the average midterm scores for the past few quarters?\nA: IIRC, they were in the mid-high 80%s\n\n\nQ: When describing the dataset in our CSs, would it be okay to format it as a data card rather than a paragraph explanation of the variables structure? Additionally, would it be okay to store this as its own read.me in the repo or should it be a part of the main report?\nA: Yes - like the data card idea. And a detailed README in the repo is great. A short description should still be included in the report and can point to the readme.\n\n\nQ: When should we cite in our case study? It is just whenever we look up and use code from the internet?\nA: There AND any time you get information elsewhere that’s not general knowledge. For example, in your background section, you’ll likely cite a bunch of sources.\n\n\n\n\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n✅ HW02 Scores/Feedback Posted\n\n\n\n\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice” -Hadley\n\n\nfunction_name <- function(input){\n  # operations using input\n}\n\n\nFor example…\n\ndouble_value <- function(val){\n  val * 2\n}\n\n\n\nTo use/execute:\n\ndouble_value(3)\n\n[1] 6\n\n\n\n\nIn what we’ve done so far, we’ve seen functions that operate on and return the whole dataframe (DF in DF out) (drop_dups) and those that carry out operations on each row of a dataframe with a number of inputs (i.e. assign_timepoint; these require the function to be map-ed)\nAdditional resource: https://r4ds.had.co.nz/functions.html\n\n\n\n\n\nPrevious Projects\nExploring the Data"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#example-case-study",
    "href": "content/lectures/12-cs01-eda.html#example-case-study",
    "title": "12-cs01-eda",
    "section": "Example Case Study",
    "text": "Example Case Study\nSee & Discuss: https://cogs137.github.io/website/content/cs/cs-example.html"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#feedback-scores",
    "href": "content/lectures/12-cs01-eda.html#feedback-scores",
    "title": "12-cs01-eda",
    "section": "Feedback & Scores",
    "text": "Feedback & Scores\nFeedback to other students here\n\n\nYou cannot see the projects, but can read all of the comments and see the associated score. Also, note that the same row is not the same group.\n\nCommon comments:\n\ncontext/explanation/guidance/lacking\nmissing citations\nfailure to introduce/describe the data\nmaking statements without evidence\nneed to edit for cohesiveness, story, clarity"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#an-example-rubric",
    "href": "content/lectures/12-cs01-eda.html#an-example-rubric",
    "title": "12-cs01-eda",
    "section": "An (Example) Rubric",
    "text": "An (Example) Rubric\nThis is NOT the rubric for your case study, but it will be similar:"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#notes",
    "href": "content/lectures/12-cs01-eda.html#notes",
    "title": "12-cs01-eda",
    "section": "Notes",
    "text": "Notes\n\n\nLots of code/plots will be provided here\nYou are free to include any of it in your own case study (no attribution needed)\nYou probably should NOT include all of them in your final report\nFor any of the “basic” plots you include in your report, you’ll want to clean them up/improve their design.\nYour final report should be polished from start to finish"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#packages",
    "href": "content/lectures/12-cs01-eda.html#packages",
    "title": "12-cs01-eda",
    "section": "Packages",
    "text": "Packages\nTwo additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#our-datasets",
    "href": "content/lectures/12-cs01-eda.html#our-datasets",
    "title": "12-cs01-eda",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#the-data",
    "href": "content/lectures/12-cs01-eda.html#the-data",
    "title": "12-cs01-eda",
    "section": "The Data",
    "text": "The Data\nReading in the .RData we wrote at the end of the last set of notes…(using load)\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nThis reads the objects stored in these files into your Environment for use."
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#what-to-do-with-all-of-these-functions",
    "href": "content/lectures/12-cs01-eda.html#what-to-do-with-all-of-these-functions",
    "title": "12-cs01-eda",
    "section": "What to do with all of these functions?",
    "text": "What to do with all of these functions?\nFor example…we discussed this function in the last set of notes.\n\n drop_dups <- function(dataset){\n  out <- dataset |> \n    filter(!is.na(timepoint_use)) |> \n    group_by(timepoint_use) |> \n    distinct(id, .keep_all=TRUE) |> \n    ungroup()\n  return(out)\n } \n\n\nWe’re going to have a lot of functions throughout…like this helper function to clean up names\n\n# helper function to clean up name of two compounds\nclean_gluc <- function(df){\n  df <- df |> \n    mutate(compound=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))),\n           compound=gsub('THCOH', '11-OH-THC', compound))\n  return(df)\n}\n\n\n\nFunctions can/should be stored in a separate .R file, probably in a src/ directory.\n\n\nTo have access to the functions in that file…\n\nsource(\"path/to/file\")\n\n\n\n\nsource(\"src/cs01_functions.R\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#single-variable-basic-plots",
    "href": "content/lectures/12-cs01-eda.html#single-variable-basic-plots",
    "title": "12-cs01-eda",
    "section": "Single Variable (basic) plots",
    "text": "Single Variable (basic) plots\nFor a single compound…\n\nggplot(WB, aes(x=thc)) + geom_histogram()\n\n\n\n\n\nBut, with wide data, that’s not easy to do for all compounds, so you may want to pivot those data….\n\nWB_long <- WB |> \n  pivot_longer(6:13) |>\n  rename(\"fluid\"=\"fluid_type\")\n\n\n\nDistribtions across all compounds (WB):\n\nggplot(WB_long, aes(x=value)) + \n  geom_histogram() +\n  facet_wrap(~name)\n\n\n\n\n\n\nNow the same for OF and BR:\n\nOF_long <- OF |> pivot_longer(6:12)\nBR_long <- BR |> pivot_longer(6)\n\n\n\nCombining long datasets:\n\ndf_full <- bind_rows(WB_long, OF_long, BR_long)\n\n\n\nPlotting some of these data…\n\ndf_full |>\n  mutate(group_compound=paste0(fluid,\": \", name)) |>\nggplot(aes(x=value)) + \n  geom_histogram() + \n  facet_wrap(~group_compound, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#two-variables-at-a-time",
    "href": "content/lectures/12-cs01-eda.html#two-variables-at-a-time",
    "title": "12-cs01-eda",
    "section": "Two variables at a time",
    "text": "Two variables at a time"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#thc-frequency",
    "href": "content/lectures/12-cs01-eda.html#thc-frequency",
    "title": "12-cs01-eda",
    "section": "THC & Frequency",
    "text": "THC & Frequency\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=group, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#thc-treatment-group",
    "href": "content/lectures/12-cs01-eda.html#thc-treatment-group",
    "title": "12-cs01-eda",
    "section": "THC & Treatment Group",
    "text": "THC & Treatment Group\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#focus-on-a-specific-timepoint",
    "href": "content/lectures/12-cs01-eda.html#focus-on-a-specific-timepoint",
    "title": "12-cs01-eda",
    "section": "Focus on a specific timepoint…",
    "text": "Focus on a specific timepoint…\n\ndf_full |> \n  filter(name==\"thc\", timepoint==\"T2A\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#at-this-point",
    "href": "content/lectures/12-cs01-eda.html#at-this-point",
    "title": "12-cs01-eda",
    "section": "At this point…",
    "text": "At this point…\nWe start to get a sense of the data with these quick and dirty plots, but we’re really only scratching the surface of what’s going on in these data.\n\nThese data require a lot of exploration due to the number of compounds, multiple matrices, and data over time aspects."
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#compounds-across-time",
    "href": "content/lectures/12-cs01-eda.html#compounds-across-time",
    "title": "12-cs01-eda",
    "section": "Compounds across time",
    "text": "Compounds across time\n\nCodeExecute (WB)Plots (WB)OFBR\n\n\n\ncompound_scatterplot_group <- function(dataset, compound, timepoints){\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      dataset |> \n        filter(!is.na(time_from_start)) |>\n        ggplot(aes_string(x=\"time_from_start\", \n                          y=compound,\n                          color=\"group\")) + \n        geom_point() +\n        geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                   linetype=\"dashed\", \n                   color=\"gray28\") +\n        scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_y_continuous(limits=c(0,3)) +\n        theme_classic() +\n        theme(legend.position=\"bottom\",\n              legend.title=element_blank()) +\n        labs(x='Time From Start (min)',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        dataset |> \n          filter(!is.na(time_from_start)) |>\n          ggplot(aes_string(x=\"time_from_start\", \n                            y=compound,\n                            color=\"group\")) + \n          geom_point() +\n          geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                     linetype=\"dashed\", \n                     color=\"gray28\")  +\n          scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          theme_classic() +\n          theme(legend.position=\"bottom\",\n                legend.title=element_blank()) +\n          labs(x='Time From Start (min)',\n               y=gsub('GLUC', 'gluc', gsub(\"_\", \"-\", toupper(compound))))\n      )\n    }\n}\n\n\n\n\nscatter_wb <- map(compounds_WB, ~ compound_scatterplot_group( \n    dataset=WB_dups, \n    compound=.x, \n    timepoints=timepoints_WB))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_of <- map(compounds_OF, ~ compound_scatterplot_group( \n    dataset=OF_dups, \n    compound=.x, \n    timepoints=timepoints_OF))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_br <- map(compounds_BR, ~ compound_scatterplot_group( \n    dataset=BR_dups, \n    compound=.x, \n    timepoints=timepoints_BR))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#pairplots",
    "href": "content/lectures/12-cs01-eda.html#pairplots",
    "title": "12-cs01-eda",
    "section": "Pairplots",
    "text": "Pairplots\n\nWBOFBR\n\n\n\npairs(WB[,unlist(compounds_WB)], \n      pch=19, \n      cex=0.3, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(WB[,unlist(compounds_WB)])))))\n\n\n\n\n\n\n\npairs(OF[,unlist(compounds_OF)], \n      pch=19, \n      cex=0.4, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(OF[,unlist(compounds_OF)])))))\n\n\n\n\n\n\n❓ Why is there no pairplot for Breath?"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#group-differences-frequency-of-use",
    "href": "content/lectures/12-cs01-eda.html#group-differences-frequency-of-use",
    "title": "12-cs01-eda",
    "section": "Group Differences: Frequency of Use",
    "text": "Group Differences: Frequency of Use\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_group_only <- function(dataset, compounds, tissue, legend=TRUE, y_lab=TRUE){\n  timepoint_to_use=levels(dataset$timepoint_use)[1]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(group, all_of(compounds)) \n  df <- df |> \n    gather(compound, value, -group) |> \n    clean_gluc() |> \n    group_by(compound) |> \n    mutate(y_max=1.2*max(value)) |> \n    group_by(compound, group) |> \n    mutate(n=n(),\n           my_label=paste0(group, ' N=', n),\n           my_label= gsub(\" \", \"\\n\", my_label))\n  \n  if(tissue == \"Blood\"){\n    df$compound=factor(df$compound, levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  y_pos <- df |> \n    group_by(compound) |> \n    summarize(y.position=mean(y_max))\n  \n  stat.test <- df |>\n    group_by(compound) |>\n    t_test(value ~ my_label) |>\n    adjust_pvalue(method=\"bonferroni\") |>\n    add_significance()\n  test <- stat.test |>\n    left_join(y_pos) |>\n    mutate(p.adj.signif=ifelse(p.adj.signif=='?', 'ns', p.adj.signif),\n           p.adj=ifelse(p.adj < 0.001, \"<0.001\", p.adj))\n\n  if(legend){\n    leg_position='right'\n  }else{\n    leg_position='none'\n  }\n  \n  if(y_lab){\n    y_text=\"Concentration (ng/mL)\"\n  }else{\n    y_text=''\n  }\n  \n  medianFunction <- function(x){\n    return(data.frame(y=round(median(x),1),label=round(median(x,na.rm=T),1)))}\n  \n  p2 <- ggplot(df, aes(x=my_label, y=value, fill=my_label)) + \n    geom_jitter(position=position_jitter(width=.3, height=0), size=0.8, color=\"gray65\")  +\n    geom_boxplot(outlier.shape=NA, alpha=0.6) +\n    stat_summary(fun=\"median\", geom=\"point\", shape=19, size=3, fill=\"black\") + \n    stat_summary(fun.data=medianFunction, geom =\"text\", color=\"black\", size=3.5, vjust=-0.65) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    geom_blank(aes(y=y_max)) + \n    scale_y_continuous(limits=c(0, NA), expand=expansion(mult=c(0, 0.1))) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    theme_classic() +\n    theme(text=element_text(size=14),\n          legend.position=leg_position,\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x='',\n         y=y_text) \n  \n  ann_text <- test |> \n    select(compound, p.adj, value=y.position, my_label=group1) |>\n    filter(p.adj < 0.05) |> \n    mutate(x1=1, x2=2)\n  \n  if(tissue == \"Whole Blood\"){\n    ann_text$compound=factor(ann_text$compound, \n                               levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  p2 + geom_text(data=ann_text, label=ann_text$p.adj, nudge_x=0.5) +\n    geom_segment(data=ann_text, aes(x=x1, xend=x2,\n                                    y=value - (0.04 * value), \n                                    yend=value - (0.04*value)))\n  \n}\n\n\n\n\ncompound_boxplot_group_only(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#group-differences-treatment",
    "href": "content/lectures/12-cs01-eda.html#group-differences-treatment",
    "title": "12-cs01-eda",
    "section": "Group Differences: Treatment",
    "text": "Group Differences: Treatment\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_treatment <- function(dataset, compounds, tissue){\n  timepoint_to_use=levels(dataset$timepoint_use)[2]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(treatment, group, compounds)\n  df <- df |> \n    gather(compound, value, -treatment, -group) |> \n    clean_gluc()\n  \n  df |> \n    ggplot(aes(x=treatment, y=value, fill=group)) + \n    # geom_jitter(color=\"gray36\") +\n    geom_boxplot(outlier.size=0.1) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    scale_x_discrete(labels=function(x) str_wrap(x, width=11)) +\n    theme_classic(base_size=10) +\n    theme(legend.position=\"bottom\",\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x=\"Treatment\",\n         y=\"Measurement (ng/mL)\")\n  \n}\n\n\n\n\ncompound_boxplot_treatment(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#compound-summaries",
    "href": "content/lectures/12-cs01-eda.html#compound-summaries",
    "title": "12-cs01-eda",
    "section": "Compound Summaries",
    "text": "Compound Summaries\n\nCodeWBOFBR\n\n\n\nT2A_plot <- function(dataset, compound, timepoint_use=2){\n  timepoint_to_use=levels(factor(dataset$timepoint_use))[timepoint_use]\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n             aes_string(x=\"group\", \n                        y=compound, \n                        fill=\"group\")) + \n        geom_boxplot(outlier.size=0.1) +\n        scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n        scale_y_continuous(limits=c(0,3)) +\n        facet_grid(~treatment) + \n        theme_classic() +\n        theme(legend.position=\"none\",\n              panel.grid=element_blank(),\n              strip.background=element_blank(),\n              plot.title.position=\"plot\") +\n        labs(title=paste0('Timepoint: ',\n                          levels(dataset$timepoint_use)[timepoint_use],\n                          ' post-smoking'),\n             x='Group',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n               aes_string(x=\"group\", \n                          y=compound, \n                          fill=\"group\")) + \n          geom_boxplot(outlier.shape=NA, alpha=0.8) +\n          scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n          facet_grid(~treatment) + \n          theme_classic() +\n          theme(legend.position=\"none\",\n                panel.grid=element_blank(),\n                strip.background=element_blank(),\n                plot.title.position=\"plot\") +\n          labs(title=paste0('Timepoint: ',\n                            levels(dataset$timepoint_use)[timepoint_use],\n                            ' post-smoking'),\n               x='Group',\n               y=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))))\n      )\n    }\n}\n\n\n\n\npost_wb <- map(compounds_WB, ~ T2A_plot( \n    dataset=WB_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_of <- map(compounds_OF, ~ T2A_plot( \n    dataset=OF_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_br <- map(compounds_BR, ~ T2A_plot( \n    dataset=BR_dups, \n    compound=.x))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#hw02-part-ii",
    "href": "content/lectures/12-cs01-eda.html#hw02-part-ii",
    "title": "12-cs01-eda",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#chess-players",
    "href": "content/lectures/12-cs01-eda.html#chess-players",
    "title": "12-cs01-eda",
    "section": "Chess Players",
    "text": "Chess Players\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n\nchess <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/chess-transfers/transfers.csv\")\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nggplot(chess_trans, aes(y = reorder(Federation, n), x = n)) +\n  geom_bar(stat = \"identity\",fill = \"#1c9099\") +\n  geom_text(aes(x = -3, y = Federation, label = n), size = 4) + #add count at the left side of the bars\n  labs(title = bquote(bold(\"More players transfer to the U.S. than to any other country\")),\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       x = \"NUMBER OF TRANSFERS\",\n       y = \"COUNTRY\") +\n  scale_fill_identity() +\n  theme_minimal() + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank()) + \n  theme(axis.text.y = element_text(size = 10, angle = 0, hjust = 0), #align to the left\n        plot.title = element_text(size=18)) #change font size"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#kushi-fandango",
    "href": "content/lectures/12-cs01-eda.html#kushi-fandango",
    "title": "12-cs01-eda",
    "section": "Kushi: Fandango",
    "text": "Kushi: Fandango\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Kushi also got font working and stars on x-axis that I'd have to spend more time to get working \nfandango_score_comparison <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv\")\n\nall_scores_comp <- fandango_score_comparison |>\n  select(FILM, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars) |>\n  pivot_longer(c(RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars), names_to = \"Site\", values_to = \"Score\")\n\ncounts_of_scores <- all_scores_comp |>\n  group_by(Site, Score) |>\n  summarize(Count = n())\n\ncounts_of_scores <- counts_of_scores |>\n  group_by(Site) |>\n  mutate(Total = sum(Count))\n\nscores_with_percents <- counts_of_scores |>\n  mutate(Percent = Count / Total * 100)\n\n# making percents 0 for any scores that don't have any values\n\nsites <- tibble(Site = c(\"RT_norm_round\", \"RT_user_norm_round\", \"Metacritic_norm_round\", \"Metacritic_user_norm_round\", \"IMDB_norm_round\", \"Fandango_Stars\"))\n\nratings <- tibble(Score = seq(0, 5, by = 0.5))\n\nall_scores <- expand.grid(Site = sites$Site, Score = ratings$Score)\n\nall_scores <- all_scores |>\n  full_join(scores_with_percents, by = c(\"Site\", \"Score\")) |>\n  mutate(Percent = case_when(is.na(Count) ~ 0,\n                             TRUE ~ Percent))\n\nscores <- ggplot(all_scores, aes(x = Score, y = Percent, color = Site)) +\n  geom_line()  +\n  geom_hline(yintercept = 0, size = 0.7, color = \"black\") +\n  labs(x = NULL, y = NULL,\n       title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distrubution of 146 films in theaters in 2015 that\\n had 30+ reviews on Fandango.com\") +\n  scale_y_continuous(labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")) +\n  scale_x_continuous(labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +\n  scale_color_manual(values = c(\"Fandango_Stars\" = \"#fa6d54\",\n                                \"IMDB_norm_round\" = \"#e5c66a\",\n                                \"Metacritic_user_norm_round\" = \"#aeca91\",\n                                \"RT_user_norm_round\" = \"#76bde0\",\n                                \"Metacritic_norm_round\" = \"#b87eb5\",\n                                \"RT_norm_round\" = \"#a3a3a3\")) +\n  geom_ribbon(data = filter(all_scores, Site != \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent), alpha = 0.1) +\n  geom_ribbon(data = filter(all_scores, Site == \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent, fill = \"#fa6d54\", alpha = 0.24)) +\n  guides(color = \"none\", fill = \"none\", alpha = \"none\")\n\nscores + \n  annotate(\"text\", x = 4.9, y = 35, label = \"Fandango\", size = 5, color = \"#fa6d54\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.9, y = 37, label = \"IMDb users\", size = 5, color = \"#e5c66a\") +\n  annotate(\"text\", x = 2.7, y = 27, label = \"Metacritic\\nusers\", size = 5, color = \"#aeca91\") +\n  annotate(\"text\", x = 2.2, y = 20, label = \"Rotten\\nTomatoes\\nusers\", size = 5, color = \"#76bde0\") +\n  annotate(\"text\", x = 1.5, y = 16, label = \"Metacritic\", size = 5, color = \"#b87eb5\") +\n  annotate(\"text\", x = 0.7, y = 13, label = \"Rotten\\nTomatoes\", size = 5, color = \"#a3a3a3\") +\n  theme(#text = element_text(family = \"NimbusSan\"), \n        plot.title = element_text(face = \"bold\", size = 25, hjust = -0.1), \n        plot.subtitle = element_text(size = 15, hjust = -0.1),\n        plot.background = element_rect(fill = \"#f0f0f0\"),\n        panel.background = element_rect(fill = \"#f0f0f0\"),\n        panel.grid.major = element_line(color = \"gray75\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text = element_text(size = 14)\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#markus-congress",
    "href": "content/lectures/12-cs01-eda.html#markus-congress",
    "title": "12-cs01-eda",
    "section": "Markus: Congress",
    "text": "Markus: Congress\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\ncongress_data <- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\")\n\ncongress_ages =\n  congress_data |>\n  select(congress, chamber, age_years) |>\n  mutate(congress_year = case_when(TRUE ~ 1787 + (2 * as.integer(congress)))) |>\n  group_by(congress_year, chamber) |>\n  mutate(mean_age = case_when(TRUE ~ mean(age_years))) |>\n  mutate(chamber = fct_recode(chamber,\n                              SENATE = \"Senate\",\n                              HOUSE = \"House\",))\n\nggplot(data = congress_ages,\n       mapping = aes(y = mean_age,\n                     x = congress_year,\n                     color = fct_rev(chamber),\n                     )) +\n  geom_step(size = 1) + \n  guides() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       caption = \"Data is based on all members who served in either the Senate or House in each Congress, which is notated\\nby the year in which it was seated. Any member who served in bothchambers in the same Congress was\\nassigned to the chamber in which they cast more votes.\\n FiveThirtyEight\\nSOURCES: BIOGRAPHICAL DIRECTORY OF THE U.S. CONGRESS, U.S. HOUSE OF REPRESENTATIVES,\\nU.S. SENATE, UNITEDSTATES GITHUB, VOTEVIEW.COM\",\n       y = NULL,\n       x = NULL\n  ) +\n  scale_color_manual(values=c(\"#6b4ddd\",\"#29ae53\")) +\n  theme_minimal(base_size = 13) + \n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(face = \"bold\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.title=element_blank(),\n        legend.position = \"top\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#voter-demographics",
    "href": "content/lectures/12-cs01-eda.html#voter-demographics",
    "title": "12-cs01-eda",
    "section": "Voter Demographics",
    "text": "Voter Demographics\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggpubr)\n\nvoters = read.csv(\"data/nonvoters_data.csv\")\n\n\n#creating subplots\nrace = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Race\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())+\n    guides(fill = guide_legend(override.aes = list(shape = 16, key_width = 1, key_height = 1))) #trying to change the shape of the legend key\n\nincome = ggplot(data = voters, mapping = aes(y = factor(income_cat), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Income\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\nage = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Age\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\neducation = ggplot(data = voters, mapping = aes(y = factor(educ), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Education\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\npartyID = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"party ID\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\n#combining subplots\nria = ggarrange(race, income, age, ncol = 3, common.legend = TRUE, legend = \"top\") +\n  theme(plot.margin = margin(0.5, 0,-2, -1,\"cm\")) \n\nep = ggarrange(education, partyID) +\n  theme(plot.margin = margin(2, 4, -2.5, 1, \"cm\"))\n\nvoterplot = ggarrange(ria, ep, nrow = 3) \n\n\n#titles\nvoterplot = annotate_figure(annotate_figure(voterplot, \n  top = text_grob(\"Demographic information of survey respondants, by voting history\")),\n  top = text_grob(\"Those who always vote and those who sometimes vote aren't that different\", face = \"bold\")\n)\n\n\nvoterplot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#fouls",
    "href": "content/lectures/12-cs01-eda.html#fouls",
    "title": "12-cs01-eda",
    "section": "Fouls",
    "text": "Fouls\n\nOriginalPlot ICode IPlot (Banso)Code (Banso)\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_csv_file <- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/foul-balls/foul-balls.csv\"\nfoulballs <- read.csv(url(raw_csv_file))\n\nfoulballs <- foulballs |>\n  mutate(over90 = case_when(\n    exit_velocity < 90 ~ \"no\",\n    exit_velocity >= 90 ~ \"yes\"\n  ))\n\nggplot(foulballs,\n       aes (y = used_zone)) +\n  geom_bar(aes(fill = over90),\n           position = position_stack(reverse = TRUE),\n           show.legend = FALSE) +\n  scale_fill_manual(labels = c(\"< 90 mph\", \"≥ 90 mph\", \"Unknown exit velocity\"),\n                    values = c(\"#97c16d\", \"#63abb0\"), na.value = \"#d3d3d3\") +\n  scale_y_discrete(limits = rev(unique(foulballs$used_zone)))+\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"#ececec\"),\n        panel.background = element_rect(fill = \"white\"),\n        axis.text.x = element_text(color = \"#9e9e9e\"),\n        plot.caption = element_text(color = \"#b5b5b5\"),\n        axis.line.y = element_line(colour = \"#343434\"),\n        axis.title.y = element_text(angle = 0, vjust = 0.12, color = \"#343434\", size = 9),\n        axis.ticks = element_blank(),\n        legend.title = element_blank()) +\n  labs(title = \"The hardest-hit fouls seem to land in unprotected areas\",\n       subtitle = str_wrap(\"Foul balls by the stadium zone they landed in and their exit velocity, among 906 fouls hit this season in the most foul-heavy day at the 10 MLB stadiums that produced the most fouls as of June 5\", 85),\n       x = \"\", y = \"Zone\",\n       caption = \"SOURCE: BASEBALL SAVANT\") +\n  annotate(\"text\", x = 75, y = 1, label = \"< 90 mph\", col = \"white\") +\n  annotate(\"text\", x = 140, y = 3.3, label = \"≥ 90 mph\", col = \"#63abb0\") +\n  annotate(\"text\", x = 215, y = 1, label = \"Unknown exit velocity\", col = \"white\")\n\n\n\n\n\n\n\n\n\n\n\nball_data <- foulballs |>\n  mutate(category = case_when(\n    exit_velocity < 90 ~ \"<90\",\n    exit_velocity >= 90 ~ \">=90\",\n    is.na(exit_velocity) ~ \"Unknown\"\n                           ))\nmy_plot <- ggplot(ball_data, aes(y = fct_rev(as.character(used_zone)), fill = category)) +\n  geom_bar(position = position_stack(reverse = TRUE)) +\n  labs(title = \"The hardest-hit fouls seem \\nto land in unprotected areas\",\n       subtitle = \"The 906 foul balls hit this season from \\nthe most foul-heavy day at each of the \\n10 MLB stadiums that produced the \\nthe most fouls as of June 5, by zone where \\nthe balls landed and their exit velocities\",\n       y = \"Zone\", x = NULL) +\n  scale_fill_manual(values = c(\"Unknown\" = \"#DEDEDE\", \"<90\" = \"#9ECE88\", \">=90\" = \"#17AFAD\")) +\n  scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250),position = \"top\") +\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major.x = element_line(size = 0.5, linetype=\"solid\", color=\"#CECECE\"),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(color = \"black\", face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(size = 10, margin = margin(b = 20))\n)\n\nmy_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#colin-nba-raptor-ratings",
    "href": "content/lectures/12-cs01-eda.html#colin-nba-raptor-ratings",
    "title": "12-cs01-eda",
    "section": "Colin: NBA RAPTOR ratings",
    "text": "Colin: NBA RAPTOR ratings\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\nraptor <- read.csv(\"data/latest_RAPTOR_by_player.csv\")\n\nraptor_rounded <- raptor |> \n  filter(mp >= 1137) |>\n  mutate(across(where(is.numeric), round, 1))\n\nraptor_plot <- raptor_rounded |>\n  ggplot(aes(x=raptor_offense, y = raptor_defense)) + \n  \n  # Annotations\n    # The colored rectangles for the 1 & 3 quadrants\n    annotate(\"rect\", xmin=0, xmax=10, ymin=0, ymax=10, fill = '#c5ecee', alpha = .85) + \n    annotate(\"rect\", xmin=-10, xmax=0, ymin=-10, ymax=0, fill = '#fecada', alpha = .85) +\n    \n    # The 3rd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -7.8, ymax = -6.8, fill = '#fd97b6') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = -7.4, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = -9, label = \" -  defense\") +\n    \n    # The 1st quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 8.4, ymax = 9.4, fill = '#8cdadf') +\n    annotate(\"text\", x = 7.5, y = 8.8, label = \" +  offense\") +\n    annotate(\"text\", x = 7.55, y = 7.1, label = \" +  defense\") +\n  \n    # The 2nd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 8.4, ymax = 9.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = 8.8, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = 7.1, label = \" +  defense\") +\n  \n    # The 4th quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -7.8, ymax = -6.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = 7.55, y = -7.4, label = \" +  offense\") +\n    annotate(\"text\", x = 7.5, y = -9, label = \" -  defense\") +\n  \n  geom_point(shape= 21, colour = \"black\", fill = \"white\", size = 4) + \n  \n  labs(\n    x = \"Offensive RAPTOR rating\", \n    y = \"Defensive RAPTOR rating\",\n    title = paste0('Nikola Jokic is the Best NBA Player Based on Overall RAPTOR Rating',\n            '<br>',\n            '<sup>',\n            'An Analytical Approach to the 2022 - 2023 NBA Season','</sup>')\n    ) + \n    \n  # Theme settings\n  theme_light() + \n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(), \n    panel.grid.major = element_line(color = \"#cdcdcd\", linewidth = 0.5), \n    plot.margin = margin(l = 100, r = 100, b = 20, t = 10),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(color = \"#cdcdcd\", size = 12),\n    axis.title.x = element_text(margin = margin(t=3)),\n    axis.title.y = element_text(margin = margin(r=3)),\n    ) + \n  coord_fixed(ratio = 1) \n\n# interactive\n# ggplotly(raptor_plot, text=player_name, hoverinfo='text') \n\nraptor_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#biden-approval",
    "href": "content/lectures/12-cs01-eda.html#biden-approval",
    "title": "12-cs01-eda",
    "section": "Biden Approval",
    "text": "Biden Approval\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n#getting only data from beginning of COVID to Jan 19, 2021 and getting rid of the \"all\" category in the party column \nyear_20 <- \n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year <= 2020)|>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n\nyear_21 <-\n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year == 2021 & end_month == 1 & end_day <= 19) |>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n       #  if (end_year == 2021 & end_month == 1 & end_day <= 19))\n\nselect_COVID_approval <-\n  full_join(year_20, year_21)\n\nggplot(select_COVID_approval, aes(x = as.Date(end_date),\n                           y = approve,\n                           color = party)) +\n  geom_smooth(aes(group = party), span = 0.05, se = FALSE) + \n  scale_color_manual(values = c(\"D\" = \"#008fd5\", \n                               \"R\" = \"#ff2700\",\n                               \"I\" = \"#a55330\")) +\n  \n  #Important Dates\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-02-29\"), y = 95, label=\"First U.S. \\n Death \\n Reported\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-05-20\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-05-20\"), y = 95, label=\"U.S. Deaths \\n surpass \\n 100,000\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype=3) + #Trump Diagnosed with COVID-19\n    annotate(\"text\", x= as.Date(\"2020-10-02\"), y = 95, label=\"Trump \\n Diagnosed \\n with \\n COVID-19\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-11-07\"), y = 95, label=\"Biden \\n declared \\n election \\n winner\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2021-01-19\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2021-01-19\"), y = 95, label=\"Biden \\n sworn \\n into \\n office\", size=3, color=\"black\") +\n  \n  labs(title = \"Approval of Trump’s response varies widely by party\",\n       subtitle = \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s \\n handling of the coronavirus outbreak\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5 ), \n        plot.subtitle = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.minor = element_line(colour=\"gray\")\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#recap",
    "href": "content/lectures/12-cs01-eda.html#recap",
    "title": "12-cs01-eda",
    "section": "Recap",
    "text": "Recap\n\nCan you explain/describe the plots generated in the context of these data?\nCan you generate EDA plots of your own for these data\nCan you understand/work through the more complicated code provided (even if you couldn’t have come up with it on your own)"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#qa",
    "href": "content/lectures/02-dplyr-slides.html#qa",
    "title": "02-dplyr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: How many people are in a group for case studies and final project?\nA: 3-4\n\n\nQ: How to turn in assignments\nA: We’ll discuss this today!\n\n\nQ: Why don’t we have uniform keyboard-shortcut (like run code, new cell) for both R and Python and other coding environment?\nA: Lack of communication? Preferences of developers? I think we’ll get there…\n\n\nQ: Wasn’t clear about the ‘single quotes’ vs “double quotes” thing\nA: When creating a string, or any time you need to use quotes in R, single and double quotes are interchangeable. R doesn’t care which you use. However, your code will be stylistically better if you consistently use one.\n\n\nQ: What are useful libraries that we can use to analyze data?\nA: We’ll be discussing lots, but the tidyverse packages (the first of which we’ll discuss is dplyr) is a great place to start. There are also different packages for basically every statistical analysis out there\n\n\nQ: Is there any way to prevent coercion? / I was wondering if you can types cast a variable when concatenation\nA: Yup. You can explicitly state as._____() when creating a variable (i.e. as.character()) and when reading in data you can specify. You’ll find that R does a pretty good job at guessing, but we can always fix to what we want after the fact.\n\n\nQ: What is the difference between mylist[1] and mylist[[1]]? It looked like class(mylist[1]) returned list and class(mylist[[1]]) returned the class of the element.\nA: Double brackets returns the element directly. Single bracket (for lists) always returns a list.\n\n\nQ: I’m curious about how to handle dataframes in R\nA: Excellent - we’ll start this discussion today and continue throughout the quarter!"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#course-announcements",
    "href": "content/lectures/02-dplyr-slides.html#course-announcements",
    "title": "02-dplyr",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 01 due tomorrow (Friday; 11:59 PM)\nStudent survey open until next Thursday\nHW01 and Lab02 will both be released Monday\nLecture Participation survey “due” after class"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#suggested-reading",
    "href": "content/lectures/02-dplyr-slides.html#suggested-reading",
    "title": "02-dplyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#agenda",
    "href": "content/lectures/02-dplyr-slides.html#agenda",
    "title": "02-dplyr",
    "section": "Agenda",
    "text": "Agenda\n\ndplyr\n\nphilosophy\npipes\ncommon operations"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#philosophy",
    "href": "content/lectures/02-dplyr-slides.html#philosophy",
    "title": "02-dplyr",
    "section": "Philosophy",
    "text": "Philosophy\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#the-pipe-in-baser",
    "href": "content/lectures/02-dplyr-slides.html#the-pipe-in-baser",
    "title": "02-dplyr",
    "section": "The pipe in baseR",
    "text": "The pipe in baseR\n\n\n\n\n|> should be read as “and then”\nfor example “Wake up |> brush teeth” would be read as “wake up and then brush teeth”"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#where-does-the-name-come-from",
    "href": "content/lectures/02-dplyr-slides.html#where-does-the-name-come-from",
    "title": "02-dplyr",
    "section": "Where does the name come from?",
    "text": "Where does the name come from?\nThe pipe operator was first implemented in the package magrittr.\n\n\n\n\n\n\n\nYou will see this frequently in code online. It’s equivalent to |>."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#review-how-does-a-pipe-work",
    "href": "content/lectures/02-dplyr-slides.html#review-how-does-a-pipe-work",
    "title": "02-dplyr",
    "section": "Review: How does a pipe work?",
    "text": "Review: How does a pipe work?\n\nYou can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.\n\n\n\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"campus\"))\n\n\n\n\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") |>\n  start_car() |>\n  drive(to = \"campus\") |>\n  park()"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#nc-dot-fatal-crashes-in-north-carolina",
    "href": "content/lectures/02-dplyr-slides.html#nc-dot-fatal-crashes-in-north-carolina",
    "title": "02-dplyr",
    "section": "NC DOT Fatal Crashes in North Carolina",
    "text": "NC DOT Fatal Crashes in North Carolina\nFrom OpenDurham’s Data Portal\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#variables",
    "href": "content/lectures/02-dplyr-slides.html#variables",
    "title": "02-dplyr",
    "section": "Variables",
    "text": "Variables\nView the names of variables via\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#viewing-your-data",
    "href": "content/lectures/02-dplyr-slides.html#viewing-your-data",
    "title": "02-dplyr",
    "section": "Viewing your data",
    "text": "Viewing your data\n\nIn the Environment, click on the name of the data frame to view it in the data viewer (or use the View function)\nUse the glimpse function to take a peek\n\n\nglimpse(bike)\n\nRows: 5,716\nColumns: 54\n$ FID        <dbl> 18, 29, 33, 35, 49, 53, 56, 60, 63, 66, 72, 75, 82, 84, 85,…\n$ OBJECTID   <dbl> 19, 30, 34, 36, 50, 54, 57, 61, 64, 67, 73, 76, 83, 85, 86,…\n$ AmbulanceR <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", …\n$ BikeAge_Gr <chr> NA, \"50-59\", NA, \"16-19\", NA, \"50-59\", \"16-19\", \"40-49\", \"1…\n$ Bike_Age   <dbl> 6, 51, 10, 17, 6, 52, 18, 40, 6, 7, 45, 30, 17, 20, 14, 15,…\n$ Bike_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bike_Dir   <chr> \"Not Applicable\", \"With Traffic\", \"With Traffic\", NA, \"Faci…\n$ Bike_Injur <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"Injury\", \"B: E…\n$ Bike_Pos   <chr> \"Driveway / Alley\", \"Travel Lane\", \"Travel Lane\", \"Travel L…\n$ Bike_Race  <chr> \"Black\", \"Black\", \"Black\", \"White\", \"Black\", \"White\", \"Blac…\n$ Bike_Sex   <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ City       <chr> \"Durham\", \"Greenville\", \"Farmville\", \"Charlotte\", \"Charlott…\n$ County     <chr> \"Durham\", \"Pitt\", \"Pitt\", \"Mecklenburg\", \"Mecklenburg\", \"Du…\n$ CrashAlcoh <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ CrashDay   <chr> \"01-01-06\", \"01-01-02\", \"01-01-07\", \"01-01-05\", NA, NA, NA,…\n$ Crash_Date <date> 2007-01-06, 2007-01-09, 2007-01-14, 2007-01-12, 2007-01-15…\n$ Crash_Grp  <chr> \"Bicyclist Failed to Yield - Midblock\", \"Crossing Paths - O…\n$ Crash_Hour <dbl> 13, 23, 16, 19, 12, 20, 19, 14, 16, 0, 17, 18, 14, 17, 19, …\n$ Crash_Loc  <chr> \"Non-Intersection\", \"Intersection-Related\", \"Intersection\",…\n$ Crash_Mont <chr> NA, NA, NA, NA, NA, \"01-04-01\", \"01-04-01\", NA, \"01-02-01\",…\n$ Crash_Time <dttm> 0001-01-01 13:17:58, 0001-01-01 23:08:58, 0001-01-01 16:44…\n$ Crash_Type <chr> \"Bicyclist Ride Out - Residential Driveway\", \"Crossing Path…\n$ Crash_Ty_1 <dbl> 353311, 211180, 111144, 119139, 112114, 311231, 119144, 132…\n$ Crash_Year <dbl> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n$ Crsh_Sevri <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"O: No Injury\",…\n$ Developmen <chr> \"Residential\", \"Commercial\", \"Residential\", \"Residential\", …\n$ DrvrAge_Gr <chr> \"60-69\", \"30-39\", \"50-59\", \"30-39\", NA, \"20-24\", \"40-49\", N…\n$ Drvr_Age   <dbl> 66, 34, 52, 33, NA, 20, 40, NA, 17, 51, NA, 64, 50, 66, 30,…\n$ Drvr_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"Missing\", \"No\", \"No\", \"Missing\", \"…\n$ Drvr_EstSp <chr> \"11-15 mph\", \"0-5 mph\", \"21-25 mph\", \"46-50 mph\", \"16-20 mp…\n$ Drvr_Injur <chr> \"O: No Injury\", \"O: No Injury\", \"O: No Injury\", \"O: No Inju…\n$ Drvr_Race  <chr> \"Black\", \"Black\", \"White\", \"White\", \"/Missing\", \"White\", \"B…\n$ Drvr_Sex   <chr> \"Male\", \"Male\", \"Female\", \"Female\", NA, \"Female\", \"Male\", N…\n$ Drvr_VehTy <chr> \"Pickup\", \"Passenger Car\", \"Passenger Car\", \"Sport Utility\"…\n$ ExcsSpdInd <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Hit_Run    <chr> \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n$ Light_Cond <chr> \"Daylight\", \"Dark - Lighted Roadway\", \"Daylight\", \"Dark - R…\n$ Locality   <chr> \"Mixed (30% To 70% Developed)\", \"Urban (>70% Developed)\", \"…\n$ Num_Lanes  <chr> \"2 lanes\", \"5 lanes\", \"2 lanes\", \"4 lanes\", \"2 lanes\", \"4 l…\n$ Num_Units  <dbl> 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Rd_Charact <chr> \"Straight - Level\", \"Straight - Level\", \"Straight - Level\",…\n$ Rd_Class   <chr> \"Local Street\", \"Local Street\", \"Local Street\", \"NC Route\",…\n$ Rd_Conditi <chr> \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr…\n$ Rd_Config  <chr> \"Two-Way, Not Divided\", \"Two-Way, Divided, Unprotected Medi…\n$ Rd_Defects <chr> \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ Rd_Feature <chr> \"No Special Feature\", \"Four-Way Intersection\", \"Four-Way In…\n$ Rd_Surface <chr> \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smoo…\n$ Region     <chr> \"Piedmont\", \"Coastal\", \"Coastal\", \"Piedmont\", \"Piedmont\", \"…\n$ Rural_Urba <chr> \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Urban\", \"Urban\", \"Urba…\n$ Speed_Limi <chr> \"20 - 25  MPH\", \"40 - 45  MPH\", \"30 - 35  MPH\", \"40 - 45  M…\n$ Traff_Cntr <chr> \"No Control Present\", \"Stop And Go Signal\", \"Stop Sign\", \"S…\n$ Weather    <chr> \"Clear\", \"Clear\", \"Clear\", \"Cloudy\", \"Clear\", \"Clear\", \"Cle…\n$ Workzone_I <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Location   <chr> \"36.002743, -78.8785\", \"35.612984, -77.39265\", \"35.595676, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#a-grammar-of-data-manipulation",
    "href": "content/lectures/02-dplyr-slides.html#a-grammar-of-data-manipulation",
    "title": "02-dplyr",
    "section": "A Grammar of Data Manipulation",
    "text": "A Grammar of Data Manipulation\ndplyr is based on the concepts of functions as verbs that manipulate data frames.\nSingle data frame functions / verbs:\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#dplyr-rules-for-functions",
    "href": "content/lectures/02-dplyr-slides.html#dplyr-rules-for-functions",
    "title": "02-dplyr",
    "section": "dplyr rules for functions",
    "text": "dplyr rules for functions\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDo not modify in place\nPerformance via lazy evaluation"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter-rows-with-filter",
    "href": "content/lectures/02-dplyr-slides.html#filter-rows-with-filter",
    "title": "02-dplyr",
    "section": "Filter rows with filter",
    "text": "Filter rows with filter\n\nSelect a subset of rows in a data frame.\nEasily filter for many conditions at once."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter",
    "href": "content/lectures/02-dplyr-slides.html#filter",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County\n\nbike |>\n  filter(County == \"Durham\")\n\n# A tibble: 253 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1    18       19 No         <NA>              6 No         Not Appl… C: Possib…\n 2    53       54 Yes        50-59            52 No         With Tra… A: Disabl…\n 3    56       57 Yes        16-19            18 No         <NA>      C: Possib…\n 4   209      210 No         16-19            16 No         Facing T… C: Possib…\n 5   228      229 Yes        40-49            40 No         With Tra… B: Eviden…\n 6   620      621 Yes        50-59            55 No         With Tra… B: Eviden…\n 7   667      668 Yes        60-69            61 No         Not Appl… B: Eviden…\n 8   458      459 Yes        60-69            62 No         With Tra… B: Eviden…\n 9   576      577 No         40-49            49 No         With Tra… C: Possib…\n10   618      619 No         20-24            23 No         With Tra… C: Possib…\n# ℹ 243 more rows\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter-1",
    "href": "content/lectures/02-dplyr-slides.html#filter-1",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County where biker was < 10 yrs old\n\nbike |>\n  filter(County == \"Durham\", Bike_Age < 10)\n\n# A tibble: 20 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1    18       19 No         <NA>              6 No         Not Appl… C: Possib…\n 2    47       48 No         10-Jun            9 No         Not Appl… O: No Inj…\n 3   124      125 Yes        10-Jun            8 No         With Tra… C: Possib…\n 4   531      532 Yes        10-Jun            7 No         With Tra… C: Possib…\n 5   704      705 Yes        10-Jun            9 No         Not Appl… C: Possib…\n 6    42       43 No         10-Jun            8 No         With Tra… O: No Inj…\n 7   392      393 Yes        0-5               2 No         Not Appl… B: Eviden…\n 8   941      942 No         10-Jun            9 No         With Tra… C: Possib…\n 9   436      437 Yes        10-Jun            6 No         Not Appl… O: No Inj…\n10   160      161 Yes        10-Jun            7 No         With Tra… C: Possib…\n11   273      274 Yes        10-Jun            7 No         Facing T… C: Possib…\n12    78       79 Yes        10-Jun            7 No         With Tra… C: Possib…\n13   422      423 No         10-Jun            9 No         Not Appl… O: No Inj…\n14   570      571 No         <NA>              0 Missing    Not Appl… Injury    \n15   683      684 Yes        10-Jun            8 No         Not Appl… C: Possib…\n16    62       63 Yes        10-Jun            7 No         With Tra… C: Possib…\n17   248      249 No         0-5               4 No         Not Appl… O: No Inj…\n18   306      307 Yes        10-Jun            8 No         With Tra… C: Possib…\n19   231      232 Yes        10-Jun            8 No         With Tra… C: Possib…\n20   361      362 Yes        10-Jun            9 No         With Tra… B: Eviden…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#aside-real-data-is-messy",
    "href": "content/lectures/02-dplyr-slides.html#aside-real-data-is-messy",
    "title": "02-dplyr",
    "section": "Aside: real data is messy!",
    "text": "Aside: real data is messy!\n   What in the world does a BikeAge_gr of 10-Jun or 15-Nov mean?\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 10-Jun             421\n 3 15-Nov             747\n 4 16-19              605\n 5 20-24              680\n 6 25-29              430\n 7 30-39              658\n 8 40-49              920\n 9 50-59              739\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#careful-data-scientists-clean-up-their-data-first",
    "href": "content/lectures/02-dplyr-slides.html#careful-data-scientists-clean-up-their-data-first",
    "title": "02-dplyr",
    "section": "Careful data scientists clean up their data first!",
    "text": "Careful data scientists clean up their data first!\n\nWe’re going to need to do some text parsing to clean up these data\n\n10-Jun should be 6-10\n15-Nov should be 11-15"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#correct-and-overwrite-mutate",
    "href": "content/lectures/02-dplyr-slides.html#correct-and-overwrite-mutate",
    "title": "02-dplyr",
    "section": "Correct and overwrite mutate",
    "text": "Correct and overwrite mutate\n\nRemember we want to do the following in the BikeAge_Gr variable\n\n10-Jun should be 6-10\n15-Nov should be 11-15\n\n\n\nbike <- bike |>\n  mutate(\n    BikeAge_Gr = case_when(\n      BikeAge_Gr == \"10-Jun\" ~ \"6-10\",\n      BikeAge_Gr == \"15-Nov\" ~ \"11-15\",\n      TRUE                   ~ BikeAge_Gr     # everything else\n    )\n  )\n\n\nNote that we’re overwriting existing data and columns, so be careful!\n\nBut remember, it’s easy to revert if you make a mistake since we didn’t touch the raw data, we can always reload it and start over"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#check-before-you-move-on",
    "href": "content/lectures/02-dplyr-slides.html#check-before-you-move-on",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr count\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#mutate-to-add-new-variables",
    "href": "content/lectures/02-dplyr-slides.html#mutate-to-add-new-variables",
    "title": "02-dplyr",
    "section": "mutate to add new variables",
    "text": "mutate to add new variables\n   How is the new alcohol variable determined?\n\nbike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#save-when-you-mutate",
    "href": "content/lectures/02-dplyr-slides.html#save-when-you-mutate",
    "title": "02-dplyr",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame.\n\nbike <- bike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#transmute-to-create-a-new-dataset",
    "href": "content/lectures/02-dplyr-slides.html#transmute-to-create-a-new-dataset",
    "title": "02-dplyr",
    "section": "transmute to create a new dataset",
    "text": "transmute to create a new dataset\nYou’ll use this much less often than mutate but when you need it, you need it.\n\nbike |> \n  transmute(ID = paste(FID, OBJECTID, sep = \"-\"))\n\n# A tibble: 5,716 × 1\n   ID   \n   <chr>\n 1 18-19\n 2 29-30\n 3 33-34\n 4 35-36\n 5 49-50\n 6 53-54\n 7 56-57\n 8 60-61\n 9 63-64\n10 66-67\n# ℹ 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#mutate-vs.-transmute",
    "href": "content/lectures/02-dplyr-slides.html#mutate-vs.-transmute",
    "title": "02-dplyr",
    "section": "mutate vs. transmute",
    "text": "mutate vs. transmute\n\nmutate adds new and keeps original\ntransmute adds new; drops existing"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#your-turn",
    "href": "content/lectures/02-dplyr-slides.html#your-turn",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nHow many accidents in our dataset required an ambulance ride (AmbulanceR) and had the Crash_Type “Bicyclist Lost Control - Mechanical Problems”?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers",
    "href": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nFirst five\n\nbike |>\n  slice(1:5)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>      <chr>     \n1    18       19 No         <NA>              6 No         Not Appli… C: Possib…\n2    29       30 Yes        50-59            51 No         With Traf… C: Possib…\n3    33       34 No         <NA>             10 No         With Traf… Injury    \n4    35       36 Yes        16-19            17 No         <NA>       B: Eviden…\n5    49       50 No         <NA>              6 No         Facing Tr… O: No Inj…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers-1",
    "href": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers-1",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nLast five\n\nlast_row <- nrow(bike)\nbike |>\n  slice((last_row - 4):last_row)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>      <chr>     \n1   460      461 Yes        6-10              7 No         Not Appli… C: Possib…\n2   474      475 Yes        50-59            50 No         With Traf… B: Eviden…\n3   479      480 Yes        16-19            16 No         Not Appli… C: Possib…\n4   487      488 No         40-49            47 Yes        With Traf… C: Possib…\n5   488      489 Yes        30-39            35 No         Facing Tr… C: Possib…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#select-to-keep-only-the-variables-you-mention",
    "href": "content/lectures/02-dplyr-slides.html#select-to-keep-only-the-variables-you-mention",
    "title": "02-dplyr",
    "section": "select to keep only the variables you mention",
    "text": "select to keep only the variables you mention\n\nbike |>\n  select(Crash_Loc, Hit_Run) |>\n  table()\n\n                      Hit_Run\nCrash_Loc                No  Yes\n  Intersection         2223  275\n  Intersection-Related  252   42\n  Location                3    7\n  Non-Intersection     2213  462\n  Non-Roadway           205   30"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#or-select-to-exclude-variables",
    "href": "content/lectures/02-dplyr-slides.html#or-select-to-exclude-variables",
    "title": "02-dplyr",
    "section": "or select to exclude variables",
    "text": "or select to exclude variables\n\nbike |>\n  select(-OBJECTID)\n\n# A tibble: 5,716 × 53\n     FID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur Bike_Pos\n   <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>      <chr>   \n 1    18 No         <NA>              6 No         Not Appl… C: Possib… Drivewa…\n 2    29 Yes        50-59            51 No         With Tra… C: Possib… Travel …\n 3    33 No         <NA>             10 No         With Tra… Injury     Travel …\n 4    35 Yes        16-19            17 No         <NA>      B: Eviden… Travel …\n 5    49 No         <NA>              6 No         Facing T… O: No Inj… Travel …\n 6    53 Yes        50-59            52 No         With Tra… A: Disabl… Travel …\n 7    56 Yes        16-19            18 No         <NA>      C: Possib… Travel …\n 8    60 No         40-49            40 No         Facing T… B: Eviden… Sidewal…\n 9    63 Yes        6-10              6 No         Facing T… B: Eviden… Travel …\n10    66 Yes        6-10              7 No         <NA>      B: Eviden… Non-Roa…\n# ℹ 5,706 more rows\n# ℹ 45 more variables: Bike_Race <chr>, Bike_Sex <chr>, City <chr>,\n#   County <chr>, CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>,\n#   Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>,\n#   Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>,\n#   Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>,\n#   Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#or-select-a-range-of-variables",
    "href": "content/lectures/02-dplyr-slides.html#or-select-a-range-of-variables",
    "title": "02-dplyr",
    "section": "or select a range of variables",
    "text": "or select a range of variables\n\nbike |>\n  select(OBJECTID:Bike_Injur)\n\n# A tibble: 5,716 × 7\n   OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir       Bike_Injur \n      <dbl> <chr>      <chr>         <dbl> <chr>      <chr>          <chr>      \n 1       19 No         <NA>              6 No         Not Applicable C: Possibl…\n 2       30 Yes        50-59            51 No         With Traffic   C: Possibl…\n 3       34 No         <NA>             10 No         With Traffic   Injury     \n 4       36 Yes        16-19            17 No         <NA>           B: Evident…\n 5       50 No         <NA>              6 No         Facing Traffic O: No Inju…\n 6       54 Yes        50-59            52 No         With Traffic   A: Disabli…\n 7       57 Yes        16-19            18 No         <NA>           C: Possibl…\n 8       61 No         40-49            40 No         Facing Traffic B: Evident…\n 9       64 Yes        6-10              6 No         Facing Traffic B: Evident…\n10       67 Yes        6-10              7 No         <NA>           B: Evident…\n# ℹ 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#pull-to-extract-a-column-as-a-vector",
    "href": "content/lectures/02-dplyr-slides.html#pull-to-extract-a-column-as-a-vector",
    "title": "02-dplyr",
    "section": "pull to extract a column as a vector",
    "text": "pull to extract a column as a vector\n\nbike |>\n  slice(1:6) |>\n  pull(Location)\n\n[1] \"36.002743, -78.8785\"  \"35.612984, -77.39265\" \"35.595676, -77.59074\"\n[4] \"35.076767, -80.7728\"  \"35.19999, -80.75713\"  \"35.966644, -78.96749\"\n\n\n\nbike |>\n  slice(1:6) |>\n  select(Location)\n\n# A tibble: 6 × 1\n  Location            \n  <chr>               \n1 36.002743, -78.8785 \n2 35.612984, -77.39265\n3 35.595676, -77.59074\n4 35.076767, -80.7728 \n5 35.19999, -80.75713 \n6 35.966644, -78.96749"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#the-two-pulls-in-your-lives",
    "href": "content/lectures/02-dplyr-slides.html#the-two-pulls-in-your-lives",
    "title": "02-dplyr",
    "section": "The two pulls in your lives",
    "text": "The two pulls in your lives\n\n\n\n\n\n\n\n\nDon’t get pull happy when wrangling data! Only extract out variables if you truly need to, otherwise keep in data frame.\nBut always ⬇️ Pull before starting your work when collaborating on GitHub."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#rename-specific-columns",
    "href": "content/lectures/02-dplyr-slides.html#rename-specific-columns",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\nUseful for correcting typos, and renaming to make variable names shorter and/or more informative\n\nOriginal names:\n\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#rename-specific-columns-1",
    "href": "content/lectures/02-dplyr-slides.html#rename-specific-columns-1",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\n\nRename Speed_Limi to Speed_Limit:\n\n\nbike <- bike |>\n  rename(Speed_Limit = Speed_Limi)"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#check-before-you-move-on-1",
    "href": "content/lectures/02-dplyr-slides.html#check-before-you-move-on-1",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nnames(bike)\n\n [1] \"FID\"         \"OBJECTID\"    \"AmbulanceR\"  \"BikeAge_Gr\"  \"Bike_Age\"   \n [6] \"Bike_Alc_D\"  \"Bike_Dir\"    \"Bike_Injur\"  \"Bike_Pos\"    \"Bike_Race\"  \n[11] \"Bike_Sex\"    \"City\"        \"County\"      \"CrashAlcoh\"  \"CrashDay\"   \n[16] \"Crash_Date\"  \"Crash_Grp\"   \"Crash_Hour\"  \"Crash_Loc\"   \"Crash_Mont\" \n[21] \"Crash_Time\"  \"Crash_Type\"  \"Crash_Ty_1\"  \"Crash_Year\"  \"Crsh_Sevri\" \n[26] \"Developmen\"  \"DrvrAge_Gr\"  \"Drvr_Age\"    \"Drvr_Alc_D\"  \"Drvr_EstSp\" \n[31] \"Drvr_Injur\"  \"Drvr_Race\"   \"Drvr_Sex\"    \"Drvr_VehTy\"  \"ExcsSpdInd\" \n[36] \"Hit_Run\"     \"Light_Cond\"  \"Locality\"    \"Num_Lanes\"   \"Num_Units\"  \n[41] \"Rd_Charact\"  \"Rd_Class\"    \"Rd_Conditi\"  \"Rd_Config\"   \"Rd_Defects\" \n[46] \"Rd_Feature\"  \"Rd_Surface\"  \"Region\"      \"Rural_Urba\"  \"Speed_Limit\"\n[51] \"Traff_Cntr\"  \"Weather\"     \"Workzone_I\"  \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#your-turn-1",
    "href": "content/lectures/02-dplyr-slides.html#your-turn-1",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nYour boss in Cumberland County gets overwhelmed by data easily, but he wants some data from you. He wants all bike accidents from his County, but he only wants to know the road’s speed limit, the age of the biker, and to know if alcohol was involved. If you have time, mine as well make the column names very clear to your boss while you’re at it…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#summarize-to-reduce-variables-to-values",
    "href": "content/lectures/02-dplyr-slides.html#summarize-to-reduce-variables-to-values",
    "title": "02-dplyr",
    "section": "summarize to reduce variables to values",
    "text": "summarize to reduce variables to values\nThe values are summarized in a data frame\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 11-15              747\n 3 16-19              605\n 4 20-24              680\n 5 25-29              430\n 6 30-39              658\n 7 40-49              920\n 8 50-59              739\n 9 6-10               421\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#and-arrange-to-order-rows",
    "href": "content/lectures/02-dplyr-slides.html#and-arrange-to-order-rows",
    "title": "02-dplyr",
    "section": "and arrange to order rows",
    "text": "and arrange to order rows\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n()) |>\n  arrange(desc(crash_count))\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 40-49              920\n 2 11-15              747\n 3 50-59              739\n 4 20-24              680\n 5 30-39              658\n 6 16-19              605\n 7 25-29              430\n 8 6-10               421\n 9 60-69              274\n10 <NA>               112\n11 0-5                 60\n12 70+                 58\n13 70                  12"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#count-to-group-by-then-count",
    "href": "content/lectures/02-dplyr-slides.html#count-to-group-by-then-count",
    "title": "02-dplyr",
    "section": "count to group by then count",
    "text": "count to group by then count\n\nbike |>\n  count(BikeAge_Gr)\n\n# A tibble: 13 × 2\n   BikeAge_Gr     n\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112\n\n\n   If you wanted to arrange these in ascending order what would you add to the pipe?"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#select-rows-with-sample_n-or-sample_frac",
    "href": "content/lectures/02-dplyr-slides.html#select-rows-with-sample_n-or-sample_frac",
    "title": "02-dplyr",
    "section": "Select rows with sample_n or sample_frac",
    "text": "Select rows with sample_n or sample_frac\n\nsample_n: randomly sample 5 observations\n\n\nbike_n5 <- bike |>\n  sample_n(5, replace = FALSE)\n\ndim(bike_n5)\n\n[1]  5 54\n\n\n\nsample_frac: randomly sample 20% of observations\n\n\nbike_perc20 <- bike |>\n  sample_frac(0.2, replace = FALSE)\n\ndim(bike_perc20)\n\n[1] 1143   54"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#distinct-to-filter-for-unique-rows",
    "href": "content/lectures/02-dplyr-slides.html#distinct-to-filter-for-unique-rows",
    "title": "02-dplyr",
    "section": "distinct to filter for unique rows",
    "text": "distinct to filter for unique rows\n\nbike |> \n  select(County, City) |> \n  distinct() |> \n  arrange(County, City)\n\n# A tibble: 360 × 2\n   County    City              \n   <chr>     <chr>             \n 1 Alamance  Alamance          \n 2 Alamance  Burlington        \n 3 Alamance  Elon College      \n 4 Alamance  Gibsonville       \n 5 Alamance  Graham            \n 6 Alamance  Green Level       \n 7 Alamance  Mebane            \n 8 Alamance  None - Rural Crash\n 9 Alexander None - Rural Crash\n10 Alleghany None - Rural Crash\n# ℹ 350 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#distinct-has-a-.keep_all-parameter",
    "href": "content/lectures/02-dplyr-slides.html#distinct-has-a-.keep_all-parameter",
    "title": "02-dplyr",
    "section": "distinct has a .keep_all parameter",
    "text": "distinct has a .keep_all parameter\n\nbike |> \n  distinct(County, City, .keep_all = TRUE) |> \n  arrange(County, City)\n\n# A tibble: 360 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1   524      525 Yes        11-15            12 No         <NA>      B: Eviden…\n 2    84       85 Yes        20-24            20 No         With Tra… B: Eviden…\n 3   571      572 Yes        16-19            16 No         Not Appl… B: Eviden…\n 4   509      510 Yes        40-49            43 Yes        With Tra… K: Killed \n 5   855      856 Yes        30-39            30 No         With Tra… A: Disabl…\n 6     5        6 Yes        40-49            44 Yes        With Tra… C: Possib…\n 7   163      164 Yes        30-39            35 No         Not Appl… C: Possib…\n 8    96       97 Yes        30-39            36 No         With Tra… C: Possib…\n 9    46       47 Yes        50-59            53 No         With Tra… B: Eviden…\n10   485      486 Yes        60-69            62 No         With Tra… C: Possib…\n# ℹ 350 more rows\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#factors-1",
    "href": "content/lectures/02-dplyr-slides.html#factors-1",
    "title": "02-dplyr",
    "section": "Factors",
    "text": "Factors\nFactor objects are how R stores data for categorical variables (fixed numbers of discrete values).\n\n(x = factor(c(\"BS\", \"MS\", \"PhD\", \"MS\")))\n\n[1] BS  MS  PhD MS \nLevels: BS MS PhD\n\n\n\nglimpse(x)\n\n Factor w/ 3 levels \"BS\",\"MS\",\"PhD\": 1 2 3 2\n\n\n\ntypeof(x)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#returning-to-cat-lovers",
    "href": "content/lectures/02-dplyr-slides.html#returning-to-cat-lovers",
    "title": "02-dplyr",
    "section": "Returning to: Cat lovers",
    "text": "Returning to: Cat lovers\nReading in the cat-lovers data…\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#read-data-in-as-character-strings",
    "href": "content/lectures/02-dplyr-slides.html#read-data-in-as-character-strings",
    "title": "02-dplyr",
    "section": "Read data in as character strings",
    "text": "Read data in as character strings\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#but-coerce-when-plotting",
    "href": "content/lectures/02-dplyr-slides.html#but-coerce-when-plotting",
    "title": "02-dplyr",
    "section": "But coerce when plotting",
    "text": "But coerce when plotting\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#use-forcats-to-manipulate-factors",
    "href": "content/lectures/02-dplyr-slides.html#use-forcats-to-manipulate-factors",
    "title": "02-dplyr",
    "section": "Use forcats to manipulate factors",
    "text": "Use forcats to manipulate factors\n\ncat_lovers <- cat_lovers |>\n  mutate(handedness = fct_relevel(handedness, \n                                  \"right\", \"left\", \"ambidextrous\"))\n\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#forcats-functionality",
    "href": "content/lectures/02-dplyr-slides.html#forcats-functionality",
    "title": "02-dplyr",
    "section": "forcats functionality ",
    "text": "forcats functionality \n\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values. Historically, factors were much easier to work with than character vectors, so many base R functions automatically convert character vectors to factors.\nfactors are still useful when you have true categorical data, and when you want to override the ordering of character vectors to improve display. The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors.\n\n\n\nSource: forcats.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr.html",
    "href": "content/lectures/02-dplyr.html",
    "title": "02-dplyr",
    "section": "",
    "text": "Q: How many people are in a group for case studies and final project?\nA: 3-4\n\n\nQ: How to turn in assignments\nA: We’ll discuss this today!\n\n\nQ: Why don’t we have uniform keyboard-shortcut (like run code, new cell) for both R and Python and other coding environment?\nA: Lack of communication? Preferences of developers? I think we’ll get there…\n\n\nQ: Wasn’t clear about the ‘single quotes’ vs “double quotes” thing\nA: When creating a string, or any time you need to use quotes in R, single and double quotes are interchangeable. R doesn’t care which you use. However, your code will be stylistically better if you consistently use one.\n\n\nQ: What are useful libraries that we can use to analyze data?\nA: We’ll be discussing lots, but the tidyverse packages (the first of which we’ll discuss is dplyr) is a great place to start. There are also different packages for basically every statistical analysis out there\n\n\nQ: Is there any way to prevent coercion? / I was wondering if you can types cast a variable when concatenation\nA: Yup. You can explicitly state as._____() when creating a variable (i.e. as.character()) and when reading in data you can specify. You’ll find that R does a pretty good job at guessing, but we can always fix to what we want after the fact.\n\n\nQ: What is the difference between mylist[1] and mylist[[1]]? It looked like class(mylist[1]) returned list and class(mylist[[1]]) returned the class of the element.\nA: Double brackets returns the element directly. Single bracket (for lists) always returns a list.\n\n\nQ: I’m curious about how to handle dataframes in R\nA: Excellent - we’ll start this discussion today and continue throughout the quarter!\n\n\n\n\nDue Dates:\n\nLab 01 due tomorrow (Friday; 11:59 PM)\nStudent survey open until next Thursday\nHW01 and Lab02 will both be released Monday\nLecture Participation survey “due” after class\n\n\n\n\nR4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors\n\n\n\n\n\ndplyr\n\nphilosophy\npipes\ncommon operations\n\n\n\n\n\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "href": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "title": "02-dplyr",
    "section": "The pipe in baseR",
    "text": "The pipe in baseR\n\n\n\n\n|> should be read as “and then”\nfor example “Wake up |> brush teeth” would be read as “wake up and then brush teeth”"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "href": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "title": "02-dplyr",
    "section": "Where does the name come from?",
    "text": "Where does the name come from?\nThe pipe operator was first implemented in the package magrittr.\n\n\n\n\n\n\n\nYou will see this frequently in code online. It’s equivalent to |>."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "href": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "title": "02-dplyr",
    "section": "Review: How does a pipe work?",
    "text": "Review: How does a pipe work?\n\nYou can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.\n\n\n\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"campus\"))\n\n\n\n\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") |>\n  start_car() |>\n  drive(to = \"campus\") |>\n  park()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "href": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "title": "02-dplyr",
    "section": "NC DOT Fatal Crashes in North Carolina",
    "text": "NC DOT Fatal Crashes in North Carolina\nFrom OpenDurham’s Data Portal\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#variables",
    "href": "content/lectures/02-dplyr.html#variables",
    "title": "02-dplyr",
    "section": "Variables",
    "text": "Variables\nView the names of variables via\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#viewing-your-data",
    "href": "content/lectures/02-dplyr.html#viewing-your-data",
    "title": "02-dplyr",
    "section": "Viewing your data",
    "text": "Viewing your data\n\nIn the Environment, click on the name of the data frame to view it in the data viewer (or use the View function)\nUse the glimpse function to take a peek\n\n\nglimpse(bike)\n\nRows: 5,716\nColumns: 54\n$ FID        <dbl> 18, 29, 33, 35, 49, 53, 56, 60, 63, 66, 72, 75, 82, 84, 85,…\n$ OBJECTID   <dbl> 19, 30, 34, 36, 50, 54, 57, 61, 64, 67, 73, 76, 83, 85, 86,…\n$ AmbulanceR <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", …\n$ BikeAge_Gr <chr> NA, \"50-59\", NA, \"16-19\", NA, \"50-59\", \"16-19\", \"40-49\", \"1…\n$ Bike_Age   <dbl> 6, 51, 10, 17, 6, 52, 18, 40, 6, 7, 45, 30, 17, 20, 14, 15,…\n$ Bike_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bike_Dir   <chr> \"Not Applicable\", \"With Traffic\", \"With Traffic\", NA, \"Faci…\n$ Bike_Injur <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"Injury\", \"B: E…\n$ Bike_Pos   <chr> \"Driveway / Alley\", \"Travel Lane\", \"Travel Lane\", \"Travel L…\n$ Bike_Race  <chr> \"Black\", \"Black\", \"Black\", \"White\", \"Black\", \"White\", \"Blac…\n$ Bike_Sex   <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ City       <chr> \"Durham\", \"Greenville\", \"Farmville\", \"Charlotte\", \"Charlott…\n$ County     <chr> \"Durham\", \"Pitt\", \"Pitt\", \"Mecklenburg\", \"Mecklenburg\", \"Du…\n$ CrashAlcoh <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ CrashDay   <chr> \"01-01-06\", \"01-01-02\", \"01-01-07\", \"01-01-05\", NA, NA, NA,…\n$ Crash_Date <date> 2007-01-06, 2007-01-09, 2007-01-14, 2007-01-12, 2007-01-15…\n$ Crash_Grp  <chr> \"Bicyclist Failed to Yield - Midblock\", \"Crossing Paths - O…\n$ Crash_Hour <dbl> 13, 23, 16, 19, 12, 20, 19, 14, 16, 0, 17, 18, 14, 17, 19, …\n$ Crash_Loc  <chr> \"Non-Intersection\", \"Intersection-Related\", \"Intersection\",…\n$ Crash_Mont <chr> NA, NA, NA, NA, NA, \"01-04-01\", \"01-04-01\", NA, \"01-02-01\",…\n$ Crash_Time <dttm> 0001-01-01 13:17:58, 0001-01-01 23:08:58, 0001-01-01 16:44…\n$ Crash_Type <chr> \"Bicyclist Ride Out - Residential Driveway\", \"Crossing Path…\n$ Crash_Ty_1 <dbl> 353311, 211180, 111144, 119139, 112114, 311231, 119144, 132…\n$ Crash_Year <dbl> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n$ Crsh_Sevri <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"O: No Injury\",…\n$ Developmen <chr> \"Residential\", \"Commercial\", \"Residential\", \"Residential\", …\n$ DrvrAge_Gr <chr> \"60-69\", \"30-39\", \"50-59\", \"30-39\", NA, \"20-24\", \"40-49\", N…\n$ Drvr_Age   <dbl> 66, 34, 52, 33, NA, 20, 40, NA, 17, 51, NA, 64, 50, 66, 30,…\n$ Drvr_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"Missing\", \"No\", \"No\", \"Missing\", \"…\n$ Drvr_EstSp <chr> \"11-15 mph\", \"0-5 mph\", \"21-25 mph\", \"46-50 mph\", \"16-20 mp…\n$ Drvr_Injur <chr> \"O: No Injury\", \"O: No Injury\", \"O: No Injury\", \"O: No Inju…\n$ Drvr_Race  <chr> \"Black\", \"Black\", \"White\", \"White\", \"/Missing\", \"White\", \"B…\n$ Drvr_Sex   <chr> \"Male\", \"Male\", \"Female\", \"Female\", NA, \"Female\", \"Male\", N…\n$ Drvr_VehTy <chr> \"Pickup\", \"Passenger Car\", \"Passenger Car\", \"Sport Utility\"…\n$ ExcsSpdInd <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Hit_Run    <chr> \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n$ Light_Cond <chr> \"Daylight\", \"Dark - Lighted Roadway\", \"Daylight\", \"Dark - R…\n$ Locality   <chr> \"Mixed (30% To 70% Developed)\", \"Urban (>70% Developed)\", \"…\n$ Num_Lanes  <chr> \"2 lanes\", \"5 lanes\", \"2 lanes\", \"4 lanes\", \"2 lanes\", \"4 l…\n$ Num_Units  <dbl> 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Rd_Charact <chr> \"Straight - Level\", \"Straight - Level\", \"Straight - Level\",…\n$ Rd_Class   <chr> \"Local Street\", \"Local Street\", \"Local Street\", \"NC Route\",…\n$ Rd_Conditi <chr> \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr…\n$ Rd_Config  <chr> \"Two-Way, Not Divided\", \"Two-Way, Divided, Unprotected Medi…\n$ Rd_Defects <chr> \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ Rd_Feature <chr> \"No Special Feature\", \"Four-Way Intersection\", \"Four-Way In…\n$ Rd_Surface <chr> \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smoo…\n$ Region     <chr> \"Piedmont\", \"Coastal\", \"Coastal\", \"Piedmont\", \"Piedmont\", \"…\n$ Rural_Urba <chr> \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Urban\", \"Urban\", \"Urba…\n$ Speed_Limi <chr> \"20 - 25  MPH\", \"40 - 45  MPH\", \"30 - 35  MPH\", \"40 - 45  M…\n$ Traff_Cntr <chr> \"No Control Present\", \"Stop And Go Signal\", \"Stop Sign\", \"S…\n$ Weather    <chr> \"Clear\", \"Clear\", \"Clear\", \"Cloudy\", \"Clear\", \"Clear\", \"Cle…\n$ Workzone_I <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Location   <chr> \"36.002743, -78.8785\", \"35.612984, -77.39265\", \"35.595676, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "href": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "title": "02-dplyr",
    "section": "A Grammar of Data Manipulation",
    "text": "A Grammar of Data Manipulation\ndplyr is based on the concepts of functions as verbs that manipulate data frames.\nSingle data frame functions / verbs:\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "href": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "title": "02-dplyr",
    "section": "dplyr rules for functions",
    "text": "dplyr rules for functions\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDo not modify in place\nPerformance via lazy evaluation"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "href": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "title": "02-dplyr",
    "section": "Filter rows with filter",
    "text": "Filter rows with filter\n\nSelect a subset of rows in a data frame.\nEasily filter for many conditions at once."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter",
    "href": "content/lectures/02-dplyr.html#filter",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County\n\nbike |>\n  filter(County == \"Durham\")\n\n# A tibble: 253 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1    18       19 No         <NA>              6 No         Not Appl… C: Possib…\n 2    53       54 Yes        50-59            52 No         With Tra… A: Disabl…\n 3    56       57 Yes        16-19            18 No         <NA>      C: Possib…\n 4   209      210 No         16-19            16 No         Facing T… C: Possib…\n 5   228      229 Yes        40-49            40 No         With Tra… B: Eviden…\n 6   620      621 Yes        50-59            55 No         With Tra… B: Eviden…\n 7   667      668 Yes        60-69            61 No         Not Appl… B: Eviden…\n 8   458      459 Yes        60-69            62 No         With Tra… B: Eviden…\n 9   576      577 No         40-49            49 No         With Tra… C: Possib…\n10   618      619 No         20-24            23 No         With Tra… C: Possib…\n# ℹ 243 more rows\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-1",
    "href": "content/lectures/02-dplyr.html#filter-1",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County where biker was < 10 yrs old\n\nbike |>\n  filter(County == \"Durham\", Bike_Age < 10)\n\n# A tibble: 20 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1    18       19 No         <NA>              6 No         Not Appl… C: Possib…\n 2    47       48 No         10-Jun            9 No         Not Appl… O: No Inj…\n 3   124      125 Yes        10-Jun            8 No         With Tra… C: Possib…\n 4   531      532 Yes        10-Jun            7 No         With Tra… C: Possib…\n 5   704      705 Yes        10-Jun            9 No         Not Appl… C: Possib…\n 6    42       43 No         10-Jun            8 No         With Tra… O: No Inj…\n 7   392      393 Yes        0-5               2 No         Not Appl… B: Eviden…\n 8   941      942 No         10-Jun            9 No         With Tra… C: Possib…\n 9   436      437 Yes        10-Jun            6 No         Not Appl… O: No Inj…\n10   160      161 Yes        10-Jun            7 No         With Tra… C: Possib…\n11   273      274 Yes        10-Jun            7 No         Facing T… C: Possib…\n12    78       79 Yes        10-Jun            7 No         With Tra… C: Possib…\n13   422      423 No         10-Jun            9 No         Not Appl… O: No Inj…\n14   570      571 No         <NA>              0 Missing    Not Appl… Injury    \n15   683      684 Yes        10-Jun            8 No         Not Appl… C: Possib…\n16    62       63 Yes        10-Jun            7 No         With Tra… C: Possib…\n17   248      249 No         0-5               4 No         Not Appl… O: No Inj…\n18   306      307 Yes        10-Jun            8 No         With Tra… C: Possib…\n19   231      232 Yes        10-Jun            8 No         With Tra… C: Possib…\n20   361      362 Yes        10-Jun            9 No         With Tra… B: Eviden…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "href": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "title": "02-dplyr",
    "section": "Aside: real data is messy!",
    "text": "Aside: real data is messy!\n   What in the world does a BikeAge_gr of 10-Jun or 15-Nov mean?\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 10-Jun             421\n 3 15-Nov             747\n 4 16-19              605\n 5 20-24              680\n 6 25-29              430\n 7 30-39              658\n 8 40-49              920\n 9 50-59              739\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "href": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "title": "02-dplyr",
    "section": "Careful data scientists clean up their data first!",
    "text": "Careful data scientists clean up their data first!\n\nWe’re going to need to do some text parsing to clean up these data\n\n10-Jun should be 6-10\n15-Nov should be 11-15"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "href": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "title": "02-dplyr",
    "section": "Correct and overwrite mutate",
    "text": "Correct and overwrite mutate\n\nRemember we want to do the following in the BikeAge_Gr variable\n\n10-Jun should be 6-10\n15-Nov should be 11-15\n\n\n\nbike <- bike |>\n  mutate(\n    BikeAge_Gr = case_when(\n      BikeAge_Gr == \"10-Jun\" ~ \"6-10\",\n      BikeAge_Gr == \"15-Nov\" ~ \"11-15\",\n      TRUE                   ~ BikeAge_Gr     # everything else\n    )\n  )\n\n\nNote that we’re overwriting existing data and columns, so be careful!\n\nBut remember, it’s easy to revert if you make a mistake since we didn’t touch the raw data, we can always reload it and start over"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr count\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "href": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "title": "02-dplyr",
    "section": "mutate to add new variables",
    "text": "mutate to add new variables\n   How is the new alcohol variable determined?\n\nbike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "href": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "title": "02-dplyr",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame.\n\nbike <- bike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "href": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "title": "02-dplyr",
    "section": "transmute to create a new dataset",
    "text": "transmute to create a new dataset\nYou’ll use this much less often than mutate but when you need it, you need it.\n\nbike |> \n  transmute(ID = paste(FID, OBJECTID, sep = \"-\"))\n\n# A tibble: 5,716 × 1\n   ID   \n   <chr>\n 1 18-19\n 2 29-30\n 3 33-34\n 4 35-36\n 5 49-50\n 6 53-54\n 7 56-57\n 8 60-61\n 9 63-64\n10 66-67\n# ℹ 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "href": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "title": "02-dplyr",
    "section": "mutate vs. transmute",
    "text": "mutate vs. transmute\n\nmutate adds new and keeps original\ntransmute adds new; drops existing"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn",
    "href": "content/lectures/02-dplyr.html#your-turn",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nHow many accidents in our dataset required an ambulance ride (AmbulanceR) and had the Crash_Type “Bicyclist Lost Control - Mechanical Problems”?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nFirst five\n\nbike |>\n  slice(1:5)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>      <chr>     \n1    18       19 No         <NA>              6 No         Not Appli… C: Possib…\n2    29       30 Yes        50-59            51 No         With Traf… C: Possib…\n3    33       34 No         <NA>             10 No         With Traf… Injury    \n4    35       36 Yes        16-19            17 No         <NA>       B: Eviden…\n5    49       50 No         <NA>              6 No         Facing Tr… O: No Inj…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nLast five\n\nlast_row <- nrow(bike)\nbike |>\n  slice((last_row - 4):last_row)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>      <chr>     \n1   460      461 Yes        6-10              7 No         Not Appli… C: Possib…\n2   474      475 Yes        50-59            50 No         With Traf… B: Eviden…\n3   479      480 Yes        16-19            16 No         Not Appli… C: Possib…\n4   487      488 No         40-49            47 Yes        With Traf… C: Possib…\n5   488      489 Yes        30-39            35 No         Facing Tr… C: Possib…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "href": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "title": "02-dplyr",
    "section": "select to keep only the variables you mention",
    "text": "select to keep only the variables you mention\n\nbike |>\n  select(Crash_Loc, Hit_Run) |>\n  table()\n\n                      Hit_Run\nCrash_Loc                No  Yes\n  Intersection         2223  275\n  Intersection-Related  252   42\n  Location                3    7\n  Non-Intersection     2213  462\n  Non-Roadway           205   30"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "href": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "title": "02-dplyr",
    "section": "or select to exclude variables",
    "text": "or select to exclude variables\n\nbike |>\n  select(-OBJECTID)\n\n# A tibble: 5,716 × 53\n     FID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur Bike_Pos\n   <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>      <chr>   \n 1    18 No         <NA>              6 No         Not Appl… C: Possib… Drivewa…\n 2    29 Yes        50-59            51 No         With Tra… C: Possib… Travel …\n 3    33 No         <NA>             10 No         With Tra… Injury     Travel …\n 4    35 Yes        16-19            17 No         <NA>      B: Eviden… Travel …\n 5    49 No         <NA>              6 No         Facing T… O: No Inj… Travel …\n 6    53 Yes        50-59            52 No         With Tra… A: Disabl… Travel …\n 7    56 Yes        16-19            18 No         <NA>      C: Possib… Travel …\n 8    60 No         40-49            40 No         Facing T… B: Eviden… Sidewal…\n 9    63 Yes        6-10              6 No         Facing T… B: Eviden… Travel …\n10    66 Yes        6-10              7 No         <NA>      B: Eviden… Non-Roa…\n# ℹ 5,706 more rows\n# ℹ 45 more variables: Bike_Race <chr>, Bike_Sex <chr>, City <chr>,\n#   County <chr>, CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>,\n#   Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>,\n#   Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>,\n#   Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>,\n#   Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "href": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "title": "02-dplyr",
    "section": "or select a range of variables",
    "text": "or select a range of variables\n\nbike |>\n  select(OBJECTID:Bike_Injur)\n\n# A tibble: 5,716 × 7\n   OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir       Bike_Injur \n      <dbl> <chr>      <chr>         <dbl> <chr>      <chr>          <chr>      \n 1       19 No         <NA>              6 No         Not Applicable C: Possibl…\n 2       30 Yes        50-59            51 No         With Traffic   C: Possibl…\n 3       34 No         <NA>             10 No         With Traffic   Injury     \n 4       36 Yes        16-19            17 No         <NA>           B: Evident…\n 5       50 No         <NA>              6 No         Facing Traffic O: No Inju…\n 6       54 Yes        50-59            52 No         With Traffic   A: Disabli…\n 7       57 Yes        16-19            18 No         <NA>           C: Possibl…\n 8       61 No         40-49            40 No         Facing Traffic B: Evident…\n 9       64 Yes        6-10              6 No         Facing Traffic B: Evident…\n10       67 Yes        6-10              7 No         <NA>           B: Evident…\n# ℹ 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "href": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "title": "02-dplyr",
    "section": "pull to extract a column as a vector",
    "text": "pull to extract a column as a vector\n\nbike |>\n  slice(1:6) |>\n  pull(Location)\n\n[1] \"36.002743, -78.8785\"  \"35.612984, -77.39265\" \"35.595676, -77.59074\"\n[4] \"35.076767, -80.7728\"  \"35.19999, -80.75713\"  \"35.966644, -78.96749\"\n\n\n\nbike |>\n  slice(1:6) |>\n  select(Location)\n\n# A tibble: 6 × 1\n  Location            \n  <chr>               \n1 36.002743, -78.8785 \n2 35.612984, -77.39265\n3 35.595676, -77.59074\n4 35.076767, -80.7728 \n5 35.19999, -80.75713 \n6 35.966644, -78.96749"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "href": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "title": "02-dplyr",
    "section": "The two pulls in your lives",
    "text": "The two pulls in your lives\n\n\n\n\n\n\n\n\nDon’t get pull happy when wrangling data! Only extract out variables if you truly need to, otherwise keep in data frame.\nBut always ⬇️ Pull before starting your work when collaborating on GitHub."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\nUseful for correcting typos, and renaming to make variable names shorter and/or more informative\n\nOriginal names:\n\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\n\nRename Speed_Limi to Speed_Limit:\n\n\nbike <- bike |>\n  rename(Speed_Limit = Speed_Limi)"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nnames(bike)\n\n [1] \"FID\"         \"OBJECTID\"    \"AmbulanceR\"  \"BikeAge_Gr\"  \"Bike_Age\"   \n [6] \"Bike_Alc_D\"  \"Bike_Dir\"    \"Bike_Injur\"  \"Bike_Pos\"    \"Bike_Race\"  \n[11] \"Bike_Sex\"    \"City\"        \"County\"      \"CrashAlcoh\"  \"CrashDay\"   \n[16] \"Crash_Date\"  \"Crash_Grp\"   \"Crash_Hour\"  \"Crash_Loc\"   \"Crash_Mont\" \n[21] \"Crash_Time\"  \"Crash_Type\"  \"Crash_Ty_1\"  \"Crash_Year\"  \"Crsh_Sevri\" \n[26] \"Developmen\"  \"DrvrAge_Gr\"  \"Drvr_Age\"    \"Drvr_Alc_D\"  \"Drvr_EstSp\" \n[31] \"Drvr_Injur\"  \"Drvr_Race\"   \"Drvr_Sex\"    \"Drvr_VehTy\"  \"ExcsSpdInd\" \n[36] \"Hit_Run\"     \"Light_Cond\"  \"Locality\"    \"Num_Lanes\"   \"Num_Units\"  \n[41] \"Rd_Charact\"  \"Rd_Class\"    \"Rd_Conditi\"  \"Rd_Config\"   \"Rd_Defects\" \n[46] \"Rd_Feature\"  \"Rd_Surface\"  \"Region\"      \"Rural_Urba\"  \"Speed_Limit\"\n[51] \"Traff_Cntr\"  \"Weather\"     \"Workzone_I\"  \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn-1",
    "href": "content/lectures/02-dplyr.html#your-turn-1",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nYour boss in Cumberland County gets overwhelmed by data easily, but he wants some data from you. He wants all bike accidents from his County, but he only wants to know the road’s speed limit, the age of the biker, and to know if alcohol was involved. If you have time, mine as well make the column names very clear to your boss while you’re at it…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "href": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "title": "02-dplyr",
    "section": "summarize to reduce variables to values",
    "text": "summarize to reduce variables to values\nThe values are summarized in a data frame\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 11-15              747\n 3 16-19              605\n 4 20-24              680\n 5 25-29              430\n 6 30-39              658\n 7 40-49              920\n 8 50-59              739\n 9 6-10               421\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "href": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "title": "02-dplyr",
    "section": "and arrange to order rows",
    "text": "and arrange to order rows\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n()) |>\n  arrange(desc(crash_count))\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 40-49              920\n 2 11-15              747\n 3 50-59              739\n 4 20-24              680\n 5 30-39              658\n 6 16-19              605\n 7 25-29              430\n 8 6-10               421\n 9 60-69              274\n10 <NA>               112\n11 0-5                 60\n12 70+                 58\n13 70                  12"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "href": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "title": "02-dplyr",
    "section": "count to group by then count",
    "text": "count to group by then count\n\nbike |>\n  count(BikeAge_Gr)\n\n# A tibble: 13 × 2\n   BikeAge_Gr     n\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112\n\n\n   If you wanted to arrange these in ascending order what would you add to the pipe?"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "href": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "title": "02-dplyr",
    "section": "Select rows with sample_n or sample_frac",
    "text": "Select rows with sample_n or sample_frac\n\nsample_n: randomly sample 5 observations\n\n\nbike_n5 <- bike |>\n  sample_n(5, replace = FALSE)\n\ndim(bike_n5)\n\n[1]  5 54\n\n\n\nsample_frac: randomly sample 20% of observations\n\n\nbike_perc20 <- bike |>\n  sample_frac(0.2, replace = FALSE)\n\ndim(bike_perc20)\n\n[1] 1143   54"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "href": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "title": "02-dplyr",
    "section": "distinct to filter for unique rows",
    "text": "distinct to filter for unique rows\n\nbike |> \n  select(County, City) |> \n  distinct() |> \n  arrange(County, City)\n\n# A tibble: 360 × 2\n   County    City              \n   <chr>     <chr>             \n 1 Alamance  Alamance          \n 2 Alamance  Burlington        \n 3 Alamance  Elon College      \n 4 Alamance  Gibsonville       \n 5 Alamance  Graham            \n 6 Alamance  Green Level       \n 7 Alamance  Mebane            \n 8 Alamance  None - Rural Crash\n 9 Alexander None - Rural Crash\n10 Alleghany None - Rural Crash\n# ℹ 350 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "href": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "title": "02-dplyr",
    "section": "distinct has a .keep_all parameter",
    "text": "distinct has a .keep_all parameter\n\nbike |> \n  distinct(County, City, .keep_all = TRUE) |> \n  arrange(County, City)\n\n# A tibble: 360 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1   524      525 Yes        11-15            12 No         <NA>      B: Eviden…\n 2    84       85 Yes        20-24            20 No         With Tra… B: Eviden…\n 3   571      572 Yes        16-19            16 No         Not Appl… B: Eviden…\n 4   509      510 Yes        40-49            43 Yes        With Tra… K: Killed \n 5   855      856 Yes        30-39            30 No         With Tra… A: Disabl…\n 6     5        6 Yes        40-49            44 Yes        With Tra… C: Possib…\n 7   163      164 Yes        30-39            35 No         Not Appl… C: Possib…\n 8    96       97 Yes        30-39            36 No         With Tra… C: Possib…\n 9    46       47 Yes        50-59            53 No         With Tra… B: Eviden…\n10   485      486 Yes        60-69            62 No         With Tra… C: Possib…\n# ℹ 350 more rows\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#factors-1",
    "href": "content/lectures/02-dplyr.html#factors-1",
    "title": "02-dplyr",
    "section": "Factors",
    "text": "Factors\nFactor objects are how R stores data for categorical variables (fixed numbers of discrete values).\n\n(x = factor(c(\"BS\", \"MS\", \"PhD\", \"MS\")))\n\n[1] BS  MS  PhD MS \nLevels: BS MS PhD\n\n\n\nglimpse(x)\n\n Factor w/ 3 levels \"BS\",\"MS\",\"PhD\": 1 2 3 2\n\n\n\ntypeof(x)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "href": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "title": "02-dplyr",
    "section": "Returning to: Cat lovers",
    "text": "Returning to: Cat lovers\nReading in the cat-lovers data…\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "href": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "title": "02-dplyr",
    "section": "Read data in as character strings",
    "text": "Read data in as character strings\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "href": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "title": "02-dplyr",
    "section": "But coerce when plotting",
    "text": "But coerce when plotting\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "href": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "title": "02-dplyr",
    "section": "Use forcats to manipulate factors",
    "text": "Use forcats to manipulate factors\n\ncat_lovers <- cat_lovers |>\n  mutate(handedness = fct_relevel(handedness, \n                                  \"right\", \"left\", \"ambidextrous\"))\n\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#forcats-functionality",
    "href": "content/lectures/02-dplyr.html#forcats-functionality",
    "title": "02-dplyr",
    "section": "forcats functionality ",
    "text": "forcats functionality \n\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values. Historically, factors were much easier to work with than character vectors, so many base R functions automatically convert character vectors to factors.\nfactors are still useful when you have true categorical data, and when you want to override the ordering of character vectors to improve display. The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors.\n\n\n\nSource: forcats.tidyverse.org"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#ad-computing-paths",
    "href": "content/lectures/01-intro-to-r-slides.html#ad-computing-paths",
    "title": "01-intro-to-r",
    "section": "[ad] Computing Paths",
    "text": "[ad] Computing Paths\n\nhttps://computingpaths.ucsd.edu/"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#ad-the-basement",
    "href": "content/lectures/01-intro-to-r-slides.html#ad-the-basement",
    "title": "01-intro-to-r",
    "section": "[ad] The Basement",
    "text": "[ad] The Basement"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#ad-calpirg",
    "href": "content/lectures/01-intro-to-r-slides.html#ad-calpirg",
    "title": "01-intro-to-r",
    "section": "[ad] CALPIRG",
    "text": "[ad] CALPIRG\n\nAPPLY NOW: Protect the environment and make social change\nCALPIRG Students is a student organization here that works to protect the environment, make college more affordable, and promote civic engagement. Last Fall we helped nearly 10,000 students register to vote in California and got the UCs to release new policy to phase out single-use plastics to protect our oceans! SIGN UP TO HELP NOW\nNow, we are working to tackle the biggest problem facing our generation - climate change. Coming off another record-setting summer of hot temperatures, it’s clear we need to take strong, swift action to reduce the impacts of climate change. That’s why we are building support from students across the state to call for 100% clean energy UC-wide - for cars, buses, buildings, lights, and more! Fill out this interest form to learn more!\nAs a volunteer or intern with CALPIRG you can:\nWork with the media and help organize events like a Solar-Powered Concert and climate week of action\nIncrease voting accessibility and voter turnout in elections\nBring down the cost of textbooks\nProtect wildlife like whales and sea otters in the Pacific\nAnd more!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#qa",
    "href": "content/lectures/01-intro-to-r-slides.html#qa",
    "title": "01-intro-to-r",
    "section": "Q&A",
    "text": "Q&A\nQ: How are groups formed for the projects?\nA: I form them randomly for the two case studies and students get to choose their own groups for the final project.\nQ: I’m curious about what the workload would be like for the case studies.\nA: We’ll discuss this when the time comes in detail but for now, students are presented with a lot of the starter code for the case studies. You and your group mates have to get teh code running, add explanations, and “extend” the case study, meaning add something meaningful onto what was presented.\nQ: Will this course go into advanced topics in tidyverse?\nA: We will certainly go beyond the basic dplyr verbs and cover multiple packages in the tidyverse, but we won’t be able to cover everything.\nQ: Would it be possible to have access to the third Case Study just to work on our own time?\nA: Yup! I’ll you all in the direction of OpenCaseStudies, which is a resource I’ll be using for 1 of the 2 case studies this quarter, and that we used for all case studies previously.\nQ: How many homework assignments are there? Some slides said 3, some said 4.\nA: Apologies. There are 3. (There were previously 4 but I removed one. Slides have been updated.)\nQ: Could we just sit in for lectures? Can I keep watching lectures?\nA: Yup - this would be fine, so long as everyone enrolled had a seat. And, the podcasts are open to anyone!\nQ: If we decide to do our projects and homework locally, how can we download/install the packages necessary for it?\nA: This is covered in your first lab!\nQ: I’m generally curious on how the language R is used in real world settings after college. What are its specific uses and what are the better ways to learn the language to maximize its utility. Also, how does this language differ from python in the data science realm.\nA: Great question that we’ll cover throughout the course. However, breifly here, R is most heavily used by individuals who do more statistics, who work in biology, psychology or economics, and/or who analyze data regularly.\nQ: What is the advantage of using R over other programming languages to do data science tasks? How much is R used for data science in the real world?\nA: Within the tidyverse and in RStudio, the advantage is the cohesiveness of the tools - once you gain familiarity you can often intuit how to use another tool in the tidyverse. R is used for data science across tons of companies; however, across industries. its use is not as widespread as Python."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#course-announcements",
    "href": "content/lectures/01-intro-to-r-slides.html#course-announcements",
    "title": "01-intro-to-r",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nStudent survey “due” today 11:59 PM\nLab 01 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\nWaitlist (Non)Update: Staff are seeing what options there are. A few people got an email from Kasey Chiang (k4chiang@ucsd.edu) to drop and then enroll. This is legitimate. Follow those instructions."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#agenda",
    "href": "content/lectures/01-intro-to-r-slides.html#agenda",
    "title": "01-intro-to-r",
    "section": "Agenda",
    "text": "Agenda\n\nVariables\nOperators\nData in R\nRMarkdown"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variables-assignment-1",
    "href": "content/lectures/01-intro-to-r-slides.html#variables-assignment-1",
    "title": "01-intro-to-r",
    "section": "Variables & Assignment",
    "text": "Variables & Assignment\nVariables are how we store information so that we can access it later.\n\nVariables are created and stored using the assignment operator <-\n\nfirst_variable <- 3\n\nThe above stores the value 3 in the variable first_variable\n\n\nNote: Other programming languages use = for assignment. R also uses that for assignment, but it is more typical to see <- in R code, so we’ll stick with that.\n\n\nThis means that if we ever want to reference the information stored in that variable later, we can “call” (mean, type in our code) the variable’s name:\n\nfirst_variable\n\n[1] 3"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-type",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-type",
    "title": "01-intro-to-r",
    "section": "Variable Type",
    "text": "Variable Type\n\nEvery variable you create in R will be of a specific type.\n\n\n\nThe type of the variable is determined dynamically on assignment.\n\n\n\n\nDetermining the type of a variable with class():\n\n\nclass(first_variable)\n\n[1] \"numeric\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#basic-variable-types",
    "href": "content/lectures/01-intro-to-r-slides.html#basic-variable-types",
    "title": "01-intro-to-r",
    "section": "Basic Variable Types",
    "text": "Basic Variable Types\n\n\n\n\n\n\n\n\nVariable Type\nExplanation\nExample\n\n\n\n\ncharacter\nstores a string\n\"cogs137\", \"hi!\"\n\n\nnumeric\nstores whole numbers and decimals\n9, 9.29\n\n\ninteger\nspecifies integer\n9L (the L specifies this is an integer)\n\n\nlogical\nBooleans\nTRUE, FALSE\n\n\nlist\nstore multiple elements\nlist(7, \"a\", TRUE)\n\n\n\nNote: There are many more. We’ll get to some but not all in this course."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#logical-character",
    "href": "content/lectures/01-intro-to-r-slides.html#logical-character",
    "title": "01-intro-to-r",
    "section": "logical & character",
    "text": "logical & character\nlogical - Boolean values TRUE and FALSE\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\ncharacter - character strings\n\nclass(\"hello\")\n\n[1] \"character\"\n\nclass('students') # equivalent...but we'll use double quotes!\n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#numeric-double-integer",
    "href": "content/lectures/01-intro-to-r-slides.html#numeric-double-integer",
    "title": "01-intro-to-r",
    "section": "numeric: double & integer",
    "text": "numeric: double & integer\ndouble - floating point numerical values (default numerical type)\n\nclass(1.335)\n\n[1] \"numeric\"\n\nclass(7)\n\n[1] \"numeric\"\n\n\n\ninteger - integer numerical values (indicated with an L)\n\nclass(7L)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#lists",
    "href": "content/lectures/01-intro-to-r-slides.html#lists",
    "title": "01-intro-to-r",
    "section": "lists",
    "text": "lists\nSo far, every variable has been an atomic vector, meaning it only stores a single piece of information.\n\nLists are 1d objects that can contain any combination of R objects\n\n\n\nmylist <- list(\"A\", 7L, TRUE, 18.4)\nmylist\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 18.4\n\n\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#your-turn",
    "href": "content/lectures/01-intro-to-r-slides.html#your-turn",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nDefine variables of each of the following types: character, numeric, integer, logical, list\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#functions",
    "href": "content/lectures/01-intro-to-r-slides.html#functions",
    "title": "01-intro-to-r",
    "section": "Functions",
    "text": "Functions\n\nclass() (and View() & median()) were our first functions…but we’ll show a few more.\n\n\n\nFunctions are (most often) verbs, followed by what they will be applied to in parentheses.\n\n\n\nFunctions are:\n\navailable from base R\navailable from packages you import\ndefined by you\n\n\n\nWe’ll start by getting comfortable with available functions, but in a few days, you’ll learn how to write your own!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#helpful-functions",
    "href": "content/lectures/01-intro-to-r-slides.html#helpful-functions",
    "title": "01-intro-to-r",
    "section": "Helpful Functions",
    "text": "Helpful Functions\n\n\n\nclass() - determine high-level variable type\n\n\nclass(mylist)\n\n[1] \"list\"\n\n\n\nlength()- determine how long an object is\n\n\n# contains 4 elements\nlength(mylist)\n\n[1] 4\n\n\n\n\nstr() - display the structure of an R object\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#coercion",
    "href": "content/lectures/01-intro-to-r-slides.html#coercion",
    "title": "01-intro-to-r",
    "section": "Coercion",
    "text": "Coercion\nR is a dynamically typed language – it will happily convert between the various types without complaint.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"\n\nc(FALSE, 3L)\n\n[1] 0 3\n\nc(1.2, 3L)\n\n[1] 1.2 3.0"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#missing-values",
    "href": "content/lectures/01-intro-to-r-slides.html#missing-values",
    "title": "01-intro-to-r",
    "section": "Missing Values",
    "text": "Missing Values\nR uses NA to represent missing values in its data structures.\n\nclass(NA)\n\n[1] \"logical\"\n\n\n\nOther Special Values\nNaN | Not a number\nInf | Positive infinity\n-Inf | Negative infinity"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#activity",
    "href": "content/lectures/01-intro-to-r-slides.html#activity",
    "title": "01-intro-to-r",
    "section": "Activity",
    "text": "Activity\nWhat is the type of the following vectors? Chat about why they have that type.\n\nc(1, NA+1L, \"C\")\nc(1L / 0, NA)\nc(1:3, 5)\nc(3L, NaN+1L)\nc(NA, TRUE)\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#operators-1",
    "href": "content/lectures/01-intro-to-r-slides.html#operators-1",
    "title": "01-intro-to-r",
    "section": "Operators",
    "text": "Operators\nAt its simplest, R is a calculator. To carry out mathematical operations, R uses operators."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators",
    "href": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\n\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\nx %% y\nmodulus (x mod y) 9%%2 is 1\n\n\nx %/% y\ninteger division 9%/%2 is 4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators-examples",
    "href": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators-examples",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators: Examples",
    "text": "Arithmetic Operators: Examples\n\n7 + 6  \n\n[1] 13\n\n2 - 3\n\n[1] -1\n\n4 * 2\n\n[1] 8\n\n9 / 2\n\n[1] 4.5"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#reminder",
    "href": "content/lectures/01-intro-to-r-slides.html#reminder",
    "title": "01-intro-to-r",
    "section": "Reminder",
    "text": "Reminder\nOutput can be stored to a variable\n\nmy_addition <- 7 + 6\n\n\n\nmy_addition\n\n[1] 13"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#comparison-operators",
    "href": "content/lectures/01-intro-to-r-slides.html#comparison-operators",
    "title": "01-intro-to-r",
    "section": "Comparison Operators",
    "text": "Comparison Operators\nThese operators return a Boolean.\n\n\n\nOperator\nDescription\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#comparison-operators-examples",
    "href": "content/lectures/01-intro-to-r-slides.html#comparison-operators-examples",
    "title": "01-intro-to-r",
    "section": "Comparison Operators: Examples",
    "text": "Comparison Operators: Examples\n\n4 < 12\n\n[1] TRUE\n\n4 >= 3\n\n[1] TRUE\n\n6 == 6\n\n[1] TRUE\n\n7 != 6\n\n[1] TRUE"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#your-turn-1",
    "href": "content/lectures/01-intro-to-r-slides.html#your-turn-1",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nUse arithmetic and comparison operators to store the value 30 in the variable var_30 and TRUE in the variable true_var.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#packages",
    "href": "content/lectures/01-intro-to-r-slides.html#packages",
    "title": "01-intro-to-r",
    "section": "Packages",
    "text": "Packages\n\nPackages are installed with the install.packages function and loaded with the library function, once per session:\n\n\ninstall.packages(\"package_name\")\nlibrary(package_name)\n\n\nIn this course, most packages we’ll use have been installed for you already on datahub, so you will only have to load the package in (using library)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-sets-in-r",
    "href": "content/lectures/01-intro-to-r-slides.html#data-sets-in-r",
    "title": "01-intro-to-r",
    "section": "Data “sets” in R",
    "text": "Data “sets” in R\n\n“set” is in quotation marks because it is not a formal data class\nA tidy data “set” can be one of the following types:\n\ntibble\ndata.frame\n\nWe’ll often work with tibbles:\n\nreadr package (e.g. read_csv function) loads data as a tibble by default\ntibbles are part of the tidyverse, so they work well with other packages we are using\nthey make minimal assumptions about your data, so are less likely to cause hard to track bugs in your code"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-frames",
    "href": "content/lectures/01-intro-to-r-slides.html#data-frames",
    "title": "01-intro-to-r",
    "section": "Data frames",
    "text": "Data frames\n\nA data frame is the most commonly used data structure in R, they are list of equal length vectors (usually atomic, but can be generic). Each vector is treated as a column and elements of the vectors as rows.\nA tibble is a type of data frame that … makes your life (i.e. data analysis) easier.\nMost often a data frame will be constructed by reading in from a file, but we can create them from scratch.\n\n\ndf <- tibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\nclass(df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(df)\n\nRows: 3\nColumns: 2\n$ x <int> 1, 2, 3\n$ y <chr> \"a\", \"b\", \"c\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-frames-cont.",
    "href": "content/lectures/01-intro-to-r-slides.html#data-frames-cont.",
    "title": "01-intro-to-r",
    "section": "Data frames (cont.)",
    "text": "Data frames (cont.)\n\nattributes(df)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n$names\n[1] \"x\" \"y\"\n\n\n\nColumns (variables) in data frames are accessed with $:\n\ndataframe$var_name\n\n\n\n\nclass(df$x)  # access variable type for column\n\n[1] \"integer\"\n\nclass(df$y)  \n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-types",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-types",
    "title": "01-intro-to-r",
    "section": "Variable Types",
    "text": "Variable Types\nData stored in columns can include different kinds of information…which would require a different type (class) of variable to be used in R.\n\n\n\n\nR Data Types:\n\nContinuous: numeric, integer\nDiscrete: factors (we haven’t talked about these yet, but will today!)\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-types-cont.",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-types-cont.",
    "title": "01-intro-to-r",
    "section": "Variable Types (cont.)",
    "text": "Variable Types (cont.)\nSometimes data are non-numeric and store words. Even when that is the case, the data can be conveying different information.\n\n\n\n\nR Data Types:\n\nNominal: character\nOrdinal: factors\nBinary: logical OR numeric OR factors 😱\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#example-cat-lovers",
    "href": "content/lectures/01-intro-to-r-slides.html#example-cat-lovers",
    "title": "01-intro-to-r",
    "section": "Example: Cat lovers",
    "text": "Example: Cat lovers\nA survey asked respondents their name and number of cats. The instructions said to enter the number of cats as a numerical value.\n\n🚨 There is code ahead that we’re not going to discuss in detail today, but we will in coming lectures.\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#the-data",
    "href": "content/lectures/01-intro-to-r-slides.html#the-data",
    "title": "01-intro-to-r",
    "section": "The Data",
    "text": "The Data\n\ncat_lovers |>\n  datatable()"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#the-question",
    "href": "content/lectures/01-intro-to-r-slides.html#the-question",
    "title": "01-intro-to-r",
    "section": "The Question",
    "text": "The Question\nHow many respondents have a below average number of cats?\n\nGiving it a first shot…\n\ncat_lovers |>\n  summarise(mean = mean(number_of_cats))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean = mean(number_of_cats)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n   mean\n  <dbl>\n1    NA\n\n\n\n\n💡 maybe there is missing data in the number_of_cats column!\nOh why will you still not work??!!\n\ncat_lovers |>\n  summarise(mean_cats = mean(number_of_cats, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean_cats = mean(number_of_cats, na.rm = TRUE)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1        NA\n\n\n\n\n💡What is the type of the number_of_cats variable?"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#take-a-breath-and-look-at-your-data",
    "href": "content/lectures/01-intro-to-r-slides.html#take-a-breath-and-look-at-your-data",
    "title": "01-intro-to-r",
    "section": "Take a breath and look at your data",
    "text": "Take a breath and look at your data\n\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#lets-take-another-look",
    "href": "content/lectures/01-intro-to-r-slides.html#lets-take-another-look",
    "title": "01-intro-to-r",
    "section": "Let’s take another look",
    "text": "Let’s take another look"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#sometimes-you-need-to-babysit-your-respondents",
    "href": "content/lectures/01-intro-to-r-slides.html#sometimes-you-need-to-babysit-your-respondents",
    "title": "01-intro-to-r",
    "section": "Sometimes you need to babysit your respondents",
    "text": "Sometimes you need to babysit your respondents\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n    name == \"Ginger Clark\" ~ 2,\n    name == \"Doug Bass\"    ~ 3,\n    TRUE                   ~ as.numeric(number_of_cats))) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `number_of_cats = case_when(...)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 60 × 3\n   name           number_of_cats handedness\n   <chr>                   <dbl> <chr>     \n 1 Bernice Warren              0 left      \n 2 Woodrow Stone               0 left      \n 3 Willie Bass                 1 left      \n 4 Tyrone Estrada              3 left      \n 5 Alex Daniels                3 left      \n 6 Jane Bates                  2 left      \n 7 Latoya Simpson              1 left      \n 8 Darin Woods                 1 left      \n 9 Agnes Cobb                  0 left      \n10 Tabitha Grant               0 left      \n# ℹ 50 more rows"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#always-respect-check-data-types",
    "href": "content/lectures/01-intro-to-r-slides.html#always-respect-check-data-types",
    "title": "01-intro-to-r",
    "section": "Always respect (& check!) data types",
    "text": "Always respect (& check!) data types\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats)) |>\n  summarise(mean_cats = mean(number_of_cats))\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1     0.817"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#now-that-we-know-what-were-doing",
    "href": "content/lectures/01-intro-to-r-slides.html#now-that-we-know-what-were-doing",
    "title": "01-intro-to-r",
    "section": "Now that we know what we’re doing…",
    "text": "Now that we know what we’re doing…\n\ncat_lovers <- cat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats))\n\n… store your data in a variable (here we’re overwriting the old cat_lovers tibble)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#moral-of-the-story",
    "href": "content/lectures/01-intro-to-r-slides.html#moral-of-the-story",
    "title": "01-intro-to-r",
    "section": "Moral of the story",
    "text": "Moral of the story\n\nIf your data does not behave how you expect it to, type coercion upon reading in the data might be the reason.\nGo in and investigate your data, apply the fix, save your data, live happily ever after."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#r-markdown-tour",
    "href": "content/lectures/01-intro-to-r-slides.html#r-markdown-tour",
    "title": "01-intro-to-r",
    "section": "R Markdown: tour",
    "text": "R Markdown: tour\n\n[DEMO]\n\nBefore we move on…\n   What is the Bechdel test?\n\nThe Bechdel test asks whether a work of fiction features at least two women who talk to each other about something other than a man, and there must be two women named characters.\n\n\nConcepts introduced:\n\nKnitting documents\nR Markdown and (some) R syntax"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#github-setup",
    "href": "content/lectures/01-intro-to-r-slides.html#github-setup",
    "title": "01-intro-to-r",
    "section": "GitHub Setup",
    "text": "GitHub Setup\nSee this week’s lab…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#giving-the-demo-a-go",
    "href": "content/lectures/01-intro-to-r-slides.html#giving-the-demo-a-go",
    "title": "01-intro-to-r",
    "section": "Giving the demo a go…",
    "text": "Giving the demo a go…\n\nNavigate to the demo URL (on Canvas)\nAccept the “assignment” (this is NOT graded)\nClone the repo\nEdit the document\nKnit the document\nPush your changes\n\nTry to play around with this after finishing your lab tomorrow!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#recap",
    "href": "content/lectures/01-intro-to-r-slides.html#recap",
    "title": "01-intro-to-r",
    "section": "Recap",
    "text": "Recap\n\nAlways best to think of data as part of a tibble\n\nThis plays nicely with the tidyverse as well\nRows are observations, columns are variables\n\nWhat are the common variable types in R\n\nHow do I create a variable of each type?\nWhen would I use each one?\n\nDo I know how to determine the class/type of a variable?\nCan I explain dynamic typing?\nCan I operate on variables and values using…\n\narithmetic operators?\ncomparison operators?\n\nWhat are dataframes/tibbles? and why are they useful?\nWhat is the difference between installing and loading a package?\nWhat are the components of an R Markdown file?\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html",
    "href": "content/lectures/01-intro-to-r.html",
    "title": "01-intro-to-r",
    "section": "",
    "text": "https://computingpaths.ucsd.edu/\n\n\n\n\n\n\n\n\n\n\nAPPLY NOW: Protect the environment and make social change\nCALPIRG Students is a student organization here that works to protect the environment, make college more affordable, and promote civic engagement. Last Fall we helped nearly 10,000 students register to vote in California and got the UCs to release new policy to phase out single-use plastics to protect our oceans! SIGN UP TO HELP NOW\nNow, we are working to tackle the biggest problem facing our generation - climate change. Coming off another record-setting summer of hot temperatures, it’s clear we need to take strong, swift action to reduce the impacts of climate change. That’s why we are building support from students across the state to call for 100% clean energy UC-wide - for cars, buses, buildings, lights, and more! Fill out this interest form to learn more!\nAs a volunteer or intern with CALPIRG you can:\nWork with the media and help organize events like a Solar-Powered Concert and climate week of action\nIncrease voting accessibility and voter turnout in elections\nBring down the cost of textbooks\nProtect wildlife like whales and sea otters in the Pacific\nAnd more!\n\n\n\n\nQ: How are groups formed for the projects?\nA: I form them randomly for the two case studies and students get to choose their own groups for the final project.\nQ: I’m curious about what the workload would be like for the case studies.\nA: We’ll discuss this when the time comes in detail but for now, students are presented with a lot of the starter code for the case studies. You and your group mates have to get teh code running, add explanations, and “extend” the case study, meaning add something meaningful onto what was presented.\nQ: Will this course go into advanced topics in tidyverse?\nA: We will certainly go beyond the basic dplyr verbs and cover multiple packages in the tidyverse, but we won’t be able to cover everything.\nQ: Would it be possible to have access to the third Case Study just to work on our own time?\nA: Yup! I’ll you all in the direction of OpenCaseStudies, which is a resource I’ll be using for 1 of the 2 case studies this quarter, and that we used for all case studies previously.\nQ: How many homework assignments are there? Some slides said 3, some said 4.\nA: Apologies. There are 3. (There were previously 4 but I removed one. Slides have been updated.)\nQ: Could we just sit in for lectures? Can I keep watching lectures?\nA: Yup - this would be fine, so long as everyone enrolled had a seat. And, the podcasts are open to anyone!\nQ: If we decide to do our projects and homework locally, how can we download/install the packages necessary for it?\nA: This is covered in your first lab!\nQ: I’m generally curious on how the language R is used in real world settings after college. What are its specific uses and what are the better ways to learn the language to maximize its utility. Also, how does this language differ from python in the data science realm.\nA: Great question that we’ll cover throughout the course. However, breifly here, R is most heavily used by individuals who do more statistics, who work in biology, psychology or economics, and/or who analyze data regularly.\nQ: What is the advantage of using R over other programming languages to do data science tasks? How much is R used for data science in the real world?\nA: Within the tidyverse and in RStudio, the advantage is the cohesiveness of the tools - once you gain familiarity you can often intuit how to use another tool in the tidyverse. R is used for data science across tons of companies; however, across industries. its use is not as widespread as Python.\n\n\n\nDue Dates:\n\nStudent survey “due” today 11:59 PM\nLab 01 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\nWaitlist (Non)Update: Staff are seeing what options there are. A few people got an email from Kasey Chiang (k4chiang@ucsd.edu) to drop and then enroll. This is legitimate. Follow those instructions.\n\n\n\n\nVariables\nOperators\nData in R\nRMarkdown"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variables-assignment-1",
    "href": "content/lectures/01-intro-to-r.html#variables-assignment-1",
    "title": "01-intro-to-r",
    "section": "Variables & Assignment",
    "text": "Variables & Assignment\nVariables are how we store information so that we can access it later.\n\nVariables are created and stored using the assignment operator <-\n\nfirst_variable <- 3\n\nThe above stores the value 3 in the variable first_variable\n\n\nNote: Other programming languages use = for assignment. R also uses that for assignment, but it is more typical to see <- in R code, so we’ll stick with that.\n\n\nThis means that if we ever want to reference the information stored in that variable later, we can “call” (mean, type in our code) the variable’s name:\n\nfirst_variable\n\n[1] 3"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-type",
    "href": "content/lectures/01-intro-to-r.html#variable-type",
    "title": "01-intro-to-r",
    "section": "Variable Type",
    "text": "Variable Type\n\nEvery variable you create in R will be of a specific type.\n\n\n\nThe type of the variable is determined dynamically on assignment.\n\n\n\n\nDetermining the type of a variable with class():\n\n\nclass(first_variable)\n\n[1] \"numeric\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#basic-variable-types",
    "href": "content/lectures/01-intro-to-r.html#basic-variable-types",
    "title": "01-intro-to-r",
    "section": "Basic Variable Types",
    "text": "Basic Variable Types\n\n\n\n\n\n\n\n\nVariable Type\nExplanation\nExample\n\n\n\n\ncharacter\nstores a string\n\"cogs137\", \"hi!\"\n\n\nnumeric\nstores whole numbers and decimals\n9, 9.29\n\n\ninteger\nspecifies integer\n9L (the L specifies this is an integer)\n\n\nlogical\nBooleans\nTRUE, FALSE\n\n\nlist\nstore multiple elements\nlist(7, \"a\", TRUE)\n\n\n\nNote: There are many more. We’ll get to some but not all in this course."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#logical-character",
    "href": "content/lectures/01-intro-to-r.html#logical-character",
    "title": "01-intro-to-r",
    "section": "logical & character",
    "text": "logical & character\nlogical - Boolean values TRUE and FALSE\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\ncharacter - character strings\n\nclass(\"hello\")\n\n[1] \"character\"\n\nclass('students') # equivalent...but we'll use double quotes!\n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#numeric-double-integer",
    "href": "content/lectures/01-intro-to-r.html#numeric-double-integer",
    "title": "01-intro-to-r",
    "section": "numeric: double & integer",
    "text": "numeric: double & integer\ndouble - floating point numerical values (default numerical type)\n\nclass(1.335)\n\n[1] \"numeric\"\n\nclass(7)\n\n[1] \"numeric\"\n\n\n\ninteger - integer numerical values (indicated with an L)\n\nclass(7L)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#lists",
    "href": "content/lectures/01-intro-to-r.html#lists",
    "title": "01-intro-to-r",
    "section": "lists",
    "text": "lists\nSo far, every variable has been an atomic vector, meaning it only stores a single piece of information.\n\nLists are 1d objects that can contain any combination of R objects\n\n\n\nmylist <- list(\"A\", 7L, TRUE, 18.4)\nmylist\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 18.4\n\n\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#your-turn",
    "href": "content/lectures/01-intro-to-r.html#your-turn",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nDefine variables of each of the following types: character, numeric, integer, logical, list\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#functions",
    "href": "content/lectures/01-intro-to-r.html#functions",
    "title": "01-intro-to-r",
    "section": "Functions",
    "text": "Functions\n\nclass() (and View() & median()) were our first functions…but we’ll show a few more.\n\n\n\nFunctions are (most often) verbs, followed by what they will be applied to in parentheses.\n\n\n\nFunctions are:\n\navailable from base R\navailable from packages you import\ndefined by you\n\n\n\nWe’ll start by getting comfortable with available functions, but in a few days, you’ll learn how to write your own!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#helpful-functions",
    "href": "content/lectures/01-intro-to-r.html#helpful-functions",
    "title": "01-intro-to-r",
    "section": "Helpful Functions",
    "text": "Helpful Functions\n\n\n\nclass() - determine high-level variable type\n\n\nclass(mylist)\n\n[1] \"list\"\n\n\n\nlength()- determine how long an object is\n\n\n# contains 4 elements\nlength(mylist)\n\n[1] 4\n\n\n\n\nstr() - display the structure of an R object\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#coercion",
    "href": "content/lectures/01-intro-to-r.html#coercion",
    "title": "01-intro-to-r",
    "section": "Coercion",
    "text": "Coercion\nR is a dynamically typed language – it will happily convert between the various types without complaint.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"\n\nc(FALSE, 3L)\n\n[1] 0 3\n\nc(1.2, 3L)\n\n[1] 1.2 3.0"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#missing-values",
    "href": "content/lectures/01-intro-to-r.html#missing-values",
    "title": "01-intro-to-r",
    "section": "Missing Values",
    "text": "Missing Values\nR uses NA to represent missing values in its data structures.\n\nclass(NA)\n\n[1] \"logical\"\n\n\n\n\nOther Special Values\nNaN | Not a number\nInf | Positive infinity\n-Inf | Negative infinity"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#activity",
    "href": "content/lectures/01-intro-to-r.html#activity",
    "title": "01-intro-to-r",
    "section": "Activity",
    "text": "Activity\nWhat is the type of the following vectors? Chat about why they have that type.\n\nc(1, NA+1L, \"C\")\nc(1L / 0, NA)\nc(1:3, 5)\nc(3L, NaN+1L)\nc(NA, TRUE)\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#operators-1",
    "href": "content/lectures/01-intro-to-r.html#operators-1",
    "title": "01-intro-to-r",
    "section": "Operators",
    "text": "Operators\nAt its simplest, R is a calculator. To carry out mathematical operations, R uses operators."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#arithmetic-operators",
    "href": "content/lectures/01-intro-to-r.html#arithmetic-operators",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\n\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\nx %% y\nmodulus (x mod y) 9%%2 is 1\n\n\nx %/% y\ninteger division 9%/%2 is 4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#arithmetic-operators-examples",
    "href": "content/lectures/01-intro-to-r.html#arithmetic-operators-examples",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators: Examples",
    "text": "Arithmetic Operators: Examples\n\n7 + 6  \n\n[1] 13\n\n2 - 3\n\n[1] -1\n\n4 * 2\n\n[1] 8\n\n9 / 2\n\n[1] 4.5"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#reminder",
    "href": "content/lectures/01-intro-to-r.html#reminder",
    "title": "01-intro-to-r",
    "section": "Reminder",
    "text": "Reminder\nOutput can be stored to a variable\n\nmy_addition <- 7 + 6\n\n\n\nmy_addition\n\n[1] 13"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#comparison-operators",
    "href": "content/lectures/01-intro-to-r.html#comparison-operators",
    "title": "01-intro-to-r",
    "section": "Comparison Operators",
    "text": "Comparison Operators\nThese operators return a Boolean.\n\n\n\nOperator\nDescription\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#comparison-operators-examples",
    "href": "content/lectures/01-intro-to-r.html#comparison-operators-examples",
    "title": "01-intro-to-r",
    "section": "Comparison Operators: Examples",
    "text": "Comparison Operators: Examples\n\n4 < 12\n\n[1] TRUE\n\n4 >= 3\n\n[1] TRUE\n\n6 == 6\n\n[1] TRUE\n\n7 != 6\n\n[1] TRUE"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#your-turn-1",
    "href": "content/lectures/01-intro-to-r.html#your-turn-1",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nUse arithmetic and comparison operators to store the value 30 in the variable var_30 and TRUE in the variable true_var.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#packages",
    "href": "content/lectures/01-intro-to-r.html#packages",
    "title": "01-intro-to-r",
    "section": "Packages",
    "text": "Packages\n\nPackages are installed with the install.packages function and loaded with the library function, once per session:\n\n\ninstall.packages(\"package_name\")\nlibrary(package_name)\n\n\nIn this course, most packages we’ll use have been installed for you already on datahub, so you will only have to load the package in (using library)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-sets-in-r",
    "href": "content/lectures/01-intro-to-r.html#data-sets-in-r",
    "title": "01-intro-to-r",
    "section": "Data “sets” in R",
    "text": "Data “sets” in R\n\n“set” is in quotation marks because it is not a formal data class\nA tidy data “set” can be one of the following types:\n\ntibble\ndata.frame\n\nWe’ll often work with tibbles:\n\nreadr package (e.g. read_csv function) loads data as a tibble by default\ntibbles are part of the tidyverse, so they work well with other packages we are using\nthey make minimal assumptions about your data, so are less likely to cause hard to track bugs in your code"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-frames",
    "href": "content/lectures/01-intro-to-r.html#data-frames",
    "title": "01-intro-to-r",
    "section": "Data frames",
    "text": "Data frames\n\nA data frame is the most commonly used data structure in R, they are list of equal length vectors (usually atomic, but can be generic). Each vector is treated as a column and elements of the vectors as rows.\nA tibble is a type of data frame that … makes your life (i.e. data analysis) easier.\nMost often a data frame will be constructed by reading in from a file, but we can create them from scratch.\n\n\ndf <- tibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\nclass(df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(df)\n\nRows: 3\nColumns: 2\n$ x <int> 1, 2, 3\n$ y <chr> \"a\", \"b\", \"c\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-frames-cont.",
    "href": "content/lectures/01-intro-to-r.html#data-frames-cont.",
    "title": "01-intro-to-r",
    "section": "Data frames (cont.)",
    "text": "Data frames (cont.)\n\nattributes(df)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n$names\n[1] \"x\" \"y\"\n\n\n\nColumns (variables) in data frames are accessed with $:\n\ndataframe$var_name\n\n\n\n\nclass(df$x)  # access variable type for column\n\n[1] \"integer\"\n\nclass(df$y)  \n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-types",
    "href": "content/lectures/01-intro-to-r.html#variable-types",
    "title": "01-intro-to-r",
    "section": "Variable Types",
    "text": "Variable Types\nData stored in columns can include different kinds of information…which would require a different type (class) of variable to be used in R.\n\n\n\n\nR Data Types:\n\nContinuous: numeric, integer\nDiscrete: factors (we haven’t talked about these yet, but will today!)\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-types-cont.",
    "href": "content/lectures/01-intro-to-r.html#variable-types-cont.",
    "title": "01-intro-to-r",
    "section": "Variable Types (cont.)",
    "text": "Variable Types (cont.)\nSometimes data are non-numeric and store words. Even when that is the case, the data can be conveying different information.\n\n\n\n\nR Data Types:\n\nNominal: character\nOrdinal: factors\nBinary: logical OR numeric OR factors 😱\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#example-cat-lovers",
    "href": "content/lectures/01-intro-to-r.html#example-cat-lovers",
    "title": "01-intro-to-r",
    "section": "Example: Cat lovers",
    "text": "Example: Cat lovers\nA survey asked respondents their name and number of cats. The instructions said to enter the number of cats as a numerical value.\n\n🚨 There is code ahead that we’re not going to discuss in detail today, but we will in coming lectures.\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#the-data",
    "href": "content/lectures/01-intro-to-r.html#the-data",
    "title": "01-intro-to-r",
    "section": "The Data",
    "text": "The Data\n\ncat_lovers |>\n  datatable()"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#the-question",
    "href": "content/lectures/01-intro-to-r.html#the-question",
    "title": "01-intro-to-r",
    "section": "The Question",
    "text": "The Question\nHow many respondents have a below average number of cats?\n\nGiving it a first shot…\n\ncat_lovers |>\n  summarise(mean = mean(number_of_cats))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean = mean(number_of_cats)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n   mean\n  <dbl>\n1    NA\n\n\n\n\n💡 maybe there is missing data in the number_of_cats column!\nOh why will you still not work??!!\n\ncat_lovers |>\n  summarise(mean_cats = mean(number_of_cats, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean_cats = mean(number_of_cats, na.rm = TRUE)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1        NA\n\n\n\n\n💡What is the type of the number_of_cats variable?"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#take-a-breath-and-look-at-your-data",
    "href": "content/lectures/01-intro-to-r.html#take-a-breath-and-look-at-your-data",
    "title": "01-intro-to-r",
    "section": "Take a breath and look at your data",
    "text": "Take a breath and look at your data\n\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#lets-take-another-look",
    "href": "content/lectures/01-intro-to-r.html#lets-take-another-look",
    "title": "01-intro-to-r",
    "section": "Let’s take another look",
    "text": "Let’s take another look"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#sometimes-you-need-to-babysit-your-respondents",
    "href": "content/lectures/01-intro-to-r.html#sometimes-you-need-to-babysit-your-respondents",
    "title": "01-intro-to-r",
    "section": "Sometimes you need to babysit your respondents",
    "text": "Sometimes you need to babysit your respondents\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n    name == \"Ginger Clark\" ~ 2,\n    name == \"Doug Bass\"    ~ 3,\n    TRUE                   ~ as.numeric(number_of_cats))) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `number_of_cats = case_when(...)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 60 × 3\n   name           number_of_cats handedness\n   <chr>                   <dbl> <chr>     \n 1 Bernice Warren              0 left      \n 2 Woodrow Stone               0 left      \n 3 Willie Bass                 1 left      \n 4 Tyrone Estrada              3 left      \n 5 Alex Daniels                3 left      \n 6 Jane Bates                  2 left      \n 7 Latoya Simpson              1 left      \n 8 Darin Woods                 1 left      \n 9 Agnes Cobb                  0 left      \n10 Tabitha Grant               0 left      \n# ℹ 50 more rows"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#always-respect-check-data-types",
    "href": "content/lectures/01-intro-to-r.html#always-respect-check-data-types",
    "title": "01-intro-to-r",
    "section": "Always respect (& check!) data types",
    "text": "Always respect (& check!) data types\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats)) |>\n  summarise(mean_cats = mean(number_of_cats))\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1     0.817"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#now-that-we-know-what-were-doing",
    "href": "content/lectures/01-intro-to-r.html#now-that-we-know-what-were-doing",
    "title": "01-intro-to-r",
    "section": "Now that we know what we’re doing…",
    "text": "Now that we know what we’re doing…\n\ncat_lovers <- cat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats))\n\n… store your data in a variable (here we’re overwriting the old cat_lovers tibble)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#moral-of-the-story",
    "href": "content/lectures/01-intro-to-r.html#moral-of-the-story",
    "title": "01-intro-to-r",
    "section": "Moral of the story",
    "text": "Moral of the story\n\nIf your data does not behave how you expect it to, type coercion upon reading in the data might be the reason.\nGo in and investigate your data, apply the fix, save your data, live happily ever after."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#r-markdown-tour",
    "href": "content/lectures/01-intro-to-r.html#r-markdown-tour",
    "title": "01-intro-to-r",
    "section": "R Markdown: tour",
    "text": "R Markdown: tour\n\n[DEMO]\n\nBefore we move on…\n   What is the Bechdel test?\n\nThe Bechdel test asks whether a work of fiction features at least two women who talk to each other about something other than a man, and there must be two women named characters.\n\n\nConcepts introduced:\n\nKnitting documents\nR Markdown and (some) R syntax"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#github-setup",
    "href": "content/lectures/01-intro-to-r.html#github-setup",
    "title": "01-intro-to-r",
    "section": "GitHub Setup",
    "text": "GitHub Setup\nSee this week’s lab…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#giving-the-demo-a-go",
    "href": "content/lectures/01-intro-to-r.html#giving-the-demo-a-go",
    "title": "01-intro-to-r",
    "section": "Giving the demo a go…",
    "text": "Giving the demo a go…\n\nNavigate to the demo URL (on Canvas)\nAccept the “assignment” (this is NOT graded)\nClone the repo\nEdit the document\nKnit the document\nPush your changes\n\nTry to play around with this after finishing your lab tomorrow!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#recap",
    "href": "content/lectures/01-intro-to-r.html#recap",
    "title": "01-intro-to-r",
    "section": "Recap",
    "text": "Recap\n\nAlways best to think of data as part of a tibble\n\nThis plays nicely with the tidyverse as well\nRows are observations, columns are variables\n\nWhat are the common variable types in R\n\nHow do I create a variable of each type?\nWhen would I use each one?\n\nDo I know how to determine the class/type of a variable?\nCan I explain dynamic typing?\nCan I operate on variables and values using…\n\narithmetic operators?\ncomparison operators?\n\nWhat are dataframes/tibbles? and why are they useful?\nWhat is the difference between installing and loading a package?\nWhat are the components of an R Markdown file?"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#qa",
    "href": "content/lectures/04-ggplot2-slides.html#qa",
    "title": "04-ggplot2",
    "section": "Q&A",
    "text": "Q&A\n\nQ: What is the difference between pull and select?\nA: select specifies which columns to display in your resulting dataframe. pull extracts the values from a column and stores them in a vector (not a dataframe)\n\n\nQ: I am a bit confused on factors and what levels mean.\nA: Factors store categorical information. The levels of a factor are all the possible unique values in a variable.\n\n\nQ: how similar is R to numpy/which scenarios are each used in the industry?\nA: Basically, anything data science-y you can do in R, you can also do in python. R has linear algebra/working with matrices built directly into its base installation, so no additional package would be need for numpy-like operations. And, dplyr does very similar things to pandas, but with a more readable and consistent syntax overall.\n\n\nQ: would we ever load just dplyr instead of the entire tidyverse package? is there a big difference?\nA: We’ll always just load tidyverse. The difference is that the tidyverse is quite big, so if you ever wanted to just use dplyr functions, you could load just that. This matters more in development where you’re trying to minimize external dependencies and make code run as fast as possible. For our purposes, there’s no real need to only load dplyr\n\n\nQ: I found the demos to be the most confusing part, because it’s very different understanding slides and applying that to actual coding. Personally, I would prefer if the lecture content were put into recordings, or just uploaded earlier so we could learn it on our own, and then have classes be more focused on data science best practices, applications, etc.\nA: I do really like this idea and would love to run this like this in the. I’m curious what y’all think of this and will add a question like this to the post-course survey to get students’ thoughts."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#course-announcements",
    "href": "content/lectures/04-ggplot2-slides.html#course-announcements",
    "title": "04-ggplot2",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 02 due Friday (11:59 PM)\nHW 01 due Monday (11:59 PM)\nLecture Participation survey “due” after class"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#suggested-reading",
    "href": "content/lectures/04-ggplot2-slides.html#suggested-reading",
    "title": "04-ggplot2",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#ggplot2-in-tidyverse",
    "href": "content/lectures/04-ggplot2-slides.html#ggplot2-in-tidyverse",
    "title": "04-ggplot2",
    "section": "ggplot2 \\(\\in\\) tidyverse",
    "text": "ggplot2 \\(\\in\\) tidyverse\n\n\n\n\n\n\n\n\n\nggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#data-palmer-penguins",
    "href": "content/lectures/04-ggplot2-slides.html#data-palmer-penguins",
    "title": "04-ggplot2",
    "section": "Data: Palmer Penguins",
    "text": "Data: Palmer Penguins\nMeasurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#the-data",
    "href": "content/lectures/04-ggplot2-slides.html#the-data",
    "title": "04-ggplot2",
    "section": "The Data",
    "text": "The Data\n\npenguins |>\n  datatable()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#a-plot",
    "href": "content/lectures/04-ggplot2-slides.html#a-plot",
    "title": "04-ggplot2",
    "section": "A Plot",
    "text": "A Plot\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section",
    "href": "content/lectures/04-ggplot2-slides.html#section",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame\n\n\n\nggplot(data = penguins)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-1",
    "href": "content/lectures/04-ggplot2-slides.html#section-1",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-2",
    "href": "content/lectures/04-ggplot2-slides.html#section-2",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-3",
    "href": "content/lectures/04-ggplot2-slides.html#section-3",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-4",
    "href": "content/lectures/04-ggplot2-slides.html#section-4",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-5",
    "href": "content/lectures/04-ggplot2-slides.html#section-5",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-6",
    "href": "content/lectures/04-ggplot2-slides.html#section-6",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-7",
    "href": "content/lectures/04-ggplot2-slides.html#section-7",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-8",
    "href": "content/lectures/04-ggplot2-slides.html#section-8",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-9",
    "href": "content/lectures/04-ggplot2-slides.html#section-9",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-10",
    "href": "content/lectures/04-ggplot2-slides.html#section-10",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#coding-out-loud-1",
    "href": "content/lectures/04-ggplot2-slides.html#coding-out-loud-1",
    "title": "04-ggplot2",
    "section": "Coding out loud",
    "text": "Coding out loud\n\nCodePlotNarrative\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStart with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\nRepresent each observation with a point and map species to the color of each point.\nTitle the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\nFinally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#argument-names",
    "href": "content/lectures/04-ggplot2-slides.html#argument-names",
    "title": "04-ggplot2",
    "section": "Argument names",
    "text": "Argument names\n\n\n\n\n\n\nTip\n\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm,  \n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\nggplot(penguins, \n       aes(x = bill_depth_mm, \n           y = bill_depth_mm,\n           color = species)) +\n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a basic plot in ggplot2 using different variables than those in the last example (last example: bill_depth_mm & bill_depth_mm).\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#aesthetics-options",
    "href": "content/lectures/04-ggplot2-slides.html#aesthetics-options",
    "title": "04-ggplot2",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#color",
    "href": "content/lectures/04-ggplot2-slides.html#color",
    "title": "04-ggplot2",
    "section": "Color",
    "text": "Color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#shape",
    "href": "content/lectures/04-ggplot2-slides.html#shape",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = island)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#shape-1",
    "href": "content/lectures/04-ggplot2-slides.html#shape-1",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#size",
    "href": "content/lectures/04-ggplot2-slides.html#size",
    "title": "04-ggplot2",
    "section": "Size",
    "text": "Size\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#alpha",
    "href": "content/lectures/04-ggplot2-slides.html#alpha",
    "title": "04-ggplot2",
    "section": "Alpha",
    "text": "Alpha\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g,\n           alpha = flipper_length_mm)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting",
    "href": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting",
    "title": "04-ggplot2",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nMapping: Determine the size, alpha, etc. of points based on the values of a variable in the data\n\ngoes into aes()\n\nSetting: Determine the size, alpha, etc. of points not based on the values of a variable in the data\n\ngoes into geom_*() (this was geom_point() in the previous example, but we’ll learn about other geoms soon!)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting-example",
    "href": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting-example",
    "title": "04-ggplot2",
    "section": "Mapping vs. Setting (example)",
    "text": "Mapping vs. Setting (example)\n\n\nMapping\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm,\n           size = body_mass_g, \n           alpha = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\nSetting\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm)) + \n  geom_point(size = 2, alpha = 0.5)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn-1",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn-1",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nEdit the basic plot you created earlier to change something about its aesthetics.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#faceting-1",
    "href": "content/lectures/04-ggplot2-slides.html#faceting-1",
    "title": "04-ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\nSmaller plots that display different subsets of the data\nUseful for exploring conditional relationships and large data\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ island) \n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#various-ways-to-facet",
    "href": "content/lectures/04-ggplot2-slides.html#various-ways-to-facet",
    "title": "04-ggplot2",
    "section": "Various ways to facet",
    "text": "Various ways to facet\n🧠 In the next few slides describe what each plot displays. Think about how the code relates to the output.\n\n\n\n\n\n\nWarning\n\n\nThe plots in the next few slides do not have proper titles, axis labels, etc. because we want you to figure out what’s happening in the plots. But you should always label your plots!"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-11",
    "href": "content/lectures/04-ggplot2-slides.html#section-11",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ sex)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-12",
    "href": "content/lectures/04-ggplot2-slides.html#section-12",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(sex ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-13",
    "href": "content/lectures/04-ggplot2-slides.html#section-13",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-14",
    "href": "content/lectures/04-ggplot2-slides.html#section-14",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(. ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-15",
    "href": "content/lectures/04-ggplot2-slides.html#section-15",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species, ncol = 2)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#faceting-summary",
    "href": "content/lectures/04-ggplot2-slides.html#faceting-summary",
    "title": "04-ggplot2",
    "section": "Faceting summary",
    "text": "Faceting summary\n\nfacet_grid():\n\n2d grid\nrows ~ cols\nuse . for no split\n\nfacet_wrap(): 1d ribbon wrapped according to number of rows and columns specified or available plotting area"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#facet-and-color",
    "href": "content/lectures/04-ggplot2-slides.html#facet-and-color",
    "title": "04-ggplot2",
    "section": "Facet and color",
    "text": "Facet and color\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) + \n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#face-and-color-no-legend",
    "href": "content/lectures/04-ggplot2-slides.html#face-and-color-no-legend",
    "title": "04-ggplot2",
    "section": "Face and color, no legend",
    "text": "Face and color, no legend\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#common-geoms",
    "href": "content/lectures/04-ggplot2-slides.html#common-geoms",
    "title": "04-ggplot2",
    "section": "Common geoms",
    "text": "Common geoms\n\n\n\ngeom 1\nDescription 2\n\n\n\n\ngeom_point\nscatterplot\n\n\ngeom_bar\nbarplot\n\n\ngeom_line\nline plot\n\n\ngeom_density\ndensityplot\n\n\ngeom_histogram\nhistogram\n\n\ngeom_boxplot\nboxplot\n\n\n\nggplot2 geoms listed hereWhen each visualization is appropriate here"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn-2",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn-2",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a plot in ggplot2 using a different geom than what you did previously. Customize as much as you can before time is “up.”\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html",
    "href": "content/lectures/04-ggplot2.html",
    "title": "04-ggplot2",
    "section": "",
    "text": "Q: What is the difference between pull and select?\nA: select specifies which columns to display in your resulting dataframe. pull extracts the values from a column and stores them in a vector (not a dataframe)\n\n\nQ: I am a bit confused on factors and what levels mean.\nA: Factors store categorical information. The levels of a factor are all the possible unique values in a variable.\n\n\nQ: how similar is R to numpy/which scenarios are each used in the industry?\nA: Basically, anything data science-y you can do in R, you can also do in python. R has linear algebra/working with matrices built directly into its base installation, so no additional package would be need for numpy-like operations. And, dplyr does very similar things to pandas, but with a more readable and consistent syntax overall.\n\n\nQ: would we ever load just dplyr instead of the entire tidyverse package? is there a big difference?\nA: We’ll always just load tidyverse. The difference is that the tidyverse is quite big, so if you ever wanted to just use dplyr functions, you could load just that. This matters more in development where you’re trying to minimize external dependencies and make code run as fast as possible. For our purposes, there’s no real need to only load dplyr\n\n\nQ: I found the demos to be the most confusing part, because it’s very different understanding slides and applying that to actual coding. Personally, I would prefer if the lecture content were put into recordings, or just uploaded earlier so we could learn it on our own, and then have classes be more focused on data science best practices, applications, etc.\nA: I do really like this idea and would love to run this like this in the. I’m curious what y’all think of this and will add a question like this to the post-course survey to get students’ thoughts.\n\n\n\n\nDue Dates:\n\nLab 02 due Friday (11:59 PM)\nHW 01 due Monday (11:59 PM)\nLecture Participation survey “due” after class\n\n\n\n\n\nR4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options\n\n\n\n\n\n\nMeasurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst \n\n\n\n\npenguins |>\n  datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section",
    "href": "content/lectures/04-ggplot2.html#section",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame\n\n\nggplot(data = penguins)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-1",
    "href": "content/lectures/04-ggplot2.html#section-1",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-2",
    "href": "content/lectures/04-ggplot2.html#section-2",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-3",
    "href": "content/lectures/04-ggplot2.html#section-3",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-4",
    "href": "content/lectures/04-ggplot2.html#section-4",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-5",
    "href": "content/lectures/04-ggplot2.html#section-5",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-6",
    "href": "content/lectures/04-ggplot2.html#section-6",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-7",
    "href": "content/lectures/04-ggplot2.html#section-7",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-8",
    "href": "content/lectures/04-ggplot2.html#section-8",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-9",
    "href": "content/lectures/04-ggplot2.html#section-9",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-10",
    "href": "content/lectures/04-ggplot2.html#section-10",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "href": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "title": "04-ggplot2",
    "section": "Coding out loud",
    "text": "Coding out loud\n\nCodePlotNarrative\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStart with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\nRepresent each observation with a point and map species to the color of each point.\nTitle the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\nFinally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#argument-names",
    "href": "content/lectures/04-ggplot2.html#argument-names",
    "title": "04-ggplot2",
    "section": "Argument names",
    "text": "Argument names\n\n\n\n\n\n\nTip\n\n\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm,  \n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\nggplot(penguins, \n       aes(x = bill_depth_mm, \n           y = bill_depth_mm,\n           color = species)) +\n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn",
    "href": "content/lectures/04-ggplot2.html#your-turn",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a basic plot in ggplot2 using different variables than those in the last example (last example: bill_depth_mm & bill_depth_mm).\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#aesthetics-options",
    "href": "content/lectures/04-ggplot2.html#aesthetics-options",
    "title": "04-ggplot2",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#color",
    "href": "content/lectures/04-ggplot2.html#color",
    "title": "04-ggplot2",
    "section": "Color",
    "text": "Color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape",
    "href": "content/lectures/04-ggplot2.html#shape",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = island)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape-1",
    "href": "content/lectures/04-ggplot2.html#shape-1",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#size",
    "href": "content/lectures/04-ggplot2.html#size",
    "title": "04-ggplot2",
    "section": "Size",
    "text": "Size\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#alpha",
    "href": "content/lectures/04-ggplot2.html#alpha",
    "title": "04-ggplot2",
    "section": "Alpha",
    "text": "Alpha\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g,\n           alpha = flipper_length_mm)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "title": "04-ggplot2",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nMapping: Determine the size, alpha, etc. of points based on the values of a variable in the data\n\ngoes into aes()\n\nSetting: Determine the size, alpha, etc. of points not based on the values of a variable in the data\n\ngoes into geom_*() (this was geom_point() in the previous example, but we’ll learn about other geoms soon!)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "title": "04-ggplot2",
    "section": "Mapping vs. Setting (example)",
    "text": "Mapping vs. Setting (example)\n\n\nMapping\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm,\n           size = body_mass_g, \n           alpha = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\nSetting\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm)) + \n  geom_point(size = 2, alpha = 0.5)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-1",
    "href": "content/lectures/04-ggplot2.html#your-turn-1",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nEdit the basic plot you created earlier to change something about its aesthetics.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-1",
    "href": "content/lectures/04-ggplot2.html#faceting-1",
    "title": "04-ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\nSmaller plots that display different subsets of the data\nUseful for exploring conditional relationships and large data\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ island) \n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "href": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "title": "04-ggplot2",
    "section": "Various ways to facet",
    "text": "Various ways to facet\n🧠 In the next few slides describe what each plot displays. Think about how the code relates to the output.\n\n\n\n\n\n\nWarning\n\n\n\nThe plots in the next few slides do not have proper titles, axis labels, etc. because we want you to figure out what’s happening in the plots. But you should always label your plots!"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-11",
    "href": "content/lectures/04-ggplot2.html#section-11",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ sex)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-12",
    "href": "content/lectures/04-ggplot2.html#section-12",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(sex ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-13",
    "href": "content/lectures/04-ggplot2.html#section-13",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-14",
    "href": "content/lectures/04-ggplot2.html#section-14",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(. ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-15",
    "href": "content/lectures/04-ggplot2.html#section-15",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species, ncol = 2)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-summary",
    "href": "content/lectures/04-ggplot2.html#faceting-summary",
    "title": "04-ggplot2",
    "section": "Faceting summary",
    "text": "Faceting summary\n\nfacet_grid():\n\n2d grid\nrows ~ cols\nuse . for no split\n\nfacet_wrap(): 1d ribbon wrapped according to number of rows and columns specified or available plotting area"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#facet-and-color",
    "href": "content/lectures/04-ggplot2.html#facet-and-color",
    "title": "04-ggplot2",
    "section": "Facet and color",
    "text": "Facet and color\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) + \n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "href": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "title": "04-ggplot2",
    "section": "Face and color, no legend",
    "text": "Face and color, no legend\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#common-geoms",
    "href": "content/lectures/04-ggplot2.html#common-geoms",
    "title": "04-ggplot2",
    "section": "Common geoms",
    "text": "Common geoms\n\n\n\ngeom 1\nDescription 2\n\n\n\n\ngeom_point\nscatterplot\n\n\ngeom_bar\nbarplot\n\n\ngeom_line\nline plot\n\n\ngeom_density\ndensityplot\n\n\ngeom_histogram\nhistogram\n\n\ngeom_boxplot\nboxplot"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-2",
    "href": "content/lectures/04-ggplot2.html#your-turn-2",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a plot in ggplot2 using a different geom than what you did previously. Customize as much as you can before time is “up.”\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COGS 137: Practical Data Science in R",
    "section": "",
    "text": "Course Info\nPractical Data Science in R focuses on teaching students how to think rigorously throughout the data science process. To this end, through interaction with unique data sets and interesting questions, this course helps students 1) gain fluency in the R programming language, 2) effectively explore & visualize data, 3) use statistical thinking to analyze data and rigorously evaluate their conclusions, and 4) effectively communicate their results. Course objectives are accomplished through hands-on practice, using real-world data to learn via case studies, and project-based learning.\n\nDays & Times\n\n\n\n\n\n Lecture: Tu/Th 2-3:20 (MOS 0204)\n Lab: Fri 3-3:50 (Peterson Hall 102)\n\nInstructional Staff & Office Hours\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11-12\nZoom by appt. (see Canvas)\n\n\n\n\n\nTu 3:30-4:30 PM\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTh 3:45-4:45 PM\nZoom (see Canvas)\n\n\nIA\nShenova Davis\n\nTBD\nTBD\n\n\n\n\n\n\n\nCourse Objectives\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports\n\n\n\nTexts\nTexts are freely available online:\n\n\n\nIntroduction to Modern Statistics\nÇetinkaya-Rundel and Hardin\nOpenIntro, 1st Edition, 2021\n\n\nR for Data Science\nGrolemund and Wickham\nO’Reilly, 1st edition, 2016\n\n\n\n\n\nMaterials\nYou should have access to a laptop and bring it to every class, fully charged (as possible).\nNote: If you do not have consistent access to the technology needed, please use this form to request a loaner laptop. (For any issues that you may have, please email vcsa@ucsd.edu, and they will work to assist you.)\n\n\nAcknowledgements\nI want to first recognize Dr. Mine Çetinkaya-Rundeland for her unparalleled efforts in support of education and educators in data science, statistics, and R programming. This course website was adapted from her course website. These course slides/labs/homework…also adapted from Mine’s course and the related datascienceinabox. I am so *very* indebted to Mine! I also want to thank the Open Case Studies team for their tireless work in putting together interesting and topical case studies, a handful of which we use throughout the course. And, finally, thanks to Allison Horst, whose artwork is inspiring, educational, and fun…and is used throughout this course. Further, thanks to the R (education) community generally; planning this course was really fun because I had so many awesome resources to choose from. Having these materials made course prep and planning is just another example of what sets the R community apart!"
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "COGS 137",
    "section": "",
    "text": "Class\nClass will include short lectures as well as interactive activities. The goal of lecture is to introduce the topics and information needed for the course. The goal of your time outside of lecture is to practice with topics that are introduced and deepen your understanding of material presented in class. Since so much of programming and statistical analysis is learned best by doing, we’ll prioritize that throughout the course, both in and outside of the classroom.\n\n\nDiversity & Inclusion\nMy goal is that every student, regardless of their background or perspective, will be well-served by this course. My philosophy is that the diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding. I intend to present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture); however, if I ever fall short or if you ever have suggestions for improvement, please do share with me! This feedback is always welcomed, and I am always in the process of learning and improving to this end. If you would like to provide that feedback anonymously, please use the anonymous Google Form.*\n\nWhat should you call me?\nMost students call me Professor/Prof Ellis, and that’s great! This is how I typically sign emails to students. I’m also totally OK with you addressing me as Shannon or Dr. Ellis.\n\n\nWhat I should call you?\nI should call you by your preferred name, with the correct pronunciation. Please correct me (in the moment or via email/Piazza after the fact…however you’re most comfortable) if I ever make a mistake.\n\n\n\nDisability Access\nStudents requesting accommodations due to a disability should provide a current Authorization for Accommodation (AFA) letter. These letters are issued by the Office for Students with Disabilities (OSD), which is located in University Center 202 behind Center Hall. If you are struggling to get necessary accommodations or want to further discuss your accommodations, please feel free to reach out to Professor Ellis directly.\nContacting the OSD can help you further:\n858.534.4382 (phone)\nosd@ucsd.edu (email)\nhttp://disabilities.ucsd.edu\n\n\nHow to get help\nIt’s great that we have so many ways to communicate, but it can get tricky to figure out who to contact or where your question belongs or when to expect a response. When in doubt, Piazza is great!\nMore specifically, if you have:\n\nquestions about course content - these are awesome! We want everyone to see them and have their questions answered too, so either ask them in class (which is recorded) and/or post these to Piazza!\na technical assignment question - come to office hours (or post to Piazza). Answering technical questions is often best accomplished ‘in person’ where we can discuss the question and talk through ideas. However, if that is not possible, post your question to Piazza. Be as specific as you can in the question you ask. And, for those answering, help your classmates as much as you can without just giving the answer. Help guide them, point them in a direction, provide pseudo code, but do not provide code that answers assignment questions.\nquestions about course logistics - first, check the course website. If you can’t find the answer there, first ask a classmate. If still unsure, post on Piazza.\nquestions about a grade - Post on Piazza with “regrades” tag in a private post to “Instructors & TAs”.\nsomething super cool to share related to class or want to talk about a topic in further depth - feel free to email Professor Ellis (sellis@ucsd.edu) or come to office hours. Please include COGS137 in the email subject line.\nsome feedback about the course you want to share anonymously - If you’ve been offended by an example in class, really liked or disliked a lesson, or wish there were something covered in class that wasn’t but would rather not share this publicly, etc., please fill out the anonymous Google Form*\n\n*This form can be taken down at any time if it’s not being used for its intended purpose; however, you all will be notified should that happen.\n\n\nAcademic integrity\nDon’t cheat.\nYou are generally encouraged to work together and help one another in this course. However, you are personally responsible for the work you submit. A helpful heuristic can be to ask yourself “Can I explain each piece of code and each analysis carried out in what I’m submitting? Could I reproduce this code/analysis on my own?”; you should be able to answer “Yes” to both questions for everything you submit in this course. For labs and assignments, you are allowed and encouraged to work together, but it is your responsibility to ensure you understand everything you’ve submitted. (For the midterm, all work has to be completed individually and communication with other humans about the exam is not allowed; this will be discussed more explicitly beforehand.)\nA note on sharing / reusing code: The Internet is an excellent resource; there will be many times you find helpful information online. You should use available resources (e.g. ChatGPT, Copilot, StackOverflow, etc.), but you must explicitly cite any code you use directly or any code you use as inspiration. This can be done by including the URL/reference to the source directly in your code (as a code comment) or in accompanying text for a given assignment/exam/lab. You should never share code directly (e.g. copy + paste; share an send an answer to a classmate), but you can discuss code and work together on everything other than take-home exams.\nPlease review UCSD’s academic integrity policies here.\nCheating and plagiarism have been and will be strongly penalized. If, for whatever reason, Canvas or DataHub is down or something else prohibits you from being able to turn in an assignment on time, immediately contact Professor Ellis by emailing (sellis@ucsd.edu) your assignment as soon as possible to avoid it being graded as late.\n\n\nCourse components\n\nLecture\nLectures will be your introduction to course topics and material. Lectures will be interactive, and you will be given time to practice with the lecture concepts during class. Attendance is not required, but is encouraged if you’re feeling well. To help incentivize coming to class, there will be a daily participation survey that will open at the end of lecture and close shortly after each Tues/Thurs lecture. Each time you fill out the lecture survey, you get a small % of credit toward your final project Completion of all surveys can provide up to 3.5% extra credit on your final project (not your final course grade).\n\nReadings\nReadings will be assigned for some class days and are best completed prior to the day’s lecture. These are meant to provide background and additional context for the upcoming day’s lecture topics. These can also be a good source after class when studying or reviewing topics discussed in class.\n\n\nPodcast\nIn case you miss class or would like to review the material covered in class, you can view the podcasts here.\n\n\n\nLabs (16%)\nLabs are meant to give you deeper understanding and hands-on experience with the technical and statistical topics introduced during lecture in a low-stakes environment. Lab sections will typically comprise of a short review and explanation of the lab and then time for you to complete the assigned weekly lab. Labs are submitted individually, but you are encouraged to work together during lab. You are free to ask and answer each others’ questions and discuss your work. Instructional staff will be present during lab to help further your understanding.\nLabs are graded for concerted effort. This is because when we learn something new, mistakes are going to happen! In fact, we learn a lot from the mistakes we make during the learning process. If your submission reflects ~50 min of work/effort, you will receive full credit for the week’s lab.\nLab attendance is not required, but is definitely encouraged if you are feeling well. While slides used are shared, lab sessions are not recorded, so being present is the best way to fully engage in the course.\n\n\nHomework (24%)\nAfter practice in lecture and labs, homework assignments are meant to demonstrate your solidified understanding of the course material. These are typically 2-4x longer and more involved than labs. Homework assignments are completed and submitted individually and are marked for correctness. You are allowed to work together on homework assignments, but academic integrity must be upheld.\n\n\nMidterm (15%)\nThere will be a single take-home midterm, and you will have at least 48 hours to complete it. This exam is meant to assess your understanding of the R programming language prior to us moving into focusing on case studies and full analyses. The exam will be completed individually and will be open-notes and open-Internet; however, you will not be permitted to ask questions of one another or instructional staff while completing the take-home exam.\n\n\nTeams\nThere will be two case study mini-projects and a final project. Teams will be randomly assigned for the mini-projects but you will choose your final project groups. (By working with teammates throughout the course, you will also be able to use one another as a resource during labs and assignments.)\n\n\nCase Study Mini-projects (25%)\nStarting week 5, we will transition to a project-based course. This will allow us to use case studies to focus on deepening statistical knowledge and carrying out interesting analyses. In this, specific case studies and statistics topics will be discussed in class. In your teams and for each of the case studies, you will: 1) extend the analysis from class and 2) communicate your findings for both a technical and general audience.\n\n\nFinal Project (20%)\nThe final project will be completed in groups. There will be two different general final projects from which your group can choose, but the idea is that whichever you choose, you will be able to tackle it using and building upon the tools and techniques discussed in class. Briefly here, the two options will be: 1. Create a technical presentation on a statistics topic and/or an R package. 2. Carry out a data analysis.\nEach will require a written technical report, a communication to a general audience, and an oral presentation, but the specific requirements will differ between the two.\nFinal Project groups will have to submit a proposal during week 8 (2%). Final projects (18%) will be due on Tues of finals week at 11:59 PM.\n\n\n\nGrading\nYour final grade will be comprised of the following:\n\n\n\nLabs (8)\n16%\n\n\nHomework (3)\n24%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n25%\n\n\nFinal Project* (Proposal + Project)\n20%\n\n\n\n* indicates group submission\n\nFinal Grades\nTo calculate final grades, I use the standard grading scale and do not round grades up (given the numerous extra credit opportunities offered):\n\n\n\n97-100%\nA+\n\n\n93-96%\nA\n\n\n90-92%\nA-\n\n\n87-89%\nB+\n\n\n83-86%\nB\n\n\n80-82%\nB-\n\n\n77-79%\nC+\n\n\n73-76%\nC\n\n\n70-72%\nC-\n\n\n67-69%\nD+\n\n\n63-66%\nD\n\n\n60-62%\nD-\n\n\n<60%\nF\n\n\n\n\n\n\nLate / missed work\nLate homework assignments and case study projects will be accepted up to 3 days (72 hours) after the assigned deadline. Late submissions will receive a 25% deduction.\nThere are no late deadlines for labs, the exam, or the final project.\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter.\n\n\nRegrade requests\nWe will work hard to grade everyone fairly and return assignments quickly. And, we know you also work hard and want you to receive the grade you’ve earned. Occasionally, grading mistakes do happen, and it’s important to us to correct them. If you think there is a mistake in your grade on an assignment, post privately on Piazza to “Instructors” using the “regrades” tag within 72 hours. This post should include evidence of why you think your answer was correct and should point to the specific part of the assignment in question.\n\n\nProfessionalism\nPlease refrain from texting or using your computer for anything other than coursework during class. Not only is this distracting to you, but it can also be distracting to those around you. (Note that there is no consequence associated with this. I know it can be difficult, but I ask that you try your best!)"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "COGS 137",
    "section": "",
    "text": "Week\nDate\nTitle\nType\n\n\n\n\n0\nTh Sep 28\nWelcome & Tooling\nLecture\n\n\n1\nTu Oct 3\nIntro to R\nLecture\n\n\n1\nTh Oct 5\nData Wrangling: dplyr\nLecture\n\n\n1\nFri Oct 6\nLab 01: Intro to R\nLab\n\n\n2\nTu Oct 10\nData Wrangling: tidyr\nLecture\n\n\n2\nTh Oct 12\nData Visualization: ggplot2 (day 1)\nLecture\n\n\n2\nFri Oct 13\nLab 02: Data Wrangling\nLab\n\n\n3\nMon Oct 16\nHW01 due (11:59 PM)\nHW\n\n\n3\nTu Oct 17\nData Visualization: ggplot2 (day 2)\nLecture\n\n\n3\nTh Oct 19\nData Analysis & Modeling\nLecture\n\n\n3\nFri Oct 20\nLab 03: Data Visualization\nLab\n\n\n4\nTu Oct 24\nLinear Models Review\nLecture\n\n\n4\nTh Oct 26\nEffective Communication\nLecture\n\n\n4\nFri Oct 27\nLab 04: Modeling\nLab\n\n\n5\nMon Oct 30\nHW02 due (11:59 PM)\nHW\n\n\n5\nTu Oct 31\nMultiple Linear Regression*\nLecture\n\n\n5\nTh Nov 2\nCase Study & Final Project Info\nLecture\n\n\n5\nFri Nov 3\nLab used for midterm review\nLab\n\n\n6\nMon Nov 6\nMIDTERM EXAM (due 11:59 PM) \nExam\n\n\n6\nTu Nov 7\nCase Study 01: THC Biomarkers (day 1)\nLecture\n\n\n6\nTh Nov 9\nCase Study 01: THC Biomarkers (day 2)\nLecture\n\n\n6\nFri Nov 10\nLab 05: Multiple Linear Regression\nLab\n\n\n7\nTu Nov 14\nCase Study 01: THC Biomarkers (day 3)\nLecture\n\n\n7\nTh Nov 16\ntidymodels\nLecture\n\n\n7\nSun Nov 19\nLab 06: CS01 [Note: Due date was extended to Sunday]\nLab\n\n\n8\nMon Nov 20\nHW03 due (11:59 PM)\nHW\n\n\n8\nMon Nov 20\nFinal Project Proposal Due\nProject\n\n\n8\nTu Nov 21\nCase Study 02: Air Pollution (day 1)\nLecture\n\n\n8\nTh Nov 23\nNo Class (Thanksgiving)\n--\n\n\n9\nMon Nov 27\nCS01 Due (11:59 PM)\nCase Study\n\n\n9\nTu Nov 28\nCase Study 02: Air Pollution (day 2)\nLecture\n\n\n9\nTh Nov 30\nCase Study 02: Air Pollution (day 3)\nLecture\n\n\n9\nFri Dec 1\nLab 07: CS02\nLab\n\n\n10\nMon Dec 4\nCS02 Due\nCase Study\n\n\n10\nTu Dec 5\nFinal Project Brainstorming\nLecture\n\n\n10\nTh Dec 7\nNext Steps\nLecture\n\n\n10\nFri Dec 8\nLab 08: Final Project\nLab\n\n\nFinals\nTu Dec 12\nFinal Project Due (11:59 PM)\nProject"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#suggested-resources",
    "href": "content/lectures/14-tidymodels-slides.html#suggested-resources",
    "title": "14-tidymodels",
    "section": "Suggested Resources",
    "text": "Suggested Resources\n\nThe package itself has some worked examples: https://www.tidymodels.org/start/models/\nThere’s a whole book (written by the developer of tidymodels) that covers the tidymodels package: https://www.tmwr.org/"
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#machine-learning-intro",
    "href": "content/lectures/14-tidymodels-slides.html#machine-learning-intro",
    "title": "14-tidymodels",
    "section": "Machine Learning: intro",
    "text": "Machine Learning: intro\nIn intro stats, you should have learned the central dogma of statistics: we sample from a population\n\n\nThe data from the sample are used to make an inference about the population:\n\n\n\nFor prediction, we have a similar sampling problem:\n\n\n\nBut now we are trying to build a rule that can be used to predict a single observation’s value of some characteristic using characteristics of the other observations."
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#ml-the-goal",
    "href": "content/lectures/14-tidymodels-slides.html#ml-the-goal",
    "title": "14-tidymodels",
    "section": "ML: the goal",
    "text": "ML: the goal\nThe goal is to:\nbuild a machine learning algorithm\n\nthat uses features as input\n\n\nand predicts an outcome variable\n\n\nin the situation where we do not know the outcome variable."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#sparse-monitoring-ph-issue",
    "href": "content/lectures/15-cs02-data-slides.html#sparse-monitoring-ph-issue",
    "title": "15-cs02-data",
    "section": "Sparse monitoring PH issue",
    "text": "Sparse monitoring PH issue\n\n\nHistorically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country.\nHowever, these monitors are relatively sparse in certain regions of the country and are not necessarily located near pollution sources.\ndramatic differences in pollution rates can be seen even within the same city. (In fact, the term micro-environments describes environments within cities or counties which may vary greatly from one block to another.)\n\n\n\n\n\n\n[source]\n\n\nLack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#features",
    "href": "content/lectures/15-cs02-data-slides.html#features",
    "title": "15-cs02-data",
    "section": "Features",
    "text": "Features\n\n\n\nVariable\nDetails\n\n\n\n\nid\nMonitor number  – the county number is indicated before the decimal  – the monitor number is indicated after the decimal  Example: 1073.0023 is Jefferson county (1073) and .0023 one of 8 monitors\n\n\nfips\nFederal information processing standard number for the county where the monitor is located  – 5 digit id code for counties (zero is often the first value and sometimes is not shown)  – the first 2 numbers indicate the state  – the last three numbers indicate the county  Example: Alabama’s state code is 01 because it is first alphabetically  (note: Alaska and Hawaii are not included because they are not part of the contiguous US)\n\n\nLat\nLatitude of the monitor in degrees\n\n\nLon\nLongitude of the monitor in degrees\n\n\nstate\nState where the monitor is located\n\n\ncounty\nCounty where the monitor is located\n\n\ncity\nCity where the monitor is located\n\n\nCMAQ\nEstimated values of air pollution from a computational model called Community Multiscale Air Quality (CMAQ)  – A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution  – Does not use any of the PM2.5 gravimetric monitoring data. (There is a version that does use the gravimetric monitoring data, but not this one!)  – Data from the EPA\n\n\nzcta\nZip Code Tabulation Area where the monitor is located  – Postal Zip codes are converted into “generalized areal representations” that are non-overlapping  – Data from the 2010 Census\n\n\nzcta_area\nLand area of the zip code area in meters squared  – Data from the 2010 Census\n\n\nzcta_pop\nPopulation in the zip code area  – Data from the 2010 Census\n\n\nimp_a500\nImpervious surface measure  – Within a circle with a radius of 500 meters around the monitor  – Impervious surface are roads, concrete, parking lots, buildings  – This is a measure of development\n\n\nimp_a1000\nImpervious surface measure  – Within a circle with a radius of 1000 meters around the monitor\n\n\nimp_a5000\nImpervious surface measure  – Within a circle with a radius of 5000 meters around the monitor\n\n\nimp_a10000\nImpervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\n\nimp_a15000\nImpervious surface measure  – Within a circle with a radius of 15000 meters around the monitor\n\n\ncounty_area\nLand area of the county of the monitor in meters squared\n\n\ncounty_pop\nPopulation of the county of the monitor\n\n\nLog_dist_to_prisec\nLog (Natural log) distance to a primary or secondary road from the monitor  – Highway or major road\n\n\nlog_pri_length_5000\nCount of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_10000\nCount of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_15000\nCount of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_25000\nCount of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_prisec_length_500\nCount of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_1000\nCount of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_5000\nCount of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_10000\nCount of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_15000\nCount of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_25000\nCount of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_nei_2008_pm25_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\npopdens_county\nPopulation density (number of people per kilometer squared area of the county)\n\n\npopdens_zcta\nPopulation density (number of people per kilometer squared area of zcta)\n\n\nnohs\nPercentage of people in zcta area where the monitor is that do not have a high school degree  – Data from the Census\n\n\nsomehs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was some high school education  – Data from the Census\n\n\nhs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing a high school degree  – Data from the Census\n\n\nsomecollege\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing some college education  – Data from the Census\n\n\nassociate\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing an associate degree  – Data from the Census\n\n\nbachelor\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a bachelor’s degree  – Data from the Census\n\n\ngrad\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a graduate degree  – Data from the Census\n\n\npov\nPercentage of people in zcta area where the monitor is that lived in poverty in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines  – Data from the Census\n\n\nhs_orless\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a high school degree or less (sum of nohs, somehs, and hs)\n\n\nurc2013\n2013 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\nurc2006\n2006 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\naod\nAerosol Optical Depth measurement from a NASA satellite  – based on the diffraction of a laser  – used as a proxy of particulate pollution  – unit-less - higher value indicates more pollution  – Data from NASA\n\n\n\n\nMany of these features have to do with the circular area around the monitor called the “buffer”. These are illustrated in the following figure:\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#sparse-monitoring-ph-issue",
    "href": "content/lectures/15-cs02-data.html#sparse-monitoring-ph-issue",
    "title": "15-cs02-data",
    "section": "Sparse monitoring PH issue",
    "text": "Sparse monitoring PH issue\n\n\nHistorically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country.\nHowever, these monitors are relatively sparse in certain regions of the country and are not necessarily located near pollution sources.\ndramatic differences in pollution rates can be seen even within the same city. (In fact, the term micro-environments describes environments within cities or counties which may vary greatly from one block to another.)\n\n\n\n\n\n\n[source]\n\n\nLack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#features",
    "href": "content/lectures/15-cs02-data.html#features",
    "title": "15-cs02-data",
    "section": "Features",
    "text": "Features\n\n\n\nVariable\nDetails\n\n\n\n\nid\nMonitor number  – the county number is indicated before the decimal  – the monitor number is indicated after the decimal  Example: 1073.0023 is Jefferson county (1073) and .0023 one of 8 monitors\n\n\nfips\nFederal information processing standard number for the county where the monitor is located  – 5 digit id code for counties (zero is often the first value and sometimes is not shown)  – the first 2 numbers indicate the state  – the last three numbers indicate the county  Example: Alabama’s state code is 01 because it is first alphabetically  (note: Alaska and Hawaii are not included because they are not part of the contiguous US)\n\n\nLat\nLatitude of the monitor in degrees\n\n\nLon\nLongitude of the monitor in degrees\n\n\nstate\nState where the monitor is located\n\n\ncounty\nCounty where the monitor is located\n\n\ncity\nCity where the monitor is located\n\n\nCMAQ\nEstimated values of air pollution from a computational model called Community Multiscale Air Quality (CMAQ)  – A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution  – Does not use any of the PM2.5 gravimetric monitoring data. (There is a version that does use the gravimetric monitoring data, but not this one!)  – Data from the EPA\n\n\nzcta\nZip Code Tabulation Area where the monitor is located  – Postal Zip codes are converted into “generalized areal representations” that are non-overlapping  – Data from the 2010 Census\n\n\nzcta_area\nLand area of the zip code area in meters squared  – Data from the 2010 Census\n\n\nzcta_pop\nPopulation in the zip code area  – Data from the 2010 Census\n\n\nimp_a500\nImpervious surface measure  – Within a circle with a radius of 500 meters around the monitor  – Impervious surface are roads, concrete, parking lots, buildings  – This is a measure of development\n\n\nimp_a1000\nImpervious surface measure  – Within a circle with a radius of 1000 meters around the monitor\n\n\nimp_a5000\nImpervious surface measure  – Within a circle with a radius of 5000 meters around the monitor\n\n\nimp_a10000\nImpervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\n\nimp_a15000\nImpervious surface measure  – Within a circle with a radius of 15000 meters around the monitor\n\n\ncounty_area\nLand area of the county of the monitor in meters squared\n\n\ncounty_pop\nPopulation of the county of the monitor\n\n\nLog_dist_to_prisec\nLog (Natural log) distance to a primary or secondary road from the monitor  – Highway or major road\n\n\nlog_pri_length_5000\nCount of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_10000\nCount of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_15000\nCount of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_25000\nCount of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_prisec_length_500\nCount of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_1000\nCount of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_5000\nCount of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_10000\nCount of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_15000\nCount of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_25000\nCount of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_nei_2008_pm25_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\npopdens_county\nPopulation density (number of people per kilometer squared area of the county)\n\n\npopdens_zcta\nPopulation density (number of people per kilometer squared area of zcta)\n\n\nnohs\nPercentage of people in zcta area where the monitor is that do not have a high school degree  – Data from the Census\n\n\nsomehs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was some high school education  – Data from the Census\n\n\nhs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing a high school degree  – Data from the Census\n\n\nsomecollege\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing some college education  – Data from the Census\n\n\nassociate\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing an associate degree  – Data from the Census\n\n\nbachelor\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a bachelor’s degree  – Data from the Census\n\n\ngrad\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a graduate degree  – Data from the Census\n\n\npov\nPercentage of people in zcta area where the monitor is that lived in poverty in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines  – Data from the Census\n\n\nhs_orless\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a high school degree or less (sum of nohs, somehs, and hs)\n\n\nurc2013\n2013 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\nurc2006\n2006 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\naod\nAerosol Optical Depth measurement from a NASA satellite  – based on the diffraction of a laser  – used as a proxy of particulate pollution  – unit-less - higher value indicates more pollution  – Data from NASA\n\n\n\n\nMany of these features have to do with the circular area around the monitor called the “buffer”. These are illustrated in the following figure:\n\n[source]"
  }
]