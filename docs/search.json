[
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#qa",
    "href": "content/lectures/17-cs02-analysis-slides.html#qa",
    "title": "17-cs02-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Just to confirm, if we choose to do the CS02 do we still do it with our final group or the assigned CS02 group?\nA: Final group!\n\n\nQ: If we choose to do the final project instead of CS2, will everything be the same as our originally planned final projects in terms of contents except the grade weighting?\nA: Yup!\n\n\nQ: Understanding some of the variable’s meaning, maybe would have been good for us to recode them to make them more intuitive.\nA: A good suggestion if you decide to go this route for the final!\n\n\nQ: For the distribution on the diagonal line of the correlation graphs, what does x/y axis represent?\nA: It’s a densityplot (shows the distribution) for each individual variable - the one that’s in that row."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#course-announcements",
    "href": "content/lectures/17-cs02-analysis-slides.html#course-announcements",
    "title": "17-cs02-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nCS01 due tonight (group work survey due Friday)\nLab07 due tomorrow (Friday)\nFinal Project due Tues of Finals week (report + presentation + general communication)\n\n\n\nAny CS01 questions?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#recall",
    "href": "content/lectures/17-cs02-analysis-slides.html#recall",
    "title": "17-cs02-analysis",
    "section": "Recall:",
    "text": "Recall:"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#the-data",
    "href": "content/lectures/17-cs02-analysis-slides.html#the-data",
    "title": "17-cs02-analysis",
    "section": "The Data",
    "text": "The Data\n\npm <- read_csv(here::here(\"OCS_data\", \"data\", \"raw\", \"pm25_data.csv\"))\n\n# Converting to factors as discussed last class\npm <- pm %>%\n  dplyr::mutate(across(c(id, fips, zcta), as.factor))"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#data-splitting",
    "href": "content/lectures/17-cs02-analysis-slides.html#data-splitting",
    "title": "17-cs02-analysis",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n\n\n\nSpecify the split:\n\nset.seed(1234)\npm_split <- rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n<Training/Testing/Total>\n<584/292/876>\n\n\n\nset.seed <- ensures we all get the exact same random split\noutput displayed: <training data sample number, testing data sample number, original sample number>\n\nMore on how people decide what proportions to use for data splitting here\n\n\nSplit the Data\n\ntrain_pm <- rsample::training(pm_split)\ntest_pm <- rsample::testing(pm_split)\n \n# Scroll through the output!\ncount(train_pm, state)\n\n# A tibble: 49 × 2\n   state                    n\n   <chr>                <int>\n 1 Alabama                 13\n 2 Arizona                 12\n 3 Arkansas                 8\n 4 California              55\n 5 Colorado                10\n 6 Connecticut             12\n 7 Delaware                 3\n 8 District Of Columbia     2\n 9 Florida                 22\n10 Georgia                 20\n# ℹ 39 more rows\n\n\n❓ What do you observe about the output?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#pre-processing-recipe-bake",
    "href": "content/lectures/17-cs02-analysis-slides.html#pre-processing-recipe-bake",
    "title": "17-cs02-analysis",
    "section": "Pre-processing: recipe() + bake()",
    "text": "Pre-processing: recipe() + bake()\nNeed to:\n\nspecify predictors vs. outcome\nscale variables\nremove redundant variables (feature engineering)\n\n\nrecipe provides a standardized format for a sequence of steps for pre-processing the data"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-variable-roles",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-variable-roles",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify variable roles",
    "text": "Step 1: Specify variable roles\nThe simplest approach…\n\nsimple_rec <- train_pm %>%\n  recipes::recipe(value ~ .)\n\nsimple_rec\n\n\n…but we need to specify which column includes ID information\n\nsimple_rec <- train_pm %>%\n  recipes::recipe(value ~ .) %>%\n  recipes::update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n\n\n…and which are our predictors and which is our outcome\n\nsimple_rec <- recipe(train_pm) %>%\n    update_role(everything(), new_role = \"predictor\") %>%\n    update_role(value, new_role = \"outcome\") %>%\n    update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n❓ Can someone summarize what this code is specifying?\n\n\nSummarizing our recipe thus far:\n\nsummary(simple_rec)\n\n# A tibble: 50 × 4\n   variable type      role        source  \n   <chr>    <list>    <chr>       <chr>   \n 1 id       <chr [3]> id variable original\n 2 value    <chr [2]> outcome     original\n 3 fips     <chr [3]> predictor   original\n 4 lat      <chr [2]> predictor   original\n 5 lon      <chr [2]> predictor   original\n 6 state    <chr [3]> predictor   original\n 7 county   <chr [3]> predictor   original\n 8 city     <chr [3]> predictor   original\n 9 CMAQ     <chr [2]> predictor   original\n10 zcta     <chr [3]> predictor   original\n# ℹ 40 more rows"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-2-specify-pre-processing-with-step",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-2-specify-pre-processing-with-step",
    "title": "17-cs02-analysis",
    "section": "Step 2: Specify pre-processing with step*()",
    "text": "Step 2: Specify pre-processing with step*()\n\n\n\n\nThere are step functions for a variety of purposes:\n\nImputation – filling in missing values based on the existing data\nTransformation – changing all values of a variable in the same way, typically to make it more normal or easier to interpret\nDiscretization – converting continuous values into discrete or nominal values - binning for example to reduce the number of possible levels (However this is generally not advisable!)\nEncoding / Creating Dummy Variables – creating a numeric code for categorical variables (More on one-hot and Dummy Variables encoding)\nData type conversions – which means changing from integer to factor or numeric to date etc.\nInteraction term addition to the model – which means that we would be modeling for predictors that would influence the capacity of each other to predict the outcome\nNormalization – centering and scaling the data to a similar range of values\nDimensionality Reduction/ Signal Extraction – reducing the space of features or predictors to a smaller set of variables that capture the variation or signal in the original variables (ex. Principal Component Analysis and Independent Component Analysis)\nFiltering – filtering options for removing variables (ex. remove variables that are highly correlated to others or remove variables with very little variance and therefore likely little predictive capacity)\nRow operations – performing functions on the values within the rows (ex. rearranging, filtering, imputing)\nChecking functions – Gut checks to look for missing values, to look at the variable classes etc.\n\nThis link and this link show the many options for recipe step functions.\n\n\nThere are several ways to select what variables to apply steps to:\n\nUsing tidyselect methods: contains(), matches(), starts_with(), ends_with(), everything(), num_range()\n\nUsing the type: all_nominal(), all_numeric() , has_type()\nUsing the role: all_predictors(), all_outcomes(), has_role()\nUsing the name - use the actual name of the variable/variables of interest\n\n\n\nOne-hot encoding categorical variables:\n\nsimple_rec %>%\n  step_dummy(state, county, city, zcta, one_hot = TRUE)\n\n❓ Can anyone remind us what one-hot encoding does?\n\n\n\nfips includes numeric code for state and county, so it’s another way to specify county\nso, we’ll change fips’ role\nwe get to decide what to call it (\"county id\")\n\n\nsimple_rec %>%\n  update_role(\"fips\", new_role = \"county id\")\n\n\n\nRemoving highly correlated variables:\n\nsimple_rec %>%\n  step_corr(all_predictors(), - CMAQ, - aod)\n\n\nspecifying to KEEP CMAQ and aod\n\n\n\nRemoving variables with non-zero variance:\n\nsimple_rec %>%\n  step_nzv(all_predictors(), - CMAQ, - aod)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#putting-our-recipe-together",
    "href": "content/lectures/17-cs02-analysis-slides.html#putting-our-recipe-together",
    "title": "17-cs02-analysis",
    "section": "Putting our recipe together",
    "text": "Putting our recipe together\n\nsimple_rec <- simple_rec %>% \n  update_role(\"fips\", new_role = \"county id\") %>%\n  step_dummy(state, county, city, zcta, one_hot = TRUE) %>%\n  step_corr(all_predictors(), - CMAQ, - aod)%>%\n  step_nzv(all_predictors(), - CMAQ, - aod)\n  \nsimple_rec\n\nNote: order of steps matters"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-3-running-the-pre-processing-prep",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-3-running-the-pre-processing-prep",
    "title": "17-cs02-analysis",
    "section": "Step 3: Running the pre-processing (prep)",
    "text": "Step 3: Running the pre-processing (prep)\nThere are some important arguments to know about:\n\ntraining - you must supply a training data set to estimate parameters for pre-processing operations (recipe steps) - this may already be included in your recipe - as is the case for us\nfresh - if fresh=TRUE, will retrain and estimate parameters for any previous steps that were already prepped if you add more steps to the recipe (default is FALSE)\nverbose - if verbose=TRUE, shows the progress as the steps are evaluated and the size of the pre-processed training set (default is FALSE)\nretain - if retain=TRUE, then the pre-processed training set will be saved within the recipe (as template). This is good if you are likely to add more steps and do not want to rerun the prep() on the previous steps. However this can make the recipe size large. This is necessary if you want to actually look at the pre-processed data (default is TRUE)\n\n\nprepped_rec <- prep(simple_rec, verbose = TRUE, retain = TRUE )\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.26 Mb  in memory.\n\nnames(prepped_rec)\n\n [1] \"var_info\"       \"term_info\"      \"steps\"          \"template\"      \n [5] \"levels\"         \"retained\"       \"requirements\"   \"tr_info\"       \n [9] \"orig_lvls\"      \"last_term_info\"\n\n\n\nThis output includes a lot of information:\n\nthe steps that were run\n\nthe original variable info (var_info)\n\nthe updated variable info after pre-processing (term_info)\nthe new levels of the variables\nthe original levels of the variables (orig_lvls)\ninfo about the training data set size and completeness (tr_info)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-4-extract-pre-processed-training-data-using-bake",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-4-extract-pre-processed-training-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 4: Extract pre-processed training data using bake()",
    "text": "Step 4: Extract pre-processed training data using bake()\n\n\n\nbake(): apply our modeling steps (in this case just pre-processing on the training data) and see what it would do the data\n\n\nbaked_train <- bake(prepped_rec, new_data = NULL)\nglimpse(baked_train)\n\nRows: 584\nColumns: 37\n$ id                          <fct> 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       <dbl> 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        <fct> 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         <dbl> 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         <dbl> -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        <dbl> 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   <dbl> 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    <dbl> 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    <dbl> 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  <dbl> 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 <dbl> 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  <dbl> 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          <dbl> 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         <dbl> 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        <dbl> 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       <dbl> 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      <dbl> 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      <dbl> 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     <dbl> 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_nei_2008_pm10_sum_10000 <dbl> 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 <dbl> 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 <dbl> 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              <dbl> 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                <dbl> 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        <dbl> 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      <dbl> 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          <dbl> 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 <dbl> 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   <dbl> 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    <dbl> 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        <dbl> 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         <dbl> 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   <dbl> 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     <dbl> 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         <dbl> 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            <dbl> 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n\nnew_data = NULL specifies that we’re not (yet) looking at our testing data\nWe only have 36 variables (33 predictors + 2 id variables + outcome)\ncategorical variables (state) are gone (one-hot encoding)\nstate_California remains - only state with nonzero variance (largest # of monitors)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-5-extract-pre-processed-testing-data-using-bake",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-5-extract-pre-processed-testing-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 5: Extract pre-processed testing data using bake()",
    "text": "Step 5: Extract pre-processed testing data using bake()\n\nbake() takes a trained recipe and applies the operations to a data set to create a design matrix. For example: it applies the centering to new data sets using these means used to create the recipe. - tidymodels documentation\n\n\nTypically, you want to avoid using your testing data…but our data set is not that large and NA values in our testing dataset could cause issues later on.\n\n\n\n\n\n\nbaked_test_pm <- recipes::bake(prepped_rec, new_data = test_pm)\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 37\n$ id                          <fct> 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       <dbl> 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        <fct> 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         <dbl> 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         <dbl> -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        <dbl> 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   <dbl> 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    <dbl> 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    <dbl> 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  <dbl> 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 <dbl> 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  <dbl> 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          <dbl> 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         <dbl> 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        <dbl> 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       <dbl> 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      <dbl> 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      <dbl> 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     <dbl> 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_nei_2008_pm10_sum_10000 <dbl> 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 <dbl> 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 <dbl> 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              <dbl> 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                <dbl> 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        <dbl> 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      <dbl> 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          <dbl> 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 <dbl> 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   <dbl> 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    <dbl> 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        <dbl> 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         <dbl> 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   <dbl> 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     <dbl> 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         <dbl> 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          <dbl> NA, NA, NA, 0, 1, 1, 1, NA, NA, NA, 0, NA,…\n\n\n\n\nHmm….lots of NAs now in city_Not.in.a.city\nLikely b/c there are cities in our testing dataset that were not in our training dataset…\n\ntraincities <- train_pm %>% distinct(city)\ntestcities <- test_pm %>% distinct(city)\n\n#get the number of cities that were different\ndim(dplyr::setdiff(traincities, testcities))\n\n[1] 381   1\n\n#get the number of cities that overlapped\ndim(dplyr::intersect(traincities, testcities))\n\n[1] 55  1"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#aside-return-to-wrangling",
    "href": "content/lectures/17-cs02-analysis-slides.html#aside-return-to-wrangling",
    "title": "17-cs02-analysis",
    "section": "Aside: return to wrangling",
    "text": "Aside: return to wrangling\nA quick return to wrangling…and re-splitting our data\n\npm <- pm %>%\n  mutate(city = case_when(city == \"Not in a city\" ~ \"Not in a city\",\n                          city != \"Not in a city\" ~ \"In a city\"))\n\nset.seed(1234) # same seed as before\npm_split <-rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n<Training/Testing/Total>\n<584/292/876>\n\n train_pm <-rsample::training(pm_split)\n test_pm <-rsample::testing(pm_split)\n\n\nAnd a recipe update…(putting it all together)\n\nnovel_rec <- recipe(train_pm) %>%\n    update_role(everything(), new_role = \"predictor\") %>%\n    update_role(value, new_role = \"outcome\") %>%\n    update_role(id, new_role = \"id variable\") %>%\n    update_role(\"fips\", new_role = \"county id\") %>%\n    step_dummy(state, county, city, zcta, one_hot = TRUE) %>%\n    step_corr(all_numeric()) %>%\n    step_nzv(all_numeric()) \n\n\n\nre-bake()\n\nprepped_rec <- prep(novel_rec, verbose = TRUE, retain = TRUE)\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.27 Mb  in memory.\n\nbaked_train <- bake(prepped_rec, new_data = NULL)\n\n\n\nLooking at the output\n\nglimpse(baked_train)\n\nRows: 584\nColumns: 38\n$ id                          <fct> 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       <dbl> 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        <fct> 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         <dbl> 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         <dbl> -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        <dbl> 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   <dbl> 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    <dbl> 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    <dbl> 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  <dbl> 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 <dbl> 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  <dbl> 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          <dbl> 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         <dbl> 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        <dbl> 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       <dbl> 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      <dbl> 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      <dbl> 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     <dbl> 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_prisec_length_25000     <dbl> 13.98749, 13.15082, 13.44293, 13.58697, 14…\n$ log_nei_2008_pm10_sum_10000 <dbl> 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 <dbl> 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 <dbl> 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              <dbl> 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                <dbl> 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        <dbl> 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      <dbl> 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          <dbl> 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 <dbl> 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   <dbl> 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    <dbl> 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        <dbl> 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         <dbl> 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   <dbl> 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     <dbl> 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         <dbl> 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            <dbl> 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n\n\nMaking sure the NA issue is taken are of:\n\nbaked_test_pm <- bake(prepped_rec, new_data = test_pm)\n\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 38\n$ id                          <fct> 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       <dbl> 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        <fct> 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         <dbl> 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         <dbl> -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        <dbl> 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   <dbl> 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    <dbl> 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    <dbl> 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  <dbl> 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 <dbl> 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  <dbl> 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          <dbl> 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         <dbl> 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        <dbl> 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       <dbl> 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      <dbl> 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      <dbl> 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     <dbl> 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_prisec_length_25000     <dbl> 13.79973, 13.70026, 13.85550, 14.45221, 13…\n$ log_nei_2008_pm10_sum_10000 <dbl> 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 <dbl> 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 <dbl> 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              <dbl> 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                <dbl> 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        <dbl> 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      <dbl> 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          <dbl> 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 <dbl> 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   <dbl> 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    <dbl> 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        <dbl> 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         <dbl> 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   <dbl> 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     <dbl> 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         <dbl> 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          <dbl> 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#specifying-our-model-parsnip",
    "href": "content/lectures/17-cs02-analysis-slides.html#specifying-our-model-parsnip",
    "title": "17-cs02-analysis",
    "section": "Specifying our model (parsnip)",
    "text": "Specifying our model (parsnip)\nThere are four things we need to define about our model:\n\n\nThe type of model (using specific functions in parsnip like rand_forest(), logistic_reg() etc.)\n\nThe package or engine that we will use to implement the type of model selected (using the set_engine() function)\nThe mode of learning - classification or regression (using the set_mode() function)\nAny arguments necessary for the model/package selected (using the set_args()function - for example the mtry = argument for random forest which is the number of variables to be used as options for splitting at each tree node)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-the-model",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify the model",
    "text": "Step 1: Specify the model\n\nWe’ll start with linear regression, but move to random forest\nSee here for modeling options in parsnip.\n\n\nlm_PM_model <- parsnip::linear_reg() %>%\n  parsnip::set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nlm_PM_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-2-fit-the-model",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-2-fit-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 2: Fit the model",
    "text": "Step 2: Fit the model\n\nworkflows package allows us to keep track of both our pre-processing steps and our model specification\nIt also allows us to implement fancier optimizations in an automated way and it can also handle post-processing operations.\n\n\nPM_wflow <- workflows::workflow() %>%\n            workflows::add_recipe(novel_rec) %>%\n            workflows::add_model(lm_PM_model)\nPM_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n❓ Who can explain the difference between a recipe, baking, and a workflow?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "title": "17-cs02-analysis",
    "section": "Step 3: Prepare the recipe (estimate the parameters)",
    "text": "Step 3: Prepare the recipe (estimate the parameters)\n\nPM_wflow_fit <- parsnip::fit(PM_wflow, data = train_pm)\nPM_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                (Intercept)                          lat  \n                  2.936e+02                    3.261e-02  \n                        lon                         CMAQ  \n                  1.586e-02                    2.463e-01  \n                  zcta_area                     zcta_pop  \n                 -3.433e-10                    1.013e-05  \n                   imp_a500                   imp_a15000  \n                  5.064e-03                   -3.066e-03  \n                county_area                   county_pop  \n                 -2.324e-11                   -7.576e-08  \n         log_dist_to_prisec          log_pri_length_5000  \n                  6.214e-02                   -2.006e-01  \n       log_pri_length_25000        log_prisec_length_500  \n                 -5.411e-02                    2.204e-01  \n     log_prisec_length_1000       log_prisec_length_5000  \n                  1.154e-01                    2.374e-01  \n    log_prisec_length_10000      log_prisec_length_25000  \n                 -3.436e-02                    5.224e-01  \nlog_nei_2008_pm10_sum_10000  log_nei_2008_pm10_sum_15000  \n                  1.829e-01                   -2.355e-02  \nlog_nei_2008_pm10_sum_25000               popdens_county  \n                  2.403e-02                    2.203e-05  \n               popdens_zcta                         nohs  \n                 -2.132e-06                   -2.983e+00  \n                     somehs                           hs  \n                 -2.956e+00                   -2.962e+00  \n                somecollege                    associate  \n                 -2.967e+00                   -2.999e+00  \n                   bachelor                         grad  \n                 -2.979e+00                   -2.978e+00  \n                        pov                    hs_orless  \n                  1.859e-03                           NA  \n                    urc2006                          aod  \n                  2.577e-01                    1.535e-02  \n           state_California           city_Not.in.a.city  \n                  3.114e+00                   -4.250e-02"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-4-assess-model-fit",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-4-assess-model-fit",
    "title": "17-cs02-analysis",
    "section": "Step 4: Assess model fit",
    "text": "Step 4: Assess model fit\n\nwflowoutput <- PM_wflow_fit %>% \n  extract_fit_parsnip() %>% \n  broom::tidy() \n\nwflowoutput\n\n# A tibble: 36 × 5\n   term         estimate std.error statistic       p.value\n   <chr>           <dbl>     <dbl>     <dbl>         <dbl>\n 1 (Intercept)  2.94e+ 2  1.18e+ 2     2.49  0.0130       \n 2 lat          3.26e- 2  2.28e- 2     1.43  0.153        \n 3 lon          1.59e- 2  1.01e- 2     1.58  0.115        \n 4 CMAQ         2.46e- 1  3.97e- 2     6.20  0.00000000108\n 5 zcta_area   -3.43e-10  1.60e-10    -2.15  0.0320       \n 6 zcta_pop     1.01e- 5  5.33e- 6     1.90  0.0578       \n 7 imp_a500     5.06e- 3  7.42e- 3     0.683 0.495        \n 8 imp_a15000  -3.07e- 3  1.16e- 2    -0.263 0.792        \n 9 county_area -2.32e-11  1.97e-11    -1.18  0.238        \n10 county_pop  -7.58e- 8  9.29e- 8    -0.815 0.415        \n# ℹ 26 more rows\n\n\n\nWe have fit our model on our training data\nWe have created a model to predict values of air pollution based on the predictors that we have included\n\n\nUnderstanding what variables are most important in our model…\n\nPM_wflow_fit %>% \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 10)\n\n\n\n\n\n\nA closer look at monitors in CA:\n\nbaked_train %>% \n  mutate(state_California = as.factor(state_California)) %>%\n  mutate(state_California = recode(state_California, \n                                   \"0\" = \"Not California\", \n                                   \"1\" = \"California\")) %>%\n  ggplot(aes(x = state_California, y = value)) + \n  geom_boxplot() +\n  geom_jitter(width = .05) + \n  xlab(\"Location of Monitor\")\n\n\n\n\n\n\nRemember: machine learning (ML) as an optimization problem that tries to minimize the distance between our predicted outcome \\(\\hat{Y} = f(X)\\) and actual outcome \\(Y\\) using our features (or predictor variables) \\(X\\) as input to a function \\(f\\) that we want to estimate.\n\\[d(Y - \\hat{Y})\\]\n\n\nLet’s pull out our predicted outcome values \\(\\hat{Y} = f(X)\\) from the models we fit (using different approaches).\n\nwf_fit <- PM_wflow_fit %>% \n  extract_fit_parsnip()\n\n\nwf_fitted_values <- \n  broom::augment(wf_fit[[\"fit\"]], data = baked_train) %>% \n  select(value, .fitted:.std.resid)\n\nhead(wf_fitted_values)\n\n# A tibble: 6 × 6\n  value .fitted   .hat .sigma   .cooksd .std.resid\n  <dbl>   <dbl>  <dbl>  <dbl>     <dbl>      <dbl>\n1 11.7    12.2  0.0370   2.05 0.0000648     -0.243\n2  6.96    9.14 0.0496   2.05 0.00179       -1.09 \n3 13.3    12.6  0.0484   2.05 0.000151       0.322\n4 10.7    10.4  0.0502   2.05 0.0000504      0.183\n5 14.5    11.9  0.0243   2.05 0.00113        1.26 \n6 12.2     9.52 0.476    2.04 0.0850         1.81"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#visualizing-model-performance",
    "href": "content/lectures/17-cs02-analysis-slides.html#visualizing-model-performance",
    "title": "17-cs02-analysis",
    "section": "Visualizing Model Performance",
    "text": "Visualizing Model Performance\n\nwf_fitted_values %>% \n  ggplot(aes(x =  value, y = .fitted)) + \n  geom_point() + \n  xlab(\"actual outcome values\") + \n  ylab(\"predicted outcome values\")\n\n\n❓ What do you notice about/learn from these results?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#quantifying-model-performance",
    "href": "content/lectures/17-cs02-analysis-slides.html#quantifying-model-performance",
    "title": "17-cs02-analysis",
    "section": "Quantifying Model Performance",
    "text": "Quantifying Model Performance\n\\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}{(\\hat{y_t}- y_t)}^2}{n}}\\]\n\nCan use the yardstick package using the rmse()` function to calculate:\n\nyardstick::metrics(wf_fitted_values,\n                   truth = value, estimate = .fitted)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       1.98 \n2 rsq     standard       0.392\n3 mae     standard       1.47 \n\n\n\nRMSE isn’t too bad\n\\(R^2\\) suggests model is only explaining 39% of the variance in the data\nThe MAE value suggests that the average difference between the value predicted and the real value was 1.47 ug/m3. The range of the values was 3-22 in the training data, so this is a relatively small amount"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#cross-validation",
    "href": "content/lectures/17-cs02-analysis-slides.html#cross-validation",
    "title": "17-cs02-analysis",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nResampling + Re-partitioning:\n\n\n\n\nPreparing the data for cross-validation:\n\n\n\nNote: this is called v-fold or k-fold CV"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#implementing-in-rsample",
    "href": "content/lectures/17-cs02-analysis-slides.html#implementing-in-rsample",
    "title": "17-cs02-analysis",
    "section": "Implementing in rsample()",
    "text": "Implementing in rsample()\n\nset.seed(1234)\nvfold_pm <- rsample::vfold_cv(data = train_pm, v = 4)\nvfold_pm\n\n#  4-fold cross-validation \n# A tibble: 4 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [438/146]> Fold1\n2 <split [438/146]> Fold2\n3 <split [438/146]> Fold3\n4 <split [438/146]> Fold4\n\n\n\n\npull(vfold_pm, splits)\n\n[[1]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n[[2]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n[[3]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n[[4]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n\n\n\nVisualizing this process:"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#model-assessment-on-v-folds",
    "href": "content/lectures/17-cs02-analysis-slides.html#model-assessment-on-v-folds",
    "title": "17-cs02-analysis",
    "section": "Model Assessment on v-folds",
    "text": "Model Assessment on v-folds\nWhere this workflow thing really shines…\n\nresample_fit <- tune::fit_resamples(PM_wflow, vfold_pm)\n\n\nGives us a sense of the RMSE across the four folds:\n\ntune::show_best(resample_fit, metric = \"rmse\")\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard    2.12     4  0.0444 Preprocessor1_Model1"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#random-forest",
    "href": "content/lectures/17-cs02-analysis-slides.html#random-forest",
    "title": "17-cs02-analysis",
    "section": "Random Forest",
    "text": "Random Forest\nFitting a different model\n\nBased on a decision tree:\n\n\n\n\n\n[source]\n\nBut…in the case of random forest:\n\n\nmultiple decision trees are created (hence: forest),\neach tree is built using a random subset of the training data (with replacement) (hence: random)\nhelps to keep the algorithm from overfitting the data\nThe mean of the predictions from each of the trees is used in the final output."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#updating-our-recipe",
    "href": "content/lectures/17-cs02-analysis-slides.html#updating-our-recipe",
    "title": "17-cs02-analysis",
    "section": "Updating our recipe()",
    "text": "Updating our recipe()\n\nRF_rec <- recipe(train_pm) %>%\n    update_role(everything(), new_role = \"predictor\")%>%\n    update_role(value, new_role = \"outcome\")%>%\n    update_role(id, new_role = \"id variable\") %>%\n    update_role(\"fips\", new_role = \"county id\") %>%\n    step_novel(\"state\") %>%\n    step_string2factor(\"state\", \"county\", \"city\") %>%\n    step_rm(\"county\") %>%\n    step_rm(\"zcta\") %>%\n    step_corr(all_numeric())%>%\n    step_nzv(all_numeric())\n\n\ncan use our categorical data as is (no dummy coding)\nstep_novel()necessary here for the state variable to get all cross validation folds to work, (b/c there will be different levels included in each fold test and training sets. The new levels for some of the test sets would otherwise result in an error.; “step_novel creates a specification of a recipe step that will assign a previously unseen factor level to a new value.”"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#model-specification",
    "href": "content/lectures/17-cs02-analysis-slides.html#model-specification",
    "title": "17-cs02-analysis",
    "section": "Model Specification",
    "text": "Model Specification\nModel parameters:\n\nmtry - The number of predictor variables (or features) that will be randomly sampled at each split when creating the tree models. The default number for regression analyses is the number of predictors divided by 3.\nmin_n - The minimum number of data points in a node that are required for the node to be split further.\ntrees - the number of trees in the ensemble\n\n\n\n# install.packages(\"randomForest\")\nRF_PM_model <- parsnip::rand_forest(mtry = 10, min_n = 3) %>% \n  set_engine(\"randomForest\") %>%\n  set_mode(\"regression\")\n\nRF_PM_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#workflow",
    "href": "content/lectures/17-cs02-analysis-slides.html#workflow",
    "title": "17-cs02-analysis",
    "section": "Workflow",
    "text": "Workflow\n\nRF_wflow <- workflows::workflow() %>%\n  workflows::add_recipe(RF_rec) %>%\n  workflows::add_model(RF_PM_model)\n\nRF_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#fit-the-data",
    "href": "content/lectures/17-cs02-analysis-slides.html#fit-the-data",
    "title": "17-cs02-analysis",
    "section": "Fit the Data",
    "text": "Fit the Data\n\nRF_wflow_fit <- parsnip::fit(RF_wflow, data = train_pm)\n\nRF_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, mtry = min_cols(~10,      x), nodesize = min_rows(~3, x)) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 10\n\n          Mean of squared residuals: 2.633639\n                    % Var explained: 59.29"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#assess-feature-importance",
    "href": "content/lectures/17-cs02-analysis-slides.html#assess-feature-importance",
    "title": "17-cs02-analysis",
    "section": "Assess Feature Importance",
    "text": "Assess Feature Importance\n\nRF_wflow_fit %>% \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 10)\n\n\n❓ What’s your interpretation of these results?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#assess-model-performance",
    "href": "content/lectures/17-cs02-analysis-slides.html#assess-model-performance",
    "title": "17-cs02-analysis",
    "section": "Assess Model Performance",
    "text": "Assess Model Performance\n\nset.seed(456)\nresample_RF_fit <- tune::fit_resamples(RF_wflow, vfold_pm)\ncollect_metrics(resample_RF_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   1.67      4  0.101  Preprocessor1_Model1\n2 rsq     standard   0.591     4  0.0514 Preprocessor1_Model1\n\n\n\nFor comparison:\n\ncollect_metrics(resample_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   2.12      4  0.0444 Preprocessor1_Model1\n2 rsq     standard   0.307     4  0.0263 Preprocessor1_Model1"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#model-tuning",
    "href": "content/lectures/17-cs02-analysis-slides.html#model-tuning",
    "title": "17-cs02-analysis",
    "section": "Model Tuning",
    "text": "Model Tuning\nHyperparameters are often things that we need to specify about a model. Instead of arbitrarily specifying this, we can try to determine the best option for model performance by a process called tuning.\n\nRather than specifying values, we can use tune():\n\ntune_RF_model <- rand_forest(mtry = tune(), min_n = tune()) %>%\n  set_engine(\"randomForest\") %>%\n  set_mode(\"regression\")\n    \ntune_RF_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\n\n\nCreate Workflow:\n\nRF_tune_wflow <- workflows::workflow() %>%\n  workflows::add_recipe(RF_rec) %>%\n  workflows::add_model(tune_RF_model)\n\nRF_tune_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\nDetect how many cores you have access to:\n\nn_cores <- parallel::detectCores()\nn_cores\n\n[1] 10\n\n\n\n\nThis code will take some time to run:\n\n# install.packages(\"doParallel\")\ndoParallel::registerDoParallel(cores = n_cores)\n\nset.seed(123)\ntune_RF_results <- tune_grid(object = RF_tune_wflow, resamples = vfold_pm, grid = 20)\ntune_RF_results\n\n# Tuning results\n# 4-fold cross-validation \n# A tibble: 4 × 4\n  splits            id    .metrics          .notes          \n  <list>            <chr> <list>            <list>          \n1 <split [438/146]> Fold1 <tibble [40 × 6]> <tibble [0 × 3]>\n2 <split [438/146]> Fold2 <tibble [40 × 6]> <tibble [0 × 3]>\n3 <split [438/146]> Fold3 <tibble [40 × 6]> <tibble [1 × 3]>\n4 <split [438/146]> Fold4 <tibble [40 × 6]> <tibble [0 × 3]>\n\nThere were issues with some computations:\n\n  - Warning(s) x1: 36 columns were requested but there were 35 predictors in the dat...\n\nRun `show_notes(.Last.tune.result)` for more information."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#check-metrics",
    "href": "content/lectures/17-cs02-analysis-slides.html#check-metrics",
    "title": "17-cs02-analysis",
    "section": "Check Metrics:",
    "text": "Check Metrics:\n\ntune_RF_results %>%\n  collect_metrics()\n\n# A tibble: 40 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1    12    33 rmse    standard   1.72      4  0.0866 Preprocessor1_Model01\n 2    12    33 rsq     standard   0.562     4  0.0466 Preprocessor1_Model01\n 3    27    35 rmse    standard   1.69      4  0.102  Preprocessor1_Model02\n 4    27    35 rsq     standard   0.563     4  0.0511 Preprocessor1_Model02\n 5    22    40 rmse    standard   1.71      4  0.106  Preprocessor1_Model03\n 6    22    40 rsq     standard   0.556     4  0.0543 Preprocessor1_Model03\n 7     1    27 rmse    standard   2.03      4  0.0501 Preprocessor1_Model04\n 8     1    27 rsq     standard   0.440     4  0.0245 Preprocessor1_Model04\n 9     6    32 rmse    standard   1.77      4  0.0756 Preprocessor1_Model05\n10     6    32 rsq     standard   0.552     4  0.0435 Preprocessor1_Model05\n# ℹ 30 more rows\n\n\n\n\nshow_best(tune_RF_results, metric = \"rmse\", n = 1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1    32    11 rmse    standard    1.65     4   0.113 Preprocessor1_Model10"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#final-model-evaluation",
    "href": "content/lectures/17-cs02-analysis-slides.html#final-model-evaluation",
    "title": "17-cs02-analysis",
    "section": "Final Model Evaluation",
    "text": "Final Model Evaluation\n\ntuned_RF_values<- select_best(tune_RF_results, \"rmse\")\ntuned_RF_values\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1    32    11 Preprocessor1_Model10\n\n\n\nThe testing data!\n\n# specify best combination from tune in workflow\nRF_tuned_wflow <-RF_tune_wflow %>%\n  tune::finalize_workflow(tuned_RF_values)\n\n# fit model with those parameters on train AND test\noverallfit <- RF_wflow %>%\n  tune::last_fit(pm_split)\n\ncollect_metrics(overallfit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       1.72  Preprocessor1_Model1\n2 rsq     standard       0.608 Preprocessor1_Model1\n\n\nResults are similar to what we saw in training (RMSE: 1.65)\n\n\nGetting the predictions for the test data:\n\ntest_predictions <- collect_predictions(overallfit)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#a-map-of-the-us",
    "href": "content/lectures/17-cs02-analysis-slides.html#a-map-of-the-us",
    "title": "17-cs02-analysis",
    "section": "A map of the US",
    "text": "A map of the US\nPackages needed:\n\nsf - the simple features package helps to convert geographical coordinates into geometry variables which are useful for making 2D plots\nmaps - this package contains geographical outlines and plotting functions to create plots with maps\nrnaturalearth- this allows for easy interaction with map data from Natural Earth which is a public domain map dataset\n\n\nlibrary(sf)\nlibrary(maps)\nlibrary(rnaturalearth)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#outline-of-the-us",
    "href": "content/lectures/17-cs02-analysis-slides.html#outline-of-the-us",
    "title": "17-cs02-analysis",
    "section": "Outline of the US",
    "text": "Outline of the US\n\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\")\nglimpse(world)\n\nRows: 241\nColumns: 64\n$ scalerank  <int> 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 5, 3, 1, 1, 1, 1, 1, 1,…\n$ featurecla <chr> \"Admin-0 country\", \"Admin-0 country\", \"Admin-0 country\", \"A…\n$ labelrank  <dbl> 5, 3, 3, 6, 6, 6, 6, 4, 2, 6, 4, 4, 5, 6, 6, 2, 4, 5, 6, 2,…\n$ sovereignt <chr> \"Netherlands\", \"Afghanistan\", \"Angola\", \"United Kingdom\", \"…\n$ sov_a3     <chr> \"NL1\", \"AFG\", \"AGO\", \"GB1\", \"ALB\", \"FI1\", \"AND\", \"ARE\", \"AR…\n$ adm0_dif   <dbl> 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,…\n$ level      <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ type       <chr> \"Country\", \"Sovereign country\", \"Sovereign country\", \"Depen…\n$ admin      <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ adm0_a3    <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ geou_dif   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ geounit    <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ gu_a3      <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ su_dif     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ subunit    <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ su_a3      <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_diff   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ name       <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_long  <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_a3     <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_name   <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_group  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ abbrev     <chr> \"Aruba\", \"Afg.\", \"Ang.\", \"Ang.\", \"Alb.\", \"Aland\", \"And.\", \"…\n$ postal     <chr> \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AI\", \"AND\", \"AE\", \"AR\", \"ARM…\n$ formal_en  <chr> \"Aruba\", \"Islamic State of Afghanistan\", \"People's Republic…\n$ formal_fr  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ note_adm0  <chr> \"Neth.\", NA, NA, \"U.K.\", NA, \"Fin.\", NA, NA, NA, NA, \"U.S.A…\n$ note_brk   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Multiple claim…\n$ name_sort  <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_alt   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ mapcolor7  <dbl> 4, 5, 3, 6, 1, 4, 1, 2, 3, 3, 4, 4, 1, 7, 2, 1, 3, 1, 2, 3,…\n$ mapcolor8  <dbl> 2, 6, 2, 6, 4, 1, 4, 1, 1, 1, 5, 5, 2, 5, 2, 2, 1, 6, 2, 2,…\n$ mapcolor9  <dbl> 2, 8, 6, 6, 1, 4, 1, 3, 3, 2, 1, 1, 2, 9, 5, 2, 3, 5, 5, 1,…\n$ mapcolor13 <dbl> 9, 7, 1, 3, 6, 6, 8, 3, 13, 10, 1, NA, 7, 11, 5, 7, 4, 8, 8…\n$ pop_est    <dbl> 103065, 28400000, 12799293, 14436, 3639453, 27153, 83888, 4…\n$ gdp_md_est <dbl> 2258.0, 22270.0, 110300.0, 108.9, 21810.0, 1563.0, 3660.0, …\n$ pop_year   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ lastcensus <dbl> 2010, 1979, 1970, NA, 2001, NA, 1989, 2010, 2010, 2001, 201…\n$ gdp_year   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ economy    <chr> \"6. Developing region\", \"7. Least developed region\", \"7. Le…\n$ income_grp <chr> \"2. High income: nonOECD\", \"5. Low income\", \"3. Upper middl…\n$ wikipedia  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fips_10    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso_a2     <chr> \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AX\", \"AD\", \"AE\", \"AR\", \"AM\",…\n$ iso_a3     <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ iso_n3     <chr> \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ un_a3      <chr> \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ wb_a2      <chr> \"AW\", \"AF\", \"AO\", NA, \"AL\", NA, \"AD\", \"AE\", \"AR\", \"AM\", \"AS…\n$ wb_a3      <chr> \"ABW\", \"AFG\", \"AGO\", NA, \"ALB\", NA, \"ADO\", \"ARE\", \"ARG\", \"A…\n$ woe_id     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_is <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_us <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_un <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_wb <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ continent  <chr> \"North America\", \"Asia\", \"Africa\", \"North America\", \"Europe…\n$ region_un  <chr> \"Americas\", \"Asia\", \"Africa\", \"Americas\", \"Europe\", \"Europe…\n$ subregion  <chr> \"Caribbean\", \"Southern Asia\", \"Middle Africa\", \"Caribbean\",…\n$ region_wb  <chr> \"Latin America & Caribbean\", \"South Asia\", \"Sub-Saharan Afr…\n$ name_len   <dbl> 5, 11, 6, 8, 7, 5, 7, 20, 9, 7, 14, 10, 23, 22, 17, 9, 7, 1…\n$ long_len   <dbl> 5, 11, 6, 8, 7, 13, 7, 20, 9, 7, 14, 10, 27, 35, 19, 9, 7, …\n$ abbrev_len <dbl> 5, 4, 4, 4, 4, 5, 4, 6, 4, 4, 9, 4, 7, 10, 6, 4, 5, 4, 4, 5…\n$ tiny       <dbl> 4, NA, NA, NA, NA, 5, 5, NA, NA, NA, 3, NA, NA, 2, 4, NA, N…\n$ homepart   <dbl> NA, 1, 1, NA, 1, NA, 1, 1, 1, 1, NA, 1, NA, NA, 1, 1, 1, 1,…\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((-69.89912 1..., MULTIPOLYGON (…\n\n\n\nWorld map:\n\nggplot(data = world) +\n    geom_sf() \n\n\n\n\n\n\nJust the US\nAccording to this link, these are the latitude and longitude bounds of the continental US:\n\ntop = 49.3457868 # north lat\nleft = -124.7844079 # west long\nright = -66.9513812 # east long\nbottom = 24.7433195 # south lat\n\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)\n\n\n\n\n\n\nAdding in our monitors…\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)+\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\")\n\n\n\n\n\n\nAdding in county lines\n\ncounties <- sf::st_as_sf(maps::map(\"county\", plot = FALSE,\n                                   fill = TRUE))\n\ncounties\n\nSimple feature collection with 3076 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.6813 ymin: 25.12993 xmax: -67.00742 ymax: 49.38323\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\nFirst 10 features:\n                               ID                           geom\nalabama,autauga   alabama,autauga MULTIPOLYGON (((-86.50517 3...\nalabama,baldwin   alabama,baldwin MULTIPOLYGON (((-87.93757 3...\nalabama,barbour   alabama,barbour MULTIPOLYGON (((-85.42801 3...\nalabama,bibb         alabama,bibb MULTIPOLYGON (((-87.02083 3...\nalabama,blount     alabama,blount MULTIPOLYGON (((-86.9578 33...\nalabama,bullock   alabama,bullock MULTIPOLYGON (((-85.66866 3...\nalabama,butler     alabama,butler MULTIPOLYGON (((-86.8604 31...\nalabama,calhoun   alabama,calhoun MULTIPOLYGON (((-85.74313 3...\nalabama,chambers alabama,chambers MULTIPOLYGON (((-85.59416 3...\nalabama,cherokee alabama,cherokee MULTIPOLYGON (((-85.46812 3...\n\n\n\n\nAnd now onto the map…\n\nmonitors <- ggplot(data = world) +\n    geom_sf(data = counties, fill = NA, color = gray(.5))+\n      coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE) +\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\") +\n    ggtitle(\"Monitor Locations\") +\n    theme(axis.title.x=element_blank(),\n          axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\nmonitors\n\n\n\n\n\n\nWrangle counties:\n\nseparate county and state into separate columns\nmake title case\ncombine with PM data\n\n\ncounties <- counties %>% \n  tidyr::separate(ID, into = c(\"state\", \"county\"), sep = \",\") %>% \n  dplyr::mutate(county = stringr::str_to_title(county))\n\nmap_data <- dplyr::inner_join(counties, pm, by = \"county\")"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#map-truth",
    "href": "content/lectures/17-cs02-analysis-slides.html#map-truth",
    "title": "17-cs02-analysis",
    "section": "Map: Truth",
    "text": "Map: Truth\n\nCodePlot\n\n\n\ntruth <- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = value)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"True PM 2.5 levels\") +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#map-predictions",
    "href": "content/lectures/17-cs02-analysis-slides.html#map-predictions",
    "title": "17-cs02-analysis",
    "section": "Map: Predictions",
    "text": "Map: Predictions\n\nDataCodePlot\n\n\n\n# fit data\nRF_final_train_fit <- parsnip::fit(RF_tuned_wflow, data = train_pm)\nRF_final_test_fit <- parsnip::fit(RF_tuned_wflow, data = test_pm)\n\n# get predictions on training data\nvalues_pred_train <- predict(RF_final_train_fit, train_pm) %>% \n  bind_cols(train_pm %>% select(value, fips, county, id)) \n\n# get predictions on testing data\nvalues_pred_test <- predict(RF_final_test_fit, test_pm) %>% \n  bind_cols(test_pm %>% select(value, fips, county, id)) \nvalues_pred_test\n\n# A tibble: 292 × 5\n   .pred value fips  county     id       \n   <dbl> <dbl> <fct> <chr>      <fct>    \n 1  11.6  11.2 1033  Colbert    1033.1002\n 2  11.9  12.4 1055  Etowah     1055.001 \n 3  11.1  10.5 1069  Houston    1069.0003\n 4  13.9  15.6 1073  Jefferson  1073.0023\n 5  12.0  12.4 1073  Jefferson  1073.1005\n 6  11.3  11.1 1073  Jefferson  1073.1009\n 7  11.5  11.8 1073  Jefferson  1073.5003\n 8  11.0  10.0 1097  Mobile     1097.0003\n 9  11.9  12.0 1101  Montgomery 1101.0007\n10  12.9  13.2 1113  Russell    1113.0001\n# ℹ 282 more rows\n\n# combine\nall_pred <- bind_rows(values_pred_test, values_pred_train)\n\n\n\n\nmap_data <- inner_join(counties, all_pred, by = \"county\")\n\npred <- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = .pred)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"Predicted PM 2.5 levels\") +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#final-plot",
    "href": "content/lectures/17-cs02-analysis-slides.html#final-plot",
    "title": "17-cs02-analysis",
    "section": "Final Plot",
    "text": "Final Plot\n\nlibrary(patchwork)\n\n(truth/pred) + \n  plot_annotation(title = \"Machine Learning Methods Allow for Prediction of Air Pollution\", subtitle = \"A random forest model predicts true monitored levels of fine particulate matter (PM 2.5) air pollution based on\\ndata about population density and other predictors reasonably well, thus suggesting that we can use similar methods to predict levels\\nof pollution in places with poor monitoring\",\n                  theme = theme(plot.title = element_text(size =12, face = \"bold\"), \n                                plot.subtitle = element_text(size = 8)))\n\n\n\n\n❓ What do you learn from these results?\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html",
    "href": "content/lectures/17-cs02-analysis.html",
    "title": "17-cs02-analysis",
    "section": "",
    "text": "Q: Just to confirm, if we choose to do the CS02 do we still do it with our final group or the assigned CS02 group?\nA: Final group!\n\n\nQ: If we choose to do the final project instead of CS2, will everything be the same as our originally planned final projects in terms of contents except the grade weighting?\nA: Yup!\n\n\nQ: Understanding some of the variable’s meaning, maybe would have been good for us to recode them to make them more intuitive.\nA: A good suggestion if you decide to go this route for the final!\n\n\nQ: For the distribution on the diagonal line of the correlation graphs, what does x/y axis represent?\nA: It’s a densityplot (shows the distribution) for each individual variable - the one that’s in that row.\n\n\n\n\n\nCS01 due tonight (group work survey due Friday)\nLab07 due tomorrow (Friday)\nFinal Project due Tues of Finals week (report + presentation + general communication)\n\n\n\nAny CS01 questions?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#qa",
    "href": "content/lectures/17-cs02-analysis.html#qa",
    "title": "17-cs02-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Just to confirm, if we choose to do the CS02 do we still do it with our final group or the assigned CS02 group?\nA: Final group!\n\n\nQ: If we choose to do the final project instead of CS2, will everything be the same as our originally planned final projects in terms of contents except the grade weighting?\nA: Yup!\n\n\nQ: Understanding some of the variable’s meaning, maybe would have been good for us to recode them to make them more intuitive.\nA: A good suggestion if you decide to go this route for the final!\n\n\nQ: For the distribution on the diagonal line of the correlation graphs, what does x/y axis represent?\nA: It’s a densityplot (shows the distribution) for each individual variable - the one that’s in that row."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#course-announcements",
    "href": "content/lectures/17-cs02-analysis.html#course-announcements",
    "title": "17-cs02-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nCS01 due tonight (group work survey due Friday)\nLab07 due tomorrow (Friday)\nFinal Project due Tues of Finals week (report + presentation + general communication)\n\n\n\nAny CS01 questions?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#recall",
    "href": "content/lectures/17-cs02-analysis.html#recall",
    "title": "17-cs02-analysis",
    "section": "Recall:",
    "text": "Recall:"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#the-data",
    "href": "content/lectures/17-cs02-analysis.html#the-data",
    "title": "17-cs02-analysis",
    "section": "The Data",
    "text": "The Data\n\npm <- read_csv(here::here(\"OCS_data\", \"data\", \"raw\", \"pm25_data.csv\"))\n\nRows: 876 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): state, county, city\ndbl (47): id, value, fips, lat, lon, CMAQ, zcta, zcta_area, zcta_pop, imp_a5...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Converting to factors as discussed last class\npm <- pm %>%\n  dplyr::mutate(across(c(id, fips, zcta), as.factor))"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#data-splitting",
    "href": "content/lectures/17-cs02-analysis.html#data-splitting",
    "title": "17-cs02-analysis",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n\n\n\nSpecify the split:\n\nset.seed(1234)\npm_split <- rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n<Training/Testing/Total>\n<584/292/876>\n\n\n\nset.seed <- ensures we all get the exact same random split\noutput displayed: <training data sample number, testing data sample number, original sample number>\n\nMore on how people decide what proportions to use for data splitting here\n\n\nSplit the Data\n\ntrain_pm <- rsample::training(pm_split)\ntest_pm <- rsample::testing(pm_split)\n \n# Scroll through the output!\ncount(train_pm, state)\n\n# A tibble: 49 × 2\n   state                    n\n   <chr>                <int>\n 1 Alabama                 13\n 2 Arizona                 12\n 3 Arkansas                 8\n 4 California              55\n 5 Colorado                10\n 6 Connecticut             12\n 7 Delaware                 3\n 8 District Of Columbia     2\n 9 Florida                 22\n10 Georgia                 20\n# ℹ 39 more rows\n\n\n❓ What do you observe about the output?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#pre-processing-recipe-bake",
    "href": "content/lectures/17-cs02-analysis.html#pre-processing-recipe-bake",
    "title": "17-cs02-analysis",
    "section": "Pre-processing: recipe() + bake()",
    "text": "Pre-processing: recipe() + bake()\nNeed to:\n\nspecify predictors vs. outcome\nscale variables\nremove redundant variables (feature engineering)\n\n\nrecipe provides a standardized format for a sequence of steps for pre-processing the data"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-1-specify-variable-roles",
    "href": "content/lectures/17-cs02-analysis.html#step-1-specify-variable-roles",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify variable roles",
    "text": "Step 1: Specify variable roles\nThe simplest approach…\n\nsimple_rec <- train_pm %>%\n  recipes::recipe(value ~ .)\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 49\n\n\n\n…but we need to specify which column includes ID information\n\nsimple_rec <- train_pm %>%\n  recipes::recipe(value ~ .) %>%\n  recipes::update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n…and which are our predictors and which is our outcome\n\nsimple_rec <- recipe(train_pm) %>%\n    update_role(everything(), new_role = \"predictor\") %>%\n    update_role(value, new_role = \"outcome\") %>%\n    update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n❓ Can someone summarize what this code is specifying?\n\n\nSummarizing our recipe thus far:\n\nsummary(simple_rec)\n\n# A tibble: 50 × 4\n   variable type      role        source  \n   <chr>    <list>    <chr>       <chr>   \n 1 id       <chr [3]> id variable original\n 2 value    <chr [2]> outcome     original\n 3 fips     <chr [3]> predictor   original\n 4 lat      <chr [2]> predictor   original\n 5 lon      <chr [2]> predictor   original\n 6 state    <chr [3]> predictor   original\n 7 county   <chr [3]> predictor   original\n 8 city     <chr [3]> predictor   original\n 9 CMAQ     <chr [2]> predictor   original\n10 zcta     <chr [3]> predictor   original\n# ℹ 40 more rows"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-2-specify-pre-processing-with-step",
    "href": "content/lectures/17-cs02-analysis.html#step-2-specify-pre-processing-with-step",
    "title": "17-cs02-analysis",
    "section": "Step 2: Specify pre-processing with step*()",
    "text": "Step 2: Specify pre-processing with step*()\n\n\n\n\nThere are step functions for a variety of purposes:\n\nImputation – filling in missing values based on the existing data\nTransformation – changing all values of a variable in the same way, typically to make it more normal or easier to interpret\nDiscretization – converting continuous values into discrete or nominal values - binning for example to reduce the number of possible levels (However this is generally not advisable!)\nEncoding / Creating Dummy Variables – creating a numeric code for categorical variables (More on one-hot and Dummy Variables encoding)\nData type conversions – which means changing from integer to factor or numeric to date etc.\nInteraction term addition to the model – which means that we would be modeling for predictors that would influence the capacity of each other to predict the outcome\nNormalization – centering and scaling the data to a similar range of values\nDimensionality Reduction/ Signal Extraction – reducing the space of features or predictors to a smaller set of variables that capture the variation or signal in the original variables (ex. Principal Component Analysis and Independent Component Analysis)\nFiltering – filtering options for removing variables (ex. remove variables that are highly correlated to others or remove variables with very little variance and therefore likely little predictive capacity)\nRow operations – performing functions on the values within the rows (ex. rearranging, filtering, imputing)\nChecking functions – Gut checks to look for missing values, to look at the variable classes etc.\n\nThis link and this link show the many options for recipe step functions.\n\n\nThere are several ways to select what variables to apply steps to:\n\nUsing tidyselect methods: contains(), matches(), starts_with(), ends_with(), everything(), num_range()\n\nUsing the type: all_nominal(), all_numeric() , has_type()\nUsing the role: all_predictors(), all_outcomes(), has_role()\nUsing the name - use the actual name of the variable/variables of interest\n\n\n\nOne-hot encoding categorical variables:\n\nsimple_rec %>%\n  step_dummy(state, county, city, zcta, one_hot = TRUE)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: state, county, city, zcta\n\n\n❓ Can anyone remind us what one-hot encoding does?\n\n\n\nfips includes numeric code for state and county, so it’s another way to specify county\nso, we’ll change fips’ role\nwe get to decide what to call it (\"county id\")\n\n\nsimple_rec %>%\n  update_role(\"fips\", new_role = \"county id\")\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   47\ncounty id:    1\nid variable:  1\n\n\n\n\nRemoving highly correlated variables:\n\nsimple_rec %>%\n  step_corr(all_predictors(), - CMAQ, - aod)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: all_predictors(), -CMAQ, -aod\n\n\n\nspecifying to KEEP CMAQ and aod\n\n\n\nRemoving variables with non-zero variance:\n\nsimple_rec %>%\n  step_nzv(all_predictors(), - CMAQ, - aod)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Sparse, unbalanced variable filter on: all_predictors(), -CMAQ, -aod"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#putting-our-recipe-together",
    "href": "content/lectures/17-cs02-analysis.html#putting-our-recipe-together",
    "title": "17-cs02-analysis",
    "section": "Putting our recipe together",
    "text": "Putting our recipe together\n\nsimple_rec <- simple_rec %>% \n  update_role(\"fips\", new_role = \"county id\") %>%\n  step_dummy(state, county, city, zcta, one_hot = TRUE) %>%\n  step_corr(all_predictors(), - CMAQ, - aod)%>%\n  step_nzv(all_predictors(), - CMAQ, - aod)\n  \nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   47\ncounty id:    1\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: state, county, city, zcta\n\n\n• Correlation filter on: all_predictors(), -CMAQ, -aod\n\n\n• Sparse, unbalanced variable filter on: all_predictors(), -CMAQ, -aod\n\n\nNote: order of steps matters"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-3-running-the-pre-processing-prep",
    "href": "content/lectures/17-cs02-analysis.html#step-3-running-the-pre-processing-prep",
    "title": "17-cs02-analysis",
    "section": "Step 3: Running the pre-processing (prep)",
    "text": "Step 3: Running the pre-processing (prep)\nThere are some important arguments to know about:\n\ntraining - you must supply a training data set to estimate parameters for pre-processing operations (recipe steps) - this may already be included in your recipe - as is the case for us\nfresh - if fresh=TRUE, will retrain and estimate parameters for any previous steps that were already prepped if you add more steps to the recipe (default is FALSE)\nverbose - if verbose=TRUE, shows the progress as the steps are evaluated and the size of the pre-processed training set (default is FALSE)\nretain - if retain=TRUE, then the pre-processed training set will be saved within the recipe (as template). This is good if you are likely to add more steps and do not want to rerun the prep() on the previous steps. However this can make the recipe size large. This is necessary if you want to actually look at the pre-processed data (default is TRUE)\n\n\nprepped_rec <- prep(simple_rec, verbose = TRUE, retain = TRUE )\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.26 Mb  in memory.\n\nnames(prepped_rec)\n\n [1] \"var_info\"       \"term_info\"      \"steps\"          \"template\"      \n [5] \"levels\"         \"retained\"       \"requirements\"   \"tr_info\"       \n [9] \"orig_lvls\"      \"last_term_info\"\n\n\n\nThis output includes a lot of information:\n\nthe steps that were run\n\nthe original variable info (var_info)\n\nthe updated variable info after pre-processing (term_info)\nthe new levels of the variables\nthe original levels of the variables (orig_lvls)\ninfo about the training data set size and completeness (tr_info)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-4-extract-pre-processed-training-data-using-bake",
    "href": "content/lectures/17-cs02-analysis.html#step-4-extract-pre-processed-training-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 4: Extract pre-processed training data using bake()",
    "text": "Step 4: Extract pre-processed training data using bake()\n\n\n\nbake(): apply our modeling steps (in this case just pre-processing on the training data) and see what it would do the data\n\n\nbaked_train <- bake(prepped_rec, new_data = NULL)\nglimpse(baked_train)\n\nRows: 584\nColumns: 37\n$ id                          <fct> 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       <dbl> 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        <fct> 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         <dbl> 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         <dbl> -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        <dbl> 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   <dbl> 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    <dbl> 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    <dbl> 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  <dbl> 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 <dbl> 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  <dbl> 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          <dbl> 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         <dbl> 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        <dbl> 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       <dbl> 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      <dbl> 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      <dbl> 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     <dbl> 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_nei_2008_pm10_sum_10000 <dbl> 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 <dbl> 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 <dbl> 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              <dbl> 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                <dbl> 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        <dbl> 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      <dbl> 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          <dbl> 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 <dbl> 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   <dbl> 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    <dbl> 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        <dbl> 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         <dbl> 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   <dbl> 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     <dbl> 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         <dbl> 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            <dbl> 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n\nnew_data = NULL specifies that we’re not (yet) looking at our testing data\nWe only have 36 variables (33 predictors + 2 id variables + outcome)\ncategorical variables (state) are gone (one-hot encoding)\nstate_California remains - only state with nonzero variance (largest # of monitors)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-5-extract-pre-processed-testing-data-using-bake",
    "href": "content/lectures/17-cs02-analysis.html#step-5-extract-pre-processed-testing-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 5: Extract pre-processed testing data using bake()",
    "text": "Step 5: Extract pre-processed testing data using bake()\n\nbake() takes a trained recipe and applies the operations to a data set to create a design matrix. For example: it applies the centering to new data sets using these means used to create the recipe. - tidymodels documentation\n\n\nTypically, you want to avoid using your testing data…but our data set is not that large and NA values in our testing dataset could cause issues later on.\n\n\n\n\n\n\nbaked_test_pm <- recipes::bake(prepped_rec, new_data = test_pm)\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 37\n$ id                          <fct> 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       <dbl> 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        <fct> 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         <dbl> 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         <dbl> -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        <dbl> 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   <dbl> 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    <dbl> 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    <dbl> 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  <dbl> 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 <dbl> 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  <dbl> 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          <dbl> 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         <dbl> 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        <dbl> 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       <dbl> 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      <dbl> 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      <dbl> 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     <dbl> 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_nei_2008_pm10_sum_10000 <dbl> 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 <dbl> 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 <dbl> 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              <dbl> 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                <dbl> 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        <dbl> 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      <dbl> 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          <dbl> 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 <dbl> 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   <dbl> 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    <dbl> 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        <dbl> 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         <dbl> 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   <dbl> 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     <dbl> 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         <dbl> 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          <dbl> NA, NA, NA, 0, 1, 1, 1, NA, NA, NA, 0, NA,…\n\n\n\n\nHmm….lots of NAs now in city_Not.in.a.city\nLikely b/c there are cities in our testing dataset that were not in our training dataset…\n\ntraincities <- train_pm %>% distinct(city)\ntestcities <- test_pm %>% distinct(city)\n\n#get the number of cities that were different\ndim(dplyr::setdiff(traincities, testcities))\n\n[1] 381   1\n\n#get the number of cities that overlapped\ndim(dplyr::intersect(traincities, testcities))\n\n[1] 55  1"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#aside-return-to-wrangling",
    "href": "content/lectures/17-cs02-analysis.html#aside-return-to-wrangling",
    "title": "17-cs02-analysis",
    "section": "Aside: return to wrangling",
    "text": "Aside: return to wrangling\nA quick return to wrangling…and re-splitting our data\n\npm <- pm %>%\n  mutate(city = case_when(city == \"Not in a city\" ~ \"Not in a city\",\n                          city != \"Not in a city\" ~ \"In a city\"))\n\nset.seed(1234) # same seed as before\npm_split <-rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n<Training/Testing/Total>\n<584/292/876>\n\n train_pm <-rsample::training(pm_split)\n test_pm <-rsample::testing(pm_split)\n\n\nAnd a recipe update…(putting it all together)\n\nnovel_rec <- recipe(train_pm) %>%\n    update_role(everything(), new_role = \"predictor\") %>%\n    update_role(value, new_role = \"outcome\") %>%\n    update_role(id, new_role = \"id variable\") %>%\n    update_role(\"fips\", new_role = \"county id\") %>%\n    step_dummy(state, county, city, zcta, one_hot = TRUE) %>%\n    step_corr(all_numeric()) %>%\n    step_nzv(all_numeric()) \n\n\n\nre-bake()\n\nprepped_rec <- prep(novel_rec, verbose = TRUE, retain = TRUE)\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.27 Mb  in memory.\n\nbaked_train <- bake(prepped_rec, new_data = NULL)\n\n\n\nLooking at the output\n\nglimpse(baked_train)\n\nRows: 584\nColumns: 38\n$ id                          <fct> 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       <dbl> 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        <fct> 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         <dbl> 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         <dbl> -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        <dbl> 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   <dbl> 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    <dbl> 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    <dbl> 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  <dbl> 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 <dbl> 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  <dbl> 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          <dbl> 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         <dbl> 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        <dbl> 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       <dbl> 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      <dbl> 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      <dbl> 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     <dbl> 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_prisec_length_25000     <dbl> 13.98749, 13.15082, 13.44293, 13.58697, 14…\n$ log_nei_2008_pm10_sum_10000 <dbl> 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 <dbl> 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 <dbl> 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              <dbl> 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                <dbl> 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        <dbl> 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      <dbl> 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          <dbl> 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 <dbl> 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   <dbl> 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    <dbl> 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        <dbl> 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         <dbl> 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   <dbl> 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     <dbl> 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         <dbl> 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            <dbl> 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n\n\nMaking sure the NA issue is taken are of:\n\nbaked_test_pm <- bake(prepped_rec, new_data = test_pm)\n\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 38\n$ id                          <fct> 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       <dbl> 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        <fct> 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         <dbl> 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         <dbl> -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        <dbl> 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   <dbl> 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    <dbl> 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    <dbl> 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  <dbl> 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 <dbl> 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  <dbl> 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          <dbl> 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         <dbl> 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        <dbl> 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       <dbl> 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      <dbl> 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      <dbl> 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     <dbl> 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_prisec_length_25000     <dbl> 13.79973, 13.70026, 13.85550, 14.45221, 13…\n$ log_nei_2008_pm10_sum_10000 <dbl> 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 <dbl> 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 <dbl> 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              <dbl> 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                <dbl> 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        <dbl> 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      <dbl> 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          <dbl> 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 <dbl> 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   <dbl> 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    <dbl> 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        <dbl> 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         <dbl> 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   <dbl> 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     <dbl> 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         <dbl> 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          <dbl> 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#specifying-our-model-parsnip",
    "href": "content/lectures/17-cs02-analysis.html#specifying-our-model-parsnip",
    "title": "17-cs02-analysis",
    "section": "Specifying our model (parsnip)",
    "text": "Specifying our model (parsnip)\nThere are four things we need to define about our model:\n\n\nThe type of model (using specific functions in parsnip like rand_forest(), logistic_reg() etc.)\n\nThe package or engine that we will use to implement the type of model selected (using the set_engine() function)\nThe mode of learning - classification or regression (using the set_mode() function)\nAny arguments necessary for the model/package selected (using the set_args()function - for example the mtry = argument for random forest which is the number of variables to be used as options for splitting at each tree node)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-1-specify-the-model",
    "href": "content/lectures/17-cs02-analysis.html#step-1-specify-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify the model",
    "text": "Step 1: Specify the model\n\nWe’ll start with linear regression, but move to random forest\nSee here for modeling options in parsnip.\n\n\nlm_PM_model <- parsnip::linear_reg() %>%\n  parsnip::set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nlm_PM_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-2-fit-the-model",
    "href": "content/lectures/17-cs02-analysis.html#step-2-fit-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 2: Fit the model",
    "text": "Step 2: Fit the model\n\nworkflows package allows us to keep track of both our pre-processing steps and our model specification\nIt also allows us to implement fancier optimizations in an automated way and it can also handle post-processing operations.\n\n\nPM_wflow <- workflows::workflow() %>%\n            workflows::add_recipe(novel_rec) %>%\n            workflows::add_model(lm_PM_model)\nPM_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n❓ Who can explain the difference between a recipe, baking, and a workflow?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "href": "content/lectures/17-cs02-analysis.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "title": "17-cs02-analysis",
    "section": "Step 3: Prepare the recipe (estimate the parameters)",
    "text": "Step 3: Prepare the recipe (estimate the parameters)\n\nPM_wflow_fit <- parsnip::fit(PM_wflow, data = train_pm)\nPM_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                (Intercept)                          lat  \n                  2.936e+02                    3.261e-02  \n                        lon                         CMAQ  \n                  1.586e-02                    2.463e-01  \n                  zcta_area                     zcta_pop  \n                 -3.433e-10                    1.013e-05  \n                   imp_a500                   imp_a15000  \n                  5.064e-03                   -3.066e-03  \n                county_area                   county_pop  \n                 -2.324e-11                   -7.576e-08  \n         log_dist_to_prisec          log_pri_length_5000  \n                  6.214e-02                   -2.006e-01  \n       log_pri_length_25000        log_prisec_length_500  \n                 -5.411e-02                    2.204e-01  \n     log_prisec_length_1000       log_prisec_length_5000  \n                  1.154e-01                    2.374e-01  \n    log_prisec_length_10000      log_prisec_length_25000  \n                 -3.436e-02                    5.224e-01  \nlog_nei_2008_pm10_sum_10000  log_nei_2008_pm10_sum_15000  \n                  1.829e-01                   -2.355e-02  \nlog_nei_2008_pm10_sum_25000               popdens_county  \n                  2.403e-02                    2.203e-05  \n               popdens_zcta                         nohs  \n                 -2.132e-06                   -2.983e+00  \n                     somehs                           hs  \n                 -2.956e+00                   -2.962e+00  \n                somecollege                    associate  \n                 -2.967e+00                   -2.999e+00  \n                   bachelor                         grad  \n                 -2.979e+00                   -2.978e+00  \n                        pov                    hs_orless  \n                  1.859e-03                           NA  \n                    urc2006                          aod  \n                  2.577e-01                    1.535e-02  \n           state_California           city_Not.in.a.city  \n                  3.114e+00                   -4.250e-02"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-4-assess-model-fit",
    "href": "content/lectures/17-cs02-analysis.html#step-4-assess-model-fit",
    "title": "17-cs02-analysis",
    "section": "Step 4: Assess model fit",
    "text": "Step 4: Assess model fit\n\nwflowoutput <- PM_wflow_fit %>% \n  extract_fit_parsnip() %>% \n  broom::tidy() \n\nwflowoutput\n\n# A tibble: 36 × 5\n   term         estimate std.error statistic       p.value\n   <chr>           <dbl>     <dbl>     <dbl>         <dbl>\n 1 (Intercept)  2.94e+ 2  1.18e+ 2     2.49  0.0130       \n 2 lat          3.26e- 2  2.28e- 2     1.43  0.153        \n 3 lon          1.59e- 2  1.01e- 2     1.58  0.115        \n 4 CMAQ         2.46e- 1  3.97e- 2     6.20  0.00000000108\n 5 zcta_area   -3.43e-10  1.60e-10    -2.15  0.0320       \n 6 zcta_pop     1.01e- 5  5.33e- 6     1.90  0.0578       \n 7 imp_a500     5.06e- 3  7.42e- 3     0.683 0.495        \n 8 imp_a15000  -3.07e- 3  1.16e- 2    -0.263 0.792        \n 9 county_area -2.32e-11  1.97e-11    -1.18  0.238        \n10 county_pop  -7.58e- 8  9.29e- 8    -0.815 0.415        \n# ℹ 26 more rows\n\n\n\nWe have fit our model on our training data\nWe have created a model to predict values of air pollution based on the predictors that we have included\n\n\nUnderstanding what variables are most important in our model…\n\nPM_wflow_fit %>% \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 10)\n\n\n\n\n\n\nA closer look at monitors in CA:\n\nbaked_train %>% \n  mutate(state_California = as.factor(state_California)) %>%\n  mutate(state_California = recode(state_California, \n                                   \"0\" = \"Not California\", \n                                   \"1\" = \"California\")) %>%\n  ggplot(aes(x = state_California, y = value)) + \n  geom_boxplot() +\n  geom_jitter(width = .05) + \n  xlab(\"Location of Monitor\")\n\n\n\n\n\n\nRemember: machine learning (ML) as an optimization problem that tries to minimize the distance between our predicted outcome \\(\\hat{Y} = f(X)\\) and actual outcome \\(Y\\) using our features (or predictor variables) \\(X\\) as input to a function \\(f\\) that we want to estimate.\n\\[d(Y - \\hat{Y})\\]\n\n\nLet’s pull out our predicted outcome values \\(\\hat{Y} = f(X)\\) from the models we fit (using different approaches).\n\nwf_fit <- PM_wflow_fit %>% \n  extract_fit_parsnip()\n\n\nwf_fitted_values <- \n  broom::augment(wf_fit[[\"fit\"]], data = baked_train) %>% \n  select(value, .fitted:.std.resid)\n\nhead(wf_fitted_values)\n\n# A tibble: 6 × 6\n  value .fitted   .hat .sigma   .cooksd .std.resid\n  <dbl>   <dbl>  <dbl>  <dbl>     <dbl>      <dbl>\n1 11.7    12.2  0.0370   2.05 0.0000648     -0.243\n2  6.96    9.14 0.0496   2.05 0.00179       -1.09 \n3 13.3    12.6  0.0484   2.05 0.000151       0.322\n4 10.7    10.4  0.0502   2.05 0.0000504      0.183\n5 14.5    11.9  0.0243   2.05 0.00113        1.26 \n6 12.2     9.52 0.476    2.04 0.0850         1.81"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#visualizing-model-performance",
    "href": "content/lectures/17-cs02-analysis.html#visualizing-model-performance",
    "title": "17-cs02-analysis",
    "section": "Visualizing Model Performance",
    "text": "Visualizing Model Performance\n\nwf_fitted_values %>% \n  ggplot(aes(x =  value, y = .fitted)) + \n  geom_point() + \n  xlab(\"actual outcome values\") + \n  ylab(\"predicted outcome values\")\n\n\n\n\n❓ What do you notice about/learn from these results?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#quantifying-model-performance",
    "href": "content/lectures/17-cs02-analysis.html#quantifying-model-performance",
    "title": "17-cs02-analysis",
    "section": "Quantifying Model Performance",
    "text": "Quantifying Model Performance\n\\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}{(\\hat{y_t}- y_t)}^2}{n}}\\]\n\nCan use the yardstick package using the rmse()` function to calculate:\n\nyardstick::metrics(wf_fitted_values,\n                   truth = value, estimate = .fitted)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       1.98 \n2 rsq     standard       0.392\n3 mae     standard       1.47 \n\n\n\nRMSE isn’t too bad\n\\(R^2\\) suggests model is only explaining 39% of the variance in the data\nThe MAE value suggests that the average difference between the value predicted and the real value was 1.47 ug/m3. The range of the values was 3-22 in the training data, so this is a relatively small amount"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#cross-validation",
    "href": "content/lectures/17-cs02-analysis.html#cross-validation",
    "title": "17-cs02-analysis",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nResampling + Re-partitioning:\n\n\n\n\nPreparing the data for cross-validation:\n\n\n\nNote: this is called v-fold or k-fold CV"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#implementing-in-rsample",
    "href": "content/lectures/17-cs02-analysis.html#implementing-in-rsample",
    "title": "17-cs02-analysis",
    "section": "Implementing in rsample()",
    "text": "Implementing in rsample()\n\nset.seed(1234)\nvfold_pm <- rsample::vfold_cv(data = train_pm, v = 4)\nvfold_pm\n\n#  4-fold cross-validation \n# A tibble: 4 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [438/146]> Fold1\n2 <split [438/146]> Fold2\n3 <split [438/146]> Fold3\n4 <split [438/146]> Fold4\n\n\n\n\npull(vfold_pm, splits)\n\n[[1]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n[[2]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n[[3]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n[[4]]\n<Analysis/Assess/Total>\n<438/146/584>\n\n\n\n\nVisualizing this process:"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#model-assessment-on-v-folds",
    "href": "content/lectures/17-cs02-analysis.html#model-assessment-on-v-folds",
    "title": "17-cs02-analysis",
    "section": "Model Assessment on v-folds",
    "text": "Model Assessment on v-folds\nWhere this workflow thing really shines…\n\nresample_fit <- tune::fit_resamples(PM_wflow, vfold_pm)\n\n→ A | warning: the standard deviation is zero, The correlation matrix has missing values. 415 columns were excluded from the filter.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: There are new levels in a factor: Maine, There are new levels in a factor: Forest, Mecklenburg, Clermont, Camden, Trumbull, Yellowstone, Caddo, Hinds, Codington, Preble, Broward, Rowan, Beaver, Dauphin, Buncombe, LaPorte, Ashley, Clayton, Talladega, Queens, Jones, Mitchell, Kalamazoo, Seminole, Henderson, Sussex, Ingham, Sangamon, Aroostook, Muscogee, Plumas, Dodge, Bennington, Sumner, Butler, Butte, Passaic, Page, Custer, Sainte Genevieve, Bullitt, Palo Alto, Rapides, Faulkner, San Francisco, Ravalli, San Mateo, Delaware, Davis, Fremont, Santa Clara, White, Carter, DeSoto, Wilkinson, Muscatine, Hampden, Yakima, Solano, Mendocino, Mobile, Roanoke City, Wake, Gwinnett, Alamance, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n→ C | warning: the standard deviation is zero, The correlation matrix has missing values. 408 columns were excluded from the filter.\nThere were issues with some computations   A: x1\n→ D | warning: There are new levels in a factor: Athens, Kent, Linn, Stark, Cabell, Arlington, St. Lucie, Grafton, Champaign, Brewster, Morgan, Lawrence, Tarrant, Yolo, Weber, Mille Lacs, Clarke, Harrison, Will, Grant, Morris, Santa Cruz, Taylor, Klamath, Prince George's, Howard, Buchanan, Cedar, Ventura, Monongalia, Bolivar, Medina, Dona Ana, Hancock, Missoula, Chittenden, Monroe, Knox, Essex, Pierce, Tuscaloosa, Ellis, Contra Costa, Apache, Harris, Edgecombe, Stearns, Outagamie, Escambia, Hidalgo, Teton, Loudoun, Belknap, Sauk, Pittsburg, Charles, Gibson, Marshall, Chester, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1\n→ E | warning: the standard deviation is zero, The correlation matrix has missing values. 417 columns were excluded from the filter.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1\n→ F | warning: There are new levels in a factor: Nevada, North Dakota, There are new levels in a factor: Placer, Bay, Niagara, DeKalb, Hampton City, Oconee, Spencer, Sutter, Cobb, Randolph, Anne Arundel, Houston, Kane, Genesee, Dane, Yavapai, Lenawee, Washtenaw, Durham, Scioto, Henry, Spartanburg, Harney, Converse, Portage, St. Croix, Colusa, Berkshire, Lenoir, Lancaster, Haywood, Iberville, Adams, Catawba, St. Clair, Lynchburg City, Nassau, Brookings, Raleigh, Summit, Sebastian, Ouachita, Westmoreland, Rock Island, Duplin, Erie, Burleigh, Vilas, Kanawha, Rutland, San Joaquin, Washoe, Sandoval, Josephine, Kenosha, Plymouth, Stanislaus, Caswell, Cameron, Lucas, Sarpy, West Baton Rouge, Mayes, Cass, Chautauqua, Terrebonne, Sweetwater, Glynn, Harford, Spokane, La Salle, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1\n→ G | warning: There are new levels in a factor: Allen, McDowell, Macon, Chaves, Yuma, Dougherty, Flathead, Ashland, Manistee, Hartford, Park, Box Elder, East Baton Rouge, Chesterfield, Woodbury, Bell, Citrus, New London, Cumberland, Fairfax, Forrest, Allegan, Ohio, Pueblo, Gaston, Bernalillo, Sullivan, Nevada, McLean, McCracken, Potter, Mahoning, Porter, Albemarle, Manitowoc, Shawnee, Ocean, Ottawa, El Paso, Baldwin, Bannock, Cheshire, Clay, Jersey, Brown, Lexington, Clinton, Peoria, Macomb, Davidson, Tooele, Dubois, Robeson, St. Lawrence, Lincoln, Virginia Beach City, Shelby, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1   E: x…\n\n\n\nGives us a sense of the RMSE across the four folds:\n\ntune::show_best(resample_fit, metric = \"rmse\")\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard    2.12     4  0.0444 Preprocessor1_Model1"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#random-forest",
    "href": "content/lectures/17-cs02-analysis.html#random-forest",
    "title": "17-cs02-analysis",
    "section": "Random Forest",
    "text": "Random Forest\nFitting a different model\n\nBased on a decision tree:\n\n\n\n\n\n\n[source]\n\nBut…in the case of random forest:\n\n\nmultiple decision trees are created (hence: forest),\neach tree is built using a random subset of the training data (with replacement) (hence: random)\nhelps to keep the algorithm from overfitting the data\nThe mean of the predictions from each of the trees is used in the final output."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#updating-our-recipe",
    "href": "content/lectures/17-cs02-analysis.html#updating-our-recipe",
    "title": "17-cs02-analysis",
    "section": "Updating our recipe()",
    "text": "Updating our recipe()\n\nRF_rec <- recipe(train_pm) %>%\n    update_role(everything(), new_role = \"predictor\")%>%\n    update_role(value, new_role = \"outcome\")%>%\n    update_role(id, new_role = \"id variable\") %>%\n    update_role(\"fips\", new_role = \"county id\") %>%\n    step_novel(\"state\") %>%\n    step_string2factor(\"state\", \"county\", \"city\") %>%\n    step_rm(\"county\") %>%\n    step_rm(\"zcta\") %>%\n    step_corr(all_numeric())%>%\n    step_nzv(all_numeric())\n\n\ncan use our categorical data as is (no dummy coding)\nstep_novel()necessary here for the state variable to get all cross validation folds to work, (b/c there will be different levels included in each fold test and training sets. The new levels for some of the test sets would otherwise result in an error.; “step_novel creates a specification of a recipe step that will assign a previously unseen factor level to a new value.”"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#model-specification",
    "href": "content/lectures/17-cs02-analysis.html#model-specification",
    "title": "17-cs02-analysis",
    "section": "Model Specification",
    "text": "Model Specification\nModel parameters:\n\nmtry - The number of predictor variables (or features) that will be randomly sampled at each split when creating the tree models. The default number for regression analyses is the number of predictors divided by 3.\nmin_n - The minimum number of data points in a node that are required for the node to be split further.\ntrees - the number of trees in the ensemble\n\n\n\n# install.packages(\"randomForest\")\nRF_PM_model <- parsnip::rand_forest(mtry = 10, min_n = 3) %>% \n  set_engine(\"randomForest\") %>%\n  set_mode(\"regression\")\n\nRF_PM_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#workflow",
    "href": "content/lectures/17-cs02-analysis.html#workflow",
    "title": "17-cs02-analysis",
    "section": "Workflow",
    "text": "Workflow\n\nRF_wflow <- workflows::workflow() %>%\n  workflows::add_recipe(RF_rec) %>%\n  workflows::add_model(RF_PM_model)\n\nRF_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#fit-the-data",
    "href": "content/lectures/17-cs02-analysis.html#fit-the-data",
    "title": "17-cs02-analysis",
    "section": "Fit the Data",
    "text": "Fit the Data\n\nRF_wflow_fit <- parsnip::fit(RF_wflow, data = train_pm)\n\nRF_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, mtry = min_cols(~10,      x), nodesize = min_rows(~3, x)) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 10\n\n          Mean of squared residuals: 2.633639\n                    % Var explained: 59.29"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#assess-feature-importance",
    "href": "content/lectures/17-cs02-analysis.html#assess-feature-importance",
    "title": "17-cs02-analysis",
    "section": "Assess Feature Importance",
    "text": "Assess Feature Importance\n\nRF_wflow_fit %>% \n  extract_fit_parsnip() %>% \n  vip::vip(num_features = 10)\n\n\n\n\n❓ What’s your interpretation of these results?"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#assess-model-performance",
    "href": "content/lectures/17-cs02-analysis.html#assess-model-performance",
    "title": "17-cs02-analysis",
    "section": "Assess Model Performance",
    "text": "Assess Model Performance\n\nset.seed(456)\nresample_RF_fit <- tune::fit_resamples(RF_wflow, vfold_pm)\ncollect_metrics(resample_RF_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   1.67      4  0.101  Preprocessor1_Model1\n2 rsq     standard   0.591     4  0.0514 Preprocessor1_Model1\n\n\n\nFor comparison:\n\ncollect_metrics(resample_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   2.12      4  0.0444 Preprocessor1_Model1\n2 rsq     standard   0.307     4  0.0263 Preprocessor1_Model1"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#model-tuning",
    "href": "content/lectures/17-cs02-analysis.html#model-tuning",
    "title": "17-cs02-analysis",
    "section": "Model Tuning",
    "text": "Model Tuning\nHyperparameters are often things that we need to specify about a model. Instead of arbitrarily specifying this, we can try to determine the best option for model performance by a process called tuning.\n\nRather than specifying values, we can use tune():\n\ntune_RF_model <- rand_forest(mtry = tune(), min_n = tune()) %>%\n  set_engine(\"randomForest\") %>%\n  set_mode(\"regression\")\n    \ntune_RF_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\n\n\nCreate Workflow:\n\nRF_tune_wflow <- workflows::workflow() %>%\n  workflows::add_recipe(RF_rec) %>%\n  workflows::add_model(tune_RF_model)\n\nRF_tune_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\nDetect how many cores you have access to:\n\nn_cores <- parallel::detectCores()\nn_cores\n\n[1] 10\n\n\n\n\nThis code will take some time to run:\n\n# install.packages(\"doParallel\")\ndoParallel::registerDoParallel(cores = n_cores)\n\nset.seed(123)\ntune_RF_results <- tune_grid(object = RF_tune_wflow, resamples = vfold_pm, grid = 20)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntune_RF_results\n\n# Tuning results\n# 4-fold cross-validation \n# A tibble: 4 × 4\n  splits            id    .metrics          .notes          \n  <list>            <chr> <list>            <list>          \n1 <split [438/146]> Fold1 <tibble [40 × 6]> <tibble [0 × 3]>\n2 <split [438/146]> Fold2 <tibble [40 × 6]> <tibble [0 × 3]>\n3 <split [438/146]> Fold3 <tibble [40 × 6]> <tibble [1 × 3]>\n4 <split [438/146]> Fold4 <tibble [40 × 6]> <tibble [0 × 3]>\n\nThere were issues with some computations:\n\n  - Warning(s) x1: 36 columns were requested but there were 35 predictors in the dat...\n\nRun `show_notes(.Last.tune.result)` for more information."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#check-metrics",
    "href": "content/lectures/17-cs02-analysis.html#check-metrics",
    "title": "17-cs02-analysis",
    "section": "Check Metrics:",
    "text": "Check Metrics:\n\ntune_RF_results %>%\n  collect_metrics()\n\n# A tibble: 40 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1    12    33 rmse    standard   1.72      4  0.0866 Preprocessor1_Model01\n 2    12    33 rsq     standard   0.562     4  0.0466 Preprocessor1_Model01\n 3    27    35 rmse    standard   1.69      4  0.102  Preprocessor1_Model02\n 4    27    35 rsq     standard   0.563     4  0.0511 Preprocessor1_Model02\n 5    22    40 rmse    standard   1.71      4  0.106  Preprocessor1_Model03\n 6    22    40 rsq     standard   0.556     4  0.0543 Preprocessor1_Model03\n 7     1    27 rmse    standard   2.03      4  0.0501 Preprocessor1_Model04\n 8     1    27 rsq     standard   0.440     4  0.0245 Preprocessor1_Model04\n 9     6    32 rmse    standard   1.77      4  0.0756 Preprocessor1_Model05\n10     6    32 rsq     standard   0.552     4  0.0435 Preprocessor1_Model05\n# ℹ 30 more rows\n\n\n\n\nshow_best(tune_RF_results, metric = \"rmse\", n = 1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1    32    11 rmse    standard    1.65     4   0.113 Preprocessor1_Model10"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#final-model-evaluation",
    "href": "content/lectures/17-cs02-analysis.html#final-model-evaluation",
    "title": "17-cs02-analysis",
    "section": "Final Model Evaluation",
    "text": "Final Model Evaluation\n\ntuned_RF_values<- select_best(tune_RF_results, \"rmse\")\ntuned_RF_values\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1    32    11 Preprocessor1_Model10\n\n\n\nThe testing data!\n\n# specify best combination from tune in workflow\nRF_tuned_wflow <-RF_tune_wflow %>%\n  tune::finalize_workflow(tuned_RF_values)\n\n# fit model with those parameters on train AND test\noverallfit <- RF_wflow %>%\n  tune::last_fit(pm_split)\n\ncollect_metrics(overallfit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       1.72  Preprocessor1_Model1\n2 rsq     standard       0.608 Preprocessor1_Model1\n\n\nResults are similar to what we saw in training (RMSE: 1.65)\n\n\nGetting the predictions for the test data:\n\ntest_predictions <- collect_predictions(overallfit)"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#a-map-of-the-us",
    "href": "content/lectures/17-cs02-analysis.html#a-map-of-the-us",
    "title": "17-cs02-analysis",
    "section": "A map of the US",
    "text": "A map of the US\nPackages needed:\n\nsf - the simple features package helps to convert geographical coordinates into geometry variables which are useful for making 2D plots\nmaps - this package contains geographical outlines and plotting functions to create plots with maps\nrnaturalearth- this allows for easy interaction with map data from Natural Earth which is a public domain map dataset\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(rnaturalearth)\n\nSupport for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#outline-of-the-us",
    "href": "content/lectures/17-cs02-analysis.html#outline-of-the-us",
    "title": "17-cs02-analysis",
    "section": "Outline of the US",
    "text": "Outline of the US\n\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\")\nglimpse(world)\n\nRows: 241\nColumns: 64\n$ scalerank  <int> 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 5, 3, 1, 1, 1, 1, 1, 1,…\n$ featurecla <chr> \"Admin-0 country\", \"Admin-0 country\", \"Admin-0 country\", \"A…\n$ labelrank  <dbl> 5, 3, 3, 6, 6, 6, 6, 4, 2, 6, 4, 4, 5, 6, 6, 2, 4, 5, 6, 2,…\n$ sovereignt <chr> \"Netherlands\", \"Afghanistan\", \"Angola\", \"United Kingdom\", \"…\n$ sov_a3     <chr> \"NL1\", \"AFG\", \"AGO\", \"GB1\", \"ALB\", \"FI1\", \"AND\", \"ARE\", \"AR…\n$ adm0_dif   <dbl> 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,…\n$ level      <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ type       <chr> \"Country\", \"Sovereign country\", \"Sovereign country\", \"Depen…\n$ admin      <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ adm0_a3    <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ geou_dif   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ geounit    <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ gu_a3      <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ su_dif     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ subunit    <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ su_a3      <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_diff   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ name       <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_long  <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_a3     <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_name   <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_group  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ abbrev     <chr> \"Aruba\", \"Afg.\", \"Ang.\", \"Ang.\", \"Alb.\", \"Aland\", \"And.\", \"…\n$ postal     <chr> \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AI\", \"AND\", \"AE\", \"AR\", \"ARM…\n$ formal_en  <chr> \"Aruba\", \"Islamic State of Afghanistan\", \"People's Republic…\n$ formal_fr  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ note_adm0  <chr> \"Neth.\", NA, NA, \"U.K.\", NA, \"Fin.\", NA, NA, NA, NA, \"U.S.A…\n$ note_brk   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Multiple claim…\n$ name_sort  <chr> \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_alt   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ mapcolor7  <dbl> 4, 5, 3, 6, 1, 4, 1, 2, 3, 3, 4, 4, 1, 7, 2, 1, 3, 1, 2, 3,…\n$ mapcolor8  <dbl> 2, 6, 2, 6, 4, 1, 4, 1, 1, 1, 5, 5, 2, 5, 2, 2, 1, 6, 2, 2,…\n$ mapcolor9  <dbl> 2, 8, 6, 6, 1, 4, 1, 3, 3, 2, 1, 1, 2, 9, 5, 2, 3, 5, 5, 1,…\n$ mapcolor13 <dbl> 9, 7, 1, 3, 6, 6, 8, 3, 13, 10, 1, NA, 7, 11, 5, 7, 4, 8, 8…\n$ pop_est    <dbl> 103065, 28400000, 12799293, 14436, 3639453, 27153, 83888, 4…\n$ gdp_md_est <dbl> 2258.0, 22270.0, 110300.0, 108.9, 21810.0, 1563.0, 3660.0, …\n$ pop_year   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ lastcensus <dbl> 2010, 1979, 1970, NA, 2001, NA, 1989, 2010, 2010, 2001, 201…\n$ gdp_year   <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ economy    <chr> \"6. Developing region\", \"7. Least developed region\", \"7. Le…\n$ income_grp <chr> \"2. High income: nonOECD\", \"5. Low income\", \"3. Upper middl…\n$ wikipedia  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fips_10    <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso_a2     <chr> \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AX\", \"AD\", \"AE\", \"AR\", \"AM\",…\n$ iso_a3     <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ iso_n3     <chr> \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ un_a3      <chr> \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ wb_a2      <chr> \"AW\", \"AF\", \"AO\", NA, \"AL\", NA, \"AD\", \"AE\", \"AR\", \"AM\", \"AS…\n$ wb_a3      <chr> \"ABW\", \"AFG\", \"AGO\", NA, \"ALB\", NA, \"ADO\", \"ARE\", \"ARG\", \"A…\n$ woe_id     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_is <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_us <chr> \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_un <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_wb <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ continent  <chr> \"North America\", \"Asia\", \"Africa\", \"North America\", \"Europe…\n$ region_un  <chr> \"Americas\", \"Asia\", \"Africa\", \"Americas\", \"Europe\", \"Europe…\n$ subregion  <chr> \"Caribbean\", \"Southern Asia\", \"Middle Africa\", \"Caribbean\",…\n$ region_wb  <chr> \"Latin America & Caribbean\", \"South Asia\", \"Sub-Saharan Afr…\n$ name_len   <dbl> 5, 11, 6, 8, 7, 5, 7, 20, 9, 7, 14, 10, 23, 22, 17, 9, 7, 1…\n$ long_len   <dbl> 5, 11, 6, 8, 7, 13, 7, 20, 9, 7, 14, 10, 27, 35, 19, 9, 7, …\n$ abbrev_len <dbl> 5, 4, 4, 4, 4, 5, 4, 6, 4, 4, 9, 4, 7, 10, 6, 4, 5, 4, 4, 5…\n$ tiny       <dbl> 4, NA, NA, NA, NA, 5, 5, NA, NA, NA, 3, NA, NA, 2, 4, NA, N…\n$ homepart   <dbl> NA, 1, 1, NA, 1, NA, 1, 1, 1, 1, NA, 1, NA, NA, 1, 1, 1, 1,…\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((-69.89912 1..., MULTIPOLYGON (…\n\n\n\nWorld map:\n\nggplot(data = world) +\n    geom_sf() \n\n\n\n\n\n\nJust the US\nAccording to this link, these are the latitude and longitude bounds of the continental US:\n\ntop = 49.3457868 # north lat\nleft = -124.7844079 # west long\nright = -66.9513812 # east long\nbottom = 24.7433195 # south lat\n\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)\n\n\n\n\n\n\nAdding in our monitors…\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)+\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\")\n\n\n\n\n\n\nAdding in county lines\n\ncounties <- sf::st_as_sf(maps::map(\"county\", plot = FALSE,\n                                   fill = TRUE))\n\ncounties\n\nSimple feature collection with 3076 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.6813 ymin: 25.12993 xmax: -67.00742 ymax: 49.38323\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\nFirst 10 features:\n                               ID                           geom\nalabama,autauga   alabama,autauga MULTIPOLYGON (((-86.50517 3...\nalabama,baldwin   alabama,baldwin MULTIPOLYGON (((-87.93757 3...\nalabama,barbour   alabama,barbour MULTIPOLYGON (((-85.42801 3...\nalabama,bibb         alabama,bibb MULTIPOLYGON (((-87.02083 3...\nalabama,blount     alabama,blount MULTIPOLYGON (((-86.9578 33...\nalabama,bullock   alabama,bullock MULTIPOLYGON (((-85.66866 3...\nalabama,butler     alabama,butler MULTIPOLYGON (((-86.8604 31...\nalabama,calhoun   alabama,calhoun MULTIPOLYGON (((-85.74313 3...\nalabama,chambers alabama,chambers MULTIPOLYGON (((-85.59416 3...\nalabama,cherokee alabama,cherokee MULTIPOLYGON (((-85.46812 3...\n\n\n\n\nAnd now onto the map…\n\nmonitors <- ggplot(data = world) +\n    geom_sf(data = counties, fill = NA, color = gray(.5))+\n      coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE) +\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\") +\n    ggtitle(\"Monitor Locations\") +\n    theme(axis.title.x=element_blank(),\n          axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\nmonitors\n\n\n\n\n\n\nWrangle counties:\n\nseparate county and state into separate columns\nmake title case\ncombine with PM data\n\n\ncounties <- counties %>% \n  tidyr::separate(ID, into = c(\"state\", \"county\"), sep = \",\") %>% \n  dplyr::mutate(county = stringr::str_to_title(county))\n\nmap_data <- dplyr::inner_join(counties, pm, by = \"county\")"
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#map-truth",
    "href": "content/lectures/17-cs02-analysis.html#map-truth",
    "title": "17-cs02-analysis",
    "section": "Map: Truth",
    "text": "Map: Truth\n\nCodePlot\n\n\n\ntruth <- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = value)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"True PM 2.5 levels\") +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#map-predictions",
    "href": "content/lectures/17-cs02-analysis.html#map-predictions",
    "title": "17-cs02-analysis",
    "section": "Map: Predictions",
    "text": "Map: Predictions\n\nDataCodePlot\n\n\n\n# fit data\nRF_final_train_fit <- parsnip::fit(RF_tuned_wflow, data = train_pm)\nRF_final_test_fit <- parsnip::fit(RF_tuned_wflow, data = test_pm)\n\n# get predictions on training data\nvalues_pred_train <- predict(RF_final_train_fit, train_pm) %>% \n  bind_cols(train_pm %>% select(value, fips, county, id)) \n\n# get predictions on testing data\nvalues_pred_test <- predict(RF_final_test_fit, test_pm) %>% \n  bind_cols(test_pm %>% select(value, fips, county, id)) \nvalues_pred_test\n\n# A tibble: 292 × 5\n   .pred value fips  county     id       \n   <dbl> <dbl> <fct> <chr>      <fct>    \n 1  11.6  11.2 1033  Colbert    1033.1002\n 2  11.9  12.4 1055  Etowah     1055.001 \n 3  11.1  10.5 1069  Houston    1069.0003\n 4  13.9  15.6 1073  Jefferson  1073.0023\n 5  12.0  12.4 1073  Jefferson  1073.1005\n 6  11.3  11.1 1073  Jefferson  1073.1009\n 7  11.5  11.8 1073  Jefferson  1073.5003\n 8  11.0  10.0 1097  Mobile     1097.0003\n 9  11.9  12.0 1101  Montgomery 1101.0007\n10  12.9  13.2 1113  Russell    1113.0001\n# ℹ 282 more rows\n\n# combine\nall_pred <- bind_rows(values_pred_test, values_pred_train)\n\n\n\n\nmap_data <- inner_join(counties, all_pred, by = \"county\")\n\npred <- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = .pred)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"Predicted PM 2.5 levels\") +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one."
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#final-plot",
    "href": "content/lectures/17-cs02-analysis.html#final-plot",
    "title": "17-cs02-analysis",
    "section": "Final Plot",
    "text": "Final Plot\n\nlibrary(patchwork)\n\n(truth/pred) + \n  plot_annotation(title = \"Machine Learning Methods Allow for Prediction of Air Pollution\", subtitle = \"A random forest model predicts true monitored levels of fine particulate matter (PM 2.5) air pollution based on\\ndata about population density and other predictors reasonably well, thus suggesting that we can use similar methods to predict levels\\nof pollution in places with poor monitoring\",\n                  theme = theme(plot.title = element_text(size =12, face = \"bold\"), \n                                plot.subtitle = element_text(size = 8)))\n\n\n\n\n❓ What do you learn from these results?"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "COGS 137",
    "section": "",
    "text": "Week\nDate\nTitle\nType\n\n\n\n\n0\nTh Sep 28\nWelcome & Tooling\nLecture\n\n\n1\nTu Oct 3\nIntro to R\nLecture\n\n\n1\nTh Oct 5\nData Wrangling: dplyr\nLecture\n\n\n1\nFri Oct 6\nLab 01: Intro to R\nLab\n\n\n2\nTu Oct 10\nData Wrangling: tidyr\nLecture\n\n\n2\nTh Oct 12\nData Visualization: ggplot2 (day 1)\nLecture\n\n\n2\nFri Oct 13\nLab 02: Data Wrangling\nLab\n\n\n3\nMon Oct 16\nHW01 due (11:59 PM)\nHW\n\n\n3\nTu Oct 17\nData Visualization: ggplot2 (day 2)\nLecture\n\n\n3\nTh Oct 19\nData Analysis & Modeling\nLecture\n\n\n3\nFri Oct 20\nLab 03: Data Visualization\nLab\n\n\n4\nTu Oct 24\nLinear Models Review\nLecture\n\n\n4\nTh Oct 26\nEffective Communication\nLecture\n\n\n4\nFri Oct 27\nLab 04: Modeling\nLab\n\n\n5\nMon Oct 30\nHW02 due (11:59 PM)\nHW\n\n\n5\nTu Oct 31\nMultiple Linear Regression*\nLecture\n\n\n5\nTh Nov 2\nCase Study & Final Project Info\nLecture\n\n\n5\nFri Nov 3\nLab used for midterm review\nLab\n\n\n6\nMon Nov 6\nMIDTERM EXAM (due 11:59 PM) \nExam\n\n\n6\nTu Nov 7\nCase Study 01: THC Biomarkers (day 1)\nLecture\n\n\n6\nTh Nov 9\nCase Study 01: THC Biomarkers (day 2)\nLecture\n\n\n6\nFri Nov 10\nLab 05: Multiple Linear Regression\nLab\n\n\n7\nTu Nov 14\nCase Study 01: THC Biomarkers (day 3)\nLecture\n\n\n7\nTh Nov 16\ntidymodels\nLecture\n\n\n7\nSun Nov 19\nLab 06: CS01 [Note: Due date was extended to Sunday]\nLab\n\n\n8\nMon Nov 20\nHW03 due (11:59 PM)\nHW\n\n\n8\nMon Nov 20\nFinal Project Proposal Due\nProject\n\n\n8\nTu Nov 21\nCase Study 02: Air Pollution (day 1)\nLecture\n\n\n8\nTh Nov 23\nNo Class (Thanksgiving)\n--\n\n\n9\nTu Nov 28\nCase Study 02: Air Pollution (day 2)\nLecture\n\n\n9\nTh Nov 30\nCase Study 02: Air Pollution (day 3)\nLecture\n\n\n9\nTh Nov 30\nCS01 Due (11:59 PM)\nCase Study\n\n\n9\nFri Dec 1\nLab 07: CS02\nLab\n\n\n10\nTu Dec 5\nFinal Project Brainstorming\nLecture\n\n\n10\nTh Dec 7\nNext Steps\nLecture\n\n\n10\nFri Dec 8\nLab 08: Final Project\nLab\n\n\nFinals\nTu Dec 12\nFinal Project Due (11:59 PM)\nProject"
  },
  {
    "objectID": "content/final/final.html",
    "href": "content/final/final.html",
    "title": "Final Project",
    "section": "",
    "text": "For the final project, you and your group mates (groups of 3-4 people) get to choose one of the following two options: 1) Technical Presentation or 2) Data Analysis.\nEach group will be provided with a private repo that all members as well as course instructional staff will have access to. Final projects will be “submitted” by pushing the project requirements to the group repo by the deadline.\nPresentations will be submitted on Canvas.\nWritten, visual, and presented content will be graded on their technical merits as well as the effectiveness of their communication."
  },
  {
    "objectID": "content/final/final.html#option-1-technical-presentation",
    "href": "content/final/final.html#option-1-technical-presentation",
    "title": "Final Project",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\nGroups who choose the technical presentation route will make slides for a presentation that effectively communicates/teaches an advanced statistical topic1 and/or an R package2.\n\nDeliverable: R Markdown + Slides\nSlides will be required for the presentation and they must be generated from either an RMarkdown document or a quarto document. Chapters 4, 7, and 8 of the R Markdown: The Definitive Guide discussion options for generating slides/presentations from R Markdown documents. (Presentations from quarto have similar documentation here.) Students should commit both the .Rmd (or .qmd) document and the rendered slides to their GitHub repo.\nThis presentation must teach the details of the R package, the statistical topic, or both at a level appropriate for students in this course. (i.e. You can assume your audience knows how to program in R, know about the tidyverse, know linear regression, etc.) And, you must demonstrate how to use the package and/or carry out the statistical analysis in R.\n\n\nDeliverable: Presentation\nStudents must also present their slides in a presentation that is 10-15min long. This presentation will be pre-recorded and submitted on Canvas. For this option, all students must participate in the presentation.\n\n\nDeliverable: General Communication\nThis will be a communication targeted to the people who you think should know about this package/statistical analysis. Here, you can assume your audience knows about R/data analysis in general, but you want to distill your presentation down to the most important aspect someone would want/need to know if they were going to use what you’ve chosen to present on."
  },
  {
    "objectID": "content/final/final.html#option-2-data-analysis",
    "href": "content/final/final.html#option-2-data-analysis",
    "title": "Final Project",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\nGroups who choose the data analysis route will carry out a full data science project. This will include question formation, finding the data, doing background research, wrangling the data, doing EDA, analyzing the data, and answering your question of interest.\nYou can think of this as a mini case report in the fact that the process is the same, but we would not expect the data wrangling to be quite as extensive as what was done in the case studies. That said, we want to see demonstration of the skills you’ve learned in the class, so we will be looking for some data wrangling in your case study. If you have a single dataset that requires no wrangling, consider if additional datasets could be incorporated to answer your question(s) of interest more deeply.\nYou are strongly encouraged to think of your topic/question before looking for datasets. More interesting case studies start with the topic/question. Boring case studies look for the dataset first.\n\nDeliverable: Report (.Rmd + HTML)\nYour analysis will be submitted as an .Rmd document and rendered to HTML (both of which should be pushed to GitHub).\nThis will likely not be quite as long as a case study in this course, but will likely have the same sections.\n\n\nDeliverable: Presentation\nStudents must present their case study in a presentation that is 3-5min long. What you use to visually support this presentation (slides, or something else) is up to you but should follow the effective communication aspects discussed in class. This presentation will pre-recorded and submitted on Canvas. For this option, at least one group member must present the project (in other words, not everyone has to “speak” but everyone in the group is responsible for the contents).\n\n\nDeliverable: General Communication\nThis will be a communication targeted to the general public (non-technical, non-data scientists) conveying the most important finding(s) from your project."
  },
  {
    "objectID": "content/final/final.html#option-3-cs02-additional-data",
    "href": "content/final/final.html#option-3-cs02-additional-data",
    "title": "Final Project",
    "section": "Option 3: CS02 + Additional Data",
    "text": "Option 3: CS02 + Additional Data\nStudents can choose to carry out CS02 for their final project; however, students will have to find an additional dataset on a related topic (pollution, climate change, etc.) and incorporate that into the ir final report. See CS02 documentation for details on report and general communication deliverables.\n\nDeliverable: Presentation\nStudents must also present their project in a presentation that is 10-15min long. This presentation will be pre-recorded and submitted on Canvas. For this option, all students must participate in the presentation."
  },
  {
    "objectID": "content/final/final.html#group-feedback",
    "href": "content/final/final.html#group-feedback",
    "title": "Final Project",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the final project to provide feedback about working with your group mates. As with the case studies, this is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary."
  },
  {
    "objectID": "content/cs/cs02.html",
    "href": "content/cs/cs02.html",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "",
    "text": "Important\n\n\n\nCS02 is not required fa23 quarter. Students have the option to complete CS02 in lieu of the typical final project. This will be completed in your final project groups and will require use of some outside source of data.\nThis is your second case study report, so you get to incorporate the general feedback from cs01 and carry out another complete data science project! This report will include your analysis from top (the background and question) to bottom (your analysis, interpretation, and conclusions.)\nWe’ll be grading to see that you have: 1) all necessary code for each section of the project; 2) explanatory text that guides the reader from start to finish; 3) polished visualizations that allow the reader to both understand the data you’re working with an your conclusions.\nThis will be submitted and graded as a group. One submission per group."
  },
  {
    "objectID": "content/cs/cs02.html#getting-started",
    "href": "content/cs/cs02.html#getting-started",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nThis will be completed in cs02 group repository that has been created for you and your group mates.\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit and commit changes (for example, once per each new part)1\nPush all your changes back to your GitHub repo\nThis case study will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading.\n\nImports\nYou are allowed to import whichever packages you like for this case study report."
  },
  {
    "objectID": "content/cs/cs02.html#case-study-report",
    "href": "content/cs/cs02.html#case-study-report",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "Case Study Report",
    "text": "Case Study Report\nYour case study can be organized however you see best fit, but we’ll be looking for the following general sections:\n\nTitle\nAuthors\nBackground/Introduction\nQuestion(s)\nData\n\nData Explanation\nData Import\nData Wrangling\n\nAnalysis\n\nExploratory Data Analysis\nData Analysis\n\nResults\nDiscussion of results\nConclusion\n\nNow, you may want to combine some of these sections (i.e. include your results and discussion among your analysis code). That’s totally allowed, but we’ll be looking to see that your report includes sufficient information to understand what you did, why you did it, and what your results are.\n\nRequired Questions\nAll groups will analyze the data and answer the following question in their report:\nCan we predict US annual average air pollution concentrations at the granularity of zip code regional levels using predictors such as data about population density, urbanization, road density, as well as, satellite pollution data and chemical modeling data?\n\n\nExtending the Analysis\nIn addition to getting the code presented in class working, adding explanatory text to your report, and generating polished visualizations, you and your group must “extend the analysis” presented in class in a meaningful way. Now “meaningful” is not a very-easily-measured term. A meaningful extension could be carrying out analysis to answer an additional sub-question beyond what was presented in class, or including a really extensive exploratory data analysis, including data from additional years, and/or or generating a really superb set of visualizations to convey your groups’ results, or finding a related dataset and incorporating it into your case study. To determine whether your extension is “meaningful,” you and your group should be able to answer “yes” to the question “Does our extension add something important to this report beyond what was presented in class?”\nThis extension should be included/weaved into your report, meaning it should only be “separated out” as its own section if it makes most sense for the story you’re telling."
  },
  {
    "objectID": "content/cs/cs02.html#general-communication",
    "href": "content/cs/cs02.html#general-communication",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "General Communication",
    "text": "General Communication\nEach group will need to convey the most important finding(s) to a general audience through some form of communication.\nThis is very open-ended in its format. It could be a short video, an infographic, an effective email, a graphic, instagram slides, a short presentation, etc. It will be submitted by one group member on Canvas. (All group members will receive credit.)\nThe specific audience you want to target can be specified (i.e. undergraduate students, policy makers, local government officials, etc.); however, the assumption is that these are NOT data scientists.\nYour communication SHOULD include your take-home message…and that may be all it includes! Basically, we want you to distill down your case study to its most important message and then convey that to the general public in an effective manner.\nIt should NOT contain specifics of your analysis or anywhere near all the information included in your report."
  },
  {
    "objectID": "content/cs/cs02.html#group-feedback",
    "href": "content/cs/cs02.html#group-feedback",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the case study to provide feedback about working with your group mates. This is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary. This form is “due” 24h after the case study, to give you time to reflect/complete your feedback after completing the case study itself."
  },
  {
    "objectID": "content/labs/03-lab-viz.html",
    "href": "content/labs/03-lab-viz.html",
    "title": "Lab 03 - Data Visualization",
    "section": "",
    "text": "A note on expectations: For each exercise, include any relevant output (tables, summary statistics, plots) in your answer along with text to guide the reader. Place any relevant R code in a code chunk, any relevant text outside of code chunks, and hit Knit HTML.\nSome define statistics as the field that focuses on turning information into knowledge. The first step in that process is to summarize and describe raw information - the data. In this lab we explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While outside the scope of this lab, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads |>\n  arrange(unemployment_rate)\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\nNote that your output here likely has a whole bunch of decimal places in the unemployment variable? You likely don’t want all those values to be displayed.\nThere are two ways we can address this problem. One would be to round the unemployment_rate variable in the dataset or we can change the number of digits displayed, without touching the input data.\nBelow are instructions for how you would do both of these:\n\nRound unemployment_rate: We create a new variable with the mutate function. In this case, we’re overwriting the existing unemployment_rate variable, by rounding it to 4 decimal places.For example, the call to mutate would be: mutate(unemployment_rate = round(unemployment_rate, digits = 4))\nChange displayed number of digits without touching data: We can add an option to our R Markdown document to change the displayed number of digits in the output. To do so, add a new chunk, and set:\n\n\noptions(digits = 2)\n\nNote that the digits in options is scientific digits, and in round they are decimal places. If you’re thinking “Wouldn’t it be nice if they were consistent?”, you’re right…\nYou don’t need to do both of these; that would be redundant. The next exercise asks you to choose one.\n\nExercise 1\nWhich of these options, changing the input data or altering the number of digits displayed without touching the input data, is the better option? Explain your reasoning. Then, implement the option you chose."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\n\nThe desc function specifies that we want unemployment_rate in descending order.\n\ncollege_recent_grads |>\n  arrange(desc(unemployment_rate)) |>\n  select(rank, major, unemployment_rate)\n\n\nExercise 2\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "Lab 03 - Data Visualization",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\n\n\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nExercise 3\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\n\nExercise 4\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\nWe can also calculate summary statistics for this distribution using the summarise function. Note here that you can calculate multiple summary statistics within a single summarise call:\n\ncollege_recent_grads |>\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n\n\nExercise 5\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\n\n\nExercise 6\nNow, plot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\n\nExercise 7\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Also note that we are looking for the highest statistic, so make sure if you arrange to do so in the correct direction.\n\n\nExercise 8\nWhich major category is the least popular in this sample?"
  },
  {
    "objectID": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "href": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "title": "Lab 03 - Data Visualization",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories <- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads <- college_recent_grads |>\n  mutate(major_type = case_when(major_category %in% stem_categories ~ \"stem\",\n                                TRUE ~ \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx < y\nless than\n\n\nx > y\ngreater than\n\n\nx <= y\nless than or equal to\n\n\nx >= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads |>\n  filter(\n    major_type == \"stem\",\n    median < 36000\n  )\n\n\nExercise 9\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "Lab 03 - Data Visualization",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nExercise 10\nCreate a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "content/lectures/00-welcome.html",
    "href": "content/lectures/00-welcome.html",
    "title": "00-welcome",
    "section": "",
    "text": "Practical Data Science in R\nPlease take one green sticky and one pink sticky as they come around. If you’re able, try and save these. We’ll use them most classes. (But, I’ll always have extra!)\n\n\n\n\n\n\n\n\nDescribe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub\n\n\n\n\n : R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data.\n\n\n\n: Data science is the scientific process of using data to answer interesting questions and/or solve important problems.\n\n\n\n\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports\n\n\n\n\n\nShannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com    MOS 0204     Tu/Th 2-3:20PM (Lab: Fri 3-3:50PM)\n\n\n\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11A-12P\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTime TBD\nLocation TBD\n\n\nIAs\nShenova Davis\n\nTime TBD\nLocation TBD\n\n\n\n\n\n\n\n\n\nKunal Rustagi (TA)\nShenova Davis (IA)\n\n\n\n\n\n\n\n\n\n\n\n\nEverything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!\n\n\n\n\n\n\n\n\n\nNope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-general-plan",
    "href": "content/lectures/00-welcome.html#the-general-plan",
    "title": "00-welcome",
    "section": "The General Plan",
    "text": "The General Plan\n\nWeeks 1-4: Learn to program in the tidyverse in R\nWeeks 5-10: Communication, Data Analysis, Statistics, & Case Studies (two Case Studies)\n\n\nNote: This course is back-loaded. But, that’s when group work happens."
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-nitty-gritty",
    "href": "content/lectures/00-welcome.html#the-nitty-gritty",
    "title": "00-welcome",
    "section": "The Nitty Gritty",
    "text": "The Nitty Gritty\n\nLectureIn-personWaitlistLab & OHMaterials\n\n\nClass Meetings\n\nInteractive\nLectures & lots of learn-by-doing\nBring your laptop to class every day\n\n\n\nIn-person, synchronous learning\n\nI will be teaching (so long as I’m healthy and have child care) in person.\nLectures and lab will be podcast.\nAttendance will be incentivized using a daily participation survey.\nIf you’re not feeling well, please stay home. I will do the same.\nExam will be take-home.\n\n\n\nThe (Dreaded) Waitlist\n\nCourse enrollment is supposed to be 50 for this course\nThere are 72 people currently enrolled\nI don’t control the waitlist (cogsadvising@ucsd.edu does)\nI’d anticipate our staff adding 3-5 people from the waitlist (but cannot guarantee this)\n\n\n\nLab & Office Hours\n\nOffice hours begin week 1\n\nProf: Tu: 3:30-4:30 (drop-in); W 11-12 (10 min slots; appt.)\n\nLab begins week 1 (next Friday)\n\nit’s not in a computer lab, so you’ll need to bring your own\ndetails about labs covered on Tues and in lab\ntypically labs will be released Monday and due Friday\n\nI will hang out after class today for questions/concerns from students\n\n\n\nCourse Materials\n\nTextbooks are free and available online\nCourse platforms:\n\nWebsite : schedule, policies, due dates, etc.\nGitHub : retrieving assignments, labs, exams, etc.\ndatahub : completing assignments, labs, exams etc.\nCanvas : grades, course-specific links\nPiazza : Q&A"
  },
  {
    "objectID": "content/lectures/00-welcome.html#diversity-inclusion",
    "href": "content/lectures/00-welcome.html#diversity-inclusion",
    "title": "00-welcome",
    "section": "Diversity & Inclusion:",
    "text": "Diversity & Inclusion:\nGoal: every student be well-served by this course\n\nPhilosophy: The diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding.\n\n\nPlan: Present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture)\n\n\nBut… if I ever fall short or if you ever have suggestions for improvement, please do share with me! There is also an anonymous Google Form if you’re more comfortable there."
  },
  {
    "objectID": "content/lectures/00-welcome.html#a-new-ish-course",
    "href": "content/lectures/00-welcome.html#a-new-ish-course",
    "title": "00-welcome",
    "section": "A new-ish course!",
    "text": "A new-ish course!\n\nOffered twice previously\nIf something doesn’t make sense, tell me!\nIf you’ve got feedback/suggestions, I’m all ears!\n\n\nChanges since last iteration (based on feedback):\n\nspread out second half\nlikely changing the heaviness of a case study\nadd in communication to public portion\none fewer HW assignments"
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-to-get-help",
    "href": "content/lectures/00-welcome.html#how-to-get-help",
    "title": "00-welcome",
    "section": "How to get help",
    "text": "How to get help\n\nLab\nOffice Hours\nPiazza\n\n\nA few (Piazza) guidelines:\n1. No duplicates.\n2. Public posts are best.\n3. Posts should include your question, what you've tried so far, & resources used.\n4. Helping others is encouraged.\n5. No assignment code in public posts.\n6. We're not robots."
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-r-community",
    "href": "content/lectures/00-welcome.html#the-r-community",
    "title": "00-welcome",
    "section": " The R Community",
    "text": "The R Community\n\n\n\nR Rollercoaster\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome.html#academic-integrity",
    "href": "content/lectures/00-welcome.html#academic-integrity",
    "title": "00-welcome",
    "section": "Academic integrity",
    "text": "Academic integrity\nDon’t cheat.\n\nTeamwork is allowed, but you should be able to answer “Yes” to each of the following:\n\nCan I explain each piece of code and each analysis carried out in what I’m submitting?\nCould I reproduce this code/analysis on my own?\n\n\n\nThe Internet is a great resource. Cite your sources.\n\n\nTeamwork is not allowed on your midterm. It is open-notes and open-Google/ChatGPT. You cannot discuss the questions on the exam with anyone."
  },
  {
    "objectID": "content/lectures/00-welcome.html#when-to-can-i-use-chatgptllms",
    "href": "content/lectures/00-welcome.html#when-to-can-i-use-chatgptllms",
    "title": "00-welcome",
    "section": "When To (Can I) Use ChatGPT/LLMs?",
    "text": "When To (Can I) Use ChatGPT/LLMs?\nFor anything in this course."
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-to-use-chatgptllms",
    "href": "content/lectures/00-welcome.html#how-to-use-chatgptllms",
    "title": "00-welcome",
    "section": "How To Use ChatGPT/LLMs",
    "text": "How To Use ChatGPT/LLMs\nProbably never first or right away.\n\nTo learn: Think first. Try first. Then use external resources.\n\n\nAlways read/think about/understand the output."
  },
  {
    "objectID": "content/lectures/00-welcome.html#chatgpt-what-to-avoid",
    "href": "content/lectures/00-welcome.html#chatgpt-what-to-avoid",
    "title": "00-welcome",
    "section": "ChatGPT: What to Avoid",
    "text": "ChatGPT: What to Avoid\n\n\nOver-reliance (thwarts learning)\nHaving to look everything up (wastes time)\nLeaving tasks to the last minute (can lead to bad decisions/academic integrity issues)\nTaking the output without thinking (thwarts learning; limits critical thinking practice)\nUsing it right away for brainstorming ideas (limits ideas generated)"
  },
  {
    "objectID": "content/lectures/00-welcome.html#course-components",
    "href": "content/lectures/00-welcome.html#course-components",
    "title": "00-welcome",
    "section": "Course components:",
    "text": "Course components:\n\n\nLabs (8): Individual submission; graded on effort\nHomework (3): Individual submission; graded on correctness\nExam (1): Individual completion & submission, take-home midterm\nCase Studies (2): Team submission, technical analysis report\nFinal Project (1) : Team submission, due Tues of finals week"
  },
  {
    "objectID": "content/lectures/00-welcome.html#grading",
    "href": "content/lectures/00-welcome.html#grading",
    "title": "00-welcome",
    "section": "Grading",
    "text": "Grading\nYour final grade will be comprised of the following:\n\n\n\nAssignment (#)\n% of grade\n\n\n\n\nLabs (8)\n16%\n\n\nHomework (3)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal project* (1)\n17%\n\n\n\n* indicates group submission"
  },
  {
    "objectID": "content/lectures/00-welcome.html#latemissed-work-policy",
    "href": "content/lectures/00-welcome.html#latemissed-work-policy",
    "title": "00-welcome",
    "section": "Late/missed work policy",
    "text": "Late/missed work policy\n\nHomework and case study projects: accepted up to 3 days (72 hours) after the assigned deadline for a 25% deduction\nNo late deadlines for labs, the exam, or the final project\n\n\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter."
  },
  {
    "objectID": "content/lectures/00-welcome.html#datahub",
    "href": "content/lectures/00-welcome.html#datahub",
    "title": "00-welcome",
    "section": "Datahub",
    "text": "Datahub\nDatahub is a platform hosted by UCSD that gives students access to computational resources.\nThis means that while you’ll be typing on your keyboard, you’ll be using UCSD’s computers in this class.\nWebsite: https://datahub.ucsd.edu/\n\nLaunch Environment\nWhen working on “stuff” for this course, select the COGS 137 environment.\n ## Datahub Usage\nQ: Do I have to use datahub?\nA: Nope. You could download and install all the packages we use and complete the course locally! However, many packages have already been installed for you on datahub, so it will be a tiny bit more work up front…but you won’t be dependent on the internet/datahub!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#toolkit",
    "href": "content/lectures/00-welcome.html#toolkit",
    "title": "00-welcome",
    "section": "Toolkit",
    "text": "Toolkit\n\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub\nThe Internet (Google/ChatGPT/etc.)"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-and-rstudio",
    "href": "content/lectures/00-welcome.html#r-and-rstudio",
    "title": "00-welcome",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR/RStudioTourTryR packages\n\n\nR & RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integreated development environment, IDE)\n\n\n\n\n[DEMO]\n\nConcepts introduced:\n\nConsole\nUsing R as a calculator\nEnvironment\nLoading and viewing a data frame\nAccessing a variable in a data frame\nR functions\n\n\n\nYour Turn\n\nLogin to datahub\nCarry out a mathematical operation in the console\nView the airquality dataframe\nAccess a column from the airquality dataframe\nCalculate the median for one of the numeric columns\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\nPackages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data 1\nAs of Sept 2023, there are ~19,941 R packages available on CRAN (the Comprehensive R Archive Network)2\nWe’re going to work with a small (but important) subset of these!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "href": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "title": "00-welcome",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\n\n\n\n\n\ntidyverse.org\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying philosophy and a common syntax."
  },
  {
    "objectID": "content/lectures/00-welcome.html#rstudio-projects3",
    "href": "content/lectures/00-welcome.html#rstudio-projects3",
    "title": "00-welcome",
    "section": "RStudio Projects3",
    "text": "RStudio Projects3\n\nBuilt-in functionality to keep all files for a single project organized"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown",
    "href": "content/lectures/00-welcome.html#r-markdown",
    "title": "00-welcome",
    "section": "R Markdown",
    "text": "R Markdown\n\nFully reproducible reports – each time you knit, the document is executed from top to bottom\nSimple markdown syntax for text\nCode goes in chunks, defined by three backticks, narrative goes outside of chunks"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown-tips",
    "href": "content/lectures/00-welcome.html#r-markdown-tips",
    "title": "00-welcome",
    "section": "R Markdown tips",
    "text": "R Markdown tips\n\nKeep the R Markdown cheat sheet and Markdown Quick Reference (Help -> Markdown Quick Reference) handy, we’ll refer to it often as the course progresses\nThe workspace of your R Markdown document is separate from the Console\n\n\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "href": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "title": "00-welcome",
    "section": "How will we use R Markdown?",
    "text": "How will we use R Markdown?\n\nEvery lab / midterm / project / homework / notes / etc. is an R Markdown document\nYou’ll always have a template R Markdown document to start with\nThe amount of scaffolding in the template will decrease over the quarter"
  },
  {
    "objectID": "content/lectures/00-welcome.html#collaboration-git-github",
    "href": "content/lectures/00-welcome.html#collaboration-git-github",
    "title": "00-welcome",
    "section": "Collaboration: Git & GitHub",
    "text": "Collaboration: Git & GitHub\n\nThe statistical programming language we’ll use is R\nThe software we use to interface with R is RStudio\nBut how do I get you the course materials that you can build on for your assignments?\n\nI’m not going to email you documents, that would be a mess!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#version-control",
    "href": "content/lectures/00-welcome.html#version-control",
    "title": "00-welcome",
    "section": "Version control",
    "text": "Version control\n\nWe introduced GitHub as a platform for collaboration\nBut it’s much more than that…\nIt’s actually designed for version control"
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning",
    "href": "content/lectures/00-welcome.html#versioning",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\n\n\n\nLego versions"
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning-1",
    "href": "content/lectures/00-welcome.html#versioning-1",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\nwith human readable messages\n\n\n\nLego versions with commit messages"
  },
  {
    "objectID": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "href": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "title": "00-welcome",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\nPhD Comics"
  },
  {
    "objectID": "content/lectures/00-welcome.html#git-and-github-tips",
    "href": "content/lectures/00-welcome.html#git-and-github-tips",
    "title": "00-welcome",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\nGit is a version control system – like “Track Changes” feature Google Docs…but optimized for code. GitHub is the home for your Git-based projects on the internet – like Drive with additional features for code.\n\n\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\n\n\nWe will be doing Git things and interfacing with GitHub through RStudio, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\n\n\n\nResource: happygitwithr.com: book for working with git in R; Some content is beyond the scope of this course, but it’s a good resource"
  },
  {
    "objectID": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "href": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "title": "00-welcome",
    "section": "Let’s take a tour – Git / GitHub",
    "text": "Let’s take a tour – Git / GitHub\nWe’ll cover this time permitting, you’ll see it again in lab this week\nConcepts introduced:\n\nConnect an R project to Github repository\nWorking with a local and remote repository\nCommitting, Pushing and Pulling\n\nThere is a bit more of GitHub that we’ll use in this class, but for today this is enough."
  },
  {
    "objectID": "content/lectures/00-welcome.html#documentation",
    "href": "content/lectures/00-welcome.html#documentation",
    "title": "00-welcome",
    "section": "Documentation",
    "text": "Documentation\nConsider ggplot2 (a package we’ll learn a lot)\n\n\nOfficial documentation (CRAN): https://cran.r-project.org/web/packages/ggplot2/index.html\nCode (Github): https://github.com/tidyverse/ggplot2\nDocumentation: https://ggplot2.tidyverse.org/reference/index.html\nSpecific Function: https://ggplot2.tidyverse.org/reference/geom_point.html"
  },
  {
    "objectID": "content/lectures/00-welcome.html#chatgpt-what-it-could-look-like",
    "href": "content/lectures/00-welcome.html#chatgpt-what-it-could-look-like",
    "title": "00-welcome",
    "section": "ChatGPT: What it could look like",
    "text": "ChatGPT: What it could look like\nImagine: You’ve been asked to carry out a number of wrangling operations on a dataset and make a plot…\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome.html#additional-help",
    "href": "content/lectures/00-welcome.html#additional-help",
    "title": "00-welcome",
    "section": "Additional help",
    "text": "Additional help\n\nclassmates\ncourse staff (OH, Piazza, class, lab)"
  },
  {
    "objectID": "content/lectures/00-welcome.html#recap",
    "href": "content/lectures/00-welcome.html#recap",
    "title": "00-welcome",
    "section": "Recap",
    "text": "Recap\nCan you answer these questions?\n\nWhat is R vs RStudio?\nWhat are RStudio Projects?\nWhat is version control, and why do we care?\nWhat is git vs GitHub (and do I need to care)?"
  },
  {
    "objectID": "content/lectures/00-welcome.html#additional-git-resources",
    "href": "content/lectures/00-welcome.html#additional-git-resources",
    "title": "00-welcome",
    "section": "Additional git Resources",
    "text": "Additional git Resources\n\nVersion Control (git and GitHub):\n\nGetting Started with git\nGitHub Guide\nGitHub Desktop App Tutorial\nGit Command Line Resource\nUsing git from the command line\n\nInstalling and using git (Part 1), by COGS 108 TA Ganesh (youtube, 22min tutorial)\nmerge conflicts and branching (Part 2), by IA Shubham Kulkarni (youtube, 8min tutorial)\n\nUsing git with GitHub Desktop, by COGS 108 TA Sidharth Suresh (youtube, 13min tutorial)\nGIT & GITHUB TUTORIAL, from edureka!\n\nwith notes from COGS 18/108 TA Holly(Yueying) Dong"
  },
  {
    "objectID": "content/lectures/00-welcome.html#slides-to-pdf",
    "href": "content/lectures/00-welcome.html#slides-to-pdf",
    "title": "00-welcome",
    "section": "Slides to PDF",
    "text": "Slides to PDF\n\nToggle into Print View using the Esc key (or using the Navigation Menu)\nOpen the in-browser print dialog (CTRL/CMD+P).\nChange the Destination setting to Save as PDF.\nChange the Layout to Landscape.\nChange the Margins to None.\nEnable the Background graphics option.\nClick Save 🎉\n\n\n\nInstructions from quarto documentation"
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class",
    "href": "content/lectures/00-welcome.html#whos-in-this-class",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster <- read_sheet('10kG09t5Uvjy2zLt4sToHvveBRnqYXTwfaAhPpgEFr3s')\n\nggplot(roster, aes(x = College)) +\n  geom_bar() +\n  labs(title = \"COGS 137\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\nNote: This code will not run for you because you don’t have access to the roster for this course."
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  mutate(major = substr(Major, 1, 2)) |>\n  ggplot(aes(fct_infreq(major))) + \n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Major\") +\n  theme_bw(base_size = 12) + \n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  ggplot(aes(fct_relevel(Level, \"SO\", \"JR\", \"SR\"))) +\n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Level\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\nWarning: 1 unknown level in `f`: SO\n1 unknown level in `f`: SO"
  },
  {
    "objectID": "content/lectures/00-welcome.html#id-like-to-know-more",
    "href": "content/lectures/00-welcome.html#id-like-to-know-more",
    "title": "00-welcome",
    "section": "I’d like to know more!",
    "text": "I’d like to know more!\n(required)Student Survey - complete by Tuesday at 11:59 PM.\nThis is required and completion will be used for CAA/#finaid. DO complete this even if you’re on the waitlist, please.\n\n(optional) Daily Post-Lecture Feedback\n\nopportunity to reflect on learning\nopportunity to ask questions (I will read and answer these.)\nopportunity for extra credit on final project\n\n\n\n\n\nNote: Links to both surveys are also on Canvas. I will try to remind you at the end of lecture, but I’ll probably forget. Feel free to remind me/one another!"
  },
  {
    "objectID": "content/lectures/02-dplyr.html",
    "href": "content/lectures/02-dplyr.html",
    "title": "02-dplyr",
    "section": "",
    "text": "Q: How many people are in a group for case studies and final project?\nA: 3-4\n\n\nQ: How to turn in assignments\nA: We’ll discuss this today!\n\n\nQ: Why don’t we have uniform keyboard-shortcut (like run code, new cell) for both R and Python and other coding environment?\nA: Lack of communication? Preferences of developers? I think we’ll get there…\n\n\nQ: Wasn’t clear about the ‘single quotes’ vs “double quotes” thing\nA: When creating a string, or any time you need to use quotes in R, single and double quotes are interchangeable. R doesn’t care which you use. However, your code will be stylistically better if you consistently use one.\n\n\nQ: What are useful libraries that we can use to analyze data?\nA: We’ll be discussing lots, but the tidyverse packages (the first of which we’ll discuss is dplyr) is a great place to start. There are also different packages for basically every statistical analysis out there\n\n\nQ: Is there any way to prevent coercion? / I was wondering if you can types cast a variable when concatenation\nA: Yup. You can explicitly state as._____() when creating a variable (i.e. as.character()) and when reading in data you can specify. You’ll find that R does a pretty good job at guessing, but we can always fix to what we want after the fact.\n\n\nQ: What is the difference between mylist[1] and mylist[[1]]? It looked like class(mylist[1]) returned list and class(mylist[[1]]) returned the class of the element.\nA: Double brackets returns the element directly. Single bracket (for lists) always returns a list.\n\n\nQ: I’m curious about how to handle dataframes in R\nA: Excellent - we’ll start this discussion today and continue throughout the quarter!\n\n\n\n\nDue Dates:\n\nLab 01 due tomorrow (Friday; 11:59 PM)\nStudent survey open until next Thursday\nHW01 and Lab02 will both be released Monday\nLecture Participation survey “due” after class\n\n\n\n\nR4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors\n\n\n\n\n\ndplyr\n\nphilosophy\npipes\ncommon operations\n\n\n\n\n\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "href": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "title": "02-dplyr",
    "section": "The pipe in baseR",
    "text": "The pipe in baseR\n\n\n\n\n|> should be read as “and then”\nfor example “Wake up |> brush teeth” would be read as “wake up and then brush teeth”"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "href": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "title": "02-dplyr",
    "section": "Where does the name come from?",
    "text": "Where does the name come from?\nThe pipe operator was first implemented in the package magrittr.\n\n\n\n\n\n\n\nYou will see this frequently in code online. It’s equivalent to |>."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "href": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "title": "02-dplyr",
    "section": "Review: How does a pipe work?",
    "text": "Review: How does a pipe work?\n\nYou can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.\n\n\n\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"campus\"))\n\n\n\n\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") |>\n  start_car() |>\n  drive(to = \"campus\") |>\n  park()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "href": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "title": "02-dplyr",
    "section": "NC DOT Fatal Crashes in North Carolina",
    "text": "NC DOT Fatal Crashes in North Carolina\nFrom OpenDurham’s Data Portal\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#variables",
    "href": "content/lectures/02-dplyr.html#variables",
    "title": "02-dplyr",
    "section": "Variables",
    "text": "Variables\nView the names of variables via\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#viewing-your-data",
    "href": "content/lectures/02-dplyr.html#viewing-your-data",
    "title": "02-dplyr",
    "section": "Viewing your data",
    "text": "Viewing your data\n\nIn the Environment, click on the name of the data frame to view it in the data viewer (or use the View function)\nUse the glimpse function to take a peek\n\n\nglimpse(bike)\n\nRows: 5,716\nColumns: 54\n$ FID        <dbl> 18, 29, 33, 35, 49, 53, 56, 60, 63, 66, 72, 75, 82, 84, 85,…\n$ OBJECTID   <dbl> 19, 30, 34, 36, 50, 54, 57, 61, 64, 67, 73, 76, 83, 85, 86,…\n$ AmbulanceR <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", …\n$ BikeAge_Gr <chr> NA, \"50-59\", NA, \"16-19\", NA, \"50-59\", \"16-19\", \"40-49\", \"1…\n$ Bike_Age   <dbl> 6, 51, 10, 17, 6, 52, 18, 40, 6, 7, 45, 30, 17, 20, 14, 15,…\n$ Bike_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bike_Dir   <chr> \"Not Applicable\", \"With Traffic\", \"With Traffic\", NA, \"Faci…\n$ Bike_Injur <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"Injury\", \"B: E…\n$ Bike_Pos   <chr> \"Driveway / Alley\", \"Travel Lane\", \"Travel Lane\", \"Travel L…\n$ Bike_Race  <chr> \"Black\", \"Black\", \"Black\", \"White\", \"Black\", \"White\", \"Blac…\n$ Bike_Sex   <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ City       <chr> \"Durham\", \"Greenville\", \"Farmville\", \"Charlotte\", \"Charlott…\n$ County     <chr> \"Durham\", \"Pitt\", \"Pitt\", \"Mecklenburg\", \"Mecklenburg\", \"Du…\n$ CrashAlcoh <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ CrashDay   <chr> \"01-01-06\", \"01-01-02\", \"01-01-07\", \"01-01-05\", NA, NA, NA,…\n$ Crash_Date <date> 2007-01-06, 2007-01-09, 2007-01-14, 2007-01-12, 2007-01-15…\n$ Crash_Grp  <chr> \"Bicyclist Failed to Yield - Midblock\", \"Crossing Paths - O…\n$ Crash_Hour <dbl> 13, 23, 16, 19, 12, 20, 19, 14, 16, 0, 17, 18, 14, 17, 19, …\n$ Crash_Loc  <chr> \"Non-Intersection\", \"Intersection-Related\", \"Intersection\",…\n$ Crash_Mont <chr> NA, NA, NA, NA, NA, \"01-04-01\", \"01-04-01\", NA, \"01-02-01\",…\n$ Crash_Time <dttm> 0001-01-01 13:17:58, 0001-01-01 23:08:58, 0001-01-01 16:44…\n$ Crash_Type <chr> \"Bicyclist Ride Out - Residential Driveway\", \"Crossing Path…\n$ Crash_Ty_1 <dbl> 353311, 211180, 111144, 119139, 112114, 311231, 119144, 132…\n$ Crash_Year <dbl> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n$ Crsh_Sevri <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"O: No Injury\",…\n$ Developmen <chr> \"Residential\", \"Commercial\", \"Residential\", \"Residential\", …\n$ DrvrAge_Gr <chr> \"60-69\", \"30-39\", \"50-59\", \"30-39\", NA, \"20-24\", \"40-49\", N…\n$ Drvr_Age   <dbl> 66, 34, 52, 33, NA, 20, 40, NA, 17, 51, NA, 64, 50, 66, 30,…\n$ Drvr_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"Missing\", \"No\", \"No\", \"Missing\", \"…\n$ Drvr_EstSp <chr> \"11-15 mph\", \"0-5 mph\", \"21-25 mph\", \"46-50 mph\", \"16-20 mp…\n$ Drvr_Injur <chr> \"O: No Injury\", \"O: No Injury\", \"O: No Injury\", \"O: No Inju…\n$ Drvr_Race  <chr> \"Black\", \"Black\", \"White\", \"White\", \"/Missing\", \"White\", \"B…\n$ Drvr_Sex   <chr> \"Male\", \"Male\", \"Female\", \"Female\", NA, \"Female\", \"Male\", N…\n$ Drvr_VehTy <chr> \"Pickup\", \"Passenger Car\", \"Passenger Car\", \"Sport Utility\"…\n$ ExcsSpdInd <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Hit_Run    <chr> \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n$ Light_Cond <chr> \"Daylight\", \"Dark - Lighted Roadway\", \"Daylight\", \"Dark - R…\n$ Locality   <chr> \"Mixed (30% To 70% Developed)\", \"Urban (>70% Developed)\", \"…\n$ Num_Lanes  <chr> \"2 lanes\", \"5 lanes\", \"2 lanes\", \"4 lanes\", \"2 lanes\", \"4 l…\n$ Num_Units  <dbl> 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Rd_Charact <chr> \"Straight - Level\", \"Straight - Level\", \"Straight - Level\",…\n$ Rd_Class   <chr> \"Local Street\", \"Local Street\", \"Local Street\", \"NC Route\",…\n$ Rd_Conditi <chr> \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr…\n$ Rd_Config  <chr> \"Two-Way, Not Divided\", \"Two-Way, Divided, Unprotected Medi…\n$ Rd_Defects <chr> \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ Rd_Feature <chr> \"No Special Feature\", \"Four-Way Intersection\", \"Four-Way In…\n$ Rd_Surface <chr> \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smoo…\n$ Region     <chr> \"Piedmont\", \"Coastal\", \"Coastal\", \"Piedmont\", \"Piedmont\", \"…\n$ Rural_Urba <chr> \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Urban\", \"Urban\", \"Urba…\n$ Speed_Limi <chr> \"20 - 25  MPH\", \"40 - 45  MPH\", \"30 - 35  MPH\", \"40 - 45  M…\n$ Traff_Cntr <chr> \"No Control Present\", \"Stop And Go Signal\", \"Stop Sign\", \"S…\n$ Weather    <chr> \"Clear\", \"Clear\", \"Clear\", \"Cloudy\", \"Clear\", \"Clear\", \"Cle…\n$ Workzone_I <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Location   <chr> \"36.002743, -78.8785\", \"35.612984, -77.39265\", \"35.595676, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "href": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "title": "02-dplyr",
    "section": "A Grammar of Data Manipulation",
    "text": "A Grammar of Data Manipulation\ndplyr is based on the concepts of functions as verbs that manipulate data frames.\nSingle data frame functions / verbs:\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "href": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "title": "02-dplyr",
    "section": "dplyr rules for functions",
    "text": "dplyr rules for functions\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDo not modify in place\nPerformance via lazy evaluation"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "href": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "title": "02-dplyr",
    "section": "Filter rows with filter",
    "text": "Filter rows with filter\n\nSelect a subset of rows in a data frame.\nEasily filter for many conditions at once."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter",
    "href": "content/lectures/02-dplyr.html#filter",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County\n\nbike |>\n  filter(County == \"Durham\")\n\n# A tibble: 253 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1    18       19 No         <NA>              6 No         Not Appl… C: Possib…\n 2    53       54 Yes        50-59            52 No         With Tra… A: Disabl…\n 3    56       57 Yes        16-19            18 No         <NA>      C: Possib…\n 4   209      210 No         16-19            16 No         Facing T… C: Possib…\n 5   228      229 Yes        40-49            40 No         With Tra… B: Eviden…\n 6   620      621 Yes        50-59            55 No         With Tra… B: Eviden…\n 7   667      668 Yes        60-69            61 No         Not Appl… B: Eviden…\n 8   458      459 Yes        60-69            62 No         With Tra… B: Eviden…\n 9   576      577 No         40-49            49 No         With Tra… C: Possib…\n10   618      619 No         20-24            23 No         With Tra… C: Possib…\n# ℹ 243 more rows\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-1",
    "href": "content/lectures/02-dplyr.html#filter-1",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County where biker was < 10 yrs old\n\nbike |>\n  filter(County == \"Durham\", Bike_Age < 10)\n\n# A tibble: 20 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1    18       19 No         <NA>              6 No         Not Appl… C: Possib…\n 2    47       48 No         10-Jun            9 No         Not Appl… O: No Inj…\n 3   124      125 Yes        10-Jun            8 No         With Tra… C: Possib…\n 4   531      532 Yes        10-Jun            7 No         With Tra… C: Possib…\n 5   704      705 Yes        10-Jun            9 No         Not Appl… C: Possib…\n 6    42       43 No         10-Jun            8 No         With Tra… O: No Inj…\n 7   392      393 Yes        0-5               2 No         Not Appl… B: Eviden…\n 8   941      942 No         10-Jun            9 No         With Tra… C: Possib…\n 9   436      437 Yes        10-Jun            6 No         Not Appl… O: No Inj…\n10   160      161 Yes        10-Jun            7 No         With Tra… C: Possib…\n11   273      274 Yes        10-Jun            7 No         Facing T… C: Possib…\n12    78       79 Yes        10-Jun            7 No         With Tra… C: Possib…\n13   422      423 No         10-Jun            9 No         Not Appl… O: No Inj…\n14   570      571 No         <NA>              0 Missing    Not Appl… Injury    \n15   683      684 Yes        10-Jun            8 No         Not Appl… C: Possib…\n16    62       63 Yes        10-Jun            7 No         With Tra… C: Possib…\n17   248      249 No         0-5               4 No         Not Appl… O: No Inj…\n18   306      307 Yes        10-Jun            8 No         With Tra… C: Possib…\n19   231      232 Yes        10-Jun            8 No         With Tra… C: Possib…\n20   361      362 Yes        10-Jun            9 No         With Tra… B: Eviden…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "href": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "title": "02-dplyr",
    "section": "Aside: real data is messy!",
    "text": "Aside: real data is messy!\n   What in the world does a BikeAge_gr of 10-Jun or 15-Nov mean?\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 10-Jun             421\n 3 15-Nov             747\n 4 16-19              605\n 5 20-24              680\n 6 25-29              430\n 7 30-39              658\n 8 40-49              920\n 9 50-59              739\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "href": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "title": "02-dplyr",
    "section": "Careful data scientists clean up their data first!",
    "text": "Careful data scientists clean up their data first!\n\nWe’re going to need to do some text parsing to clean up these data\n\n10-Jun should be 6-10\n15-Nov should be 11-15"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "href": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "title": "02-dplyr",
    "section": "Correct and overwrite mutate",
    "text": "Correct and overwrite mutate\n\nRemember we want to do the following in the BikeAge_Gr variable\n\n10-Jun should be 6-10\n15-Nov should be 11-15\n\n\n\nbike <- bike |>\n  mutate(\n    BikeAge_Gr = case_when(\n      BikeAge_Gr == \"10-Jun\" ~ \"6-10\",\n      BikeAge_Gr == \"15-Nov\" ~ \"11-15\",\n      TRUE                   ~ BikeAge_Gr     # everything else\n    )\n  )\n\n\nNote that we’re overwriting existing data and columns, so be careful!\n\nBut remember, it’s easy to revert if you make a mistake since we didn’t touch the raw data, we can always reload it and start over"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr count\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "href": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "title": "02-dplyr",
    "section": "mutate to add new variables",
    "text": "mutate to add new variables\n   How is the new alcohol variable determined?\n\nbike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "href": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "title": "02-dplyr",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame.\n\nbike <- bike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "href": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "title": "02-dplyr",
    "section": "transmute to create a new dataset",
    "text": "transmute to create a new dataset\nYou’ll use this much less often than mutate but when you need it, you need it.\n\nbike |> \n  transmute(ID = paste(FID, OBJECTID, sep = \"-\"))\n\n# A tibble: 5,716 × 1\n   ID   \n   <chr>\n 1 18-19\n 2 29-30\n 3 33-34\n 4 35-36\n 5 49-50\n 6 53-54\n 7 56-57\n 8 60-61\n 9 63-64\n10 66-67\n# ℹ 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "href": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "title": "02-dplyr",
    "section": "mutate vs. transmute",
    "text": "mutate vs. transmute\n\nmutate adds new and keeps original\ntransmute adds new; drops existing"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn",
    "href": "content/lectures/02-dplyr.html#your-turn",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nHow many accidents in our dataset required an ambulance ride (AmbulanceR) and had the Crash_Type “Bicyclist Lost Control - Mechanical Problems”?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nFirst five\n\nbike |>\n  slice(1:5)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>      <chr>     \n1    18       19 No         <NA>              6 No         Not Appli… C: Possib…\n2    29       30 Yes        50-59            51 No         With Traf… C: Possib…\n3    33       34 No         <NA>             10 No         With Traf… Injury    \n4    35       36 Yes        16-19            17 No         <NA>       B: Eviden…\n5    49       50 No         <NA>              6 No         Facing Tr… O: No Inj…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nLast five\n\nlast_row <- nrow(bike)\nbike |>\n  slice((last_row - 4):last_row)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>      <chr>     \n1   460      461 Yes        6-10              7 No         Not Appli… C: Possib…\n2   474      475 Yes        50-59            50 No         With Traf… B: Eviden…\n3   479      480 Yes        16-19            16 No         Not Appli… C: Possib…\n4   487      488 No         40-49            47 Yes        With Traf… C: Possib…\n5   488      489 Yes        30-39            35 No         Facing Tr… C: Possib…\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>,\n#   Drvr_Race <chr>, Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "href": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "title": "02-dplyr",
    "section": "select to keep only the variables you mention",
    "text": "select to keep only the variables you mention\n\nbike |>\n  select(Crash_Loc, Hit_Run) |>\n  table()\n\n                      Hit_Run\nCrash_Loc                No  Yes\n  Intersection         2223  275\n  Intersection-Related  252   42\n  Location                3    7\n  Non-Intersection     2213  462\n  Non-Roadway           205   30"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "href": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "title": "02-dplyr",
    "section": "or select to exclude variables",
    "text": "or select to exclude variables\n\nbike |>\n  select(-OBJECTID)\n\n# A tibble: 5,716 × 53\n     FID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur Bike_Pos\n   <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>      <chr>   \n 1    18 No         <NA>              6 No         Not Appl… C: Possib… Drivewa…\n 2    29 Yes        50-59            51 No         With Tra… C: Possib… Travel …\n 3    33 No         <NA>             10 No         With Tra… Injury     Travel …\n 4    35 Yes        16-19            17 No         <NA>      B: Eviden… Travel …\n 5    49 No         <NA>              6 No         Facing T… O: No Inj… Travel …\n 6    53 Yes        50-59            52 No         With Tra… A: Disabl… Travel …\n 7    56 Yes        16-19            18 No         <NA>      C: Possib… Travel …\n 8    60 No         40-49            40 No         Facing T… B: Eviden… Sidewal…\n 9    63 Yes        6-10              6 No         Facing T… B: Eviden… Travel …\n10    66 Yes        6-10              7 No         <NA>      B: Eviden… Non-Roa…\n# ℹ 5,706 more rows\n# ℹ 45 more variables: Bike_Race <chr>, Bike_Sex <chr>, City <chr>,\n#   County <chr>, CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>,\n#   Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>,\n#   Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>,\n#   Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>,\n#   Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "href": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "title": "02-dplyr",
    "section": "or select a range of variables",
    "text": "or select a range of variables\n\nbike |>\n  select(OBJECTID:Bike_Injur)\n\n# A tibble: 5,716 × 7\n   OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir       Bike_Injur \n      <dbl> <chr>      <chr>         <dbl> <chr>      <chr>          <chr>      \n 1       19 No         <NA>              6 No         Not Applicable C: Possibl…\n 2       30 Yes        50-59            51 No         With Traffic   C: Possibl…\n 3       34 No         <NA>             10 No         With Traffic   Injury     \n 4       36 Yes        16-19            17 No         <NA>           B: Evident…\n 5       50 No         <NA>              6 No         Facing Traffic O: No Inju…\n 6       54 Yes        50-59            52 No         With Traffic   A: Disabli…\n 7       57 Yes        16-19            18 No         <NA>           C: Possibl…\n 8       61 No         40-49            40 No         Facing Traffic B: Evident…\n 9       64 Yes        6-10              6 No         Facing Traffic B: Evident…\n10       67 Yes        6-10              7 No         <NA>           B: Evident…\n# ℹ 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "href": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "title": "02-dplyr",
    "section": "pull to extract a column as a vector",
    "text": "pull to extract a column as a vector\n\nbike |>\n  slice(1:6) |>\n  pull(Location)\n\n[1] \"36.002743, -78.8785\"  \"35.612984, -77.39265\" \"35.595676, -77.59074\"\n[4] \"35.076767, -80.7728\"  \"35.19999, -80.75713\"  \"35.966644, -78.96749\"\n\n\n\nbike |>\n  slice(1:6) |>\n  select(Location)\n\n# A tibble: 6 × 1\n  Location            \n  <chr>               \n1 36.002743, -78.8785 \n2 35.612984, -77.39265\n3 35.595676, -77.59074\n4 35.076767, -80.7728 \n5 35.19999, -80.75713 \n6 35.966644, -78.96749"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "href": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "title": "02-dplyr",
    "section": "The two pulls in your lives",
    "text": "The two pulls in your lives\n\n\n\n\n\n\n\n\nDon’t get pull happy when wrangling data! Only extract out variables if you truly need to, otherwise keep in data frame.\nBut always ⬇️ Pull before starting your work when collaborating on GitHub."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\nUseful for correcting typos, and renaming to make variable names shorter and/or more informative\n\nOriginal names:\n\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\n\nRename Speed_Limi to Speed_Limit:\n\n\nbike <- bike |>\n  rename(Speed_Limit = Speed_Limi)"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nnames(bike)\n\n [1] \"FID\"         \"OBJECTID\"    \"AmbulanceR\"  \"BikeAge_Gr\"  \"Bike_Age\"   \n [6] \"Bike_Alc_D\"  \"Bike_Dir\"    \"Bike_Injur\"  \"Bike_Pos\"    \"Bike_Race\"  \n[11] \"Bike_Sex\"    \"City\"        \"County\"      \"CrashAlcoh\"  \"CrashDay\"   \n[16] \"Crash_Date\"  \"Crash_Grp\"   \"Crash_Hour\"  \"Crash_Loc\"   \"Crash_Mont\" \n[21] \"Crash_Time\"  \"Crash_Type\"  \"Crash_Ty_1\"  \"Crash_Year\"  \"Crsh_Sevri\" \n[26] \"Developmen\"  \"DrvrAge_Gr\"  \"Drvr_Age\"    \"Drvr_Alc_D\"  \"Drvr_EstSp\" \n[31] \"Drvr_Injur\"  \"Drvr_Race\"   \"Drvr_Sex\"    \"Drvr_VehTy\"  \"ExcsSpdInd\" \n[36] \"Hit_Run\"     \"Light_Cond\"  \"Locality\"    \"Num_Lanes\"   \"Num_Units\"  \n[41] \"Rd_Charact\"  \"Rd_Class\"    \"Rd_Conditi\"  \"Rd_Config\"   \"Rd_Defects\" \n[46] \"Rd_Feature\"  \"Rd_Surface\"  \"Region\"      \"Rural_Urba\"  \"Speed_Limit\"\n[51] \"Traff_Cntr\"  \"Weather\"     \"Workzone_I\"  \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn-1",
    "href": "content/lectures/02-dplyr.html#your-turn-1",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nYour boss in Cumberland County gets overwhelmed by data easily, but he wants some data from you. He wants all bike accidents from his County, but he only wants to know the road’s speed limit, the age of the biker, and to know if alcohol was involved. If you have time, mine as well make the column names very clear to your boss while you’re at it…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "href": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "title": "02-dplyr",
    "section": "summarize to reduce variables to values",
    "text": "summarize to reduce variables to values\nThe values are summarized in a data frame\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 11-15              747\n 3 16-19              605\n 4 20-24              680\n 5 25-29              430\n 6 30-39              658\n 7 40-49              920\n 8 50-59              739\n 9 6-10               421\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "href": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "title": "02-dplyr",
    "section": "and arrange to order rows",
    "text": "and arrange to order rows\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n()) |>\n  arrange(desc(crash_count))\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 40-49              920\n 2 11-15              747\n 3 50-59              739\n 4 20-24              680\n 5 30-39              658\n 6 16-19              605\n 7 25-29              430\n 8 6-10               421\n 9 60-69              274\n10 <NA>               112\n11 0-5                 60\n12 70+                 58\n13 70                  12"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "href": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "title": "02-dplyr",
    "section": "count to group by then count",
    "text": "count to group by then count\n\nbike |>\n  count(BikeAge_Gr)\n\n# A tibble: 13 × 2\n   BikeAge_Gr     n\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112\n\n\n   If you wanted to arrange these in ascending order what would you add to the pipe?"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "href": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "title": "02-dplyr",
    "section": "Select rows with sample_n or sample_frac",
    "text": "Select rows with sample_n or sample_frac\n\nsample_n: randomly sample 5 observations\n\n\nbike_n5 <- bike |>\n  sample_n(5, replace = FALSE)\n\ndim(bike_n5)\n\n[1]  5 54\n\n\n\nsample_frac: randomly sample 20% of observations\n\n\nbike_perc20 <- bike |>\n  sample_frac(0.2, replace = FALSE)\n\ndim(bike_perc20)\n\n[1] 1143   54"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "href": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "title": "02-dplyr",
    "section": "distinct to filter for unique rows",
    "text": "distinct to filter for unique rows\n\nbike |> \n  select(County, City) |> \n  distinct() |> \n  arrange(County, City)\n\n# A tibble: 360 × 2\n   County    City              \n   <chr>     <chr>             \n 1 Alamance  Alamance          \n 2 Alamance  Burlington        \n 3 Alamance  Elon College      \n 4 Alamance  Gibsonville       \n 5 Alamance  Graham            \n 6 Alamance  Green Level       \n 7 Alamance  Mebane            \n 8 Alamance  None - Rural Crash\n 9 Alexander None - Rural Crash\n10 Alleghany None - Rural Crash\n# ℹ 350 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "href": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "title": "02-dplyr",
    "section": "distinct has a .keep_all parameter",
    "text": "distinct has a .keep_all parameter\n\nbike |> \n  distinct(County, City, .keep_all = TRUE) |> \n  arrange(County, City)\n\n# A tibble: 360 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   <dbl>    <dbl> <chr>      <chr>         <dbl> <chr>      <chr>     <chr>     \n 1   524      525 Yes        11-15            12 No         <NA>      B: Eviden…\n 2    84       85 Yes        20-24            20 No         With Tra… B: Eviden…\n 3   571      572 Yes        16-19            16 No         Not Appl… B: Eviden…\n 4   509      510 Yes        40-49            43 Yes        With Tra… K: Killed \n 5   855      856 Yes        30-39            30 No         With Tra… A: Disabl…\n 6     5        6 Yes        40-49            44 Yes        With Tra… C: Possib…\n 7   163      164 Yes        30-39            35 No         Not Appl… C: Possib…\n 8    96       97 Yes        30-39            36 No         With Tra… C: Possib…\n 9    46       47 Yes        50-59            53 No         With Tra… B: Eviden…\n10   485      486 Yes        60-69            62 No         With Tra… C: Possib…\n# ℹ 350 more rows\n# ℹ 46 more variables: Bike_Pos <chr>, Bike_Race <chr>, Bike_Sex <chr>,\n#   City <chr>, County <chr>, CrashAlcoh <chr>, CrashDay <chr>,\n#   Crash_Date <date>, Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>,\n#   Crash_Mont <chr>, Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>,\n#   Crash_Year <dbl>, Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>,\n#   Drvr_Age <dbl>, Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#factors-1",
    "href": "content/lectures/02-dplyr.html#factors-1",
    "title": "02-dplyr",
    "section": "Factors",
    "text": "Factors\nFactor objects are how R stores data for categorical variables (fixed numbers of discrete values).\n\n(x = factor(c(\"BS\", \"MS\", \"PhD\", \"MS\")))\n\n[1] BS  MS  PhD MS \nLevels: BS MS PhD\n\n\n\nglimpse(x)\n\n Factor w/ 3 levels \"BS\",\"MS\",\"PhD\": 1 2 3 2\n\n\n\ntypeof(x)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "href": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "title": "02-dplyr",
    "section": "Returning to: Cat lovers",
    "text": "Returning to: Cat lovers\nReading in the cat-lovers data…\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "href": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "title": "02-dplyr",
    "section": "Read data in as character strings",
    "text": "Read data in as character strings\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "href": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "title": "02-dplyr",
    "section": "But coerce when plotting",
    "text": "But coerce when plotting\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "href": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "title": "02-dplyr",
    "section": "Use forcats to manipulate factors",
    "text": "Use forcats to manipulate factors\n\ncat_lovers <- cat_lovers |>\n  mutate(handedness = fct_relevel(handedness, \n                                  \"right\", \"left\", \"ambidextrous\"))\n\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#forcats-functionality",
    "href": "content/lectures/02-dplyr.html#forcats-functionality",
    "title": "02-dplyr",
    "section": "forcats functionality ",
    "text": "forcats functionality \n\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values. Historically, factors were much easier to work with than character vectors, so many base R functions automatically convert character vectors to factors.\nfactors are still useful when you have true categorical data, and when you want to override the ordering of character vectors to improve display. The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors.\n\n\n\nSource: forcats.tidyverse.org"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html",
    "href": "content/lectures/04-ggplot2.html",
    "title": "04-ggplot2",
    "section": "",
    "text": "Q: What is the difference between pull and select?\nA: select specifies which columns to display in your resulting dataframe. pull extracts the values from a column and stores them in a vector (not a dataframe)\n\n\nQ: I am a bit confused on factors and what levels mean.\nA: Factors store categorical information. The levels of a factor are all the possible unique values in a variable.\n\n\nQ: how similar is R to numpy/which scenarios are each used in the industry?\nA: Basically, anything data science-y you can do in R, you can also do in python. R has linear algebra/working with matrices built directly into its base installation, so no additional package would be need for numpy-like operations. And, dplyr does very similar things to pandas, but with a more readable and consistent syntax overall.\n\n\nQ: would we ever load just dplyr instead of the entire tidyverse package? is there a big difference?\nA: We’ll always just load tidyverse. The difference is that the tidyverse is quite big, so if you ever wanted to just use dplyr functions, you could load just that. This matters more in development where you’re trying to minimize external dependencies and make code run as fast as possible. For our purposes, there’s no real need to only load dplyr\n\n\nQ: I found the demos to be the most confusing part, because it’s very different understanding slides and applying that to actual coding. Personally, I would prefer if the lecture content were put into recordings, or just uploaded earlier so we could learn it on our own, and then have classes be more focused on data science best practices, applications, etc.\nA: I do really like this idea and would love to run this like this in the. I’m curious what y’all think of this and will add a question like this to the post-course survey to get students’ thoughts.\n\n\n\n\nDue Dates:\n\nLab 02 due Friday (11:59 PM)\nHW 01 due Monday (11:59 PM)\nLecture Participation survey “due” after class\n\n\n\n\n\nR4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options\n\n\n\n\n\n\nMeasurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst \n\n\n\n\npenguins |>\n  datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section",
    "href": "content/lectures/04-ggplot2.html#section",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame\n\n\nggplot(data = penguins)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-1",
    "href": "content/lectures/04-ggplot2.html#section-1",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-2",
    "href": "content/lectures/04-ggplot2.html#section-2",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-3",
    "href": "content/lectures/04-ggplot2.html#section-3",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-4",
    "href": "content/lectures/04-ggplot2.html#section-4",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-5",
    "href": "content/lectures/04-ggplot2.html#section-5",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-6",
    "href": "content/lectures/04-ggplot2.html#section-6",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-7",
    "href": "content/lectures/04-ggplot2.html#section-7",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-8",
    "href": "content/lectures/04-ggplot2.html#section-8",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-9",
    "href": "content/lectures/04-ggplot2.html#section-9",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-10",
    "href": "content/lectures/04-ggplot2.html#section-10",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "href": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "title": "04-ggplot2",
    "section": "Coding out loud",
    "text": "Coding out loud\n\nCodePlotNarrative\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStart with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\nRepresent each observation with a point and map species to the color of each point.\nTitle the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\nFinally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#argument-names",
    "href": "content/lectures/04-ggplot2.html#argument-names",
    "title": "04-ggplot2",
    "section": "Argument names",
    "text": "Argument names\n\n\n\n\n\n\nTip\n\n\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm,  \n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\nggplot(penguins, \n       aes(x = bill_depth_mm, \n           y = bill_depth_mm,\n           color = species)) +\n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn",
    "href": "content/lectures/04-ggplot2.html#your-turn",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a basic plot in ggplot2 using different variables than those in the last example (last example: bill_depth_mm & bill_depth_mm).\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#aesthetics-options",
    "href": "content/lectures/04-ggplot2.html#aesthetics-options",
    "title": "04-ggplot2",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#color",
    "href": "content/lectures/04-ggplot2.html#color",
    "title": "04-ggplot2",
    "section": "Color",
    "text": "Color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape",
    "href": "content/lectures/04-ggplot2.html#shape",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = island)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape-1",
    "href": "content/lectures/04-ggplot2.html#shape-1",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#size",
    "href": "content/lectures/04-ggplot2.html#size",
    "title": "04-ggplot2",
    "section": "Size",
    "text": "Size\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#alpha",
    "href": "content/lectures/04-ggplot2.html#alpha",
    "title": "04-ggplot2",
    "section": "Alpha",
    "text": "Alpha\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g,\n           alpha = flipper_length_mm)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "title": "04-ggplot2",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nMapping: Determine the size, alpha, etc. of points based on the values of a variable in the data\n\ngoes into aes()\n\nSetting: Determine the size, alpha, etc. of points not based on the values of a variable in the data\n\ngoes into geom_*() (this was geom_point() in the previous example, but we’ll learn about other geoms soon!)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "title": "04-ggplot2",
    "section": "Mapping vs. Setting (example)",
    "text": "Mapping vs. Setting (example)\n\n\nMapping\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm,\n           size = body_mass_g, \n           alpha = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\nSetting\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm)) + \n  geom_point(size = 2, alpha = 0.5)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-1",
    "href": "content/lectures/04-ggplot2.html#your-turn-1",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nEdit the basic plot you created earlier to change something about its aesthetics.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-1",
    "href": "content/lectures/04-ggplot2.html#faceting-1",
    "title": "04-ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\nSmaller plots that display different subsets of the data\nUseful for exploring conditional relationships and large data\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ island) \n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "href": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "title": "04-ggplot2",
    "section": "Various ways to facet",
    "text": "Various ways to facet\n🧠 In the next few slides describe what each plot displays. Think about how the code relates to the output.\n\n\n\n\n\n\nWarning\n\n\n\nThe plots in the next few slides do not have proper titles, axis labels, etc. because we want you to figure out what’s happening in the plots. But you should always label your plots!"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-11",
    "href": "content/lectures/04-ggplot2.html#section-11",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ sex)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-12",
    "href": "content/lectures/04-ggplot2.html#section-12",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(sex ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-13",
    "href": "content/lectures/04-ggplot2.html#section-13",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-14",
    "href": "content/lectures/04-ggplot2.html#section-14",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(. ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-15",
    "href": "content/lectures/04-ggplot2.html#section-15",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species, ncol = 2)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-summary",
    "href": "content/lectures/04-ggplot2.html#faceting-summary",
    "title": "04-ggplot2",
    "section": "Faceting summary",
    "text": "Faceting summary\n\nfacet_grid():\n\n2d grid\nrows ~ cols\nuse . for no split\n\nfacet_wrap(): 1d ribbon wrapped according to number of rows and columns specified or available plotting area"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#facet-and-color",
    "href": "content/lectures/04-ggplot2.html#facet-and-color",
    "title": "04-ggplot2",
    "section": "Facet and color",
    "text": "Facet and color\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) + \n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "href": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "title": "04-ggplot2",
    "section": "Face and color, no legend",
    "text": "Face and color, no legend\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#common-geoms",
    "href": "content/lectures/04-ggplot2.html#common-geoms",
    "title": "04-ggplot2",
    "section": "Common geoms",
    "text": "Common geoms\n\n\n\ngeom 1\nDescription 2\n\n\n\n\ngeom_point\nscatterplot\n\n\ngeom_bar\nbarplot\n\n\ngeom_line\nline plot\n\n\ngeom_density\ndensityplot\n\n\ngeom_histogram\nhistogram\n\n\ngeom_boxplot\nboxplot"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-2",
    "href": "content/lectures/04-ggplot2.html#your-turn-2",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a plot in ggplot2 using a different geom than what you did previously. Customize as much as you can before time is “up.”\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/05-viz.html",
    "href": "content/lectures/05-viz.html",
    "title": "05-viz",
    "section": "",
    "text": "Slides modified from datascienceinabox.org\n\n\n\nQ: I just think it’s a bit fast and I would appreciate if you made sure to show like how to import and stuff rather then jumping to the final product\nA: That’s really valuable feedback. I don’t think you’re alone. I’m going to work to better get everyone on the same page going forward.\n\nNote: a handful of comments on needing more time on joins & pivots…which makes sense! They’re new :)\n\n\n\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHW01 can be submitted by tonight for full credit (datahub issues)\nLab02 Ans Posted\nHW02 Now Available\nMy OH will now be at the tables along the back of this building\n\n\n\n\n\nR4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit: Angela Zoss and Eric Monson, Duke DVS"
  },
  {
    "objectID": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "href": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "title": "05-viz",
    "section": "Principles for effective visualizations",
    "text": "Principles for effective visualizations\n\nOrder matters\nPut long categories on the y-axis\nKeep scales consistent\nSelect meaningful colors\nUse meaningful and nonredundant labels"
  },
  {
    "objectID": "content/lectures/05-viz.html#data",
    "href": "content/lectures/05-viz.html#data",
    "title": "05-viz",
    "section": "Data",
    "text": "Data\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\n\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\n\n\n\n\n\n\n\n\n\n\nSource: YouGov Survey Results, retrieved Oct 7, 2019"
  },
  {
    "objectID": "content/lectures/05-viz.html#the-data-code",
    "href": "content/lectures/05-viz.html#the-data-code",
    "title": "05-viz",
    "section": "The Data: Code",
    "text": "The Data: Code\n\nbrexit <- tibble(\n  opinion = c(\n    rep(\"Right\", 664), rep(\"Wrong\", 787), rep(\"Don't know\", 188)\n  ),\n  region = c(\n    rep(\"london\", 63), rep(\"rest_of_south\", 241), rep(\"midlands_wales\", 145), rep(\"north\", 176), rep(\"scot\", 39),\n    rep(\"london\", 110), rep(\"rest_of_south\", 257), rep(\"midlands_wales\", 152), rep(\"north\", 176), rep(\"scot\", 92),\n    rep(\"london\", 24), rep(\"rest_of_south\", 49), rep(\"midlands_wales\", 57), rep(\"north\", 48), rep(\"scot\", 10)\n  )\n)"
  },
  {
    "objectID": "content/lectures/05-viz.html#order-matters",
    "href": "content/lectures/05-viz.html#order-matters",
    "title": "05-viz",
    "section": "Order matters",
    "text": "Order matters\nAlphabetical is rarely ideal\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#order-by-frequency",
    "href": "content/lectures/05-viz.html#order-by-frequency",
    "title": "05-viz",
    "section": "Order by frequency",
    "text": "Order by frequency\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_infreq: Reorder factors’ levels by frequency\n\nggplot(brexit, aes(x = fct_infreq(opinion))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels",
    "href": "content/lectures/05-viz.html#clean-up-labels",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar() +\n  labs( \n    x = \"Opinion\", \n    y = \"Count\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "href": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "title": "05-viz",
    "section": "Avoiding Alphabetical Order",
    "text": "Avoiding Alphabetical Order\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = region)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-inherent-level-order",
    "href": "content/lectures/05-viz.html#use-inherent-level-order",
    "title": "05-viz",
    "section": "Use inherent level order",
    "text": "Use inherent level order\n\nRelevelPlot\n\n\nfct_relevel: Reorder factor levels using a custom order\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_relevel( \n      region,\n      \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-1",
    "href": "content/lectures/05-viz.html#clean-up-labels-1",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nRecodePlot\n\n\nfct_recode: Change factor levels by hand\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_recode( \n      region,\n      London = \"london\",\n      `Rest of South` = \"rest_of_south\",\n      `Midlands / Wales` = \"midlands_wales\",\n      North = \"north\",\n      Scotland = \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "href": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "title": "05-viz",
    "section": "Put long categories on the y-axis",
    "text": "Put long categories on the y-axis\nLong categories can be hard to read"
  },
  {
    "objectID": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "href": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "title": "05-viz",
    "section": "Move them to the y-axis",
    "text": "Move them to the y-axis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "href": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "title": "05-viz",
    "section": "And reverse the order of levels",
    "text": "And reverse the order of levels\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_rev: Reverse order of factor levels\n\nggplot(brexit, aes(y = fct_rev(region))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-2",
    "href": "content/lectures/05-viz.html#clean-up-labels-2",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = fct_rev(region))) +\n  geom_bar() +\n  labs( \n    x = \"Count\", \n    y = \"Region\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "href": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "title": "05-viz",
    "section": "Segmented bar plots can be hard to read",
    "text": "Segmented bar plots can be hard to read\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region, fill = opinion)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-facets",
    "href": "content/lectures/05-viz.html#use-facets",
    "title": "05-viz",
    "section": "Use facets",
    "text": "Use facets\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = region)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz.html#avoid-redundancy",
    "href": "content/lectures/05-viz.html#avoid-redundancy",
    "title": "05-viz",
    "section": "Avoid redundancy?",
    "text": "Avoid redundancy?"
  },
  {
    "objectID": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "href": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "title": "05-viz",
    "section": "Redundancy can help tell a story",
    "text": "Redundancy can help tell a story\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "href": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "title": "05-viz",
    "section": "Be selective with redundancy",
    "text": "Be selective with redundancy\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-informative-labels",
    "href": "content/lectures/05-viz.html#use-informative-labels",
    "title": "05-viz",
    "section": "Use informative labels",
    "text": "Use informative labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#a-bit-more-info",
    "href": "content/lectures/05-viz.html#a-bit-more-info",
    "title": "05-viz",
    "section": "A bit more info",
    "text": "A bit more info\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\", \n    caption = \"Source: https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#lets-do-better",
    "href": "content/lectures/05-viz.html#lets-do-better",
    "title": "05-viz",
    "section": "Let’s do better",
    "text": "Let’s do better\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#fix-up-facet-labels",
    "href": "content/lectures/05-viz.html#fix-up-facet-labels",
    "title": "05-viz",
    "section": "Fix up facet labels",
    "text": "Fix up facet labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1,\n    labeller = label_wrap_gen(width = 12) \n  ) + \n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "href": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "title": "05-viz",
    "section": "Rainbow colors not always best",
    "text": "Rainbow colors not always best"
  },
  {
    "objectID": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "href": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "title": "05-viz",
    "section": "Manually choose colors when needed",
    "text": "Manually choose colors when needed\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c( \n    \"Wrong\" = \"red\", \n    \"Right\" = \"green\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz.html#choosing-better-colors",
    "href": "content/lectures/05-viz.html#choosing-better-colors",
    "title": "05-viz",
    "section": "Choosing better colors",
    "text": "Choosing better colors\ncolorbrewer2.org"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-better-colors",
    "href": "content/lectures/05-viz.html#use-better-colors",
    "title": "05-viz",
    "section": "Use better colors",
    "text": "Use better colors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\", \n    \"Right\" = \"#67a9cf\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz.html#select-theme",
    "href": "content/lectures/05-viz.html#select-theme",
    "title": "05-viz",
    "section": "Select theme",
    "text": "Select theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal() \n\n\n\n\n\n\nggthemes described here"
  },
  {
    "objectID": "content/lectures/05-viz.html#customize-theme",
    "href": "content/lectures/05-viz.html#customize-theme",
    "title": "05-viz",
    "section": "Customize theme",
    "text": "Customize theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal(base_size = 16) + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "content/lectures/05-viz.html#your-turn",
    "href": "content/lectures/05-viz.html#your-turn",
    "title": "05-viz",
    "section": "Your Turn",
    "text": "Your Turn\n\nRead in the data (Data slide)\nThink of at least three different ways to tell slightly different stories with these data\nTry to implement at least one of these ideas!"
  },
  {
    "objectID": "content/lectures/07-linear-models.html",
    "href": "content/lectures/07-linear-models.html",
    "title": "07-linear-models",
    "section": "",
    "text": "Q: I was just wondering if you could still provide those videos you talked about with like syntax stuff so that we can follow along. I would also appreciate if it was really through with like a walk through of how you got to each point\nA: I like this suggestion, and I’ll try to make these. No promises though. Just have to find magical time in the schedule to make them. Note that lectures will be the more detailed walk through with more time on your own to figure it out (rather than detailed walk through outside of class), so you will get the information…the time/location will just differ. But, I’ll try to make some supplemental videos for those interested.\n\n\nQ: Had a question about the modeling and the last bit showing how decreasing the alpha showed greater clustering at the bottom left corner. If the bottom left corner looks like 0 height and 0 width, how does that translate into the dimensions of an actual painting?\nA: Good observation. The follow-up to this is…are there any paintings with zero width or zero height? And, if you dig in the data (i.e. min(pp$Height_in, na.rm=TRUE)), you’ll see that there are some very small paintings, but that none are zero.\n\n\nQ: If we want to built our own model, can we plot them with ggplot2?\nA: Yup! This post starts to get at that. It does so for a linear model, but the logic follows for other models.\n\n\nQ: For the paintings dataset, how could we perform EDA on specific subject matter, like seeing how many portraits include Jesus as part of the subject?\nA: Love this question. There is a whole field of natural language processing that would have sophisticated ways to analyze this. A simple first pass would be to, for example, filter for paintings that include “Jesus” in the subject column.\n\n\nQ: I think the segmented bar plots based on proportion seem difficult to read. I’m not sure why we should be using this instead of the stacked plots?\nA: Grouped bar blots are typically most quickly understood. Proportion stacked plots are then easiest to understand proportion across categories. Stacked plots of raw numbers take longer (for most) to understand and thus are often avoided, but like all viz, it depends on context and audience.\n\n\nQ: In the last lecture, we talked about how segmented bar plots might not be the ideal choice for data visualization, but we still demonstrated them in today’s lecture. So, specifically in what cases should we choose segmented bar plots as a data analysis tool?\nA: They can be helpful when the audience is familiar with them, but typically are most helpful when you want to display relative proportions rather than counts\n\n\n\n\nDue Dates:\n\nLab 04 due Friday\n\nModel Interpretations\nText, code, & viz all matter\n\nLecture Participation survey “due” after class\nHW02 due Monday (10/30; 11:59 PM)\ndiscuss displaying image in Markdown\n\n\n\n\n\nLinear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & >2 levels)\nresiduals\ndata transformations\n\n\n\n\n\n\nR4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nNumber of observations: 3393\nNumber of variables: 61"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "title": "07-linear-models",
    "section": "Goal: Predict height from width",
    "text": "Goal: Predict height from width\n\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#tidymodels-1",
    "href": "content/lectures/07-linear-models.html#tidymodels-1",
    "title": "07-linear-models",
    "section": "tidymodels",
    "text": "tidymodels\n\nNOT a core tidyverse package\nfollows the structure of a tidyverse package\n\n\n\n\n\n\n\n# should already be installed for you on datahub\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-1-specify-model",
    "href": "content/lectures/07-linear-models.html#step-1-specify-model",
    "title": "07-linear-models",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "href": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "title": "07-linear-models",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |>\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "href": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "title": "07-linear-models",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\n… using formula syntax\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "title": "07-linear-models",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n\n\n\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "title": "07-linear-models",
    "section": "A tidy look at model output",
    "text": "A tidy look at model output\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n\n\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#slope-and-intercept",
    "href": "content/lectures/07-linear-models.html#slope-and-intercept",
    "title": "07-linear-models",
    "section": "Slope and intercept",
    "text": "Slope and intercept\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]\n\n\nSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n\n\n\nIntercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "href": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "title": "07-linear-models",
    "section": "Correlation does not imply causation",
    "text": "Correlation does not imply causation\nRemember this when interpreting model coefficients\n\n\n\n\n\n\n\nSource: XKCD, Cell phones"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "href": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "title": "07-linear-models",
    "section": "Linear model with a single predictor",
    "text": "Linear model with a single predictor\n\nWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\n\n\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n\n\nTough luck, you can’t have them…\n\n\n\n\nSo we use sample statistics to estimate them:\n\n\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#least-squares-regression",
    "href": "content/lectures/07-linear-models.html#least-squares-regression",
    "title": "07-linear-models",
    "section": "Least squares regression",
    "text": "Least squares regression\n\nThe regression line minimizes the sum of squared residuals.\n\n\n\nIf \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\)."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals",
    "title": "07-linear-models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "href": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "title": "07-linear-models",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\n\n\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\nThe slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\n\n\n\n\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\n\n\n\n\nThe residuals and \\(x\\) values are uncorrelated"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 2 levels",
    "text": "Categorical predictor with 2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   <chr>         <dbl>    <dbl>\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# ℹ 3,373 more rows\n\n\n\n\nlandsALL = 0: No landscape features\nlandsALL = 1: Some landscape features"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features",
    "href": "content/lectures/07-linear-models.html#height-landscape-features",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\nm_ht_lands <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |> tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "href": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\nSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n\nCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\n\nIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "title": "07-linear-models",
    "section": "Categorical predictor with >2 levels",
    "text": "Categorical predictor with >2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows\n\n\n\n\nschool from which painting came (details in a few slides)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship between height and school",
    "text": "Relationship between height and school\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ school_pntg, data = pp) |>\n  tidy()\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#dummy-variables",
    "href": "content/lectures/07-linear-models.html#dummy-variables",
    "title": "07-linear-models",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nEach coefficient describes the expected difference between heights in that particular school compared to the baseline level"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 3+ levels",
    "text": "Categorical predictor with 3+ levels\n\n\n\n\n\n\n\nschool_pntg\nD_FL\nF\nG\nI\nS\nX\n\n\n\n\nA\n0\n0\n0\n0\n0\n0\n\n\nD/FL\n1\n0\n0\n0\n0\n0\n\n\nF\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n1\n0\n0\n0\n\n\nI\n0\n0\n0\n1\n0\n0\n\n\nS\n0\n0\n0\n0\n1\n0\n\n\nX\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "href": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "title": "07-linear-models",
    "section": "The linear model with multiple predictors",
    "text": "The linear model with multiple predictors\n\nPopulation model:\n\n\\[ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k \\]\n\n\nSample model that we use to estimate the population model:\n\n\\[ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k \\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship b/w height and school",
    "text": "Relationship b/w height and school\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nAustrian school (A) paintings are expected, on average, to be 14 inches tall.\nDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\nFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\nGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\nItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\nSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\nPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings. ]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#predict-height-from-width",
    "title": "07-linear-models",
    "section": "Predict height from width",
    "text": "Predict height from width\n❓ On average, how tall are paintings that are 60 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]\n\n\n3.62 + 0.78 * 60\n\n[1] 50.42\n\n\n“On average, we expect paintings that are 60 inches wide to be 50.42 inches high.”\nWarning: We “expect” this to happen, but there will be some variability. (We’ll learn about measuring the variability around the prediction later.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "href": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "title": "07-linear-models",
    "section": "Prediction vs. extrapolation",
    "text": "Prediction vs. extrapolation\n❓ On average, how tall are paintings that are 400 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "href": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "title": "07-linear-models",
    "section": "Watch out for extrapolation!",
    "text": "Watch out for extrapolation!\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”1  Stephen Colbert, April 6th, 2010"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "href": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "title": "07-linear-models",
    "section": "Measuring the strength of the fit",
    "text": "Measuring the strength of the fit\n\nThe strength of the fit of a linear model is most commonly evaluated using \\(R^2\\).\nIt tells us what percent of variability in the response variable is explained by the model.\nThe remainder of the variability is explained by variables not included in the model.\n\\(R^2\\) is sometimes called the coefficient of determination."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "href": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "title": "07-linear-models",
    "section": "Obtaining \\(R^2\\) in R",
    "text": "Obtaining \\(R^2\\) in R\n\nHeight vs. width\n\n\nglance(m_ht_wt)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>\n1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n\nglance(m_ht_wt)$r.squared # extract R-squared\n\n[1] 0.6829468\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n\n\nHeight vs. landscape features\n\n\nglance(m_ht_lands)$r.squared\n\n[1] 0.03456724"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width",
    "href": "content/lectures/07-linear-models.html#price-vs.-width",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Describe the relationship between price and width of paintings whose width is less than 100in."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Which plot shows a more linear relationship?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "title": "07-linear-models",
    "section": "Price vs. width, residuals",
    "text": "Price vs. width, residuals\n❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❓What’s the unit of residuals?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#transforming-the-data",
    "href": "content/lectures/07-linear-models.html#transforming-the-data",
    "title": "07-linear-models",
    "section": "Transforming the data",
    "text": "Transforming the data\n\nWe saw that price has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n\n\nIn these situations a transformation applied to the response variable may be useful.\n\n\n\n\nIn order to decide which transformation to use, we should examine the distribution of the response variable.\n\n\n\n\nThe extremely right skewed distribution suggests that a log transformation may be useful.\n\nlog = natural log, \\(ln\\)\nDefault base of the log function in R is the natural log:  log(x, base = exp(1))"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "href": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "title": "07-linear-models",
    "section": "Logged price vs. width",
    "text": "Logged price vs. width\n❓ How do we interpret the slope of this model?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\n\nm_lprice_wt <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(log(price) ~ Width_in, data = pp_wt_lt_100)\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "title": "07-linear-models",
    "section": "Linear model with log transformation",
    "text": "Linear model with log transformation\n\\[ \\widehat{log(price)} = 4.67 + 0.02 Width \\]\n\n\nFor each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n\n\n\nwhich is not a very useful statement…"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#working-with-logs",
    "href": "content/lectures/07-linear-models.html#working-with-logs",
    "title": "07-linear-models",
    "section": "Working with logs",
    "text": "Working with logs\n\nSubtraction and logs: \\(log(a) − log(b) = log(a / b)\\)\n\n\n\nNatural logarithm: \\(e^{log(x)} = x\\)\n\n\n\n\nWe can use these identities to “undo” the log transformation"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n\n\\[ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 \\]\n\n\n\\[ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 \\]\n\n\n\\[ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} \\]\n\n\n\\[ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 \\]\n\n\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "href": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "title": "07-linear-models",
    "section": "Shortcuts in R",
    "text": "Shortcuts in R\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019\n\n\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(exp(estimate), 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   107.  \n2 Width_in        1.02"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap-log-transformations",
    "href": "content/lectures/07-linear-models.html#recap-log-transformations",
    "title": "07-linear-models",
    "section": "Recap: Log Transformations",
    "text": "Recap: Log Transformations\n\nNon-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n\n\nThe most common transformation when the response variable is right skewed is the log transform: \\(log(y)\\), especially useful when the response variable is (extremely) right skewed.\n\n\n\n\nThis transformation is also useful for variance stabilization.\n\n\n\n\nWhen using a log transformation on the response variable the interpretation of the slope changes: *“For each unit increase in x, y is expected on average to be higher/lower*  by a factor of \\(e^{b_1}\\).”\n\n\n\n\nAnother useful transformation is the square root: \\(\\sqrt{y}\\), especially useful when the response variable is counts."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#aside-when-y-0",
    "href": "content/lectures/07-linear-models.html#aside-when-y-0",
    "title": "07-linear-models",
    "section": "Aside: when \\(y = 0\\)",
    "text": "Aside: when \\(y = 0\\)\nIn some cases the value of the response variable might be 0, and\n\nlog(0)\n\n[1] -Inf\n\n\n\nThe trick is to add a very small number to the value of the response variable for these cases so that the log function can still be applied:\n\nlog(0 + 0.00001)\n\n[1] -11.51293"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap",
    "href": "content/lectures/07-linear-models.html#recap",
    "title": "07-linear-models",
    "section": "Recap",
    "text": "Recap\n\nCan I carry out linear regression using the tidymodels approach?\nCan I interpret and explain the results from a linear model with a single predictor?\nDo I understand the limitations of modelling data w/ linear regression?\nCan I describe and implement the use of a dummy variable in linear regression?\nCan I determine when logistic transformation may be appropriate? Can I interpret these results?"
  },
  {
    "objectID": "content/lectures/06-analysis.html",
    "href": "content/lectures/06-analysis.html",
    "title": "06-analysis",
    "section": "",
    "text": "Q: I was most confused about the differences between facet_wrap and facet_grid. I didn’t understand what made them so different\nA: You’re right that they’re very similar and can even at times be used/coerced to do what the other does. Basically, if you want a 2D comparison (variable1 x variable2), facet_grid. If you just want to break down the plots by a single variable, facet_wrap.\n\n\nQ: Will future HW assignments also include things that were not explicitly covered in class? For example, HW1 which was due last night, included concepts and materials that were just covered today.\nA: Nope. That was my fault. We were behind where I intended to be. This will not be an issue going forward.\n\n\nQ: can we plot data from 2 separate data frames somehow?\nA: Not typically in ggplot2. Instead, data would have to be wrangled into the tidy format (single dataset) first.\n\n\nQ: does filling= mean seperate and color by the value passed in, and we don’t need to assign color again? What if we don’t want color?\nA: Fill indicates how to fill the plot with color. If you don’t want color, you wouldn’t use fill\n\n\nQ: How we would do the second suggestion in how to visualize the brexit data differently?\nA: We’ll do this together today!\n\n\nQ: for datasets that are included in a library (like penguins one), could we read it into a df to see it in the environment? or is that redundant and we should only view it, use functions to see var names, etc\nA: That would be ok!\n\n\nQ: I was wondering where I can find all available themes.\nA: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n\nQ: I have heard about plotly before. I was wondering what will be its difference between this and ggplot?\nA: There is a ggplotly package! Plotly enables interaction on top of ggplot2 packages\n\n\n\n\nDue Dates:\n\nLab 03 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\n\nNotes:\n\nHow To: Chunk output in console\n\n\n\n\nCase Studies Discussion\n\nBiomarkers of recent marijuana usage (Linear Regression)\nVaping in American Youth (Logistic Regression)\nRight-to-Carry Laws Effect on Violent Crime (Multiple Linear Regression)\nPredicting Annual Air Pollution (Machine Learning)\n\n\n\n\n\n\n\n\n\n\n\nDiscuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)\n\n\n\n\n\n\nIntroduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building"
  },
  {
    "objectID": "content/lectures/06-analysis.html#what-is-eda",
    "href": "content/lectures/06-analysis.html#what-is-eda",
    "title": "06-analysis",
    "section": "What is EDA?",
    "text": "What is EDA?\n\nExploratory data analysis (EDA) is an aproach to analyzing data sets to summarize and understand its main characteristics.\nOften, this is visual….but the visuals do not have to be perfect. (Save that for communication)\nCalculating summary statistics is also part of EDA.\n\n\n\nData tidying/wrangling/manipulation/transformation typically happens before this stage of the analysis.\n\n\n\nThe Goal: KNOW YOUR DATA!"
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "href": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "title": "06-analysis",
    "section": "Consider: No. of variables involved",
    "text": "Consider: No. of variables involved\n\nUnivariate data analysis - distribution of single variable\nBivariate data analysis - relationship between two variables\nMultivariate data analysis - relationship between many variables at once, usually focusing on the relationship between two while conditioning for others"
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-types-of-variables",
    "href": "content/lectures/06-analysis.html#consider-types-of-variables",
    "title": "06-analysis",
    "section": "Consider: Types of variables",
    "text": "Consider: Types of variables\n\nNumerical variables can be classified as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.\nIf the variable is categorical, we can determine if it is ordinal based on whether or not the levels have a natural ordering."
  },
  {
    "objectID": "content/lectures/06-analysis.html#data-visualization",
    "href": "content/lectures/06-analysis.html#data-visualization",
    "title": "06-analysis",
    "section": "Data visualization",
    "text": "Data visualization\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\n\nData visualization is the creation and study of the visual representation of data.\nThere are many tools for visualizing data (R is one of them), and many approaches/systems within R for making data visualizations (ggplot2 is what we’ll continue to use).\nEDA will involve making plots/visualizing your data"
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-paintings",
    "href": "content/lectures/06-analysis.html#paris-paintings",
    "title": "06-analysis",
    "section": "Paris Paintings",
    "text": "Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nSource: Printed catalogs of 28 auction sales in Paris, 1764 - 1780\nData curators Sandra van Ginhoven and Hilary Coe Cronheim (who were PhD students in the Duke Art, Law, and Markets Initiative at the time of putting together this dataset) translated and tabulated the catalogues\n3393 paintings, their prices, and descriptive details from sales catalogues over 60 variables"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-today",
    "href": "content/lectures/06-analysis.html#auctions-today",
    "title": "06-analysis",
    "section": "Auctions today",
    "text": "Auctions today\n\n\n\n\n\n\n\nSource: Sothebys"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "href": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "title": "06-analysis",
    "section": "Auctions back in the day",
    "text": "Auctions back in the day\n\n\n\n\n\nSource: Pierre-Antoine de Machy, Public Sale at the Hôtel Bullion, Musée Carnavalet, Paris (18th century)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-auction-market",
    "href": "content/lectures/06-analysis.html#paris-auction-market",
    "title": "06-analysis",
    "section": "Paris auction market",
    "text": "Paris auction market"
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "title": "06-analysis",
    "section": "Depart pour la chasse",
    "text": "Depart pour la chasse"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auction-catalogue-text",
    "href": "content/lectures/06-analysis.html#auction-catalogue-text",
    "title": "06-analysis",
    "section": "Auction catalogue text",
    "text": "Auction catalogue text\n\n\n\n\nTwo paintings very rich in composition, of a beautiful execution, and whose merit is very remarkable, each 17 inches 3 lines high, 23 inches wide; the first, painted on wood, comes from the Cabinet of Madame la Comtesse de Verrue; it represents a departure for the hunt: it shows in the front a child on a white horse, a man who gives the horn to gather the dogs, a falconer and other figures nicely distributed across the width of the painting; two horses drinking from a fountain; on the right in the corner a lovely country house topped by a terrace, on which people are at the table, others who play instruments; trees and fabriques pleasantly enrich the background."
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "title": "06-analysis",
    "section": "Depart pour la chasse as Data",
    "text": "Depart pour la chasse as Data"
  },
  {
    "objectID": "content/lectures/06-analysis.html#section",
    "href": "content/lectures/06-analysis.html#section",
    "title": "06-analysis",
    "section": "",
    "text": "pp |>\n  filter(name == \"R1777-89a\") |>\n  glimpse()\n\nRows: 1\nColumns: 61\n$ name              <chr> \"R1777-89a\"\n$ sale              <chr> \"R1777\"\n$ lot               <chr> \"89\"\n$ position          <dbl> 0.3755274\n$ dealer            <chr> \"R\"\n$ year              <dbl> 1777\n$ origin_author     <chr> \"D/FL\"\n$ origin_cat        <chr> \"D/FL\"\n$ school_pntg       <chr> \"D/FL\"\n$ diff_origin       <dbl> 0\n$ logprice          <dbl> 8.575462\n$ price             <dbl> 5300\n$ count             <dbl> 1\n$ subject           <chr> \"D\\x8epart pour la chasse\"\n$ authorstandard    <chr> \"Wouwerman, Philips\"\n$ artistliving      <dbl> 0\n$ authorstyle       <chr> NA\n$ author            <chr> \"Philippe Wouwermans\"\n$ winningbidder     <chr> \"Langlier, Jacques for Poullain, Antoine\"\n$ winningbiddertype <chr> \"DC\"\n$ endbuyer          <chr> \"C\"\n$ Interm            <dbl> 1\n$ type_intermed     <chr> \"D\"\n$ Height_in         <dbl> 17.25\n$ Width_in          <dbl> 23\n$ Surface_Rect      <dbl> 396.75\n$ Diam_in           <dbl> NA\n$ Surface_Rnd       <dbl> NA\n$ Shape             <chr> \"squ_rect\"\n$ Surface           <dbl> 396.75\n$ material          <chr> \"bois\"\n$ mat               <chr> \"b\"\n$ materialCat       <chr> \"wood\"\n$ quantity          <dbl> 1\n$ nfigures          <dbl> 0\n$ engraved          <dbl> 0\n$ original          <dbl> 0\n$ prevcoll          <dbl> 1\n$ othartist         <dbl> 0\n$ paired            <dbl> 1\n$ figures           <dbl> 0\n$ finished          <dbl> 0\n$ lrgfont           <dbl> 0\n$ relig             <dbl> 0\n$ landsALL          <dbl> 1\n$ lands_sc          <dbl> 0\n$ lands_elem        <dbl> 1\n$ lands_figs        <dbl> 1\n$ lands_ment        <dbl> 0\n$ arch              <dbl> 1\n$ mytho             <dbl> 0\n$ peasant           <dbl> 0\n$ othgenre          <dbl> 0\n$ singlefig         <dbl> 0\n$ portrait          <dbl> 0\n$ still_life        <dbl> 0\n$ discauth          <dbl> 0\n$ history           <dbl> 0\n$ allegory          <dbl> 0\n$ pastorale         <dbl> 0\n$ other             <dbl> 0"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "href": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "title": "06-analysis",
    "section": "Visualizing numerical data",
    "text": "Visualizing numerical data\nDescribing shapes of numerical distributions\n\nshape:\n\nskewness: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail)\nmodality: unimodal, bimodal, multimodal, uniform\n\ncenter: mean (mean), median (median), mode (not always useful)\nspread: range (range), standard deviation (sd), inter-quartile range (IQR)\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis.html#histograms",
    "href": "content/lectures/06-analysis.html#histograms",
    "title": "06-analysis",
    "section": "Histograms",
    "text": "Histograms\n\nHeightsWidthsPrices\n\n\n\nggplot(data = pp, aes(x = Height_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Height, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = Width_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Width, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 100) +\n  labs(x = \"Price\", y = NULL)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#density-plots",
    "href": "content/lectures/06-analysis.html#density-plots",
    "title": "06-analysis",
    "section": "Density plots",
    "text": "Density plots\n\nggplot(data = pp, mapping = aes(x = Height_in)) +\n  geom_density()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "href": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "title": "06-analysis",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = pp, mapping = aes(y = Height_in, x = as.factor(landsALL))) +\n  geom_boxplot()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "href": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "title": "06-analysis",
    "section": "Visualizing categorical data",
    "text": "Visualizing categorical data\n\ncount/proportion of values\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis.html#bar-plots",
    "href": "content/lectures/06-analysis.html#bar-plots",
    "title": "06-analysis",
    "section": "Bar plots",
    "text": "Bar plots\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL))) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "title": "06-analysis",
    "section": "Segmented bar plots, counts",
    "text": "Segmented bar plots, counts\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "title": "06-analysis",
    "section": "Segmented bar plots, proportions",
    "text": "Segmented bar plots, proportions\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\")"
  },
  {
    "objectID": "content/lectures/06-analysis.html#your-turn",
    "href": "content/lectures/06-analysis.html#your-turn",
    "title": "06-analysis",
    "section": "Your Turn",
    "text": "Your Turn\n❓ Which of the previous two bar plots is a more useful representation for visualizing the relationship between landscape and painting material?\n\n❓ What else would you want to do/know to complete EDA?\n\n\n🧠 Try to answer at least one thing you’d want to know from the dataset.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-1",
    "href": "content/lectures/06-analysis.html#modelling-1",
    "title": "06-analysis",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we focus on linear models (but remember there are other types of models too!)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#packages",
    "href": "content/lectures/06-analysis.html#packages",
    "title": "06-analysis",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nYou’re familiar with the tidyverse:\n\n\nlibrary(tidyverse)\n\n\nThe broom package takes the messy output of built-in functions in R, such as lm, and turns them into tidy data frames.\n\n\nlibrary(broom)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "href": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "title": "06-analysis",
    "section": "Modelling the relationship between variables",
    "text": "Modelling the relationship between variables\nEDA: Prices\n❗ Describe the distribution of prices of paintings.\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 1000)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#models-as-functions",
    "href": "content/lectures/06-analysis.html#models-as-functions",
    "title": "06-analysis",
    "section": "Models as functions",
    "text": "Models as functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs.\n\nPlug in the inputs and receive back the output\nExample: the formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\), when \\(x\\) is \\(5\\), the output \\(y\\) is \\(22\\)\n\n\ny = 3 * 5 + 7 = 22"
  },
  {
    "objectID": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "href": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "title": "06-analysis",
    "section": "Height as a function of width",
    "text": "Height as a function of width\n❗ Describe the relationship between height and width of paintings."
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… without the measure of uncertainty around the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… with different cosmetic choices for the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, \n              col = \"pink\",      # color\n              lty = 2,           # line type\n              linewidth = 3)     # line weight"
  },
  {
    "objectID": "content/lectures/06-analysis.html#vocabulary",
    "href": "content/lectures/06-analysis.html#vocabulary",
    "title": "06-analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis (dependent variable)\n\n\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis (independent variables)\n\n\n\n\nPredicted value: Output of the function model function\n\nThe model function gives the typical value of the response variable conditioning on the explanatory variables\n\n\n\n\n\nResiduals: Show how far each case is from its model value\n\nResidual = Observed value - Predicted value\nTells how far above/below the model function each case is"
  },
  {
    "objectID": "content/lectures/06-analysis.html#residuals",
    "href": "content/lectures/06-analysis.html#residuals",
    "title": "06-analysis",
    "section": "Residuals",
    "text": "Residuals\n❓ What does a negative residual mean? Which paintings on the plot have have negative residuals?"
  },
  {
    "objectID": "content/lectures/06-analysis.html#section-1",
    "href": "content/lectures/06-analysis.html#section-1",
    "title": "06-analysis",
    "section": "",
    "text": "The plot below displays the relationship between height and width of paintings. It uses a lower alpha level for the points than the previous plots we looked at.\n\n\n\n\n\n❓ What feature is apparent in this plot that was not (as) apparent in the previous plots? What might be the reason for this feature?"
  },
  {
    "objectID": "content/lectures/06-analysis.html#landscape-paintings",
    "href": "content/lectures/06-analysis.html#landscape-paintings",
    "title": "06-analysis",
    "section": "Landscape paintings",
    "text": "Landscape paintings\n\nLandscape painting is the depiction in art of landscapes – natural scenery such as mountains, valleys, trees, rivers, and forests, especially where the main subject is a wide view – with its elements arranged into a coherent composition.1\n\nLandscape paintings tend to be wider than longer.\n\nPortrait painting is a genre in painting, where the intent is to depict a human subject.2\n\nPortrait paintings tend to be longer than wider."
  },
  {
    "objectID": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "href": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "title": "06-analysis",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\n❓ How, if at all, does the relationship between width and height of paintings vary by whether or not they have any landscape elements?\n\nggplot(data = pp, aes(x = Width_in, y = Height_in, \n                      color = factor(landsALL))) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"landscape\")"
  },
  {
    "objectID": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "href": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "title": "06-analysis",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modelling over simple visual inspection of data.\n\n\n\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "content/lectures/06-analysis.html#variation-around-the-model",
    "href": "content/lectures/06-analysis.html#variation-around-the-model",
    "title": "06-analysis",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\n\nStatistics is the explanation of variation in the context of what remains unexplained.\n\n\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#how-do-we-use-models",
    "href": "content/lectures/06-analysis.html#how-do-we-use-models",
    "title": "06-analysis",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nExplanation: Characterize the relationship between \\(y\\) and \\(x\\) via slopes for numerical explanatory variables or differences for categorical explanatory variables (Inference)\nPrediction: Plug in \\(x\\), get the predicted \\(y\\) (Machine Learning)"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html",
    "href": "content/lectures/08-effective-communication.html",
    "title": "08-effective-communication",
    "section": "",
    "text": "Q: for the last part of lecture 07, I tried glance(m_ht_wt) but it didn’t work because m_ht_wt doesn’t exist. Is “m_ht_wt” supposed be a model?\nA: Yup, this model was the height by weight model:\n\n\nm_ht_wt <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\n\nQ: I was not too sure what was going on when talking about the relationship between painting height and school.\nA: I don’t think you were the only one confused! Briefly here (and I’m happy to chat more before/after class and in OH), we were looking to determine/quantify the relationship between the size (height) of a painting and the school from which the painting originated. This was an example of having more than two categories for a categorical (factor) predictor. The important points were undersatnding that each level is compared to the baseline and the linear model that results from multiple categories. Part 4 of the lab gets into this a bit more too. Definitely follow up if you’re unsure after doing that part of the lab!\n\n\nQ: How do you calculate the linear regression model when you have non-numeric values? For example, on lab 04, when it asks to calculate the linear regression model by gender, the gender appears only as male and female. Suppose male is 1 and female is 0 (interpreted by the function), then male linear regression model is y =ax + 1?\nA: Close! the “1” would be plugged in as the value of x (in what you suggested)m not for the intercept. So the function would be \\(y=\\beta_1*1 + \\beta_0\\)\n\n\n\n\n\nLab04 due Friday\nHW02 due Monday\n\n\n\nPractice Midterms Now Available\n\nanswers posted next week\n\nMidterm Exam\n\nwill cover material through “Multiple Linear Regression”\nwill be released/posted next Friday after lab\nwill be due Monday Nov 6th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be completed individually (open Notes; open Internet)\n\n\n\n\n\nLink for Later\n\n\n\n\n\n\n\n\n\nCommunicating for your audience\nOral Communication\nWritten Communication\nVisual Communication\n\n\n\n\n\nBookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "href": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "title": "08-effective-communication",
    "section": "What does this mean?",
    "text": "What does this mean?\n❓ What does it mean to “consider your audience?”\n\nSimply: You do the work so they don’t have to.\n\n\n…also the aesthetic-usability effect exists."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "href": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "title": "08-effective-communication",
    "section": "What’s the right level?",
    "text": "What’s the right level?\n\n\nGeneral Audience\n✔ background\n🚫 limit technical details\n🎉 emphasize take-home\n\n\n\nTechnical Audience\n⬇ limit background\n💻 all-the-details\n🎉 emphasize take-home"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#considerations",
    "href": "content/lectures/08-effective-communication.html#considerations",
    "title": "08-effective-communication",
    "section": "Considerations",
    "text": "Considerations\n\nPlatform: written? oral?\n\n\n\nSetting: informal? formal?\n\n\n\n\nTiming: never go over your time limit!"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#storytelling",
    "href": "content/lectures/08-effective-communication.html#storytelling",
    "title": "08-effective-communication",
    "section": "Storytelling",
    "text": "Storytelling\n\nStories have a beginning, a middle, and an end.\n\n\n\nStories do not need every detail of what you’ve tried\n\n\n\n\nReports and presentations should tell a story\nPlanning out your report/presentation can help\n\n\n\n\nHold the audience’s attention with what needs to be said; do so effectively\nTell your audience why they should care; why it matters\nYou should explain your choices and the “why”"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "href": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "title": "08-effective-communication",
    "section": "Choose informative titles",
    "text": "Choose informative titles\nOn presentations: Balance b/w short and informative (goal: concise)\n\n\nAvoid: “Analyzing NHANES”\n\nBetter: “Data from the NHANES study shows that diet is related to overall health”\n\n\nOn visualizations: emphasize the take-home! (what’s learned or what action to take)\n\n\n\nAvoid: “Boxplot of gender”\n\nBetter: “Twice as many females as males included for analysis”\n\n\n\nAvoid: “Tickets vs. Time”\n\nBetter: “Staff unable to respond to incoming tickets; need to hire 2 FTEs”"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses",
    "href": "content/lectures/08-effective-communication.html#student-responses",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nconsider your audience\n\n\nspeak organizedly and logically\n\n\nA narrative format is preferable to an enumeration or a nonlinear presentation such as what would arise from reading off an infographic, for example.\n\n\nBe clear and direct!\n\n\nDon't say filler words like \"uhm\" \"like\". Take a pause instead\n\n\nConsciously speak slowly than you normally do (for fast talkers)\n\n\nspeak confidently and know material well enough to sound natural/not just memorize material\n\n\ninteract with the audience\n\n\nTalk slowly and clearly\n\n\nPut yourself in the shoes of your audience\n\n\nSpeak clearly at a good pace (not too fast or slow), make eye contact and engage with your audience\n\n\nEnunciation, proper volume, etc.\n\n\nI tend to speak in long sentences which can confuse the audience.\n\n\nTalk slower and clearer. Enunciation. Eye contact while talking. Avoid filler words.\n\n\nspeak clearly, slow down if you need to, don't just read off slides when presenting\n\n\nbe sure to point out areas of interest on your plots and explain them\n\n\nUse simply words if possible\n\n\nSpeaking slowly at someone\n\n\nDon't assume the audience know the same thing (like the research background or the research design) as you do. Another thing is: try to make sentence as simple as possible.\n\n\nTake moments to pause in between you sentences if you get lost.\n\n\nSpeak slowly and clearly\n\n\nKeep it engaging, involve audience participation, make eye contact, be confident\n\n\nTalk clearly and stick with the theme\n\n\nCater to your audience. Be conscious of what they know and don't know.\n\n\nUse appropriate font.\n\n\nSpeak clearly and slow down when you're picking up a fast pace."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "href": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "title": "08-effective-communication",
    "section": "Presentations are for listening",
    "text": "Presentations are for listening\n\nAdvantage: words to explain out loud what you’re showing\n\n\n\nYou are presenting for the person in the back of the room.\n\n\n\nTo accomplish:\n\ndon’t read directly off slides\nrepetition is ok: tell what you’re going to tell them, tell them, tell them what you told them\nuse animation to build your story (not to distract)\nintroduce your axes\ntext/labels larger\nwatch your speech speed\npractice!"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "href": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "title": "08-effective-communication",
    "section": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood",
    "text": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood\n\nRed Riding Hood (RRH) has to walk 0.54 mi from Point A (home) to Point B (Grandma’s)\nRRH meets Wolf who (1) runs ahead to Grandma’s, (2) eats her, and (3) dresses in her clothes\nRRH arrives at Grandmas at 2PM, asks her three questions\nIdentified problem: after third question, Wolf eats RRH\nSolution: vendor (Woodsman) employs tool (ax)\nExpected outcome: Grandma and RRH alive, wolf is not"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-1",
    "href": "content/lectures/08-effective-communication.html#student-responses-1",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nWhen sending a status update on a project to people in my team, I often had a habit of over explaining things such as specific terms or what a specific p-value indicates, etc. and it was redundant to my team, who all knew what these terms mean and how they are defined. On the flip side, during a meeting with non-technical people, a lot of my team's work didn't make sense to some people in the meeting and they requested info in \"layman terms\". My mentor advised me to not over-explain terms in depth to technical people, but keep things simple, clear and concise to those without a technical background.\n\n\nNA\n\n\nBe as concise as possible while getting your point across\n\n\nno need to write full sentences for bullet points\n\n\nwrite in a concise manner, don’t use big words unless it’s relevant\n\n\nkeep things succinct and write in a neutral tone\n\n\nBold or italicize important ideas/ key words in long writing\n\n\nMain idea sentence in the beginning of your text (report, essay, email).\n\n\nRefrain from using the first person. Talk in the past-tense\n\n\nUse grammarly\n\n\nWatch repetition of certain words. Occasionally change the structure of sentences. Know your audience.\n\n\nDont be too repetitive and Don’t have run on long sentences and get caught up in the details too much - I do that a lot ://\n\n\nAvoid ambiguity, have someone else proof read to double check what you've written, try not to make your sentences too wordy.\n\n\nWrite for your audience, avoid overuse of jargon and if necessary be sure to define the terms in a way appropriate for how you’re actually using them.\n\n\nBe clear and concise with the points you're trying to make and don't lose them with sentences that run on for too long\n\n\nBe concise and use as few words to effectively get point across. Don't go off on tangents.\n\n\nOrganize using subheadings, highlight main points using bold or colors if appropriate, vary sentence structure\n\n\nmake your sentences simpler to understand\n\n\nWhen giving a status report to a technical team, no need to over-explain terms. It is a lot of times effective to make a concise bullet point list such as p value= x, correlation coefficinet = y, instead of overexplaining what each value means b/c a technical team probably would know the signifance anyways\n\n\nWrite in words that the readers will understand, and do not assume that the readers will know what you mean.\n\n\nuse an outline to help organize the order of your paper. it helps you figure out where to place images, plots, and text\n\n\nOrganize arguments, don't be overly repetitive"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "href": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "title": "08-effective-communication",
    "section": "Benefits of written communciation",
    "text": "Benefits of written communciation\nYour audience has time to process…but the explanation has to be there!\n\nVisually: more on a single visualization\n\n\nYes, often there are different visualizations for reports/papers than for presentations/lectures."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "href": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "title": "08-effective-communication",
    "section": "When you have time to digest (read)",
    "text": "When you have time to digest (read)\n\n\n❓ What makes this an effective visualization for a written communication?”\nSource: Storytelling wtih data by cole nussbaumer knaflic"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#written-explanations",
    "href": "content/lectures/08-effective-communication.html#written-explanations",
    "title": "08-effective-communication",
    "section": "Written Explanations",
    "text": "Written Explanations\n\nVisualizations should be explained/interpreted\nModels should be explained\n\nshould be clear what question is being answered\nwhat conclusions is being drawn\nand what numbers were used to draw that conclusion"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "title": "08-effective-communication",
    "section": "Data Science Reports in .Rmd",
    "text": "Data Science Reports in .Rmd\n\nAs concise as possible\nNecessary details (for your audience); nothing more\n\nBe sure that the knit output contains what you intended (plots displayed; headers etc.)\n…and does NOT display stuff that doesn’t need to be there (messages/warnings suppressed, brainstorming, etc.)\n\nTypical Sections: Introduction/Background, Setup, Data, Analysis, Conclusion, References"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "href": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "title": "08-effective-communication",
    "section": "Controlling HTML document settings",
    "text": "Controlling HTML document settings\n\nTable of Contents\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\nTheme\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    theme: united\n    highlight: tango\n---\n\n\n\nFigure Options\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    fig_width: 7\n    fig_height: 6\n    fig_caption: true\n---\n\n\n\nCode Folding\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    code_folding: hide\n---"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "href": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "title": "08-effective-communication",
    "section": "Controlling code chunk output",
    "text": "Controlling code chunk output\n\nSpecified in the curly braces, separated by commas\n\n\n\neval: whether to execute the code chunk\necho: whether to include the code in the output\nwarning, message, and error: whether to show warnings, messages, or errors in the knit document\nfig.width and fig.height: control the width/height of plots\n\n\n\n\nControlling for the whole document:\n\nknitr::opts_chunk$set(fig.width = 8, collapse = TRUE)"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#editing-proofreading",
    "href": "content/lectures/08-effective-communication.html#editing-proofreading",
    "title": "08-effective-communication",
    "section": "Editing & Proofreading",
    "text": "Editing & Proofreading\n\nDid you end up telling a story?\n\nThings missing?\nThings to delete?\n\n\n\n\nDo not fall in love with your words/code/plots\n\n\n\n\nDo spell check\nDo read it over before sending/presenting/submitting"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "href": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "title": "08-effective-communication",
    "section": "Aside: Citing Sources",
    "text": "Aside: Citing Sources\nWhen are citations needed?\n\n\n\n“We will be doing our analysis using two different data sets created by two different groups: Donohue and Mustard + Lott, or simply Lott”\n\n\n\n\n\n\n“What turned from the idea of carrying firearms to protect oneself from enemies such as the British monarchy and the unknown frontier of North America has now become a nationwide issue.”\n\n\n\n\n\n\n“Right to Carry Laws refer to laws that specify how citizens are allowed to carry concealed handguns when they’re away from home without a permit”\n\n\n\n\n\n\n“In this case study, we are examining the relationship between unemployment rate, poverty rate, police staffing, and violent crime rate.”\n\n\n\n\n\n\n“In the United States, the second amendment permits the right to bear arms, and this law has not been changed since its creation in 1791.”\n\n\n\n\n\n\n“The Right to Carry Laws (RTC) is defined as”a law that specifies if and how citizens are allowed to have a firearm on their person or nearby in public.””\n\n\n\n\nReminder: You do NOT get docked points for citing others’ work. You can be at risk of AI Violation if you don’t. When in doubt, give credit."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "title": "08-effective-communication",
    "section": "Footnotes in .Rmd",
    "text": "Footnotes in .Rmd\nHow to specify a footnote in text:\nHere is some body text.[^1]\nHow to include the footnote’s reference:\n[^1]: This footnote will appear at the bottom of the page.\n\n\nNote: .bib files can be included with BibTeX references using the bibliography parameter in your YAML"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-2",
    "href": "content/lectures/08-effective-communication.html#student-responses-2",
    "title": "08-effective-communication",
    "section": "Student Responses",
    "text": "Student Responses\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nDon’t make slides overly colorful\n\n\nno borders on plots, graphs.don't write the whole info on one slide. take advantage of white space\n\n\nuse images to help audience understand\n\n\nHighlight important points in visualization.\n\n\ndon’t write everything on slides, just main points, try to use pictures that model/reflect/support talking points\n\n\nuse a legend for graphs\n\n\nmake sure your plot is relevant to the point you are trying to make\n\n\nreduce the number of words on the slide\n\n\nIt should be easy to understand/digest relatively quickly, only put absolutely necessary/relevant things\n\n\nPick a font and size for body+headings and commit to it\n\n\nTry to keep the design minimalistic and aesthetic, no cognitive overload that way.\n\n\nTitle your plots & graphs\n\n\nImages/visuals should help strengthen your presentation/story, not distracting from it\n\n\nUse lots of pictures!\n\n\nIt's better to have meaningful and intuitive color selection.\n\n\nSpecific graphs are more beneficial to a technical audience, while others are better for a non-technical one. My coworkers like graphs such as boxplots, but when presenting to partners, I have found that they prefer more intuitive/popular graphs like histograms or line plots.\n\n\nComplementary colors, appropriate graphs for the type of information you have and want to get across, neat and not cluttered\n\n\nConcise and clear, use colors and space effectively\n\n\nUse color responsibly in graphs/tables, make text large enough for everyone in the room to see, don't overload slides with information\n\n\ndont put too much words\n\n\nDon’t put too many animations (if any)\n\n\nLess is more. Too much can distract and detract from the main point\n\n\ngood contrast color between background and text\n\n\nmake clear visual guide, don’t make it too complicated\n\n\nAvoid neon colors\n\n\nKeep accessibility in mind when presenting visuals. (e.g. using texture instead of color, image descriptions, etc.)\n\n\nMake presentations look cleaner. Seems like you know what youre talking about."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "href": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "title": "08-effective-communication",
    "section": "The Glamour of Graphics",
    "text": "The Glamour of Graphics\n\nbuilds on top of the grammar (components) of a graphic\nconsiderations for the design of a graphic\ncolor, typography, layout\ngoing from accurate to 😍effective\n\n\n\nThese ideas and slides are all modified from Will Chase’s rstudio::conf2020 slides/talk"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "href": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "title": "08-effective-communication",
    "section": "Left-align titles at top-left",
    "text": "Left-align titles at top-left\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "href": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "title": "08-effective-communication",
    "section": "Avoid head-tilting",
    "text": "Avoid head-tilting\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "href": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "title": "08-effective-communication",
    "section": "Borders & Backgrounds: 👎",
    "text": "Borders & Backgrounds: 👎\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_bw() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "href": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "title": "08-effective-communication",
    "section": "Organize & Remove/Lighten as much as possible",
    "text": "Organize & Remove/Lighten as much as possible\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#legends-suck",
    "href": "content/lectures/08-effective-communication.html#legends-suck",
    "title": "08-effective-communication",
    "section": "Legends suck",
    "text": "Legends suck\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 7) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 20) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#additional-guidance",
    "href": "content/lectures/08-effective-communication.html#additional-guidance",
    "title": "08-effective-communication",
    "section": "Additional Guidance",
    "text": "Additional Guidance\n\nWhite space is like garlic - take the amount you need and triple it\nFonts Matter\nUse Color Effectively"
  },
  {
    "objectID": "content/lectures/09-mlr.html",
    "href": "content/lectures/09-mlr.html",
    "title": "09-mlr",
    "section": "",
    "text": "Q: What are some ways we can make our visuals accessible? For example, how can we account for color blind people when incorporating graphs and figures in our presentation?\nA: Love this question! One way to do this is to use tools built for just this purpose! For example, for color blindness, ColorBrewer2 allows you to toggle for color palletes that are “colorblind safe.” Similarly, for written content online, ensuring that you’re including alt-text for all images, to enable visual understanding by those with vision impariments/differences. For those with learning differences (i.e. dyslexia, ADHD), I’m sure there’s research out there with respect to viz, and I should make myself more familiar, but ensuring that you’re not using very similar acronyms/initialisms for different context or using consistent icons (Rather than letters) across visualizations, etc. can really help individuals with learning differences and overall make your viz more consistent/understandable. This is just the tip of the iceberg. Additional suggestions here.\n\n\n\n\n\n🎃 Happy Halloween!\n\nNo OH today after class\nMake up: tomorrow 10-11AM (CSB 243)\n\n🔬 There is NO lab to turn in this week\n\nLab will be used for midterm review\nCome to lab with questions!\n\n🏫 Midterm is due next Monday at 11:59PM\n\nwill be released 5PM Friday\ncompleted individually\nopen notes/open Internet\nNOT timed\ntypically takes students ~2.5h to complete (big range: 2-15h)\n\nLab05 & HW03 will be released next Monday\n\n\n\n\nIntroduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors\n\n\n\n\nMultiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nData: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |> \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62"
  },
  {
    "objectID": "content/lectures/09-mlr.html#multiple-predictors",
    "href": "content/lectures/09-mlr.html#multiple-predictors",
    "title": "09-mlr",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nResponse variable: log_price\nExplanatory variables: Width and height\n\n\n\nlin_mod <- linear_reg() |>\n  set_engine(\"lm\")\n\npp_fit <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data = pp)\ntidy(pp_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4"
  },
  {
    "objectID": "content/lectures/09-mlr.html#linear-model-with-multiple-predictors",
    "href": "content/lectures/09-mlr.html#linear-model-with-multiple-predictors",
    "title": "09-mlr",
    "section": "Linear model with multiple predictors",
    "text": "Linear model with multiple predictors\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4\n\n\n\n\\[\\widehat{log\\_price} = 4.77 + 0.0269 \\times width - 0.0133 \\times height\\]\n\n❓ How do we interpret this model?"
  },
  {
    "objectID": "content/lectures/09-mlr.html#price-surface-area-and-living-artist",
    "href": "content/lectures/09-mlr.html#price-surface-area-and-living-artist",
    "title": "09-mlr",
    "section": "Price, surface area, and living artist",
    "text": "Price, surface area, and living artist\n\nExplore the relationship between price of paintings and surface area, conditioned on whether or not the artist is still living\nFirst visualize and explore, then model\n\n\n\nBut first, prep the data:\n\n\npp <- pp |>\n  mutate(artistliving = case_when(artistliving == 0 ~ \"Deceased\", \n                                  TRUE ~ \"Living\"))\n\npp |>\n  count(artistliving)\n\n# A tibble: 2 × 2\n  artistliving     n\n  <chr>        <int>\n1 Deceased      2937\n2 Living         456"
  },
  {
    "objectID": "content/lectures/09-mlr.html#typical-surface-area",
    "href": "content/lectures/09-mlr.html#typical-surface-area",
    "title": "09-mlr",
    "section": "Typical surface area",
    "text": "Typical surface area\n\nPlotCode\n\n\n\n\n\n\n\nTypical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.\n\n\n\nggplot(data = pp, aes(x = Surface, fill = artistliving)) +\n  geom_histogram(binwidth = 500) + \n  facet_grid(artistliving ~ .) +\n  scale_fill_manual(values = c(\"#E48957\", \"#071381\")) +\n  guides(fill = \"none\") +\n  labs(x = \"Surface area\", y = NULL) +\n  geom_vline(xintercept = 1000) +\n  geom_vline(xintercept = 5000, linetype = \"dashed\", color = \"gray\")"
  },
  {
    "objectID": "content/lectures/09-mlr.html#narrowing-the-scope",
    "href": "content/lectures/09-mlr.html#narrowing-the-scope",
    "title": "09-mlr",
    "section": "Narrowing the scope",
    "text": "Narrowing the scope\n\nPlotCode\n\n\nFor simplicity let’s focus on the paintings with Surface < 5000:\n\n\n\n\n\n\n\n\npp_Surf_lt_5000 <- pp |>\n  filter(Surface < 5000)\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Artist\", shape = \"Artist\") +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\"))"
  },
  {
    "objectID": "content/lectures/09-mlr.html#facet-to-get-a-better-look",
    "href": "content/lectures/09-mlr.html#facet-to-get-a-better-look",
    "title": "09-mlr",
    "section": "Facet to get a better look",
    "text": "Facet to get a better look\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~artistliving) +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\")) +\n  labs(color = \"Artist\", shape = \"Artist\")"
  },
  {
    "objectID": "content/lectures/09-mlr.html#two-ways-to-model",
    "href": "content/lectures/09-mlr.html#two-ways-to-model",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living.\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living."
  },
  {
    "objectID": "content/lectures/09-mlr.html#interacting-explanatory-variables",
    "href": "content/lectures/09-mlr.html#interacting-explanatory-variables",
    "title": "09-mlr",
    "section": "Interacting explanatory variables",
    "text": "Interacting explanatory variables\n\nIncluding an interaction effect in the model allows for different slopes, i.e. nonparallel lines.\nThis implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\nThis can be accomplished by adding an interaction variable: the product of two explanatory variables."
  },
  {
    "objectID": "content/lectures/09-mlr.html#two-ways-to-model-1",
    "href": "content/lectures/09-mlr.html#two-ways-to-model-1",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\n\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living\n\n\n\n\n\n\n\n\n\n\n❓ Which does your intuition/knowledge of the data suggest is more appropriate?\nPut a green sticky if you think main; pink if you think interaction."
  },
  {
    "objectID": "content/lectures/09-mlr.html#fit-model-with-main-effects",
    "href": "content/lectures/09-mlr.html#fit-model-with-main-effects",
    "title": "09-mlr",
    "section": "Fit model with main effects",
    "text": "Fit model with main effects\n\nResponse variable: log_price\nExplanatory variables: Surface area and artistliving\n\n\npp_main_fit <- lin_mod |>\n  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)\ntidy(pp_main_fit)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        4.88     0.0424       115.   0       \n2 Surface            0.000265 0.0000415      6.39 1.85e-10\n3 artistlivingLiving 0.137    0.0970         1.41 1.57e- 1\n\n\n\n\\[\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times artistliving\\]"
  },
  {
    "objectID": "content/lectures/09-mlr.html#solving-the-model",
    "href": "content/lectures/09-mlr.html#solving-the-model",
    "title": "09-mlr",
    "section": "Solving the model",
    "text": "Solving the model\n\nNon-living artist: Plug in 0 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 0\\)\n\\(= 4.88 + 0.000265 \\times surface\\)\n\n\nLiving artist: Plug in 1 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 1\\)\n\\(= 5.017 + 0.000265 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/09-mlr.html#visualizing-main-effects",
    "href": "content/lectures/09-mlr.html#visualizing-main-effects",
    "title": "09-mlr",
    "section": "Visualizing main effects",
    "text": "Visualizing main effects\n\n\n\nSame slope: Rate of change in price as the surface area increases does not vary between paintings by living and non-living artists.\nDifferent intercept: Paintings by living artists are consistently more expensive than paintings by non-living artists."
  },
  {
    "objectID": "content/lectures/09-mlr.html#interpreting-main-effects",
    "href": "content/lectures/09-mlr.html#interpreting-main-effects",
    "title": "09-mlr",
    "section": "Interpreting main effects",
    "text": "Interpreting main effects\n\ntidy(pp_main_fit) |> \n  mutate(exp_estimate = exp(estimate)) |>\n  select(term, estimate, exp_estimate)\n\n# A tibble: 3 × 3\n  term               estimate exp_estimate\n  <chr>                 <dbl>        <dbl>\n1 (Intercept)        4.88           132.  \n2 Surface            0.000265         1.00\n3 artistlivingLiving 0.137            1.15\n\n\n\n\nAll else held constant, for each additional square inch in painting’s surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.\nAll else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.\nPaintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres."
  },
  {
    "objectID": "content/lectures/09-mlr.html#main-vs.-interaction-effects",
    "href": "content/lectures/09-mlr.html#main-vs.-interaction-effects",
    "title": "09-mlr",
    "section": "Main vs. interaction effects",
    "text": "Main vs. interaction effects\n\nThe way we specified our main effects model only lets artistliving affect the intercept.\nModel implicitly assumes that paintings with living and deceased artists have the same slope and only allows for different intercepts.\n\n\n❓ What seems more appropriate in this case?\n\nSame slope and same intercept for both colors\nSame slope and different intercept for both colors\nDifferent slope and different intercept for both colors"
  },
  {
    "objectID": "content/lectures/09-mlr.html#interaction-surface-artistliving",
    "href": "content/lectures/09-mlr.html#interaction-surface-artistliving",
    "title": "09-mlr",
    "section": "Interaction: Surface * artistliving",
    "text": "Interaction: Surface * artistliving"
  },
  {
    "objectID": "content/lectures/09-mlr.html#fit-model-with-interaction-effects",
    "href": "content/lectures/09-mlr.html#fit-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Fit model with interaction effects",
    "text": "Fit model with interaction effects\n\nResponse variable: log_price\nExplanatory variables: Surface area, artistliving, and their interaction\n\n\npp_int_fit <- lin_mod |>\n  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)\ntidy(pp_int_fit)\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139"
  },
  {
    "objectID": "content/lectures/09-mlr.html#linear-model-with-interaction-effects",
    "href": "content/lectures/09-mlr.html#linear-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Linear model with interaction effects",
    "text": "Linear model with interaction effects\n\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139  \n\n\n\\[\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface - 0.126 \\times artistliving\\] \\[+ ~ 0.00048 \\times surface * artistliving\\]"
  },
  {
    "objectID": "content/lectures/09-mlr.html#interpretation-of-interaction-effects",
    "href": "content/lectures/09-mlr.html#interpretation-of-interaction-effects",
    "title": "09-mlr",
    "section": "Interpretation of interaction effects",
    "text": "Interpretation of interaction effects\n\n\nRate of change in price as the surface area of the painting increases does vary between paintings by living and non-living artists (different slopes)\nSome paintings by living artists are more expensive than paintings by non-living artists, and some are not (different intercept).\n\n\n\n\n\n\n\nNon-living artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 0 + 0.00048 \\times surface \\times 0\\) \\(= 4.91 + 0.00021 \\times surface\\)\nLiving artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 1 + 0.00048 \\times surface \\times 1\\) \\(= 4.91 + 0.00021 \\times surface\\) \\(- 0.126 + 0.00048 \\times surface\\) \\(= 4.784 + 0.00069 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/09-mlr.html#r-squared",
    "href": "content/lectures/09-mlr.html#r-squared",
    "title": "09-mlr",
    "section": "R-squared",
    "text": "R-squared\n\n\\(R^2\\) is the percentage of variability in the response variable explained by the regression model.\n\n\nglance(pp_main_fit)$r.squared\n\n[1] 0.01320884\n\nglance(pp_int_fit)$r.squared\n\n[1] 0.0176922\n\n\n\n\nClearly the model with interactions has a higher \\(R^2\\).\n\n\n\n\nHowever using \\(R^2\\) for model selection in models with multiple explanatory variables is not a good idea as \\(R^2\\) increases when any variable is added to the model."
  },
  {
    "objectID": "content/lectures/09-mlr.html#adjusted-r-squared",
    "href": "content/lectures/09-mlr.html#adjusted-r-squared",
    "title": "09-mlr",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\nIt appears that adding the interaction actually increased adjusted \\(R^2\\), so we should indeed use the model with the interactions.\n\nglance(pp_main_fit)$adj.r.squared\n\n[1] 0.01258977\n\nglance(pp_int_fit)$adj.r.squared\n\n[1] 0.01676753"
  },
  {
    "objectID": "content/lectures/09-mlr.html#third-order-interactions",
    "href": "content/lectures/09-mlr.html#third-order-interactions",
    "title": "09-mlr",
    "section": "Third order interactions",
    "text": "Third order interactions\n\nCan you? Yes\nShould you? Probably not if you want to interpret these interactions in context of the data."
  },
  {
    "objectID": "content/lectures/09-mlr.html#in-pursuit-of-occams-razor",
    "href": "content/lectures/09-mlr.html#in-pursuit-of-occams-razor",
    "title": "09-mlr",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model."
  },
  {
    "objectID": "content/lectures/09-mlr.html#backward-selection",
    "href": "content/lectures/09-mlr.html#backward-selection",
    "title": "09-mlr",
    "section": "Backward selection",
    "text": "Backward selection\nFor this demo, we’ll ignore interaction effects…and just model main effects to start:\n\npp_full <-  lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp) \n\nglance(pp_full)$adj.r.squared\n\n[1] 0.02570141\n\n\n\n\\(R^2\\) (full): 0.0257014\n\n\n\nRemove artistliving\n\npp_noartist <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface, data=pp) \n\nglance(pp_noartist)$adj.r.squared\n\n[1] 0.02579859\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\n…Improved variance explained\n\n\n\nRemove Surface\n\npp_noartist_nosurface <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data=pp) \n\nglance(pp_noartist_nosurface)$adj.r.squared\n\n[1] 0.02231559\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\\(R^2\\) (no artistliving or Surface): 0.0223\n\n\n\n…no longer gaining improvement, so we stick with: log_price ~ Width_in + Height_in + Surface"
  },
  {
    "objectID": "content/lectures/09-mlr.html#other-approach",
    "href": "content/lectures/09-mlr.html#other-approach",
    "title": "09-mlr",
    "section": "Other approach:",
    "text": "Other approach:\n\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n\nStep 1: Fit model (w/o tidymodels)\n\n# fit the model (not using tidymodels)\nmod <- lm(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp_Surf_lt_5000)\n\n\n\nStep 2: Determine which variables to remove\n\nols_step_backward_p(mod)\n\n\n                             Elimination Summary                               \n------------------------------------------------------------------------------\n        Variable                      Adj.                                        \nStep      Removed       R-Square    R-Square     C(p)        AIC         RMSE     \n------------------------------------------------------------------------------\n   1    artistliving      0.0261      0.0251    3.8495    12603.7727    1.8315    \n------------------------------------------------------------------------------\n\n\n…specifies that artistliving should be removed\n\n\nStep 2 (alternate): Compare all possible models…\n\nols_step_all_possible(mod) |>\n  arrange(desc(adjr))\n\n   Index N                              Predictors     R-Square Adj. R-Square\n1     11 3              Width_in Height_in Surface 0.0260749939  0.0251349118\n2     15 4 Width_in Height_in Surface artistliving 0.0263412027  0.0250876993\n3      5 2                      Width_in Height_in 0.0256902566  0.0250634893\n4     12 3         Width_in Height_in artistliving 0.0259732581  0.0250330779\n5      6 2                        Width_in Surface 0.0249136264  0.0242863596\n6     13 3           Width_in Surface artistliving 0.0251787948  0.0242378477\n7      7 2                   Width_in artistliving 0.0212864021  0.0206568018\n8      1 1                                Width_in 0.0209415833  0.0206267736\n9      8 2                    Surface artistliving 0.0132088377  0.0125897717\n10     2 1                                 Surface 0.0125899681  0.0122803381\n11    14 3          Height_in Surface artistliving 0.0130836930  0.0121310711\n12     9 2                       Height_in Surface 0.0126782901  0.0120431523\n13    10 2                  Height_in artistliving 0.0062698155  0.0056305552\n14     3 1                               Height_in 0.0058797727  0.0055601199\n15     4 1                            artistliving 0.0005531617  0.0002397573\n   Mallow's Cp\n1     3.849487\n2     5.000000\n3     3.077206\n4     4.174132\n5     5.555476\n6     6.709309\n7    17.130153\n8    16.230489\n9    51.971190\n10   52.001268\n11   45.305459\n12   44.599123\n13   65.048926\n14   64.293574\n15   91.485605"
  },
  {
    "objectID": "content/lectures/09-mlr.html#recap",
    "href": "content/lectures/09-mlr.html#recap",
    "title": "09-mlr",
    "section": "Recap",
    "text": "Recap\n\nCan you model and interpret linear models with multiple predictors?\nCan you explain the difference in a model with main effects vs. interaction effects?\nCan you compare different models and determine how to proceed?\nCan you carry out and explain backward selection?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html",
    "href": "content/lectures/11-cs01-data.html",
    "title": "11-cs01-data",
    "section": "",
    "text": "Q: How much time are we expected to spend on the case studies?\nA: That’s hard to say. I would recommend spending a bit of time after each lecture ensuring I understand the code presented. It will eventually be included in your final report, so you’ll need to understand/describe/explain it. After the case study has been presented, I would expect a few hours from each group member to complete the extension and write the report. Last year students reported typically spending 4-6h on case studies (with a big range around that median).\n\n\nQ: For the general project plan how much time should we budget towards working on this?\nA: Students report spending ~10h on their final project\n\n\nQ: Are we allowed to work with some of our case study partners for a final project?\nA: Absolutely! My hope is through the case studies students will get to know one another a bit and hopefully want to work together again!\n\n\n\n\n\n💻 Midterm is due next Monday at 11:59PM (released Friday 5PM; practice answer keys are on website)\n❓ Mid-course survey will open with midterm - please complete after finishing midterm; will have a week to complete\n🔬 Lab is for midterm review; Lab05 & HW03 will be released next Monday\n\n\n\n\n\nBackground\nData Intro\nPaper Results\nWrangle"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#motor-vehicle-accidents-mvas",
    "href": "content/lectures/11-cs01-data.html#motor-vehicle-accidents-mvas",
    "title": "11-cs01-data",
    "section": "Motor Vehicle Accidents (MVAs)",
    "text": "Motor Vehicle Accidents (MVAs)\n\n2/3 of US trauma center admissions are due to MVAs\n~60% of such patients testing positive for drugs or alcohol\nAlcohol and cannabis are most frequently detected\n\nSource: https://academic.oup.com/clinchem/article/59/3/478/5621997"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#legalization-of-marijuana",
    "href": "content/lectures/11-cs01-data.html#legalization-of-marijuana",
    "title": "11-cs01-data",
    "section": "Legalization of Marijuana",
    "text": "Legalization of Marijuana\n\nFederally illegal in the US\nDecriminalized in many states\nMedically available in 15 states\nLegal for recreational use in 24 states (including CA)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#increased-roadside-surveys",
    "href": "content/lectures/11-cs01-data.html#increased-roadside-surveys",
    "title": "11-cs01-data",
    "section": "Increased roadside surveys",
    "text": "Increased roadside surveys\n\n\n25% increase in use nationwide from 2002 to 2015 (survey)\nTHC detection in drivers increased by 48% from 2007 to 2014\nIncreased prevalence of consumption -> possible intoxication -> possible impaired driving -> public health concern"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#dui-of-alcohol-duia",
    "href": "content/lectures/11-cs01-data.html#dui-of-alcohol-duia",
    "title": "11-cs01-data",
    "section": "DUI of Alcohol (DUIA)",
    "text": "DUI of Alcohol (DUIA)\n\n\nThe science is there. Don’t do it.\nDUIA has decreased since the 1970s\n\n% of nighttime, weekend drivers testing over the legal limit (BAC > 0.08 g/dL) decreased from 7.5% (1973) to 2.2% (2007) link"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#dui-of-cannabis",
    "href": "content/lectures/11-cs01-data.html#dui-of-cannabis",
    "title": "11-cs01-data",
    "section": "DUI of Cannabis",
    "text": "DUI of Cannabis\n\nIn a 2007 survey, 16.3% of nighttime drivers were drug-positive link\n\n8.6% of these tested positive for THC\n\nExperimental and cognitive studies suggest cannabis-induced impairment increases risk of motor vehicle crashes:\n\n\n\nEvidence suggests recent smoking and/or blood THC concentrations 2–5 ng/mL are associated with substantial driving impairment, particularly in occasional smokers.link"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#roadside-detection",
    "href": "content/lectures/11-cs01-data.html#roadside-detection",
    "title": "11-cs01-data",
    "section": "Roadside Detection",
    "text": "Roadside Detection\n\nper se laws: “a driver is deemed to have committed an offense if THC is detected at or above a pre-determined cutoff” link\n\n\n\nDefining cutoffs for safe driving is difficult\nTHC concentration differs by:\n\n“smoking topography” (time to smoke; number of puffs)\nfrequency of use\nroute of ingestion\n\n\n\n\nAs of 2021…link\n\n\n19 states have per se or zero tolerance cannabis laws\nStates with per se laws (Illinois, Montana, Nevada, Ohio, Pennsylvania, Washington and West Virginia), cutoffs range from 1 to 5 ng/mL THC in whole blood.\nIn 3 states, per se limits also apply to THC metabolites\nColorado: “reasonable inference” - blood contained >5 ng/mL THC at the time of the offense\n3 states zero tolerance for THC; 8 states for THC and metabolites"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#metabolism",
    "href": "content/lectures/11-cs01-data.html#metabolism",
    "title": "11-cs01-data",
    "section": "Metabolism",
    "text": "Metabolism\n\n\npeak blood concentrations occur during smoking, then drop rapidly link\nsubjective ‘high’ persists for several hours, varies greatly between individuals\nTHC concentrations remain detectable in frequent users longer than occasional users link\nTHC and certain metabolites can be detected in blood for weeks to months after use and do not necessarily indicate impairment"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#detection",
    "href": "content/lectures/11-cs01-data.html#detection",
    "title": "11-cs01-data",
    "section": "Detection",
    "text": "Detection\nVarious approaches:\n\nDetect impairment (officers detect DUIC)\nDetect recent use (test for compounds)\nCombine recent use + impairment\n\n\nFocus here: Can we identify a biomarker of recent use?\n\nrecent use: defined here as within 3h\ntesting THC and metabolites in blood, oral fluid (OF), and breath"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#aside-case-study-report",
    "href": "content/lectures/11-cs01-data.html#aside-case-study-report",
    "title": "11-cs01-data",
    "section": "Aside: Case Study Report",
    "text": "Aside: Case Study Report\n\nYour Case study will need a background section\nIt can use/summarize/paraphrase the information here (you should cite the source, not me)\nBut, you’re not limited to this information\nYou are allowed/encouraged to dig deeper, include what’s most important, add to, remove, etc.\nThere are a lot of citations in this section - go ahead and peruse them/others/use references in these papers"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#participants",
    "href": "content/lectures/11-cs01-data.html#participants",
    "title": "11-cs01-data",
    "section": "Participants",
    "text": "Participants\n\nplacebo-controlled, double-blinded, randomized study\n\n\n\nrecruited:\n\nvolunteers 21-55y/o\nhad a driver’s license\nself-reported cannabis use >= 4x in the past month\n\n\n\n\n\nParticipants were:\n\ncompensated\nmedically evaluated (for safety)\nasked to refrain from use for 2d prior to participation\nexclusion criteria: OF THC concentration ≥5 ng/mL on day of study (n=7)\n\n\n\n\n\nStudy included 191 participants"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#demographics",
    "href": "content/lectures/11-cs01-data.html#demographics",
    "title": "11-cs01-data",
    "section": "Demographics",
    "text": "Demographics\n\nSource: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#experimental-design",
    "href": "content/lectures/11-cs01-data.html#experimental-design",
    "title": "11-cs01-data",
    "section": "Experimental Design",
    "text": "Experimental Design\nParticipants were:\n\n\nrandomly assigned to receive a cigarette containing placebo (0.02%), or 5.9% or 13.4% THC\nBlood, OF and breath were collected prior to smoking\nsmoked a 700 mg cigarette ad libitum within 10 min, with a minimum of four puffs.\nAfter smoking, 4 additional OF and breath and 8 blood collections were completed at time points up to ∼6h from the start of smoking.\nParticipants ate and drank water between collections, although not within 10 min of OF collection."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#timeline",
    "href": "content/lectures/11-cs01-data.html#timeline",
    "title": "11-cs01-data",
    "section": "Timeline",
    "text": "Timeline\n\nSource: Fitzgerald et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#consumption",
    "href": "content/lectures/11-cs01-data.html#consumption",
    "title": "11-cs01-data",
    "section": "Consumption",
    "text": "Consumption\n Source: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#topography",
    "href": "content/lectures/11-cs01-data.html#topography",
    "title": "11-cs01-data",
    "section": "Topography",
    "text": "Topography\n Source: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#subjective-highness",
    "href": "content/lectures/11-cs01-data.html#subjective-highness",
    "title": "11-cs01-data",
    "section": "Subjective Highness",
    "text": "Subjective Highness\n\nSource: Hoffman et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#our-datasets",
    "href": "content/lectures/11-cs01-data.html#our-datasets",
    "title": "11-cs01-data",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#the-data-1",
    "href": "content/lectures/11-cs01-data.html#the-data-1",
    "title": "11-cs01-data",
    "section": "The Data",
    "text": "The Data\nYou’ll have access once your groups/repos are created…(today I want people to follow along; there will be time to try on your own soon!)\n\nWB <- read_csv(\"data/Blood.csv\")\nBR <- read_csv(\"data/Breath.csv\")\nOF <- read_csv(\"data/OF.csv\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-wb",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-wb",
    "title": "11-cs01-data",
    "section": "First Look at the data (WB)",
    "text": "First Look at the data (WB)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-of",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-of",
    "title": "11-cs01-data",
    "section": "First Look at the data (OF)",
    "text": "First Look at the data (OF)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-br",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-br",
    "title": "11-cs01-data",
    "section": "First Look at the data (BR)",
    "text": "First Look at the data (BR)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-1-pre-smoking",
    "href": "content/lectures/11-cs01-data.html#fig-1-pre-smoking",
    "title": "11-cs01-data",
    "section": "Fig 1: Pre-smoking",
    "text": "Fig 1: Pre-smoking"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-2-sensitivity-and-specificity",
    "href": "content/lectures/11-cs01-data.html#fig-2-sensitivity-and-specificity",
    "title": "11-cs01-data",
    "section": "Fig 2: Sensitivity and Specificity",
    "text": "Fig 2: Sensitivity and Specificity"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-3-cross-compound-relationship",
    "href": "content/lectures/11-cs01-data.html#fig-3-cross-compound-relationship",
    "title": "11-cs01-data",
    "section": "Fig 3: Cross-compound relationship",
    "text": "Fig 3: Cross-compound relationship"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-4-cutoffs",
    "href": "content/lectures/11-cs01-data.html#fig-4-cutoffs",
    "title": "11-cs01-data",
    "section": "Fig 4: Cutoffs",
    "text": "Fig 4: Cutoffs"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-5-youden",
    "href": "content/lectures/11-cs01-data.html#fig-5-youden",
    "title": "11-cs01-data",
    "section": "Fig 5: Youden",
    "text": "Fig 5: Youden\n\n\n…and if there’s time PPV and Accuracy post 3h"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#what-came-after",
    "href": "content/lectures/11-cs01-data.html#what-came-after",
    "title": "11-cs01-data",
    "section": "What Came After",
    "text": "What Came After\n Source: Fiztgerald et al."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#oral-fluid",
    "href": "content/lectures/11-cs01-data.html#oral-fluid",
    "title": "11-cs01-data",
    "section": "Oral Fluid",
    "text": "Oral Fluid\n\nOF <- OF |>\n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |>  \n  janitor::clean_names() |>\n  rename(thcoh = x11_oh_thc,\n         thcv = thc_v)\n\n❓ What’s this accomplishing?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#whole-blood",
    "href": "content/lectures/11-cs01-data.html#whole-blood",
    "title": "11-cs01-data",
    "section": "Whole Blood",
    "text": "Whole Blood\n\nWB <- WB |> \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\")) |> \n  janitor::clean_names() |>\n  rename(thcoh = x11_oh_thc,\n         thccooh = thc_cooh,\n         thccooh_gluc = thc_cooh_gluc,\n         thcv = thc_v)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#breath",
    "href": "content/lectures/11-cs01-data.html#breath",
    "title": "11-cs01-data",
    "section": "Breath",
    "text": "Breath\n\nBR <- BR |> \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |> \n  janitor::clean_names() |> \n  rename(thc = thc_pg_pad)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#question-1",
    "href": "content/lectures/11-cs01-data.html#question-1",
    "title": "11-cs01-data",
    "section": "Question",
    "text": "Question\n❓ We’re doing very similar things across three similar (albeit different) datasets. What would be a better approach?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#storing-compounds",
    "href": "content/lectures/11-cs01-data.html#storing-compounds",
    "title": "11-cs01-data",
    "section": "Storing compounds",
    "text": "Storing compounds\nWe’ll need these later in our functions\n\n# whole blood\ncompounds_WB <-  as.list(colnames(Filter(function(x) !all(is.na(x)), WB[6:13])))\n\n# breath\ncompounds_BR <-  as.list(colnames(Filter(function(x) !all(is.na(x)), BR[6])))\n\n# oral fluid\ncompounds_OF <-  as.list(colnames(Filter(function(x) !all(is.na(x)), OF[6:12])))\n\n\n\n# to get a sense of output\ncompounds_WB\n\n[[1]]\n[1] \"cbn\"\n\n[[2]]\n[1] \"cbd\"\n\n[[3]]\n[1] \"thc\"\n\n[[4]]\n[1] \"thcoh\"\n\n[[5]]\n[1] \"thccooh\"\n\n[[6]]\n[1] \"thccooh_gluc\"\n\n[[7]]\n[1] \"cbg\"\n\n[[8]]\n[1] \"thcv\""
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#storing-timepoints",
    "href": "content/lectures/11-cs01-data.html#storing-timepoints",
    "title": "11-cs01-data",
    "section": "Storing timepoints",
    "text": "Storing timepoints\n\ntimepoints_WB = tibble(start = c(-400, 0, 30, 70, 100, 180, 210, 240, 270, 300), \n                       stop = c(0, 30, 70, 100, 180, 210, 240, 270, 300, max(WB$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-70 min\",\n                                     \"71-100 min\",\"101-180 min\",\"181-210 min\",\n                                     \"211-240 min\",\"241-270 min\",\n                                     \"271-300 min\", \"301+ min\") )\n\n\n\ntimepoints_WB\n\n# A tibble: 10 × 3\n   start  stop timepoint  \n   <dbl> <dbl> <chr>      \n 1  -400     0 pre-smoking\n 2     0    30 0-30 min   \n 3    30    70 31-70 min  \n 4    70   100 71-100 min \n 5   100   180 101-180 min\n 6   180   210 181-210 min\n 7   210   240 211-240 min\n 8   240   270 241-270 min\n 9   270   300 271-300 min\n10   300   382 301+ min   \n\n\n\n\n…and in BR and OF\n\ntimepoints_BR = tibble(start = c(-400, 0, 40, 90, 180, 210, 240, 270), \n                       stop = c(0, 40, 90, 180, 210, 240, 270, \n                                max(BR$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-40 min\",\"41-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\"))\ntimepoints_OF = tibble(start = c(-400, 0, 30, 90, 180, 210, 240, 270), \n                       stop = c(0, 30, 90, 180, 210, 240, 270, \n                                max(OF$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\") )"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-udf-assign_timepoint",
    "href": "content/lectures/11-cs01-data.html#first-udf-assign_timepoint",
    "title": "11-cs01-data",
    "section": "First UDF: assign_timepoint",
    "text": "First UDF: assign_timepoint\n\nassign_timepoint <- function(x, timepoints){\n  if(!is.na(x)){ \n    timepoints$timepoint[x > timepoints$start & x <= timepoints$stop]\n  }else{\n    NA\n  }\n}\n\n🧠 What’s a UDF? What do you think this is doing?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#timepoints-to-use",
    "href": "content/lectures/11-cs01-data.html#timepoints-to-use",
    "title": "11-cs01-data",
    "section": "Timepoints to use",
    "text": "Timepoints to use\n\n WB <- WB |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_WB),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_WB$timepoint))\n\n# let's get a sense for what this did\nlevels(WB$timepoint_use)\n\n [1] \"pre-smoking\" \"0-30 min\"    \"31-70 min\"   \"71-100 min\"  \"101-180 min\"\n [6] \"181-210 min\" \"211-240 min\" \"241-270 min\" \"271-300 min\" \"301+ min\"   \n\n\nNote: map_* allow you to apply a function across multiple “things” (here: across all rows in a dataframe)\n❓What do you think the above is doing?\n\n\nOF <- OF |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_OF),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_OF$timepoint))\n\nBR <- BR |> \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_BR),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_BR$timepoint))"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#drop-duplicates",
    "href": "content/lectures/11-cs01-data.html#drop-duplicates",
    "title": "11-cs01-data",
    "section": "Drop Duplicates",
    "text": "Drop Duplicates\n\n drop_dups <- function(dataset){\n  out <- dataset |> \n    filter(!is.na(timepoint_use)) |> \n    group_by(timepoint_use) |> \n    distinct(id, .keep_all = TRUE) |> \n    ungroup()\n  return(out)\n} \n\n❓What do you think the above is doing?\n\n\nWB_dups <- drop_dups(WB)\nOF_dups <- drop_dups(OF)\nBR_dups <- drop_dups(BR)\n\n❓What would you do to try to understand what this has done?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#saving-intermediate-files",
    "href": "content/lectures/11-cs01-data.html#saving-intermediate-files",
    "title": "11-cs01-data",
    "section": "Saving Intermediate Files",
    "text": "Saving Intermediate Files\nCleaned/wrangled files as CSVs:\n\nwrite_csv(WB, \"data/WB_clean.csv\")\nwrite_csv(BR, \"data/BR_clean.csv\")\nwrite_csv(OF, \"data/OF_clean.csv\")\n\nNote: can lose “type” of object (factor levels)\n\n(Alt) Save as RData:\n\nsave(compounds_WB, compounds_BR, compounds_OF, file=\"data/compounds.RData\")\nsave(timepoints_WB, timepoints_BR, timepoints_OF, file=\"data/timepoints.RData\")\nsave(WB, BR, OF, WB_dups, BR_dups, OF_dups, file=\"data/data_clean.RData\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#recap",
    "href": "content/lectures/11-cs01-data.html#recap",
    "title": "11-cs01-data",
    "section": "Recap",
    "text": "Recap\n\nCould you summarize/explain background presented?\nCould you summarize the experiment that was done?\nCould you describe the datasets? (variables, observations, values, etc.)\nDo you understand/could you explain the wrangling that was done?"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html",
    "href": "content/lectures/12-cs01-eda.html",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Q: was curious about the sensitivity/cut-offs/specificity, but we mainly discussed it in class already; was still slightly confused about it?\nA: We’ll discuss this in detail later this week and early next week!\n\n\nQ: Are Youden’s indices related to ROC curves?\nA: Related, yes! We’ll discuss both soon!\n\n\nQ: I wasn’t sure how to interpret some of the visuals towards the end of lecture.\nA: That’s OK! We’ll be recreating these and discussing them more as we do this case study in class.\n\n\nQ: Did they actually use intravenous blood draws or just thumb pricks because multiple intravenous would not be fun.\nA: It was venous blood from the arm. This unfunness is one of the reasons participants were compensated.\n\n\nQ: Did this study (or other studies on THC) impairment end up influencing any legislation at the local or state level?\nA: Great question! The state is currently reviewing these and other study’s data. The state was definitely aware of this study and waited (im)patiently while we analyzed and worked to publish.\n\n\nQ: How long should our reports be?\nA: It’s hard to say. We’ll discuss an example today so you have a sense!\n\n\nQ: Regarding the final project, will there be a Google form that we can fill out that will help us form groups if we can’t form one ourselves?  A: Yup - I’d say try to find a group using Piazza, in class, or during lab. However, if you’re unable, when you fill out the form to indicate your group next week, you’ll select that you’d like to be placed into a group.\n\n\n\n\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n\nNotes:\n\nCS01 Groups have been sent out\n\nemail for contact\nGitHub repo <- please accept and open; make sure you have access\ngroup mate feedback is required\nif you made changes to repo yesterday, be sure to pull to get data in your repo\n\nFinal Project: can use Piazza to help find group mates\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe CS01 data are data for you only. My collaborator is excited that y’all will be working on this…but these are still research data, so please do not share with others or post publicly.\n\n\n\n\n\n\n\nQ: Are you allowed to share the average midterm scores for the past few quarters?\nA: IIRC, they were in the mid-high 80%s\n\n\nQ: When describing the dataset in our CSs, would it be okay to format it as a data card rather than a paragraph explanation of the variables structure? Additionally, would it be okay to store this as its own read.me in the repo or should it be a part of the main report?\nA: Yes - like the data card idea. And a detailed README in the repo is great. A short description should still be included in the report and can point to the readme.\n\n\nQ: When should we cite in our case study? It is just whenever we look up and use code from the internet?\nA: There AND any time you get information elsewhere that’s not general knowledge. For example, in your background section, you’ll likely cite a bunch of sources.\n\n\n\n\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n✅ HW02 Scores/Feedback Posted\n\n\n\n\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice” -Hadley\n\n\nfunction_name <- function(input){\n  # operations using input\n}\n\n\nFor example…\n\ndouble_value <- function(val){\n  val * 2\n}\n\n\n\nTo use/execute:\n\ndouble_value(3)\n\n[1] 6\n\n\n\n\nIn what we’ve done so far, we’ve seen functions that operate on and return the whole dataframe (DF in DF out) (drop_dups) and those that carry out operations on each row of a dataframe with a number of inputs (i.e. assign_timepoint; these require the function to be map-ed)\nAdditional resource: https://r4ds.had.co.nz/functions.html\n\n\n\n\n\nPrevious Projects\nExploring the Data"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#example-case-study",
    "href": "content/lectures/12-cs01-eda.html#example-case-study",
    "title": "12-cs01-eda",
    "section": "Example Case Study",
    "text": "Example Case Study\nSee & Discuss: https://cogs137.github.io/website/content/cs/cs-example.html"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#feedback-scores",
    "href": "content/lectures/12-cs01-eda.html#feedback-scores",
    "title": "12-cs01-eda",
    "section": "Feedback & Scores",
    "text": "Feedback & Scores\nFeedback to other students here\n\n\nYou cannot see the projects, but can read all of the comments and see the associated score. Also, note that the same row is not the same group.\n\nCommon comments:\n\ncontext/explanation/guidance/lacking\nmissing citations\nfailure to introduce/describe the data\nmaking statements without evidence\nneed to edit for cohesiveness, story, clarity"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#an-example-rubric",
    "href": "content/lectures/12-cs01-eda.html#an-example-rubric",
    "title": "12-cs01-eda",
    "section": "An (Example) Rubric",
    "text": "An (Example) Rubric\nThis is NOT the rubric for your case study, but it will be similar:"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#notes",
    "href": "content/lectures/12-cs01-eda.html#notes",
    "title": "12-cs01-eda",
    "section": "Notes",
    "text": "Notes\n\n\nLots of code/plots will be provided here\nYou are free to include any of it in your own case study (no attribution needed)\nYou probably should NOT include all of them in your final report\nFor any of the “basic” plots you include in your report, you’ll want to clean them up/improve their design.\nYour final report should be polished from start to finish"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#packages",
    "href": "content/lectures/12-cs01-eda.html#packages",
    "title": "12-cs01-eda",
    "section": "Packages",
    "text": "Packages\nTwo additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#our-datasets",
    "href": "content/lectures/12-cs01-eda.html#our-datasets",
    "title": "12-cs01-eda",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#the-data",
    "href": "content/lectures/12-cs01-eda.html#the-data",
    "title": "12-cs01-eda",
    "section": "The Data",
    "text": "The Data\nReading in the .RData we wrote at the end of the last set of notes…(using load)\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nThis reads the objects stored in these files into your Environment for use."
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#what-to-do-with-all-of-these-functions",
    "href": "content/lectures/12-cs01-eda.html#what-to-do-with-all-of-these-functions",
    "title": "12-cs01-eda",
    "section": "What to do with all of these functions?",
    "text": "What to do with all of these functions?\nFor example…we discussed this function in the last set of notes.\n\n drop_dups <- function(dataset){\n  out <- dataset |> \n    filter(!is.na(timepoint_use)) |> \n    group_by(timepoint_use) |> \n    distinct(id, .keep_all=TRUE) |> \n    ungroup()\n  return(out)\n } \n\n\nWe’re going to have a lot of functions throughout…like this helper function to clean up names\n\n# helper function to clean up name of two compounds\nclean_gluc <- function(df){\n  df <- df |> \n    mutate(compound=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))),\n           compound=gsub('THCOH', '11-OH-THC', compound))\n  return(df)\n}\n\n\n\nFunctions can/should be stored in a separate .R file, probably in a src/ directory.\n\n\nTo have access to the functions in that file…\n\nsource(\"path/to/file\")\n\n\n\n\nsource(\"src/cs01_functions.R\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#single-variable-basic-plots",
    "href": "content/lectures/12-cs01-eda.html#single-variable-basic-plots",
    "title": "12-cs01-eda",
    "section": "Single Variable (basic) plots",
    "text": "Single Variable (basic) plots\nFor a single compound…\n\nggplot(WB, aes(x=thc)) + geom_histogram()\n\n\n\n\n\nBut, with wide data, that’s not easy to do for all compounds, so you may want to pivot those data….\n\nWB_long <- WB |> \n  pivot_longer(6:13) |>\n  rename(\"fluid\"=\"fluid_type\")\n\n\n\nDistribtions across all compounds (WB):\n\nggplot(WB_long, aes(x=value)) + \n  geom_histogram() +\n  facet_wrap(~name)\n\n\n\n\n\n\nNow the same for OF and BR:\n\nOF_long <- OF |> pivot_longer(6:12)\nBR_long <- BR |> pivot_longer(6)\n\n\n\nCombining long datasets:\n\ndf_full <- bind_rows(WB_long, OF_long, BR_long)\n\n\n\nPlotting some of these data…\n\ndf_full |>\n  mutate(group_compound=paste0(fluid,\": \", name)) |>\nggplot(aes(x=value)) + \n  geom_histogram() + \n  facet_wrap(~group_compound, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#two-variables-at-a-time",
    "href": "content/lectures/12-cs01-eda.html#two-variables-at-a-time",
    "title": "12-cs01-eda",
    "section": "Two variables at a time",
    "text": "Two variables at a time"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#thc-frequency",
    "href": "content/lectures/12-cs01-eda.html#thc-frequency",
    "title": "12-cs01-eda",
    "section": "THC & Frequency",
    "text": "THC & Frequency\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=group, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#thc-treatment-group",
    "href": "content/lectures/12-cs01-eda.html#thc-treatment-group",
    "title": "12-cs01-eda",
    "section": "THC & Treatment Group",
    "text": "THC & Treatment Group\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#focus-on-a-specific-timepoint",
    "href": "content/lectures/12-cs01-eda.html#focus-on-a-specific-timepoint",
    "title": "12-cs01-eda",
    "section": "Focus on a specific timepoint…",
    "text": "Focus on a specific timepoint…\n\ndf_full |> \n  filter(name==\"thc\", timepoint==\"T2A\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#at-this-point",
    "href": "content/lectures/12-cs01-eda.html#at-this-point",
    "title": "12-cs01-eda",
    "section": "At this point…",
    "text": "At this point…\nWe start to get a sense of the data with these quick and dirty plots, but we’re really only scratching the surface of what’s going on in these data.\n\nThese data require a lot of exploration due to the number of compounds, multiple matrices, and data over time aspects."
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#compounds-across-time",
    "href": "content/lectures/12-cs01-eda.html#compounds-across-time",
    "title": "12-cs01-eda",
    "section": "Compounds across time",
    "text": "Compounds across time\n\nCodeExecute (WB)Plots (WB)OFBR\n\n\n\ncompound_scatterplot_group <- function(dataset, compound, timepoints){\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      dataset |> \n        filter(!is.na(time_from_start)) |>\n        ggplot(aes_string(x=\"time_from_start\", \n                          y=compound,\n                          color=\"group\")) + \n        geom_point() +\n        geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                   linetype=\"dashed\", \n                   color=\"gray28\") +\n        scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_y_continuous(limits=c(0,3)) +\n        theme_classic() +\n        theme(legend.position=\"bottom\",\n              legend.title=element_blank()) +\n        labs(x='Time From Start (min)',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        dataset |> \n          filter(!is.na(time_from_start)) |>\n          ggplot(aes_string(x=\"time_from_start\", \n                            y=compound,\n                            color=\"group\")) + \n          geom_point() +\n          geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                     linetype=\"dashed\", \n                     color=\"gray28\")  +\n          scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          theme_classic() +\n          theme(legend.position=\"bottom\",\n                legend.title=element_blank()) +\n          labs(x='Time From Start (min)',\n               y=gsub('GLUC', 'gluc', gsub(\"_\", \"-\", toupper(compound))))\n      )\n    }\n}\n\n\n\n\nscatter_wb <- map(compounds_WB, ~ compound_scatterplot_group( \n    dataset=WB_dups, \n    compound=.x, \n    timepoints=timepoints_WB))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_of <- map(compounds_OF, ~ compound_scatterplot_group( \n    dataset=OF_dups, \n    compound=.x, \n    timepoints=timepoints_OF))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_br <- map(compounds_BR, ~ compound_scatterplot_group( \n    dataset=BR_dups, \n    compound=.x, \n    timepoints=timepoints_BR))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#pairplots",
    "href": "content/lectures/12-cs01-eda.html#pairplots",
    "title": "12-cs01-eda",
    "section": "Pairplots",
    "text": "Pairplots\n\nWBOFBR\n\n\n\npairs(WB[,unlist(compounds_WB)], \n      pch=19, \n      cex=0.3, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(WB[,unlist(compounds_WB)])))))\n\n\n\n\n\n\n\npairs(OF[,unlist(compounds_OF)], \n      pch=19, \n      cex=0.4, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(OF[,unlist(compounds_OF)])))))\n\n\n\n\n\n\n❓ Why is there no pairplot for Breath?"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#group-differences-frequency-of-use",
    "href": "content/lectures/12-cs01-eda.html#group-differences-frequency-of-use",
    "title": "12-cs01-eda",
    "section": "Group Differences: Frequency of Use",
    "text": "Group Differences: Frequency of Use\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_group_only <- function(dataset, compounds, tissue, legend=TRUE, y_lab=TRUE){\n  timepoint_to_use=levels(dataset$timepoint_use)[1]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(group, all_of(compounds)) \n  df <- df |> \n    gather(compound, value, -group) |> \n    clean_gluc() |> \n    group_by(compound) |> \n    mutate(y_max=1.2*max(value)) |> \n    group_by(compound, group) |> \n    mutate(n=n(),\n           my_label=paste0(group, ' N=', n),\n           my_label= gsub(\" \", \"\\n\", my_label))\n  \n  if(tissue == \"Blood\"){\n    df$compound=factor(df$compound, levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  y_pos <- df |> \n    group_by(compound) |> \n    summarize(y.position=mean(y_max))\n  \n  stat.test <- df |>\n    group_by(compound) |>\n    t_test(value ~ my_label) |>\n    adjust_pvalue(method=\"bonferroni\") |>\n    add_significance()\n  test <- stat.test |>\n    left_join(y_pos) |>\n    mutate(p.adj.signif=ifelse(p.adj.signif=='?', 'ns', p.adj.signif),\n           p.adj=ifelse(p.adj < 0.001, \"<0.001\", p.adj))\n\n  if(legend){\n    leg_position='right'\n  }else{\n    leg_position='none'\n  }\n  \n  if(y_lab){\n    y_text=\"Concentration (ng/mL)\"\n  }else{\n    y_text=''\n  }\n  \n  medianFunction <- function(x){\n    return(data.frame(y=round(median(x),1),label=round(median(x,na.rm=T),1)))}\n  \n  p2 <- ggplot(df, aes(x=my_label, y=value, fill=my_label)) + \n    geom_jitter(position=position_jitter(width=.3, height=0), size=0.8, color=\"gray65\")  +\n    geom_boxplot(outlier.shape=NA, alpha=0.6) +\n    stat_summary(fun=\"median\", geom=\"point\", shape=19, size=3, fill=\"black\") + \n    stat_summary(fun.data=medianFunction, geom =\"text\", color=\"black\", size=3.5, vjust=-0.65) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    geom_blank(aes(y=y_max)) + \n    scale_y_continuous(limits=c(0, NA), expand=expansion(mult=c(0, 0.1))) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    theme_classic() +\n    theme(text=element_text(size=14),\n          legend.position=leg_position,\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x='',\n         y=y_text) \n  \n  ann_text <- test |> \n    select(compound, p.adj, value=y.position, my_label=group1) |>\n    filter(p.adj < 0.05) |> \n    mutate(x1=1, x2=2)\n  \n  if(tissue == \"Whole Blood\"){\n    ann_text$compound=factor(ann_text$compound, \n                               levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  p2 + geom_text(data=ann_text, label=ann_text$p.adj, nudge_x=0.5) +\n    geom_segment(data=ann_text, aes(x=x1, xend=x2,\n                                    y=value - (0.04 * value), \n                                    yend=value - (0.04*value)))\n  \n}\n\n\n\n\ncompound_boxplot_group_only(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#group-differences-treatment",
    "href": "content/lectures/12-cs01-eda.html#group-differences-treatment",
    "title": "12-cs01-eda",
    "section": "Group Differences: Treatment",
    "text": "Group Differences: Treatment\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_treatment <- function(dataset, compounds, tissue){\n  timepoint_to_use=levels(dataset$timepoint_use)[2]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(treatment, group, compounds)\n  df <- df |> \n    gather(compound, value, -treatment, -group) |> \n    clean_gluc()\n  \n  df |> \n    ggplot(aes(x=treatment, y=value, fill=group)) + \n    # geom_jitter(color=\"gray36\") +\n    geom_boxplot(outlier.size=0.1) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    scale_x_discrete(labels=function(x) str_wrap(x, width=11)) +\n    theme_classic(base_size=10) +\n    theme(legend.position=\"bottom\",\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x=\"Treatment\",\n         y=\"Measurement (ng/mL)\")\n  \n}\n\n\n\n\ncompound_boxplot_treatment(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#compound-summaries",
    "href": "content/lectures/12-cs01-eda.html#compound-summaries",
    "title": "12-cs01-eda",
    "section": "Compound Summaries",
    "text": "Compound Summaries\n\nCodeWBOFBR\n\n\n\nT2A_plot <- function(dataset, compound, timepoint_use=2){\n  timepoint_to_use=levels(factor(dataset$timepoint_use))[timepoint_use]\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n             aes_string(x=\"group\", \n                        y=compound, \n                        fill=\"group\")) + \n        geom_boxplot(outlier.size=0.1) +\n        scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n        scale_y_continuous(limits=c(0,3)) +\n        facet_grid(~treatment) + \n        theme_classic() +\n        theme(legend.position=\"none\",\n              panel.grid=element_blank(),\n              strip.background=element_blank(),\n              plot.title.position=\"plot\") +\n        labs(title=paste0('Timepoint: ',\n                          levels(dataset$timepoint_use)[timepoint_use],\n                          ' post-smoking'),\n             x='Group',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n               aes_string(x=\"group\", \n                          y=compound, \n                          fill=\"group\")) + \n          geom_boxplot(outlier.shape=NA, alpha=0.8) +\n          scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n          facet_grid(~treatment) + \n          theme_classic() +\n          theme(legend.position=\"none\",\n                panel.grid=element_blank(),\n                strip.background=element_blank(),\n                plot.title.position=\"plot\") +\n          labs(title=paste0('Timepoint: ',\n                            levels(dataset$timepoint_use)[timepoint_use],\n                            ' post-smoking'),\n               x='Group',\n               y=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))))\n      )\n    }\n}\n\n\n\n\npost_wb <- map(compounds_WB, ~ T2A_plot( \n    dataset=WB_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_of <- map(compounds_OF, ~ T2A_plot( \n    dataset=OF_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_br <- map(compounds_BR, ~ T2A_plot( \n    dataset=BR_dups, \n    compound=.x))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#hw02-part-ii",
    "href": "content/lectures/12-cs01-eda.html#hw02-part-ii",
    "title": "12-cs01-eda",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#chess-players",
    "href": "content/lectures/12-cs01-eda.html#chess-players",
    "title": "12-cs01-eda",
    "section": "Chess Players",
    "text": "Chess Players\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n\nchess <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/chess-transfers/transfers.csv\")\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nggplot(chess_trans, aes(y = reorder(Federation, n), x = n)) +\n  geom_bar(stat = \"identity\",fill = \"#1c9099\") +\n  geom_text(aes(x = -3, y = Federation, label = n), size = 4) + #add count at the left side of the bars\n  labs(title = bquote(bold(\"More players transfer to the U.S. than to any other country\")),\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       x = \"NUMBER OF TRANSFERS\",\n       y = \"COUNTRY\") +\n  scale_fill_identity() +\n  theme_minimal() + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank()) + \n  theme(axis.text.y = element_text(size = 10, angle = 0, hjust = 0), #align to the left\n        plot.title = element_text(size=18)) #change font size"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#kushi-fandango",
    "href": "content/lectures/12-cs01-eda.html#kushi-fandango",
    "title": "12-cs01-eda",
    "section": "Kushi: Fandango",
    "text": "Kushi: Fandango\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Kushi also got font working and stars on x-axis that I'd have to spend more time to get working \nfandango_score_comparison <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv\")\n\nall_scores_comp <- fandango_score_comparison |>\n  select(FILM, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars) |>\n  pivot_longer(c(RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars), names_to = \"Site\", values_to = \"Score\")\n\ncounts_of_scores <- all_scores_comp |>\n  group_by(Site, Score) |>\n  summarize(Count = n())\n\ncounts_of_scores <- counts_of_scores |>\n  group_by(Site) |>\n  mutate(Total = sum(Count))\n\nscores_with_percents <- counts_of_scores |>\n  mutate(Percent = Count / Total * 100)\n\n# making percents 0 for any scores that don't have any values\n\nsites <- tibble(Site = c(\"RT_norm_round\", \"RT_user_norm_round\", \"Metacritic_norm_round\", \"Metacritic_user_norm_round\", \"IMDB_norm_round\", \"Fandango_Stars\"))\n\nratings <- tibble(Score = seq(0, 5, by = 0.5))\n\nall_scores <- expand.grid(Site = sites$Site, Score = ratings$Score)\n\nall_scores <- all_scores |>\n  full_join(scores_with_percents, by = c(\"Site\", \"Score\")) |>\n  mutate(Percent = case_when(is.na(Count) ~ 0,\n                             TRUE ~ Percent))\n\nscores <- ggplot(all_scores, aes(x = Score, y = Percent, color = Site)) +\n  geom_line()  +\n  geom_hline(yintercept = 0, size = 0.7, color = \"black\") +\n  labs(x = NULL, y = NULL,\n       title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distrubution of 146 films in theaters in 2015 that\\n had 30+ reviews on Fandango.com\") +\n  scale_y_continuous(labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")) +\n  scale_x_continuous(labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +\n  scale_color_manual(values = c(\"Fandango_Stars\" = \"#fa6d54\",\n                                \"IMDB_norm_round\" = \"#e5c66a\",\n                                \"Metacritic_user_norm_round\" = \"#aeca91\",\n                                \"RT_user_norm_round\" = \"#76bde0\",\n                                \"Metacritic_norm_round\" = \"#b87eb5\",\n                                \"RT_norm_round\" = \"#a3a3a3\")) +\n  geom_ribbon(data = filter(all_scores, Site != \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent), alpha = 0.1) +\n  geom_ribbon(data = filter(all_scores, Site == \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent, fill = \"#fa6d54\", alpha = 0.24)) +\n  guides(color = \"none\", fill = \"none\", alpha = \"none\")\n\nscores + \n  annotate(\"text\", x = 4.9, y = 35, label = \"Fandango\", size = 5, color = \"#fa6d54\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.9, y = 37, label = \"IMDb users\", size = 5, color = \"#e5c66a\") +\n  annotate(\"text\", x = 2.7, y = 27, label = \"Metacritic\\nusers\", size = 5, color = \"#aeca91\") +\n  annotate(\"text\", x = 2.2, y = 20, label = \"Rotten\\nTomatoes\\nusers\", size = 5, color = \"#76bde0\") +\n  annotate(\"text\", x = 1.5, y = 16, label = \"Metacritic\", size = 5, color = \"#b87eb5\") +\n  annotate(\"text\", x = 0.7, y = 13, label = \"Rotten\\nTomatoes\", size = 5, color = \"#a3a3a3\") +\n  theme(#text = element_text(family = \"NimbusSan\"), \n        plot.title = element_text(face = \"bold\", size = 25, hjust = -0.1), \n        plot.subtitle = element_text(size = 15, hjust = -0.1),\n        plot.background = element_rect(fill = \"#f0f0f0\"),\n        panel.background = element_rect(fill = \"#f0f0f0\"),\n        panel.grid.major = element_line(color = \"gray75\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text = element_text(size = 14)\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#markus-congress",
    "href": "content/lectures/12-cs01-eda.html#markus-congress",
    "title": "12-cs01-eda",
    "section": "Markus: Congress",
    "text": "Markus: Congress\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\ncongress_data <- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\")\n\ncongress_ages =\n  congress_data |>\n  select(congress, chamber, age_years) |>\n  mutate(congress_year = case_when(TRUE ~ 1787 + (2 * as.integer(congress)))) |>\n  group_by(congress_year, chamber) |>\n  mutate(mean_age = case_when(TRUE ~ mean(age_years))) |>\n  mutate(chamber = fct_recode(chamber,\n                              SENATE = \"Senate\",\n                              HOUSE = \"House\",))\n\nggplot(data = congress_ages,\n       mapping = aes(y = mean_age,\n                     x = congress_year,\n                     color = fct_rev(chamber),\n                     )) +\n  geom_step(size = 1) + \n  guides() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       caption = \"Data is based on all members who served in either the Senate or House in each Congress, which is notated\\nby the year in which it was seated. Any member who served in bothchambers in the same Congress was\\nassigned to the chamber in which they cast more votes.\\n FiveThirtyEight\\nSOURCES: BIOGRAPHICAL DIRECTORY OF THE U.S. CONGRESS, U.S. HOUSE OF REPRESENTATIVES,\\nU.S. SENATE, UNITEDSTATES GITHUB, VOTEVIEW.COM\",\n       y = NULL,\n       x = NULL\n  ) +\n  scale_color_manual(values=c(\"#6b4ddd\",\"#29ae53\")) +\n  theme_minimal(base_size = 13) + \n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(face = \"bold\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.title=element_blank(),\n        legend.position = \"top\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#voter-demographics",
    "href": "content/lectures/12-cs01-eda.html#voter-demographics",
    "title": "12-cs01-eda",
    "section": "Voter Demographics",
    "text": "Voter Demographics\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggpubr)\n\nvoters = read.csv(\"data/nonvoters_data.csv\")\n\n\n#creating subplots\nrace = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Race\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())+\n    guides(fill = guide_legend(override.aes = list(shape = 16, key_width = 1, key_height = 1))) #trying to change the shape of the legend key\n\nincome = ggplot(data = voters, mapping = aes(y = factor(income_cat), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Income\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\nage = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Age\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\neducation = ggplot(data = voters, mapping = aes(y = factor(educ), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Education\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\npartyID = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"party ID\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\n#combining subplots\nria = ggarrange(race, income, age, ncol = 3, common.legend = TRUE, legend = \"top\") +\n  theme(plot.margin = margin(0.5, 0,-2, -1,\"cm\")) \n\nep = ggarrange(education, partyID) +\n  theme(plot.margin = margin(2, 4, -2.5, 1, \"cm\"))\n\nvoterplot = ggarrange(ria, ep, nrow = 3) \n\n\n#titles\nvoterplot = annotate_figure(annotate_figure(voterplot, \n  top = text_grob(\"Demographic information of survey respondants, by voting history\")),\n  top = text_grob(\"Those who always vote and those who sometimes vote aren't that different\", face = \"bold\")\n)\n\n\nvoterplot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#fouls",
    "href": "content/lectures/12-cs01-eda.html#fouls",
    "title": "12-cs01-eda",
    "section": "Fouls",
    "text": "Fouls\n\nOriginalPlot ICode IPlot (Banso)Code (Banso)\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_csv_file <- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/foul-balls/foul-balls.csv\"\nfoulballs <- read.csv(url(raw_csv_file))\n\nfoulballs <- foulballs |>\n  mutate(over90 = case_when(\n    exit_velocity < 90 ~ \"no\",\n    exit_velocity >= 90 ~ \"yes\"\n  ))\n\nggplot(foulballs,\n       aes (y = used_zone)) +\n  geom_bar(aes(fill = over90),\n           position = position_stack(reverse = TRUE),\n           show.legend = FALSE) +\n  scale_fill_manual(labels = c(\"< 90 mph\", \"≥ 90 mph\", \"Unknown exit velocity\"),\n                    values = c(\"#97c16d\", \"#63abb0\"), na.value = \"#d3d3d3\") +\n  scale_y_discrete(limits = rev(unique(foulballs$used_zone)))+\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"#ececec\"),\n        panel.background = element_rect(fill = \"white\"),\n        axis.text.x = element_text(color = \"#9e9e9e\"),\n        plot.caption = element_text(color = \"#b5b5b5\"),\n        axis.line.y = element_line(colour = \"#343434\"),\n        axis.title.y = element_text(angle = 0, vjust = 0.12, color = \"#343434\", size = 9),\n        axis.ticks = element_blank(),\n        legend.title = element_blank()) +\n  labs(title = \"The hardest-hit fouls seem to land in unprotected areas\",\n       subtitle = str_wrap(\"Foul balls by the stadium zone they landed in and their exit velocity, among 906 fouls hit this season in the most foul-heavy day at the 10 MLB stadiums that produced the most fouls as of June 5\", 85),\n       x = \"\", y = \"Zone\",\n       caption = \"SOURCE: BASEBALL SAVANT\") +\n  annotate(\"text\", x = 75, y = 1, label = \"< 90 mph\", col = \"white\") +\n  annotate(\"text\", x = 140, y = 3.3, label = \"≥ 90 mph\", col = \"#63abb0\") +\n  annotate(\"text\", x = 215, y = 1, label = \"Unknown exit velocity\", col = \"white\")\n\n\n\n\n\n\n\n\n\n\n\nball_data <- foulballs |>\n  mutate(category = case_when(\n    exit_velocity < 90 ~ \"<90\",\n    exit_velocity >= 90 ~ \">=90\",\n    is.na(exit_velocity) ~ \"Unknown\"\n                           ))\nmy_plot <- ggplot(ball_data, aes(y = fct_rev(as.character(used_zone)), fill = category)) +\n  geom_bar(position = position_stack(reverse = TRUE)) +\n  labs(title = \"The hardest-hit fouls seem \\nto land in unprotected areas\",\n       subtitle = \"The 906 foul balls hit this season from \\nthe most foul-heavy day at each of the \\n10 MLB stadiums that produced the \\nthe most fouls as of June 5, by zone where \\nthe balls landed and their exit velocities\",\n       y = \"Zone\", x = NULL) +\n  scale_fill_manual(values = c(\"Unknown\" = \"#DEDEDE\", \"<90\" = \"#9ECE88\", \">=90\" = \"#17AFAD\")) +\n  scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250),position = \"top\") +\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major.x = element_line(size = 0.5, linetype=\"solid\", color=\"#CECECE\"),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(color = \"black\", face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(size = 10, margin = margin(b = 20))\n)\n\nmy_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#colin-nba-raptor-ratings",
    "href": "content/lectures/12-cs01-eda.html#colin-nba-raptor-ratings",
    "title": "12-cs01-eda",
    "section": "Colin: NBA RAPTOR ratings",
    "text": "Colin: NBA RAPTOR ratings\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\nraptor <- read.csv(\"data/latest_RAPTOR_by_player.csv\")\n\nraptor_rounded <- raptor |> \n  filter(mp >= 1137) |>\n  mutate(across(where(is.numeric), round, 1))\n\nraptor_plot <- raptor_rounded |>\n  ggplot(aes(x=raptor_offense, y = raptor_defense)) + \n  \n  # Annotations\n    # The colored rectangles for the 1 & 3 quadrants\n    annotate(\"rect\", xmin=0, xmax=10, ymin=0, ymax=10, fill = '#c5ecee', alpha = .85) + \n    annotate(\"rect\", xmin=-10, xmax=0, ymin=-10, ymax=0, fill = '#fecada', alpha = .85) +\n    \n    # The 3rd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -7.8, ymax = -6.8, fill = '#fd97b6') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = -7.4, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = -9, label = \" -  defense\") +\n    \n    # The 1st quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 8.4, ymax = 9.4, fill = '#8cdadf') +\n    annotate(\"text\", x = 7.5, y = 8.8, label = \" +  offense\") +\n    annotate(\"text\", x = 7.55, y = 7.1, label = \" +  defense\") +\n  \n    # The 2nd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 8.4, ymax = 9.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = 8.8, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = 7.1, label = \" +  defense\") +\n  \n    # The 4th quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -7.8, ymax = -6.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = 7.55, y = -7.4, label = \" +  offense\") +\n    annotate(\"text\", x = 7.5, y = -9, label = \" -  defense\") +\n  \n  geom_point(shape= 21, colour = \"black\", fill = \"white\", size = 4) + \n  \n  labs(\n    x = \"Offensive RAPTOR rating\", \n    y = \"Defensive RAPTOR rating\",\n    title = paste0('Nikola Jokic is the Best NBA Player Based on Overall RAPTOR Rating',\n            '<br>',\n            '<sup>',\n            'An Analytical Approach to the 2022 - 2023 NBA Season','</sup>')\n    ) + \n    \n  # Theme settings\n  theme_light() + \n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(), \n    panel.grid.major = element_line(color = \"#cdcdcd\", linewidth = 0.5), \n    plot.margin = margin(l = 100, r = 100, b = 20, t = 10),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(color = \"#cdcdcd\", size = 12),\n    axis.title.x = element_text(margin = margin(t=3)),\n    axis.title.y = element_text(margin = margin(r=3)),\n    ) + \n  coord_fixed(ratio = 1) \n\n# interactive\n# ggplotly(raptor_plot, text=player_name, hoverinfo='text') \n\nraptor_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#biden-approval",
    "href": "content/lectures/12-cs01-eda.html#biden-approval",
    "title": "12-cs01-eda",
    "section": "Biden Approval",
    "text": "Biden Approval\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n#getting only data from beginning of COVID to Jan 19, 2021 and getting rid of the \"all\" category in the party column \nyear_20 <- \n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year <= 2020)|>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n\nyear_21 <-\n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year == 2021 & end_month == 1 & end_day <= 19) |>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n       #  if (end_year == 2021 & end_month == 1 & end_day <= 19))\n\nselect_COVID_approval <-\n  full_join(year_20, year_21)\n\nggplot(select_COVID_approval, aes(x = as.Date(end_date),\n                           y = approve,\n                           color = party)) +\n  geom_smooth(aes(group = party), span = 0.05, se = FALSE) + \n  scale_color_manual(values = c(\"D\" = \"#008fd5\", \n                               \"R\" = \"#ff2700\",\n                               \"I\" = \"#a55330\")) +\n  \n  #Important Dates\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-02-29\"), y = 95, label=\"First U.S. \\n Death \\n Reported\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-05-20\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-05-20\"), y = 95, label=\"U.S. Deaths \\n surpass \\n 100,000\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype=3) + #Trump Diagnosed with COVID-19\n    annotate(\"text\", x= as.Date(\"2020-10-02\"), y = 95, label=\"Trump \\n Diagnosed \\n with \\n COVID-19\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-11-07\"), y = 95, label=\"Biden \\n declared \\n election \\n winner\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2021-01-19\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2021-01-19\"), y = 95, label=\"Biden \\n sworn \\n into \\n office\", size=3, color=\"black\") +\n  \n  labs(title = \"Approval of Trump’s response varies widely by party\",\n       subtitle = \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s \\n handling of the coronavirus outbreak\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5 ), \n        plot.subtitle = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.minor = element_line(colour=\"gray\")\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#recap",
    "href": "content/lectures/12-cs01-eda.html#recap",
    "title": "12-cs01-eda",
    "section": "Recap",
    "text": "Recap\n\nCan you explain/describe the plots generated in the context of these data?\nCan you generate EDA plots of your own for these data\nCan you understand/work through the more complicated code provided (even if you couldn’t have come up with it on your own)"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#qa-tu-117",
    "href": "content/lectures/12-cs01-eda-slides.html#qa-tu-117",
    "title": "12-cs01-eda",
    "section": "Q&A (Tu 11/7)",
    "text": "Q&A (Tu 11/7)\n\nQ: was curious about the sensitivity/cut-offs/specificity, but we mainly discussed it in class already; was still slightly confused about it?\nA: We’ll discuss this in detail later this week and early next week!\n\n\nQ: Are Youden’s indices related to ROC curves?\nA: Related, yes! We’ll discuss both soon!\n\n\nQ: I wasn’t sure how to interpret some of the visuals towards the end of lecture.\nA: That’s OK! We’ll be recreating these and discussing them more as we do this case study in class.\n\n\nQ: Did they actually use intravenous blood draws or just thumb pricks because multiple intravenous would not be fun.\nA: It was venous blood from the arm. This unfunness is one of the reasons participants were compensated.\n\n\nQ: Did this study (or other studies on THC) impairment end up influencing any legislation at the local or state level?\nA: Great question! The state is currently reviewing these and other study’s data. The state was definitely aware of this study and waited (im)patiently while we analyzed and worked to publish.\n\n\nQ: How long should our reports be?\nA: It’s hard to say. We’ll discuss an example today so you have a sense!\n\n\nQ: Regarding the final project, will there be a Google form that we can fill out that will help us form groups if we can’t form one ourselves?  A: Yup - I’d say try to find a group using Piazza, in class, or during lab. However, if you’re unable, when you fill out the form to indicate your group next week, you’ll select that you’d like to be placed into a group."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#course-announcements-tu-117",
    "href": "content/lectures/12-cs01-eda-slides.html#course-announcements-tu-117",
    "title": "12-cs01-eda",
    "section": "Course Announcements (Tu 11/7)",
    "text": "Course Announcements (Tu 11/7)\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n\nNotes:\n\nCS01 Groups have been sent out\n\nemail for contact\nGitHub repo <- please accept and open; make sure you have access\ngroup mate feedback is required\nif you made changes to repo yesterday, be sure to pull to get data in your repo\n\nFinal Project: can use Piazza to help find group mates\n\n\n\n\n\n\n\n\nImportant\n\n\nThe CS01 data are data for you only. My collaborator is excited that y’all will be working on this…but these are still research data, so please do not share with others or post publicly."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#qa-th-119",
    "href": "content/lectures/12-cs01-eda-slides.html#qa-th-119",
    "title": "12-cs01-eda",
    "section": "Q&A (Th 11/9)",
    "text": "Q&A (Th 11/9)\n\nQ: Are you allowed to share the average midterm scores for the past few quarters?\nA: IIRC, they were in the mid-high 80%s\n\n\nQ: When describing the dataset in our CSs, would it be okay to format it as a data card rather than a paragraph explanation of the variables structure? Additionally, would it be okay to store this as its own read.me in the repo or should it be a part of the main report?\nA: Yes - like the data card idea. And a detailed README in the repo is great. A short description should still be included in the report and can point to the readme.\n\n\nQ: When should we cite in our case study? It is just whenever we look up and use code from the internet?\nA: There AND any time you get information elsewhere that’s not general knowledge. For example, in your background section, you’ll likely cite a bunch of sources."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#course-announcements-th-119",
    "href": "content/lectures/12-cs01-eda-slides.html#course-announcements-th-119",
    "title": "12-cs01-eda",
    "section": "Course Announcements (Th 11/9)",
    "text": "Course Announcements (Th 11/9)\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n✅ HW02 Scores/Feedback Posted"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#functions-in-r",
    "href": "content/lectures/12-cs01-eda-slides.html#functions-in-r",
    "title": "12-cs01-eda",
    "section": "Functions in R",
    "text": "Functions in R\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice” -Hadley\n\n\nfunction_name <- function(input){\n  # operations using input\n}\n\n\nFor example…\n\ndouble_value <- function(val){\n  val * 2\n}\n\n\n\nTo use/execute:\n\ndouble_value(3)\n\n[1] 6\n\n\n\n\nIn what we’ve done so far, we’ve seen functions that operate on and return the whole dataframe (DF in DF out) (drop_dups) and those that carry out operations on each row of a dataframe with a number of inputs (i.e. assign_timepoint; these require the function to be map-ed)\nAdditional resource: https://r4ds.had.co.nz/functions.html"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#agenda",
    "href": "content/lectures/12-cs01-eda-slides.html#agenda",
    "title": "12-cs01-eda",
    "section": "Agenda",
    "text": "Agenda\n\nPrevious Projects\nExploring the Data"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#example-case-study",
    "href": "content/lectures/12-cs01-eda-slides.html#example-case-study",
    "title": "12-cs01-eda",
    "section": "Example Case Study",
    "text": "Example Case Study\nSee & Discuss: https://cogs137.github.io/website/content/cs/cs-example.html"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#feedback-scores",
    "href": "content/lectures/12-cs01-eda-slides.html#feedback-scores",
    "title": "12-cs01-eda",
    "section": "Feedback & Scores",
    "text": "Feedback & Scores\nFeedback to other students here\n\n\nCommon comments:\n\ncontext/explanation/guidance/lacking\nmissing citations\nfailure to introduce/describe the data\nmaking statements without evidence\nneed to edit for cohesiveness, story, clarity\n\n\n\nYou cannot see the projects, but can read all of the comments and see the associated score. Also, note that the same row is not the same group."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#an-example-rubric",
    "href": "content/lectures/12-cs01-eda-slides.html#an-example-rubric",
    "title": "12-cs01-eda",
    "section": "An (Example) Rubric",
    "text": "An (Example) Rubric\nThis is NOT the rubric for your case study, but it will be similar:"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#notes",
    "href": "content/lectures/12-cs01-eda-slides.html#notes",
    "title": "12-cs01-eda",
    "section": "Notes",
    "text": "Notes\n\n\nLots of code/plots will be provided here\nYou are free to include any of it in your own case study (no attribution needed)\nYou probably should NOT include all of them in your final report\nFor any of the “basic” plots you include in your report, you’ll want to clean them up/improve their design.\nYour final report should be polished from start to finish"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#packages",
    "href": "content/lectures/12-cs01-eda-slides.html#packages",
    "title": "12-cs01-eda",
    "section": "Packages",
    "text": "Packages\nTwo additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#our-datasets",
    "href": "content/lectures/12-cs01-eda-slides.html#our-datasets",
    "title": "12-cs01-eda",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#the-data",
    "href": "content/lectures/12-cs01-eda-slides.html#the-data",
    "title": "12-cs01-eda",
    "section": "The Data",
    "text": "The Data\nReading in the .RData we wrote at the end of the last set of notes…(using load)\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nThis reads the objects stored in these files into your Environment for use."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#what-to-do-with-all-of-these-functions",
    "href": "content/lectures/12-cs01-eda-slides.html#what-to-do-with-all-of-these-functions",
    "title": "12-cs01-eda",
    "section": "What to do with all of these functions?",
    "text": "What to do with all of these functions?\nFor example…we discussed this function in the last set of notes.\n\n drop_dups <- function(dataset){\n  out <- dataset |> \n    filter(!is.na(timepoint_use)) |> \n    group_by(timepoint_use) |> \n    distinct(id, .keep_all=TRUE) |> \n    ungroup()\n  return(out)\n } \n\n\nWe’re going to have a lot of functions throughout…like this helper function to clean up names\n\n# helper function to clean up name of two compounds\nclean_gluc <- function(df){\n  df <- df |> \n    mutate(compound=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))),\n           compound=gsub('THCOH', '11-OH-THC', compound))\n  return(df)\n}\n\n\n\nFunctions can/should be stored in a separate .R file, probably in a src/ directory.\n\n\nTo have access to the functions in that file…\n\nsource(\"path/to/file\")\n\n\n\n\nsource(\"src/cs01_functions.R\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#single-variable-basic-plots",
    "href": "content/lectures/12-cs01-eda-slides.html#single-variable-basic-plots",
    "title": "12-cs01-eda",
    "section": "Single Variable (basic) plots",
    "text": "Single Variable (basic) plots\nFor a single compound…\n\nggplot(WB, aes(x=thc)) + geom_histogram()\n\n\n\n\n\nBut, with wide data, that’s not easy to do for all compounds, so you may want to pivot those data….\n\nWB_long <- WB |> \n  pivot_longer(6:13) |>\n  rename(\"fluid\"=\"fluid_type\")\n\n\n\nDistribtions across all compounds (WB):\n\nggplot(WB_long, aes(x=value)) + \n  geom_histogram() +\n  facet_wrap(~name)\n\n\n\n\n\n\nNow the same for OF and BR:\n\nOF_long <- OF |> pivot_longer(6:12)\nBR_long <- BR |> pivot_longer(6)\n\n\n\nCombining long datasets:\n\ndf_full <- bind_rows(WB_long, OF_long, BR_long)\n\n\n\nPlotting some of these data…\n\ndf_full |>\n  mutate(group_compound=paste0(fluid,\": \", name)) |>\nggplot(aes(x=value)) + \n  geom_histogram() + \n  facet_wrap(~group_compound, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#two-variables-at-a-time",
    "href": "content/lectures/12-cs01-eda-slides.html#two-variables-at-a-time",
    "title": "12-cs01-eda",
    "section": "Two variables at a time",
    "text": "Two variables at a time"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#thc-frequency",
    "href": "content/lectures/12-cs01-eda-slides.html#thc-frequency",
    "title": "12-cs01-eda",
    "section": "THC & Frequency",
    "text": "THC & Frequency\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=group, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#thc-treatment-group",
    "href": "content/lectures/12-cs01-eda-slides.html#thc-treatment-group",
    "title": "12-cs01-eda",
    "section": "THC & Treatment Group",
    "text": "THC & Treatment Group\n\ndf_full |> \n  filter(name==\"thc\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#focus-on-a-specific-timepoint",
    "href": "content/lectures/12-cs01-eda-slides.html#focus-on-a-specific-timepoint",
    "title": "12-cs01-eda",
    "section": "Focus on a specific timepoint…",
    "text": "Focus on a specific timepoint…\n\ndf_full |> \n  filter(name==\"thc\", timepoint==\"T2A\") |>\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#at-this-point",
    "href": "content/lectures/12-cs01-eda-slides.html#at-this-point",
    "title": "12-cs01-eda",
    "section": "At this point…",
    "text": "At this point…\nWe start to get a sense of the data with these quick and dirty plots, but we’re really only scratching the surface of what’s going on in these data.\n\nThese data require a lot of exploration due to the number of compounds, multiple matrices, and data over time aspects."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#compounds-across-time",
    "href": "content/lectures/12-cs01-eda-slides.html#compounds-across-time",
    "title": "12-cs01-eda",
    "section": "Compounds across time",
    "text": "Compounds across time\n\nCodeExecute (WB)Plots (WB)OFBR\n\n\n\ncompound_scatterplot_group <- function(dataset, compound, timepoints){\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      dataset |> \n        filter(!is.na(time_from_start)) |>\n        ggplot(aes_string(x=\"time_from_start\", \n                          y=compound,\n                          color=\"group\")) + \n        geom_point() +\n        geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                   linetype=\"dashed\", \n                   color=\"gray28\") +\n        scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_y_continuous(limits=c(0,3)) +\n        theme_classic() +\n        theme(legend.position=\"bottom\",\n              legend.title=element_blank()) +\n        labs(x='Time From Start (min)',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        dataset |> \n          filter(!is.na(time_from_start)) |>\n          ggplot(aes_string(x=\"time_from_start\", \n                            y=compound,\n                            color=\"group\")) + \n          geom_point() +\n          geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                     linetype=\"dashed\", \n                     color=\"gray28\")  +\n          scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          theme_classic() +\n          theme(legend.position=\"bottom\",\n                legend.title=element_blank()) +\n          labs(x='Time From Start (min)',\n               y=gsub('GLUC', 'gluc', gsub(\"_\", \"-\", toupper(compound))))\n      )\n    }\n}\n\n\n\n\nscatter_wb <- map(compounds_WB, ~ compound_scatterplot_group( \n    dataset=WB_dups, \n    compound=.x, \n    timepoints=timepoints_WB))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_of <- map(compounds_OF, ~ compound_scatterplot_group( \n    dataset=OF_dups, \n    compound=.x, \n    timepoints=timepoints_OF))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_br <- map(compounds_BR, ~ compound_scatterplot_group( \n    dataset=BR_dups, \n    compound=.x, \n    timepoints=timepoints_BR))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#pairplots",
    "href": "content/lectures/12-cs01-eda-slides.html#pairplots",
    "title": "12-cs01-eda",
    "section": "Pairplots",
    "text": "Pairplots\n\nWBOFBR\n\n\n\npairs(WB[,unlist(compounds_WB)], \n      pch=19, \n      cex=0.3, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(WB[,unlist(compounds_WB)])))))\n\n\n\n\n\n\n\npairs(OF[,unlist(compounds_OF)], \n      pch=19, \n      cex=0.4, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(OF[,unlist(compounds_OF)])))))\n\n\n\n\n\n\n❓ Why is there no pairplot for Breath?"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#group-differences-frequency-of-use",
    "href": "content/lectures/12-cs01-eda-slides.html#group-differences-frequency-of-use",
    "title": "12-cs01-eda",
    "section": "Group Differences: Frequency of Use",
    "text": "Group Differences: Frequency of Use\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_group_only <- function(dataset, compounds, tissue, legend=TRUE, y_lab=TRUE){\n  timepoint_to_use=levels(dataset$timepoint_use)[1]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(group, all_of(compounds)) \n  df <- df |> \n    gather(compound, value, -group) |> \n    clean_gluc() |> \n    group_by(compound) |> \n    mutate(y_max=1.2*max(value)) |> \n    group_by(compound, group) |> \n    mutate(n=n(),\n           my_label=paste0(group, ' N=', n),\n           my_label= gsub(\" \", \"\\n\", my_label))\n  \n  if(tissue == \"Blood\"){\n    df$compound=factor(df$compound, levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  y_pos <- df |> \n    group_by(compound) |> \n    summarize(y.position=mean(y_max))\n  \n  stat.test <- df |>\n    group_by(compound) |>\n    t_test(value ~ my_label) |>\n    adjust_pvalue(method=\"bonferroni\") |>\n    add_significance()\n  test <- stat.test |>\n    left_join(y_pos) |>\n    mutate(p.adj.signif=ifelse(p.adj.signif=='?', 'ns', p.adj.signif),\n           p.adj=ifelse(p.adj < 0.001, \"<0.001\", p.adj))\n\n  if(legend){\n    leg_position='right'\n  }else{\n    leg_position='none'\n  }\n  \n  if(y_lab){\n    y_text=\"Concentration (ng/mL)\"\n  }else{\n    y_text=''\n  }\n  \n  medianFunction <- function(x){\n    return(data.frame(y=round(median(x),1),label=round(median(x,na.rm=T),1)))}\n  \n  p2 <- ggplot(df, aes(x=my_label, y=value, fill=my_label)) + \n    geom_jitter(position=position_jitter(width=.3, height=0), size=0.8, color=\"gray65\")  +\n    geom_boxplot(outlier.shape=NA, alpha=0.6) +\n    stat_summary(fun=\"median\", geom=\"point\", shape=19, size=3, fill=\"black\") + \n    stat_summary(fun.data=medianFunction, geom =\"text\", color=\"black\", size=3.5, vjust=-0.65) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    geom_blank(aes(y=y_max)) + \n    scale_y_continuous(limits=c(0, NA), expand=expansion(mult=c(0, 0.1))) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    theme_classic() +\n    theme(text=element_text(size=14),\n          legend.position=leg_position,\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x='',\n         y=y_text) \n  \n  ann_text <- test |> \n    select(compound, p.adj, value=y.position, my_label=group1) |>\n    filter(p.adj < 0.05) |> \n    mutate(x1=1, x2=2)\n  \n  if(tissue == \"Whole Blood\"){\n    ann_text$compound=factor(ann_text$compound, \n                               levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  p2 + geom_text(data=ann_text, label=ann_text$p.adj, nudge_x=0.5) +\n    geom_segment(data=ann_text, aes(x=x1, xend=x2,\n                                    y=value - (0.04 * value), \n                                    yend=value - (0.04*value)))\n  \n}\n\n\n\n\ncompound_boxplot_group_only(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_group_only(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#group-differences-treatment",
    "href": "content/lectures/12-cs01-eda-slides.html#group-differences-treatment",
    "title": "12-cs01-eda",
    "section": "Group Differences: Treatment",
    "text": "Group Differences: Treatment\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_treatment <- function(dataset, compounds, tissue){\n  timepoint_to_use=levels(dataset$timepoint_use)[2]\n  df <- dataset |> \n    filter(timepoint_use == timepoint_to_use) |>\n    select(treatment, group, compounds)\n  df <- df |> \n    gather(compound, value, -treatment, -group) |> \n    clean_gluc()\n  \n  df |> \n    ggplot(aes(x=treatment, y=value, fill=group)) + \n    # geom_jitter(color=\"gray36\") +\n    geom_boxplot(outlier.size=0.1) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    scale_x_discrete(labels=function(x) str_wrap(x, width=11)) +\n    theme_classic(base_size=10) +\n    theme(legend.position=\"bottom\",\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x=\"Treatment\",\n         y=\"Measurement (ng/mL)\")\n  \n}\n\n\n\n\ncompound_boxplot_treatment(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\ncompound_boxplot_treatment(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#compound-summaries",
    "href": "content/lectures/12-cs01-eda-slides.html#compound-summaries",
    "title": "12-cs01-eda",
    "section": "Compound Summaries",
    "text": "Compound Summaries\n\nCodeWBOFBR\n\n\n\nT2A_plot <- function(dataset, compound, timepoint_use=2){\n  timepoint_to_use=levels(factor(dataset$timepoint_use))[timepoint_use]\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n             aes_string(x=\"group\", \n                        y=compound, \n                        fill=\"group\")) + \n        geom_boxplot(outlier.size=0.1) +\n        scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n        scale_y_continuous(limits=c(0,3)) +\n        facet_grid(~treatment) + \n        theme_classic() +\n        theme(legend.position=\"none\",\n              panel.grid=element_blank(),\n              strip.background=element_blank(),\n              plot.title.position=\"plot\") +\n        labs(title=paste0('Timepoint: ',\n                          levels(dataset$timepoint_use)[timepoint_use],\n                          ' post-smoking'),\n             x='Group',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n               aes_string(x=\"group\", \n                          y=compound, \n                          fill=\"group\")) + \n          geom_boxplot(outlier.shape=NA, alpha=0.8) +\n          scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n          facet_grid(~treatment) + \n          theme_classic() +\n          theme(legend.position=\"none\",\n                panel.grid=element_blank(),\n                strip.background=element_blank(),\n                plot.title.position=\"plot\") +\n          labs(title=paste0('Timepoint: ',\n                            levels(dataset$timepoint_use)[timepoint_use],\n                            ' post-smoking'),\n               x='Group',\n               y=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))))\n      )\n    }\n}\n\n\n\n\npost_wb <- map(compounds_WB, ~ T2A_plot( \n    dataset=WB_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_of <- map(compounds_OF, ~ T2A_plot( \n    dataset=OF_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_br <- map(compounds_BR, ~ T2A_plot( \n    dataset=BR_dups, \n    compound=.x))"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#hw02-part-ii",
    "href": "content/lectures/12-cs01-eda-slides.html#hw02-part-ii",
    "title": "12-cs01-eda",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#chess-players",
    "href": "content/lectures/12-cs01-eda-slides.html#chess-players",
    "title": "12-cs01-eda",
    "section": "Chess Players",
    "text": "Chess Players\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n\nchess <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/chess-transfers/transfers.csv\")\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nchess_trans <- chess |>\n  count(Federation) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\nchess_trans <- chess_trans |>\n  mutate(rank = row_number()) |>\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nggplot(chess_trans, aes(y = reorder(Federation, n), x = n)) +\n  geom_bar(stat = \"identity\",fill = \"#1c9099\") +\n  geom_text(aes(x = -3, y = Federation, label = n), size = 4) + #add count at the left side of the bars\n  labs(title = bquote(bold(\"More players transfer to the U.S. than to any other country\")),\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       x = \"NUMBER OF TRANSFERS\",\n       y = \"COUNTRY\") +\n  scale_fill_identity() +\n  theme_minimal() + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank()) + \n  theme(axis.text.y = element_text(size = 10, angle = 0, hjust = 0), #align to the left\n        plot.title = element_text(size=18)) #change font size"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#kushi-fandango",
    "href": "content/lectures/12-cs01-eda-slides.html#kushi-fandango",
    "title": "12-cs01-eda",
    "section": "Kushi: Fandango",
    "text": "Kushi: Fandango\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Kushi also got font working and stars on x-axis that I'd have to spend more time to get working \nfandango_score_comparison <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv\")\n\nall_scores_comp <- fandango_score_comparison |>\n  select(FILM, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars) |>\n  pivot_longer(c(RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars), names_to = \"Site\", values_to = \"Score\")\n\ncounts_of_scores <- all_scores_comp |>\n  group_by(Site, Score) |>\n  summarize(Count = n())\n\ncounts_of_scores <- counts_of_scores |>\n  group_by(Site) |>\n  mutate(Total = sum(Count))\n\nscores_with_percents <- counts_of_scores |>\n  mutate(Percent = Count / Total * 100)\n\n# making percents 0 for any scores that don't have any values\n\nsites <- tibble(Site = c(\"RT_norm_round\", \"RT_user_norm_round\", \"Metacritic_norm_round\", \"Metacritic_user_norm_round\", \"IMDB_norm_round\", \"Fandango_Stars\"))\n\nratings <- tibble(Score = seq(0, 5, by = 0.5))\n\nall_scores <- expand.grid(Site = sites$Site, Score = ratings$Score)\n\nall_scores <- all_scores |>\n  full_join(scores_with_percents, by = c(\"Site\", \"Score\")) |>\n  mutate(Percent = case_when(is.na(Count) ~ 0,\n                             TRUE ~ Percent))\n\nscores <- ggplot(all_scores, aes(x = Score, y = Percent, color = Site)) +\n  geom_line()  +\n  geom_hline(yintercept = 0, size = 0.7, color = \"black\") +\n  labs(x = NULL, y = NULL,\n       title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distrubution of 146 films in theaters in 2015 that\\n had 30+ reviews on Fandango.com\") +\n  scale_y_continuous(labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")) +\n  scale_x_continuous(labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +\n  scale_color_manual(values = c(\"Fandango_Stars\" = \"#fa6d54\",\n                                \"IMDB_norm_round\" = \"#e5c66a\",\n                                \"Metacritic_user_norm_round\" = \"#aeca91\",\n                                \"RT_user_norm_round\" = \"#76bde0\",\n                                \"Metacritic_norm_round\" = \"#b87eb5\",\n                                \"RT_norm_round\" = \"#a3a3a3\")) +\n  geom_ribbon(data = filter(all_scores, Site != \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent), alpha = 0.1) +\n  geom_ribbon(data = filter(all_scores, Site == \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent, fill = \"#fa6d54\", alpha = 0.24)) +\n  guides(color = \"none\", fill = \"none\", alpha = \"none\")\n\nscores + \n  annotate(\"text\", x = 4.9, y = 35, label = \"Fandango\", size = 5, color = \"#fa6d54\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.9, y = 37, label = \"IMDb users\", size = 5, color = \"#e5c66a\") +\n  annotate(\"text\", x = 2.7, y = 27, label = \"Metacritic\\nusers\", size = 5, color = \"#aeca91\") +\n  annotate(\"text\", x = 2.2, y = 20, label = \"Rotten\\nTomatoes\\nusers\", size = 5, color = \"#76bde0\") +\n  annotate(\"text\", x = 1.5, y = 16, label = \"Metacritic\", size = 5, color = \"#b87eb5\") +\n  annotate(\"text\", x = 0.7, y = 13, label = \"Rotten\\nTomatoes\", size = 5, color = \"#a3a3a3\") +\n  theme(#text = element_text(family = \"NimbusSan\"), \n        plot.title = element_text(face = \"bold\", size = 25, hjust = -0.1), \n        plot.subtitle = element_text(size = 15, hjust = -0.1),\n        plot.background = element_rect(fill = \"#f0f0f0\"),\n        panel.background = element_rect(fill = \"#f0f0f0\"),\n        panel.grid.major = element_line(color = \"gray75\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text = element_text(size = 14)\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#markus-congress",
    "href": "content/lectures/12-cs01-eda-slides.html#markus-congress",
    "title": "12-cs01-eda",
    "section": "Markus: Congress",
    "text": "Markus: Congress\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\ncongress_data <- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\")\n\ncongress_ages =\n  congress_data |>\n  select(congress, chamber, age_years) |>\n  mutate(congress_year = case_when(TRUE ~ 1787 + (2 * as.integer(congress)))) |>\n  group_by(congress_year, chamber) |>\n  mutate(mean_age = case_when(TRUE ~ mean(age_years))) |>\n  mutate(chamber = fct_recode(chamber,\n                              SENATE = \"Senate\",\n                              HOUSE = \"House\",))\n\nggplot(data = congress_ages,\n       mapping = aes(y = mean_age,\n                     x = congress_year,\n                     color = fct_rev(chamber),\n                     )) +\n  geom_step(size = 1) + \n  guides() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       caption = \"Data is based on all members who served in either the Senate or House in each Congress, which is notated\\nby the year in which it was seated. Any member who served in bothchambers in the same Congress was\\nassigned to the chamber in which they cast more votes.\\n FiveThirtyEight\\nSOURCES: BIOGRAPHICAL DIRECTORY OF THE U.S. CONGRESS, U.S. HOUSE OF REPRESENTATIVES,\\nU.S. SENATE, UNITEDSTATES GITHUB, VOTEVIEW.COM\",\n       y = NULL,\n       x = NULL\n  ) +\n  scale_color_manual(values=c(\"#6b4ddd\",\"#29ae53\")) +\n  theme_minimal(base_size = 13) + \n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(face = \"bold\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.title=element_blank(),\n        legend.position = \"top\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#voter-demographics",
    "href": "content/lectures/12-cs01-eda-slides.html#voter-demographics",
    "title": "12-cs01-eda",
    "section": "Voter Demographics",
    "text": "Voter Demographics\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggpubr)\n\nvoters = read.csv(\"data/nonvoters_data.csv\")\n\n\n#creating subplots\nrace = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Race\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())+\n    guides(fill = guide_legend(override.aes = list(shape = 16, key_width = 1, key_height = 1))) #trying to change the shape of the legend key\n\nincome = ggplot(data = voters, mapping = aes(y = factor(income_cat), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Income\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\nage = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Age\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\neducation = ggplot(data = voters, mapping = aes(y = factor(educ), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Education\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\npartyID = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"party ID\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\n#combining subplots\nria = ggarrange(race, income, age, ncol = 3, common.legend = TRUE, legend = \"top\") +\n  theme(plot.margin = margin(0.5, 0,-2, -1,\"cm\")) \n\nep = ggarrange(education, partyID) +\n  theme(plot.margin = margin(2, 4, -2.5, 1, \"cm\"))\n\nvoterplot = ggarrange(ria, ep, nrow = 3) \n\n\n#titles\nvoterplot = annotate_figure(annotate_figure(voterplot, \n  top = text_grob(\"Demographic information of survey respondants, by voting history\")),\n  top = text_grob(\"Those who always vote and those who sometimes vote aren't that different\", face = \"bold\")\n)\n\n\nvoterplot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#fouls",
    "href": "content/lectures/12-cs01-eda-slides.html#fouls",
    "title": "12-cs01-eda",
    "section": "Fouls",
    "text": "Fouls\n\nOriginalPlot ICode IPlot (Banso)Code (Banso)\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_csv_file <- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/foul-balls/foul-balls.csv\"\nfoulballs <- read.csv(url(raw_csv_file))\n\nfoulballs <- foulballs |>\n  mutate(over90 = case_when(\n    exit_velocity < 90 ~ \"no\",\n    exit_velocity >= 90 ~ \"yes\"\n  ))\n\nggplot(foulballs,\n       aes (y = used_zone)) +\n  geom_bar(aes(fill = over90),\n           position = position_stack(reverse = TRUE),\n           show.legend = FALSE) +\n  scale_fill_manual(labels = c(\"< 90 mph\", \"≥ 90 mph\", \"Unknown exit velocity\"),\n                    values = c(\"#97c16d\", \"#63abb0\"), na.value = \"#d3d3d3\") +\n  scale_y_discrete(limits = rev(unique(foulballs$used_zone)))+\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"#ececec\"),\n        panel.background = element_rect(fill = \"white\"),\n        axis.text.x = element_text(color = \"#9e9e9e\"),\n        plot.caption = element_text(color = \"#b5b5b5\"),\n        axis.line.y = element_line(colour = \"#343434\"),\n        axis.title.y = element_text(angle = 0, vjust = 0.12, color = \"#343434\", size = 9),\n        axis.ticks = element_blank(),\n        legend.title = element_blank()) +\n  labs(title = \"The hardest-hit fouls seem to land in unprotected areas\",\n       subtitle = str_wrap(\"Foul balls by the stadium zone they landed in and their exit velocity, among 906 fouls hit this season in the most foul-heavy day at the 10 MLB stadiums that produced the most fouls as of June 5\", 85),\n       x = \"\", y = \"Zone\",\n       caption = \"SOURCE: BASEBALL SAVANT\") +\n  annotate(\"text\", x = 75, y = 1, label = \"< 90 mph\", col = \"white\") +\n  annotate(\"text\", x = 140, y = 3.3, label = \"≥ 90 mph\", col = \"#63abb0\") +\n  annotate(\"text\", x = 215, y = 1, label = \"Unknown exit velocity\", col = \"white\")\n\n\n\n\n\n\n\n\n\n\n\nball_data <- foulballs |>\n  mutate(category = case_when(\n    exit_velocity < 90 ~ \"<90\",\n    exit_velocity >= 90 ~ \">=90\",\n    is.na(exit_velocity) ~ \"Unknown\"\n                           ))\nmy_plot <- ggplot(ball_data, aes(y = fct_rev(as.character(used_zone)), fill = category)) +\n  geom_bar(position = position_stack(reverse = TRUE)) +\n  labs(title = \"The hardest-hit fouls seem \\nto land in unprotected areas\",\n       subtitle = \"The 906 foul balls hit this season from \\nthe most foul-heavy day at each of the \\n10 MLB stadiums that produced the \\nthe most fouls as of June 5, by zone where \\nthe balls landed and their exit velocities\",\n       y = \"Zone\", x = NULL) +\n  scale_fill_manual(values = c(\"Unknown\" = \"#DEDEDE\", \"<90\" = \"#9ECE88\", \">=90\" = \"#17AFAD\")) +\n  scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250),position = \"top\") +\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major.x = element_line(size = 0.5, linetype=\"solid\", color=\"#CECECE\"),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(color = \"black\", face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(size = 10, margin = margin(b = 20))\n)\n\nmy_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#colin-nba-raptor-ratings",
    "href": "content/lectures/12-cs01-eda-slides.html#colin-nba-raptor-ratings",
    "title": "12-cs01-eda",
    "section": "Colin: NBA RAPTOR ratings",
    "text": "Colin: NBA RAPTOR ratings\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\nraptor <- read.csv(\"data/latest_RAPTOR_by_player.csv\")\n\nraptor_rounded <- raptor |> \n  filter(mp >= 1137) |>\n  mutate(across(where(is.numeric), round, 1))\n\nraptor_plot <- raptor_rounded |>\n  ggplot(aes(x=raptor_offense, y = raptor_defense)) + \n  \n  # Annotations\n    # The colored rectangles for the 1 & 3 quadrants\n    annotate(\"rect\", xmin=0, xmax=10, ymin=0, ymax=10, fill = '#c5ecee', alpha = .85) + \n    annotate(\"rect\", xmin=-10, xmax=0, ymin=-10, ymax=0, fill = '#fecada', alpha = .85) +\n    \n    # The 3rd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -7.8, ymax = -6.8, fill = '#fd97b6') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = -7.4, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = -9, label = \" -  defense\") +\n    \n    # The 1st quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 8.4, ymax = 9.4, fill = '#8cdadf') +\n    annotate(\"text\", x = 7.5, y = 8.8, label = \" +  offense\") +\n    annotate(\"text\", x = 7.55, y = 7.1, label = \" +  defense\") +\n  \n    # The 2nd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 8.4, ymax = 9.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = 8.8, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = 7.1, label = \" +  defense\") +\n  \n    # The 4th quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -7.8, ymax = -6.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = 7.55, y = -7.4, label = \" +  offense\") +\n    annotate(\"text\", x = 7.5, y = -9, label = \" -  defense\") +\n  \n  geom_point(shape= 21, colour = \"black\", fill = \"white\", size = 4) + \n  \n  labs(\n    x = \"Offensive RAPTOR rating\", \n    y = \"Defensive RAPTOR rating\",\n    title = paste0('Nikola Jokic is the Best NBA Player Based on Overall RAPTOR Rating',\n            '<br>',\n            '<sup>',\n            'An Analytical Approach to the 2022 - 2023 NBA Season','</sup>')\n    ) + \n    \n  # Theme settings\n  theme_light() + \n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(), \n    panel.grid.major = element_line(color = \"#cdcdcd\", linewidth = 0.5), \n    plot.margin = margin(l = 100, r = 100, b = 20, t = 10),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(color = \"#cdcdcd\", size = 12),\n    axis.title.x = element_text(margin = margin(t=3)),\n    axis.title.y = element_text(margin = margin(r=3)),\n    ) + \n  coord_fixed(ratio = 1) \n\n# interactive\n# ggplotly(raptor_plot, text=player_name, hoverinfo='text') \n\nraptor_plot"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#biden-approval",
    "href": "content/lectures/12-cs01-eda-slides.html#biden-approval",
    "title": "12-cs01-eda",
    "section": "Biden Approval",
    "text": "Biden Approval\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval <- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n#getting only data from beginning of COVID to Jan 19, 2021 and getting rid of the \"all\" category in the party column \nyear_20 <- \n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year <= 2020)|>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n\nyear_21 <-\n  COVID_approval |>\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |>\n  filter(party != \"all\", \n         end_year == 2021 & end_month == 1 & end_day <= 19) |>\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n       #  if (end_year == 2021 & end_month == 1 & end_day <= 19))\n\nselect_COVID_approval <-\n  full_join(year_20, year_21)\n\nggplot(select_COVID_approval, aes(x = as.Date(end_date),\n                           y = approve,\n                           color = party)) +\n  geom_smooth(aes(group = party), span = 0.05, se = FALSE) + \n  scale_color_manual(values = c(\"D\" = \"#008fd5\", \n                               \"R\" = \"#ff2700\",\n                               \"I\" = \"#a55330\")) +\n  \n  #Important Dates\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-02-29\"), y = 95, label=\"First U.S. \\n Death \\n Reported\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-05-20\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-05-20\"), y = 95, label=\"U.S. Deaths \\n surpass \\n 100,000\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype=3) + #Trump Diagnosed with COVID-19\n    annotate(\"text\", x= as.Date(\"2020-10-02\"), y = 95, label=\"Trump \\n Diagnosed \\n with \\n COVID-19\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-11-07\"), y = 95, label=\"Biden \\n declared \\n election \\n winner\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2021-01-19\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2021-01-19\"), y = 95, label=\"Biden \\n sworn \\n into \\n office\", size=3, color=\"black\") +\n  \n  labs(title = \"Approval of Trump’s response varies widely by party\",\n       subtitle = \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s \\n handling of the coronavirus outbreak\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5 ), \n        plot.subtitle = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.minor = element_line(colour=\"gray\")\n        )"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#recap",
    "href": "content/lectures/12-cs01-eda-slides.html#recap",
    "title": "12-cs01-eda",
    "section": "Recap",
    "text": "Recap\n\nCan you explain/describe the plots generated in the context of these data?\nCan you generate EDA plots of your own for these data\nCan you understand/work through the more complicated code provided (even if you couldn’t have come up with it on your own)\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html",
    "href": "content/lectures/13-cs01-analysis.html",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Q: How extensive does our extension component need to be?\nA: A bit hard to answer in certain terms. We’ll discuss some examples today to hopefully set expectaions well. To explain in writing here, the most typical extension is students using the data provided to ask and answer a question not directly presented in class. Thus, simply generating a visualization not presented in class would NOT be sufficient. At the other end, finding external data on the topic and analyzing that data, while certainly allowed, would far exceed expectations. In between those extremes is what we expect: add significantly to the analysis, beyond what was presented in class.\n\n\n\n\nDue Dates:\n\nHW03 (MLR) due Mon 11/20\nProject Proposal (it will be a Google Form) due 11/20\nCS01 Deadlines:\n\nLab06 due Friday - cs01-focused\nReport & “General Communication” due 11/27\nsurvey about how working with group went - due 11/28\n\n\n\nNotes:\nMidterm scores & Feedback posted\n\noverall, did very well\n\navg: 13.85/15 (92%)\n6 perfect scores\n\nanswer key on course website\n\nI am behind on emails and Piazza posts.\n\n\n\n\n\nN=73 (~75%)\nPacing workload (so far) about right\nCourse notes most helpful in the course overall\nAlso helpful: completing labs, doing homework,\nMany are not checking labs against answer keys; most are not doing suggested readings\nOf those that attend lecture, most find it helpful\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging/Understanding Code Strategies\nSensitivity & Specificity\nCross-compound correlations\nExtensions\n\n\n\n\nSuggestions (as discussed in class):\n\n\nLook up documentation (i.e. ?...) / Google the function\nRun it on different input; see how output changing\nRun the code line-by-line, understanding output at each step\nAsk ChatGPT"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#packages",
    "href": "content/lectures/13-cs01-analysis.html#packages",
    "title": "13-cs01-analysis",
    "section": "Packages",
    "text": "Packages\nThree additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)\nlibrary(cowplot)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#the-data",
    "href": "content/lectures/13-cs01-analysis.html#the-data",
    "title": "13-cs01-analysis",
    "section": "The Data",
    "text": "The Data\nReading in the data from the end of data wrangling notes:\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nAnd the functions…\n\nsource(\"src/cs01_functions.R\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#sensitivity-specificity",
    "href": "content/lectures/13-cs01-analysis.html#sensitivity-specificity",
    "title": "13-cs01-analysis",
    "section": "Sensitivity & Specificity",
    "text": "Sensitivity & Specificity\nSensitivity | the ability of a test to correctly identify patients with a disease/trait/condition. \\[TP/(TP + FN)\\]\n\nSpecificity | the ability of a test to correctly identify people without the disease/trait/condition. \\[TN/(TN + FP)\\]\n\n\n❓ For this analysis, do you care more about sensitivity? about specificity? equally about both?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#what-is-a-tp-here-tn-fp-fn",
    "href": "content/lectures/13-cs01-analysis.html#what-is-a-tp-here-tn-fp-fn",
    "title": "13-cs01-analysis",
    "section": "What is a TP here? TN? FP? FN?",
    "text": "What is a TP here? TN? FP? FN?\nPost-smoking (cutoff > 0)\n\n\nTP = THC group, value >= cutoff\nFN = THC group, value < cutoff\nFP = Placebo group, value >= cutoff\nTN = Placebo group, value < cutoff\n\n\n\nPost-smoking (cutoff == 0)\nCannot be a TP or FP if zero…\n\nTP = THC group, value > cutoff),\nFN = THC group, value <= cutoff),\nFP = Placebo group, value > cutoff),\nTN = Placebo group, value < cutoff)\n\n\n\nPre-smoking\nCannot be a TP or FN before consuming…\n\nTP = 0\nFN = 0\nFP = value >= cutoff\nTN = value < cutoff"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#roc",
    "href": "content/lectures/13-cs01-analysis.html#roc",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\nReceiver-Operator Characteristic (ROC) Curve: TPR (Sensitivity) vs FPR (1-Specificity)\n\nImage Credit: By cmglee, MartinThoma - Roc-draft-xkcd-style.svg, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=109730045"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculating-sensitivity-and-specificity",
    "href": "content/lectures/13-cs01-analysis.html#calculating-sensitivity-and-specificity",
    "title": "13-cs01-analysis",
    "section": "Calculating Sensitivity and Specificity",
    "text": "Calculating Sensitivity and Specificity\n\nCalculateRunApplyDo it!\n\n\n\nmake_calculations <- function(dataset, dataset_removedups, split, compound, \n                              start = start, stop = stop, tpt_use = tpt_use){\n  ## remove NAs\n  df <- dataset_removedups %>% \n    dplyr::select(treatment, compound, timepoint_use) %>%\n    rename(compound = 2) %>%\n    filter(complete.cases(.))\n  if(nrow(df)>0){\n    if(stop <= 0){\n      output <- df %>% \n        summarise(TP = 0,\n                  FN = 0,\n                  FP = sum(compound >= split),\n                  TN = sum(compound < split)) \n    }else{\n      if(split == 0){\n        output_pre <- df %>% \n          filter(tpt_use == \"pre-smoking\") %>%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound >= split),\n                    TN = sum(compound < split)) \n        \n        output <- df %>% \n          filter(tpt_use != \"pre-smoking\") %>%\n          summarise(TP = sum(treatment != \"Placebo\" & compound > split),\n                    FN = sum(treatment != \"Placebo\" & compound <= split),\n                    FP = sum(treatment == \"Placebo\" & compound > split),\n                    TN = sum(treatment == \"Placebo\" & compound < split))\n        \n        output <- output + output_pre\n      }else{\n        ## calculate values if pre-smoking\n        output_pre <- df %>% \n          filter(tpt_use == \"pre-smoking\") %>%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound >= split),\n                    TN = sum(compound < split)) \n        \n        output <- df %>% \n          filter(tpt_use != \"pre-smoking\") %>%\n          summarise(TP = sum(treatment != \"Placebo\" & compound >= split),\n                    FN = sum(treatment != \"Placebo\" & compound < split),\n                    FP = sum(treatment == \"Placebo\" & compound >= split),\n                    TN = sum(treatment == \"Placebo\" & compound < split))\n        \n        output <- output + output_pre\n      }\n    }\n  }\n  # clean things up; make calculations on above values\n  output <- output %>%\n    mutate(detection_limit = split,\n           compound = compound,\n           time_start = start,\n           time_stop = stop,\n           time_window = tpt_use,\n           NAs = nrow(dataset) - nrow(df),\n           N = nrow(dataset_removedups),\n           N_removed = nrow(dataset) - nrow(dataset_removedups),\n           Sensitivity = (TP/(TP + FN)), \n           Specificity = (TN /(TN + FP)),\n           PPV = (TP/(TP+FP)),\n           NPV = (TN/(TN + FN)),\n           Efficiency = ((TP + TN)/(TP + TN + FP + FN))*100\n    )\n  \n  return(output)\n}\n\n\n\n\ndetermine what cutoff values to try\ncarry out above function on those cutoffs\n\n\nsens_spec <- function(dataset, compound, start, stop, tpt_use, \n                      lowest_value = 0.5, splits = NULL, ...){\n  # if it's not all NAs...\n  if(sum(is.na(dataset[,compound])) != nrow(dataset)){\n    # specify what splits should be used for calculations\n    if(is.null(splits)){\n      limits <- dataset[is.finite(rowSums(dataset[,compound])),compound]\n      ## define lower and upper limits\n      lower = min(limits, na.rm=TRUE)\n      upper = max(limits, na.rm=TRUE)\n      ## determine splits to use for calculations\n      tosplit = pull(limits[,1])[limits[,1]>0]\n      ## only split if there are detectable limits:\n      if(length(tosplit)>=1){\n        splits = c(lowest_value, quantile(tosplit, probs=seq(0, 1, by = 0.01), na.rm=TRUE))\n        splits = unique(splits)\n      }else{\n        splits = 0\n      }\n    }else{\n      splits = splits\n    }\n    # filter to include timepoint of interest\n    dataset <- dataset %>% \n      filter(time_from_start > start & time_from_start <= stop & !is.na(timepoint_use))\n    dataset_removedups <- dataset %>%\n      filter(!is.na(timepoint_use)) %>% \n      group_by(timepoint_use) %>% \n      distinct(id, .keep_all = TRUE) %>% \n      ungroup()\n\n    ## create empty output variable which we'll fill in\n    ## iterate through each possible dose and calculate\n    output <- map_dfr(as.list(splits), ~make_calculations(dataset, \n                                                          dataset_removedups, \n                                                          split = .x,\n                                                          compound,\n                                                          start = start,\n                                                          stop = stop, \n                                                          tpt_use = tpt_use))\n  }\n  \n  return(output)\n}\n\n\n\nMap the above for each matrix…\n\nsens_spec_cpd <- function(dataset, cpd, timepoints, splits = NULL){\n  args2 <- list(start = timepoints$start, \n                stop = timepoints$stop, \n                tpt_use = timepoints$timepoint)\n  out <- args2 %>% \n    pmap_dfr(sens_spec, dataset, compound = cpd, splits = splits)\n  return(out)\n}\n\n\n\nThis takes a few minutes to run… (reminder: cache=TRUE)\n\noutput_WB <- map_dfr(compounds_WB, \n                     ~sens_spec_cpd(dataset = WB, cpd = all_of(.x), \n                                    timepoints = timepoints_WB)) %>% clean_gluc()\n\noutput_BR <- map_dfr(compounds_BR, \n                     ~sens_spec_cpd(dataset = BR,  cpd = all_of(.x),\n                                    timepoints = timepoints_BR))  %>% clean_gluc()\n\noutput_OF <- map_dfr(compounds_OF, \n                     ~sens_spec_cpd(dataset = OF, cpd = all_of(.x),\n                                    timepoints = timepoints_OF))  %>% clean_gluc()"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#roc-1",
    "href": "content/lectures/13-cs01-analysis.html#roc-1",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\n\nCodeCalculatePlot\n\n\n\nss_plot <- function(output, tpts=8, tissue){\n  to_include = output %>%\n    group_by(compound) %>% \n    summarize(mean_detection = mean(detection_limit)) %>% \n    filter(mean_detection > 0)\n  \n  output <-  output %>% \n    mutate(iszero = ifelse(time_start<0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %>%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %>%\n    clean_gluc() %>% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output <- output %>%  mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#A2DDED', '#86BEDC', '#6C9FCA', \n                  '#547EB9', '#3F5EA8', '#2D4096', '#1E2385',\n                  '#181173', '#180762', '#180051')\n  values = c(blue_colors[1:tpts])\n  \n  print(ggplot(output, aes(x = detection_limit, y = Sensitivity, group = fct_inorder(legend))) + \n          geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n          geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n          facet_grid(~compound, scales = \"free_x\") +\n          labs(x = 'Detection Limit',\n               y = 'Sensitivity') +\n          ylim(0,1) +\n          scale_color_manual(values = values, name = 'Time Window') +\n          theme_classic(base_size = 12) + \n          theme(axis.title = element_text(size=16), \n                panel.grid = element_blank(),\n                strip.background = element_blank(),\n                strip.text.x = element_text(size = 12))  \n  )\n  print(\n    ggplot(output, aes(x = detection_limit, y = Specificity, group = fct_inorder(legend))) + \n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound, scales = \"free_x\") +\n      ylim(0,100) +\n      labs(title = tissue,\n           x = 'Detection Limit',\n           y = 'Specificity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12))\n  )\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12))\n  )\n}\n\nroc_plot <- function(output, tpts=8, tissue){\n  to_include = output %>%\n    group_by(compound) %>% \n    summarize(mean_detection = mean(detection_limit)) %>% \n    filter(mean_detection > 0)\n  \n  output <-  output %>% \n    mutate(iszero = ifelse(time_start<0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %>%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %>%\n    clean_gluc() %>% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output <- output %>% mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#86BEDC', \n                  '#547EB9', '#2D4096',\n                  '#181173', '#180051')\n  values = c(blue_colors[1:tpts])\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12) )\n  )\n}\n\n\n\n\nss1_a <- ss_plot(output_WB, tpts = length(unique(output_WB$time_start)), tissue = \"Blood\")\n\n\n\n\n\n\n\n\n\nss2_a <- ss_plot(output_OF, tpts = length(unique(output_OF$time_start)), tissue = \"Oral Fluid\")\n\n\n\n\n\n\n\n\n\nss3_a <- roc_plot(output_BR, tpts = length(unique(output_BR$time_start)), tissue = \"Breath\")\n\n\n\n\n\n\n\nbottom_row <- plot_grid(ss2_a, ss3_a, labels = c('B', 'C'), label_size = 12, ncol = 2, rel_widths = c(0.66, .33))\nplot_grid(ss1_a, bottom_row, labels = c('A', ''), label_size = 12, ncol = 1)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculate-thc",
    "href": "content/lectures/13-cs01-analysis.html#calculate-thc",
    "title": "13-cs01-analysis",
    "section": "Calculate: THC",
    "text": "Calculate: THC\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOFBR\n\n\n\ncutoffs = c(0.5, 1, 2, 5, 10)\nWB_THC <- sens_spec_cpd(dataset = WB, cpd = 'thc',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %>% clean_gluc()\n\nWB_THC\n\n# A tibble: 50 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int>           <dbl> <chr>         <dbl>     <dbl>\n 1     0     0    81   108             0.5 THC            -400         0\n 2     0     0    61   128             1   THC            -400         0\n 3     0     0    45   144             2   THC            -400         0\n 4     0     0    10   179             5   THC            -400         0\n 5     0     0     1   188            10   THC            -400         0\n 6   124     2    28    33             0.5 THC               0        30\n 7   123     3    22    39             1   THC               0        30\n 8   119     7    15    46             2   THC               0        30\n 9   106    20     4    57             5   THC               0        30\n10   101    25     0    61            10   THC               0        30\n# ℹ 40 more rows\n# ℹ 9 more variables: time_window <chr>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>\n\n\n\n\n\nOF_THC <- sens_spec_cpd(dataset = OF, cpd = 'thc',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %>% clean_gluc()\n\nOF_THC\n\n# A tibble: 40 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int>           <dbl> <chr>         <dbl>     <dbl>\n 1     0     0    35   157             0.5 THC            -400         0\n 2     0     0    20   172             1   THC            -400         0\n 3     0     0     9   183             2   THC            -400         0\n 4     0     0     0   192             5   THC            -400         0\n 5     0     0     0   192            10   THC            -400         0\n 6   129     0    39    24             0.5 THC               0        30\n 7   129     0    30    33             1   THC               0        30\n 8   128     1    19    44             2   THC               0        30\n 9   128     1     3    60             5   THC               0        30\n10   125     4     1    62            10   THC               0        30\n# ℹ 30 more rows\n# ℹ 9 more variables: time_window <chr>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>\n\n\n\n\nWhy is there no calculation for breath?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#cutoffs",
    "href": "content/lectures/13-cs01-analysis.html#cutoffs",
    "title": "13-cs01-analysis",
    "section": "Cutoffs",
    "text": "Cutoffs\n\nCodeWBOF\n\n\n\nplot_cutoffs <- function(dataset, timepoint_use_variable, tissue, labels = c(\"A\", \"B\"), vertline, cpd, x_labels){\n    col_val = c(\"#D9D9D9\", \"#BDBDBD\", \"#969696\", \"#636363\", \"#252525\")\n    lines = rep(\"solid\", 5)\n    \n  df_ss <- dataset %>% \n    mutate(time_window = fct_relevel(as.factor(time_window), \n                                     levels(timepoint_use_variable)),\n           detection_limit = as.factor(detection_limit),\n           Sensitivity =  round(Sensitivity*100,0),\n           Specificity =  round(Specificity*100,0),\n           my_label = paste0(time_window, ' N=', N),\n           my_label =  gsub(\" \", \"\\n\", my_label),\n           my_label = fct_relevel(as.factor(my_label), x_labels)) #%>%          \n    \n    p1 <- df_ss %>% \n    ggplot(aes(x = my_label, y = Sensitivity, \n               colour = detection_limit)) + \n    geom_line(size = 1.2, aes(group = detection_limit, \n                              linetype = detection_limit)) + \n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point(show.legend=FALSE) + \n    ylim(0,100) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values=lines) +\n      scale_color_manual(values = col_val, name = \"Cutoff \\n (ng/mL)\",\n                         guide = guide_legend(override.aes = list(linetype = c(1),\n                                                                  shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme( axis.title = element_text(size=16),\n           axis.text = element_text(size=10),\n           legend.position = c(0.08, 0.4),\n           panel.grid = element_blank(),\n           strip.background = element_blank()\n           ) +\n      guides(linetype = FALSE) +\n    labs(x = \"Time Window\", \n         y = \"Sensitivity\", \n         title = paste0(tissue,\": \", cpd) )\n \n  p2 <- df_ss %>% \n    ggplot(aes(x = my_label, y = Specificity,\n               group = detection_limit, \n               colour = detection_limit, \n               linetype = detection_limit)) + \n    geom_line(size = 1.2) +\n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point() + \n    ylim(0,100) +\n    scale_color_manual(values = col_val) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values = lines, \n                          guide = guide_legend(override.aes = list(linetype = \"solid\",\n                                                                   shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme(axis.title = element_text(size=16),\n          axis.text = element_text(size=10),\n          legend.position = \"none\", \n          panel.grid = element_blank(),\n          strip.background = element_blank()) +\n    labs(x = \"Time Window\", \n         y = \"Specificity\",\n         title = \"\" )\n  \n  title <- ggdraw() + \n    draw_label(\n      tissue,\n      x = 0.05,\n      hjust = 0\n    )\n  \n  plot_row <- plot_grid(p1, p2, labels = labels, label_size = 12)\n  \n  plot_grid(\n    title, plot_row,\n    ncol = 1,\n    # rel_heights values control vertical title margins\n    rel_heights = c(0.1, 1)\n  )\n  \n  return(list(plot_row, df_ss))\n\n}\n\n\n\n\nblood_levels <- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(dataset=WB_THC, \n             timepoint_use_variable=WB$timepoint_use, \n             tissue=\"Blood\", \n             vertline=levels(WB$timepoint_use)[5], \n             cpd=\"THC\", \n             x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0    81   108 0.5             THC            -400         0\n 2     0     0    61   128 1               THC            -400         0\n 3     0     0    45   144 2               THC            -400         0\n 4     0     0    10   179 5               THC            -400         0\n 5     0     0     1   188 10              THC            -400         0\n 6   124     2    28    33 0.5             THC               0        30\n 7   123     3    22    39 1               THC               0        30\n 8   119     7    15    46 2               THC               0        30\n 9   106    20     4    57 5               THC               0        30\n10   101    25     0    61 10              THC               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>\n\n\n\n\n\nof_levels <- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_THC, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"THC\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0    35   157 0.5             THC            -400         0\n 2     0     0    20   172 1               THC            -400         0\n 3     0     0     9   183 2               THC            -400         0\n 4     0     0     0   192 5               THC            -400         0\n 5     0     0     0   192 10              THC            -400         0\n 6   129     0    39    24 0.5             THC               0        30\n 7   129     0    30    33 1               THC               0        30\n 8   128     1    19    44 2               THC               0        30\n 9   128     1     3    60 5               THC               0        30\n10   125     4     1    62 10              THC               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculate-cbn",
    "href": "content/lectures/13-cs01-analysis.html#calculate-cbn",
    "title": "13-cs01-analysis",
    "section": "Calculate: CBN",
    "text": "Calculate: CBN\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOF\n\n\n\nWB_CBN =  sens_spec_cpd(dataset = WB, cpd = 'cbn',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %>% clean_gluc()\n\nblood_levels <- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(WB_CBN, WB$timepoint_use, tissue = \"Blood\", vertline=levels(WB$timepoint_use)[5], cpd=\"CBN\", x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0     1   188 0.5             CBN            -400         0\n 2     0     0     0   189 1               CBN            -400         0\n 3     0     0     0   189 2               CBN            -400         0\n 4     0     0     0   189 5               CBN            -400         0\n 5     0     0     0   189 10              CBN            -400         0\n 6   106    20     7    54 0.5             CBN               0        30\n 7    97    29     0    61 1               CBN               0        30\n 8    82    44     0    61 2               CBN               0        30\n 9    40    86     0    61 5               CBN               0        30\n10     9   117     0    61 10              CBN               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>\n\n\n\n\n\nOF_CBN =  sens_spec_cpd(dataset = OF, cpd = 'cbn',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %>% clean_gluc()\n\nof_levels <- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_CBN, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"CBN\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   <dbl> <dbl> <int> <int> <fct>           <chr>         <dbl>     <dbl>\n 1     0     0     5   187 0.5             CBN            -400         0\n 2     0     0     1   191 1               CBN            -400         0\n 3     0     0     1   191 2               CBN            -400         0\n 4     0     0     1   191 5               CBN            -400         0\n 5     0     0     0   192 10              CBN            -400         0\n 6   127     2    41    22 0.5             CBN               0        30\n 7   125     4    32    31 1               CBN               0        30\n 8   122     7    18    45 2               CBN               0        30\n 9   116    13     7    56 5               CBN               0        30\n10   107    22     3    60 10              CBN               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window <fct>, NAs <int>, N <int>, N_removed <int>,\n#   Sensitivity <dbl>, Specificity <dbl>, PPV <dbl>, NPV <dbl>,\n#   Efficiency <dbl>, my_label <fct>"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#compound-correlations",
    "href": "content/lectures/13-cs01-analysis.html#compound-correlations",
    "title": "13-cs01-analysis",
    "section": "Compound Correlations",
    "text": "Compound Correlations\n\nCodePlot\n\n\n\nggplotRegression <- function (x, y, xlab, ylab, x_text, y_text,  y_text2, title) {\n  fit <- lm(y ~ x)\n  if(max(fit$model[,1],na.rm=TRUE)!=0){\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), \n                                               digits=2)),\n               vjust=1, hjust=0, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))),\n        vjust=1, hjust=0, size=4.5) + \n      theme_minimal(base_size=14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n  } else{\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      scale_y_continuous(limits = c(0,3)) +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), digits=2)),vjust=1, hjust=1, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))), vjust=1, hjust=1,size=4.5) + \n      theme_minimal(base_size = 14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n    \n    \n  }\n}\n\n\n\n\nwb_reg <- ggplotRegression(WB$thc, WB$cbn, xlab = 'THC (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 150, y_text = 7, y_text2 = 5, title = \"Blood\")\n\nof_reg <- ggplotRegression(OF$thc, OF$cbn, xlab = 'THC  (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 12500, y_text = 750, y_text2 = 500, title = \"Oral Fluid\")\n\nplot_grid(wb_reg, of_reg, labels = 'AUTO', label_size = 12, ncol = 2, scale = 1)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#possible-extensions",
    "href": "content/lectures/13-cs01-analysis.html#possible-extensions",
    "title": "13-cs01-analysis",
    "section": "Possible Extensions",
    "text": "Possible Extensions\nOur current question asks for a single compound…and you’ll need to decide that.\n\n…but you could imagine a world where more than one compound or more than one matrix could be measured at the roadside.\n\n\nSo:\n\n\ncombination of the oral fluid and blood that would better predict recent use? (For example if an officer stopped a driver and got a high oral fluid, but could not get a blood sample for a couple of hours and got a relatively low result would this predict recent use better than blood (or OF) alone?\nIs there a ratio of OF/blood that predicts recent use?\nMachine learning model to determine optimal combination of measurements/cutoffs to detect recent use?\n\n\n\n\nThings to keep in mind:\n\nsome matrices are easier to get at the roadside\ntime from use matters (trying to detect recent use)\nwe may not care equally about sensitivity and specificity"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#cs01-what-to-do-now",
    "href": "content/lectures/13-cs01-analysis.html#cs01-what-to-do-now",
    "title": "13-cs01-analysis",
    "section": "cs01: what to do now?",
    "text": "cs01: what to do now?\n\nCommunicate with your group!\nDiscuss possible extensions\nMake a plan; figure out who’s doing what; set deadlines\nImplement the plan!"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#what-has-to-be-done",
    "href": "content/lectures/13-cs01-analysis.html#what-has-to-be-done",
    "title": "13-cs01-analysis",
    "section": "What has to be done:",
    "text": "What has to be done:\n\n\nQuestion | include in Rmd; add extension if applicable\nBackground | summarize and add to what was discussed in classed\nData\n\nDescribe data & variables\nData wrangling | likely copy + paste from notes; add explanation as you go\n\nAnalysis\n\nEDA | likely borrowing parts from notes and adding more in; be sure to include interpretations of output & guide the reader\nAnalysis | likely borrowing most/all from class; interpretations/guiding reader/contextualizing is essential\nExtension | must be completed\n\nConclusion | summarize\nProofread | ensure it makes sense from top to bottom\nGeneral Audience communication (submit on Canvas; 1 submission per group)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#collaborating-on-github",
    "href": "content/lectures/13-cs01-analysis.html#collaborating-on-github",
    "title": "13-cs01-analysis",
    "section": "Collaborating on GitHub",
    "text": "Collaborating on GitHub\n\nBe sure to pull changes every time you sit down to work\nAvoid working on the same part of the same file as another teammate OR work in separate files and combine at the end\npush your changes once you’re ready to add them to the group"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#recap",
    "href": "content/lectures/13-cs01-analysis.html#recap",
    "title": "13-cs01-analysis",
    "section": "Recap",
    "text": "Recap\n\nCan you describe sensitivity? Specificity?\nCan you explain how TP, TN, FP, and FN were calculated/defined in this experiment?\nCan you describe the code used to carry out the calculations?\nCan you interpret the results from these data?"
  },
  {
    "objectID": "content/lectures/14-tidymodels.html",
    "href": "content/lectures/14-tidymodels.html",
    "title": "14-tidymodels",
    "section": "",
    "text": "Q: I had a question about the presentations for the final projects; since it is due during finals week, is it a live presentation in class or do we submit a video? If it is a live presentation, do we present during our designated final day/time on webreg?\nA: Video submission!\n\n\nQ: I also wanted to mention that the mid/pre-course extra credit surveys doesn’t reflect a change in grade on canvas. (For ex. if i put a 0 or 100 for E.C my grade stays the same).\nA: Correct - I add these in at the end. Canvas can do many things, but it doesn’t handle EC well (from what I can tell).\n\n\nQ: I’m overwhelmed/confused by “the code :’) it’s quite a bit to take in”\nA: Yes! It’s a lot! This is why we have group mates on the case study. I encourage everyone to sit with the code after class and then work through it together as you complete the case study!\n\n\nQ: For oral fluid you mentioned looking more into why there’s that big dip in specificity and that we should look more into that on Friday with eda but would that be slightly guided because I have no idea where to start with that.\nA: I would make some plots that specifically look at the data/numbers there to figure out what could be leading to that drop at that particular time window.\n\n\nQ: Why are specificity graphs so high?  A: Good question - this is generally b/c people who didn’t smoke have values very close to zero across compounds…so they will rarely be above the cutoff, making this very effective at identifying individuals who did not smoke\n\n\nQ: What is the dplyr::select notation, like is it a way to use select from dplyr without librarying first?\nA: Yes!\n\n\nQ: Also separate topic, but do we have information on impairment so we can account for that with recent use?  A: Great question - impairment is very hard to define here. We (the researchers) have data on self-reported high and what the police officers determined, but y’all don’t have that data. So, we’re using knowledge from other studies (see 11-cs01-data notes) to understand what we know on impoairment but only focusing on detecting recent use here.\n\n\nQ: I am unable to locate where to sign up for groups for the final project\nA: This form was just released (sorry for delay). link to survey\n\n\nQ: I think I need more time to digest how the code works together to produce the visuals that we saw.\nA: I agree. I think I could balance and give more time in class…but I will say this is an exercise I want groups to work through together!\n\n\n\n\nDue Dates:\n\nNo class This Th; No Lab this Fri (Happy Thanksgiving!)\nCS01 due Monday 11/27\n\ngroup work survey due Tues 11/28\n\n\n\nNotes:\n\nBe sure you watch the video from last Thursday on Canvas\nAny questions about CS01?\n\n\n\n\n\n\nmachine learning intro\n(re)introduce tidymodels\nworked example: ML in tidymodels\n\n\n\n\n\nThe package itself has some worked examples: https://www.tidymodels.org/start/models/\nThere’s a whole book (written by the developer of tidymodels) that covers the tidymodels package: https://www.tmwr.org/\n\n\n\n\n\n“Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: pre-processing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret. The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do.” - Max Kuhn (tidymodels developer)\n\n\nBenefits:\n\nStandardized workflow/format/notation across different types of machine learning algorithms\nCan easily modify pre-processing, algorithm choice, and hyper-parameter tuning making optimization easy\n\n\n\n\n\nThe main packages (and their roles):\n\n\n\n\n\n\nIn intro stats, you should have learned the central dogma of statistics: we sample from a population\n\n\nThe data from the sample are used to make an inference about the population:\n\n\n\nFor prediction, we have a similar sampling problem:\n\n\n\nBut now we are trying to build a rule that can be used to predict a single observation’s value of some characteristic using characteristics of the other observations.\n\n\n\n\n\nThe goal is to:\nbuild a machine learning algorithm\n\nthat uses features as input\n\n\nand predicts an outcome variable\n\n\nin the situation where we do not know the outcome variable.\n\n\n\n\nTypically, you use data where you have both the input and output data to train a machine learning algorithm.\n\nWhat you need:\n\n\nA data set to train from.\nAn algorithm or set of algorithms you can use to try values of \\(f\\).\nA distance metric \\(d\\) for measuring how close \\(Y\\) is to \\(\\hat{Y}\\).\nA definition of what a “good” distance is.\n\n\n\n\n\n\nHow these packages fit together for carrying out machine learning:\n\n\n\n\n\n\n\n\n\nCan you describe the basics of machine learning?\nCan you describe the goals of and general steps in tidymodels?"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html",
    "href": "content/lectures/15-cs02-data.html",
    "title": "15-cs02-data",
    "section": "",
    "text": "Background\nQuestion\nData Intro\nWrangle"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#opencasestudies",
    "href": "content/lectures/15-cs02-data.html#opencasestudies",
    "title": "15-cs02-data",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\nWright, Carrie and Meng, Qier and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com//opencasestudies/ocs-bp-air-pollution. Predicting Annual Air Pollution (Version v1.0.0)."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#air-pollutants",
    "href": "content/lectures/15-cs02-data.html#air-pollutants",
    "title": "15-cs02-data",
    "section": "Air Pollutants",
    "text": "Air Pollutants\nSome sources are natural while others are anthropogenic (human-derived):\n\n\n\n[source]\n\nMajor types of air pollutants\n\nGaseous - Carbon Monoxide (CO), Ozone (O3), Nitrogen Oxides(NO, NO2), Sulfur Dioxide (SO2)\nParticulate - small liquids and solids suspended in the air (includes lead- can include certain types of dust)\nDust - small solids (larger than particulates) that can be suspended in the air for some time but eventually settle\nBiological - pollen, bacteria, viruses, mold spores\n\nSee here for more detail on the types of pollutants in the air."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#particulate-pollution",
    "href": "content/lectures/15-cs02-data.html#particulate-pollution",
    "title": "15-cs02-data",
    "section": "Particulate Pollution",
    "text": "Particulate Pollution\nAir pollution particulates are generally described by their size:\n\nLarge Coarse Particulate Matter - has diameter of >10 micrometers (10 µm)\nCoarse Particulate Matter (called PM10-2.5) - has diameter of between 2.5 µm and 10 µm\nFine Particulate Matter (called PM2.5) - has diameter of < 2.5 µm\n\nPM10 includes any particulate matter <10 µm (both coarse and fine particulate matter)\n\nIn relation to a piece of human hair:\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#common-pollutants-and-their-size",
    "href": "content/lectures/15-cs02-data.html#common-pollutants-and-their-size",
    "title": "15-cs02-data",
    "section": "Common Pollutants and their size",
    "text": "Common Pollutants and their size\n\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#penetration-into-the-human-body",
    "href": "content/lectures/15-cs02-data.html#penetration-into-the-human-body",
    "title": "15-cs02-data",
    "section": "Penetration into the human body",
    "text": "Penetration into the human body\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#negative-health-impacts",
    "href": "content/lectures/15-cs02-data.html#negative-health-impacts",
    "title": "15-cs02-data",
    "section": "Negative Health Impacts",
    "text": "Negative Health Impacts\nExposure to air pollution is:\n\nassociated with higher rates of mortality in older adults\nknown to be a risk factor for many diseases and conditions including (but not limited to):\n\n\n\nAsthma - fine particle exposure (PM2.5) was found to be associated with higher rates of asthma in children\nInflammation in type 1 diabetes - fine particle exposure (PM2.5) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with Type 1 diabetes\nLung function and emphysema - higher concentrations of ozone (O3), nitrogen oxides (NOx), black carbon, and fine particle exposure PM2.5 , at study baseline were significantly associated with greater increases in percent emphysema per 10 years\nLow birthweight - fine particle exposure(PM2.5) was associated with lower birth weight in full-term live births\nViral Infection - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (PM2.5)\n\n\nSee this review article for more information about sources of air pollution and the influence of air pollution on health."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#sparse-monitoring-ph-issue",
    "href": "content/lectures/15-cs02-data.html#sparse-monitoring-ph-issue",
    "title": "15-cs02-data",
    "section": "Sparse monitoring PH issue",
    "text": "Sparse monitoring PH issue\n\n\nHistorically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country.\nHowever, these monitors are relatively sparse in certain regions of the country and are not necessarily located near pollution sources.\ndramatic differences in pollution rates can be seen even within the same city. (In fact, the term micro-environments describes environments within cities or counties which may vary greatly from one block to another.)\n\n\n\n\n\n\n[source]\n\n\nLack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#machine-learning-offers-a-solution",
    "href": "content/lectures/15-cs02-data.html#machine-learning-offers-a-solution",
    "title": "15-cs02-data",
    "section": "Machine Learning offers a solution",
    "text": "Machine Learning offers a solution\nAn article published in the Environmental Health journal dealt with this issue by using data, including population density and road density, among other features, to model or predict air pollution levels at a more localized scale using machine learning (ML) methods.\n\n\n\n[source]\n\nThe authors of this article state that:\n\n“Exposure to atmospheric particulate matter (PM) remains an important public health concern, although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or county-specific ambient concentrations.”\n\n\n\nThe article above demonstrates that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems.\n\n\nSo…we’re going to do the same"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#the-state-of-global-air",
    "href": "content/lectures/15-cs02-data.html#the-state-of-global-air",
    "title": "15-cs02-data",
    "section": "The State of Global Air",
    "text": "The State of Global Air\nThe State of Global Air is a report released every year to communicate the impact of air pollution on public health.\n\nThe State of Global Air 2019 report (which uses data from 2017) stated that:\n\nAir pollution is the fifth leading risk factor for mortality worldwide. It is responsible for more deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity. Each year, more people die from air pollution–related disease than from road traffic injuries or malaria.\n\n\n\n\n[source]\n\n\n\nIn 2017, air pollution is estimated to have contributed to close to 5 million deaths globally — nearly 1 in every 10 deaths.\n\n\n[source]\n\n\nThe State of Global Air 2018 report (using data from 2016) separated different types of air pollution & found that particulate pollution was particularly associated with mortality.\n\n[source]\n\n\nThe 2019 report shows that the highest levels of fine particulate pollution occur in Africa and Asia and that:\n\nMore than 90% of people worldwide live in areas exceeding the World Health Organization (WHO) Guideline for healthy air. More than half live in areas that do not even meet WHO’s least-stringent air quality target.\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#overall-improvement",
    "href": "content/lectures/15-cs02-data.html#overall-improvement",
    "title": "15-cs02-data",
    "section": "Overall Improvement",
    "text": "Overall Improvement\nLooking at the US specifically, air pollution levels are generally improving, with declining national air pollutant concentration averages as shown from the 2019 Our Nation’s Air report from the US Environmental Protection Agency (EPA):\n\n\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#an-issue-nonetheless",
    "href": "content/lectures/15-cs02-data.html#an-issue-nonetheless",
    "title": "15-cs02-data",
    "section": "An Issue Nonetheless",
    "text": "An Issue Nonetheless\n\nair pollution continues to contribute to health risk for Americans, in particular in regions with higher than national average rates of pollution that, at times, exceed the WHO’s recommended level.\nimportant to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.\n\n\nYou can see that current air quality conditions at this website, and you will notice variation across different cities.\nFor example, here are the conditions in San Francisco yesterday:\n\n[source]\n\n\n\nreports particulate values using what is called the Air Quality Index (AQI).\nThis calculator indicates that 138 AQI is equivalent to 50.5 ug/m3 and is considered unhealthy for sensitive individuals.\nThus, some areas exceed the WHO annual exposure guideline (10 ug/m3), and this may adversely affect the health of people living in these locations."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#adverse-health-effects",
    "href": "content/lectures/15-cs02-data.html#adverse-health-effects",
    "title": "15-cs02-data",
    "section": "Adverse health effects",
    "text": "Adverse health effects\n\nAdverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines.\nit appears that the composition of the particulate matter and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. (For example, see this article for more details.)"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#monitor-data",
    "href": "content/lectures/15-cs02-data.html#monitor-data",
    "title": "15-cs02-data",
    "section": "Monitor Data",
    "text": "Monitor Data\n\n\nMonitor data in this case study come from a system of monitors in which roughly 90% are located within cities.\nThere is an equity issue in terms of capturing the air pollution levels of more rural areas.\nTo get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be useful to estimate air pollution levels in areas with little to no monitoring.\nSpecifically, these methods can be used to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:\n\n\n\n\n\n\n[source]\nThis is what we aim to achieve in this case study."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#limitations",
    "href": "content/lectures/15-cs02-data.html#limitations",
    "title": "15-cs02-data",
    "section": "Limitations",
    "text": "Limitations\n\n\nThe data do not include information about the composition of particulate matter. Different types of particulates may be more benign or deleterious for health outcomes.\nOutdoor pollution levels are not necessarily an indication of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. Researchers are now developing personal monitoring systems to track air pollution levels on the personal level.\nOur analysis will use annual mean estimates of pollution levels, but these can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data; however, we are interested in long term exposures, as these appear to be the most influential for health outcomes.\nThese data are US-focused."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#supervised-ml",
    "href": "content/lectures/15-cs02-data.html#supervised-ml",
    "title": "15-cs02-data",
    "section": "Supervised ML",
    "text": "Supervised ML\nHere, we’ll need:\n\nA continuous outcome variable that we want to predict\nA set of feature(s) (or predictor variables) that we use to predict the outcome variable\n\n\nTo build (or train) our model, we use both the outcome and features.\n\n\nThe goal is to identify informative features that can explain a large amount of variation in our outcome variable.\n\n\nUsing this model, we can then predict the outcome from new observations with the same features where have not observed the outcome.\n(More details here)"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#outcome",
    "href": "content/lectures/15-cs02-data.html#outcome",
    "title": "15-cs02-data",
    "section": "Outcome",
    "text": "Outcome\nThe monitor data that we will be using comes from gravimetric monitors (see picture below) operated by the US Environmental Protection Agency (EPA).\n\n[image courtesy of Kirsten Koehler]\n\nThese monitors use a filtration system to specifically capture fine particulate matter.\n\n[source]\n\n\nThe weight of this particulate matter is manually measured daily or weekly.\nFor the EPA standard operating procedure for PM gravimetric analysis in 2008, we refer the reader to here.\n\n\nIn our data set, the value column indicates the PM2.5 monitor average for 2008 in mass of fine particles/volume of air for 876 gravimetric monitors.\n\n\nThe units are micrograms of fine particulate matter (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m3).\n\n\nRecall the WHO exposure guideline is < 10 ug/m3 on average annually for PM2.5."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-import",
    "href": "content/lectures/15-cs02-data.html#data-import",
    "title": "15-cs02-data",
    "section": "Data Import",
    "text": "Data Import\nAll of our data was previously collected by a researcher at the Johns Hopkins School of Public Health who studies air pollution and climate change. (Roger now works at UT Austin)\n\nWe have one CSV file that contains both our single outcome variable and all of our features (or predictor variables). You can download this file using the OCSdata package:\n\n# install.packages(\"OCSdata\")\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\n\n\n\nhere::here() helps manage file paths; will always locate files relative to your project root\n\n# install.packages(\"here\")\npm <- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#pm-2.5-data",
    "href": "content/lectures/15-cs02-data.html#pm-2.5-data",
    "title": "15-cs02-data",
    "section": "PM 2.5 Data",
    "text": "PM 2.5 Data\n\n876 monitors\n40 columns\n\nvalue | outcome variable\n\n\n\npm |>\n  glimpse()\n\nRows: 876\nColumns: 50\n$ id                          <dbl> 1003.001, 1027.000, 1033.100, 1049.100, 10…\n$ value                       <dbl> 9.597647, 10.800000, 11.212174, 11.659091,…\n$ fips                        <dbl> 1003, 1027, 1033, 1049, 1055, 1069, 1073, …\n$ lat                         <dbl> 30.49800, 33.28126, 34.75878, 34.28763, 33…\n$ lon                         <dbl> -87.88141, -85.80218, -87.65056, -85.96830…\n$ state                       <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\"…\n$ county                      <chr> \"Baldwin\", \"Clay\", \"Colbert\", \"DeKalb\", \"E…\n$ city                        <chr> \"Fairhope\", \"Ashland\", \"Muscle Shoals\", \"C…\n$ CMAQ                        <dbl> 8.098836, 9.766208, 9.402679, 8.534772, 9.…\n$ zcta                        <dbl> 36532, 36251, 35660, 35962, 35901, 36303, …\n$ zcta_area                   <dbl> 190980522, 374132430, 16716984, 203836235,…\n$ zcta_pop                    <dbl> 27829, 5103, 9042, 8300, 20045, 30217, 901…\n$ imp_a500                    <dbl> 0.01730104, 1.96972318, 19.17301038, 5.782…\n$ imp_a1000                   <dbl> 1.4096021, 0.8531574, 11.1448962, 3.867647…\n$ imp_a5000                   <dbl> 3.3360118, 0.9851479, 15.1786154, 1.231141…\n$ imp_a10000                  <dbl> 1.9879187, 0.5208189, 9.7253870, 1.0316469…\n$ imp_a15000                  <dbl> 1.4386207, 0.3359198, 5.2472094, 0.9730444…\n$ county_area                 <dbl> 4117521611, 1564252280, 1534877333, 201266…\n$ county_pop                  <dbl> 182265, 13932, 54428, 71109, 104430, 10154…\n$ log_dist_to_prisec          <dbl> 4.648181, 7.219907, 5.760131, 3.721489, 5.…\n$ log_pri_length_5000         <dbl> 8.517193, 8.517193, 8.517193, 8.517193, 9.…\n$ log_pri_length_10000        <dbl> 9.210340, 9.210340, 9.274303, 10.409411, 1…\n$ log_pri_length_15000        <dbl> 9.630228, 9.615805, 9.658899, 11.173626, 1…\n$ log_pri_length_25000        <dbl> 11.32735, 10.12663, 10.15769, 11.90959, 12…\n$ log_prisec_length_500       <dbl> 7.295356, 6.214608, 8.611945, 7.310155, 8.…\n$ log_prisec_length_1000      <dbl> 8.195119, 7.600902, 9.735569, 8.585843, 9.…\n$ log_prisec_length_5000      <dbl> 10.815042, 10.170878, 11.770407, 10.214200…\n$ log_prisec_length_10000     <dbl> 11.88680, 11.40554, 12.84066, 11.50894, 12…\n$ log_prisec_length_15000     <dbl> 12.205723, 12.042963, 13.282656, 12.353663…\n$ log_prisec_length_25000     <dbl> 13.41395, 12.79980, 13.79973, 13.55979, 13…\n$ log_nei_2008_pm25_sum_10000 <dbl> 0.318035438, 3.218632928, 6.573127301, 0.0…\n$ log_nei_2008_pm25_sum_15000 <dbl> 1.967358961, 3.218632928, 6.581917457, 3.2…\n$ log_nei_2008_pm25_sum_25000 <dbl> 5.067308, 3.218633, 6.875900, 4.887665, 4.…\n$ log_nei_2008_pm10_sum_10000 <dbl> 1.35588511, 3.31111648, 6.69187313, 0.0000…\n$ log_nei_2008_pm10_sum_15000 <dbl> 2.26783411, 3.31111648, 6.70127741, 3.3500…\n$ log_nei_2008_pm10_sum_25000 <dbl> 5.628728, 3.311116, 7.148858, 5.171920, 4.…\n$ popdens_county              <dbl> 44.265706, 8.906492, 35.460814, 35.330814,…\n$ popdens_zcta                <dbl> 145.716431, 13.639555, 540.887040, 40.7189…\n$ nohs                        <dbl> 3.3, 11.6, 7.3, 14.3, 4.3, 5.8, 7.1, 2.7, …\n$ somehs                      <dbl> 4.9, 19.1, 15.8, 16.7, 13.3, 11.6, 17.1, 6…\n$ hs                          <dbl> 25.1, 33.9, 30.6, 35.0, 27.8, 29.8, 37.2, …\n$ somecollege                 <dbl> 19.7, 18.8, 20.9, 14.9, 29.2, 21.4, 23.5, …\n$ associate                   <dbl> 8.2, 8.0, 7.6, 5.5, 10.1, 7.9, 7.3, 8.0, 4…\n$ bachelor                    <dbl> 25.3, 5.5, 12.7, 7.9, 10.0, 13.7, 5.9, 17.…\n$ grad                        <dbl> 13.5, 3.1, 5.1, 5.8, 5.4, 9.8, 2.0, 8.7, 2…\n$ pov                         <dbl> 6.1, 19.5, 19.0, 13.8, 8.8, 15.6, 25.5, 7.…\n$ hs_orless                   <dbl> 33.3, 64.6, 53.7, 66.0, 45.4, 47.2, 61.4, …\n$ urc2013                     <dbl> 4, 6, 4, 6, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ urc2006                     <dbl> 5, 6, 4, 5, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ aod                         <dbl> 37.36364, 34.81818, 36.00000, 33.08333, 43…\n\n\n\nThere are 48 features with values for each of the 876 monitors (observations).\nThe data comes from the US Environmental Protection Agency (EPA), the National Aeronautics and Space Administration (NASA), the US Census, and the National Center for Health Statistics (NCHS)."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#features",
    "href": "content/lectures/15-cs02-data.html#features",
    "title": "15-cs02-data",
    "section": "Features",
    "text": "Features\n\n\n\nVariable\nDetails\n\n\n\n\nid\nMonitor number  – the county number is indicated before the decimal  – the monitor number is indicated after the decimal  Example: 1073.0023 is Jefferson county (1073) and .0023 one of 8 monitors\n\n\nfips\nFederal information processing standard number for the county where the monitor is located  – 5 digit id code for counties (zero is often the first value and sometimes is not shown)  – the first 2 numbers indicate the state  – the last three numbers indicate the county  Example: Alabama’s state code is 01 because it is first alphabetically  (note: Alaska and Hawaii are not included because they are not part of the contiguous US)\n\n\nLat\nLatitude of the monitor in degrees\n\n\nLon\nLongitude of the monitor in degrees\n\n\nstate\nState where the monitor is located\n\n\ncounty\nCounty where the monitor is located\n\n\ncity\nCity where the monitor is located\n\n\nCMAQ\nEstimated values of air pollution from a computational model called Community Multiscale Air Quality (CMAQ)  – A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution  – Does not use any of the PM2.5 gravimetric monitoring data. (There is a version that does use the gravimetric monitoring data, but not this one!)  – Data from the EPA\n\n\nzcta\nZip Code Tabulation Area where the monitor is located  – Postal Zip codes are converted into “generalized areal representations” that are non-overlapping  – Data from the 2010 Census\n\n\nzcta_area\nLand area of the zip code area in meters squared  – Data from the 2010 Census\n\n\nzcta_pop\nPopulation in the zip code area  – Data from the 2010 Census\n\n\nimp_a500\nImpervious surface measure  – Within a circle with a radius of 500 meters around the monitor  – Impervious surface are roads, concrete, parking lots, buildings  – This is a measure of development\n\n\nimp_a1000\nImpervious surface measure  – Within a circle with a radius of 1000 meters around the monitor\n\n\nimp_a5000\nImpervious surface measure  – Within a circle with a radius of 5000 meters around the monitor\n\n\nimp_a10000\nImpervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\n\nimp_a15000\nImpervious surface measure  – Within a circle with a radius of 15000 meters around the monitor\n\n\ncounty_area\nLand area of the county of the monitor in meters squared\n\n\ncounty_pop\nPopulation of the county of the monitor\n\n\nLog_dist_to_prisec\nLog (Natural log) distance to a primary or secondary road from the monitor  – Highway or major road\n\n\nlog_pri_length_5000\nCount of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_10000\nCount of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_15000\nCount of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_25000\nCount of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_prisec_length_500\nCount of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_1000\nCount of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_5000\nCount of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_10000\nCount of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_15000\nCount of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_25000\nCount of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_nei_2008_pm25_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\npopdens_county\nPopulation density (number of people per kilometer squared area of the county)\n\n\npopdens_zcta\nPopulation density (number of people per kilometer squared area of zcta)\n\n\nnohs\nPercentage of people in zcta area where the monitor is that do not have a high school degree  – Data from the Census\n\n\nsomehs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was some high school education  – Data from the Census\n\n\nhs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing a high school degree  – Data from the Census\n\n\nsomecollege\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing some college education  – Data from the Census\n\n\nassociate\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing an associate degree  – Data from the Census\n\n\nbachelor\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a bachelor’s degree  – Data from the Census\n\n\ngrad\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a graduate degree  – Data from the Census\n\n\npov\nPercentage of people in zcta area where the monitor is that lived in poverty in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines  – Data from the Census\n\n\nhs_orless\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a high school degree or less (sum of nohs, somehs, and hs)\n\n\nurc2013\n2013 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\nurc2006\n2006 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\naod\nAerosol Optical Depth measurement from a NASA satellite  – based on the diffraction of a laser  – used as a proxy of particulate pollution  – unit-less - higher value indicates more pollution  – Data from NASA\n\n\n\n\nMany of these features have to do with the circular area around the monitor called the “buffer”. These are illustrated in the following figure:\n\n[source]"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#skimr",
    "href": "content/lectures/15-cs02-data.html#skimr",
    "title": "15-cs02-data",
    "section": "skimr",
    "text": "skimr\nskimr | A helpful way to get an overall sense of a dataset\n\n# install.packages(\"skimr\")\nskimr::skim(pm)\n\n\nData summary\n\n\nName\npm\n\n\nNumber of rows\n876\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n47\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n49\n0\n\n\ncounty\n0\n1\n3\n20\n0\n471\n0\n\n\ncity\n0\n1\n4\n48\n0\n607\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n26987.96\n1.578761e+04\n1003.00\n13089.15\n26132.00\n39118.00\n5.603910e+04\n▇▇▆▇▆\n\n\nvalue\n0\n1\n10.81\n2.580000e+00\n3.02\n9.27\n11.15\n12.37\n2.316000e+01\n▂▆▇▁▁\n\n\nfips\n0\n1\n26987.89\n1.578763e+04\n1003.00\n13089.00\n26132.00\n39118.00\n5.603900e+04\n▇▇▆▇▆\n\n\nlat\n0\n1\n38.48\n4.620000e+00\n25.47\n35.03\n39.30\n41.66\n4.840000e+01\n▁▃▅▇▂\n\n\nlon\n0\n1\n-91.74\n1.496000e+01\n-124.18\n-99.16\n-87.47\n-80.69\n-6.804000e+01\n▃▂▃▇▃\n\n\nCMAQ\n0\n1\n8.41\n2.970000e+00\n1.63\n6.53\n8.62\n10.24\n2.313000e+01\n▃▇▃▁▁\n\n\nzcta\n0\n1\n50890.29\n2.778447e+04\n1022.00\n28788.25\n48172.00\n74371.00\n9.920200e+04\n▅▇▇▅▇\n\n\nzcta_area\n0\n1\n183173481.91\n5.425989e+08\n15459.00\n14204601.75\n37653560.50\n160041508.25\n8.164821e+09\n▇▁▁▁▁\n\n\nzcta_pop\n0\n1\n24227.58\n1.777216e+04\n0.00\n9797.00\n22014.00\n35004.75\n9.539700e+04\n▇▇▃▁▁\n\n\nimp_a500\n0\n1\n24.72\n1.934000e+01\n0.00\n3.70\n25.12\n40.22\n6.961000e+01\n▇▅▆▃▂\n\n\nimp_a1000\n0\n1\n24.26\n1.802000e+01\n0.00\n5.32\n24.53\n38.59\n6.750000e+01\n▇▅▆▃▁\n\n\nimp_a5000\n0\n1\n19.93\n1.472000e+01\n0.05\n6.79\n19.07\n30.11\n7.460000e+01\n▇▆▃▁▁\n\n\nimp_a10000\n0\n1\n15.82\n1.381000e+01\n0.09\n4.54\n12.36\n24.17\n7.209000e+01\n▇▃▂▁▁\n\n\nimp_a15000\n0\n1\n13.43\n1.312000e+01\n0.11\n3.24\n9.67\n20.55\n7.110000e+01\n▇▃▁▁▁\n\n\ncounty_area\n0\n1\n3768701992.12\n6.212830e+09\n33703512.00\n1116536297.50\n1690826566.50\n2878192209.00\n5.194723e+10\n▇▁▁▁▁\n\n\ncounty_pop\n0\n1\n687298.44\n1.293489e+06\n783.00\n100948.00\n280730.50\n743159.00\n9.818605e+06\n▇▁▁▁▁\n\n\nlog_dist_to_prisec\n0\n1\n6.19\n1.410000e+00\n-1.46\n5.43\n6.36\n7.15\n1.045000e+01\n▁▁▃▇▁\n\n\nlog_pri_length_5000\n0\n1\n9.82\n1.080000e+00\n8.52\n8.52\n10.05\n10.73\n1.205000e+01\n▇▂▆▅▂\n\n\nlog_pri_length_10000\n0\n1\n10.92\n1.130000e+00\n9.21\n9.80\n11.17\n11.83\n1.302000e+01\n▇▂▇▇▃\n\n\nlog_pri_length_15000\n0\n1\n11.50\n1.150000e+00\n9.62\n10.87\n11.72\n12.40\n1.359000e+01\n▆▂▇▇▃\n\n\nlog_pri_length_25000\n0\n1\n12.24\n1.100000e+00\n10.13\n11.69\n12.46\n13.05\n1.436000e+01\n▅▃▇▇▃\n\n\nlog_prisec_length_500\n0\n1\n6.99\n9.500000e-01\n6.21\n6.21\n6.21\n7.82\n9.400000e+00\n▇▁▂▂▁\n\n\nlog_prisec_length_1000\n0\n1\n8.56\n7.900000e-01\n7.60\n7.60\n8.66\n9.20\n1.047000e+01\n▇▅▆▃▁\n\n\nlog_prisec_length_5000\n0\n1\n11.28\n7.800000e-01\n8.52\n10.91\n11.42\n11.83\n1.278000e+01\n▁▁▃▇▃\n\n\nlog_prisec_length_10000\n0\n1\n12.41\n7.300000e-01\n9.21\n11.99\n12.53\n12.94\n1.385000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_15000\n0\n1\n13.03\n7.200000e-01\n9.62\n12.59\n13.13\n13.57\n1.441000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_25000\n0\n1\n13.82\n7.000000e-01\n10.13\n13.38\n13.92\n14.35\n1.523000e+01\n▁▁▃▇▆\n\n\nlog_nei_2008_pm25_sum_10000\n0\n1\n3.97\n2.350000e+00\n0.00\n2.15\n4.29\n5.69\n9.120000e+00\n▆▅▇▆▂\n\n\nlog_nei_2008_pm25_sum_15000\n0\n1\n4.72\n2.250000e+00\n0.00\n3.47\n5.00\n6.35\n9.420000e+00\n▃▃▇▇▂\n\n\nlog_nei_2008_pm25_sum_25000\n0\n1\n5.67\n2.110000e+00\n0.00\n4.66\n5.91\n7.28\n9.650000e+00\n▂▂▇▇▃\n\n\nlog_nei_2008_pm10_sum_10000\n0\n1\n4.35\n2.320000e+00\n0.00\n2.69\n4.62\n6.07\n9.340000e+00\n▅▅▇▇▂\n\n\nlog_nei_2008_pm10_sum_15000\n0\n1\n5.10\n2.180000e+00\n0.00\n3.87\n5.39\n6.72\n9.710000e+00\n▂▃▇▇▂\n\n\nlog_nei_2008_pm10_sum_25000\n0\n1\n6.07\n2.010000e+00\n0.00\n5.10\n6.37\n7.52\n9.880000e+00\n▁▂▆▇▃\n\n\npopdens_county\n0\n1\n551.76\n1.711510e+03\n0.26\n40.77\n156.67\n510.81\n2.682191e+04\n▇▁▁▁▁\n\n\npopdens_zcta\n0\n1\n1279.66\n2.757490e+03\n0.00\n101.15\n610.35\n1382.52\n3.041884e+04\n▇▁▁▁▁\n\n\nnohs\n0\n1\n6.99\n7.210000e+00\n0.00\n2.70\n5.10\n8.80\n1.000000e+02\n▇▁▁▁▁\n\n\nsomehs\n0\n1\n10.17\n6.200000e+00\n0.00\n5.90\n9.40\n13.90\n7.220000e+01\n▇▂▁▁▁\n\n\nhs\n0\n1\n30.32\n1.140000e+01\n0.00\n23.80\n30.75\n36.10\n1.000000e+02\n▂▇▂▁▁\n\n\nsomecollege\n0\n1\n21.58\n8.600000e+00\n0.00\n17.50\n21.30\n24.70\n1.000000e+02\n▆▇▁▁▁\n\n\nassociate\n0\n1\n7.13\n4.010000e+00\n0.00\n4.90\n7.10\n8.80\n7.140000e+01\n▇▁▁▁▁\n\n\nbachelor\n0\n1\n14.90\n9.710000e+00\n0.00\n8.80\n12.95\n19.22\n1.000000e+02\n▇▂▁▁▁\n\n\ngrad\n0\n1\n8.91\n8.650000e+00\n0.00\n3.90\n6.70\n11.00\n1.000000e+02\n▇▁▁▁▁\n\n\npov\n0\n1\n14.95\n1.133000e+01\n0.00\n6.50\n12.10\n21.22\n6.590000e+01\n▇▅▂▁▁\n\n\nhs_orless\n0\n1\n47.48\n1.675000e+01\n0.00\n37.92\n48.65\n59.10\n1.000000e+02\n▁▃▇▃▁\n\n\nurc2013\n0\n1\n2.92\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\nurc2006\n0\n1\n2.97\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\naod\n0\n1\n43.70\n1.956000e+01\n5.00\n31.66\n40.17\n49.67\n1.430000e+02\n▃▇▁▁▁\n\n\n\n\n\n\n❓ Given the dataset we’re working with, what wrangling should we consider doing here?”\n❓ What’s something you’ve learned about the data from the skimr output?\n\nConsider variable type - need more factors?\nUnderstand why ID is not uniformally distributed; figure out which are overrepresented; decide what to do\nlog or other transformations necessary? decide during EDA\n\nReminder: to read the data in and run skimr if you haven’t already:\n\n# install.packages(\"OCSdata\")\n# install.packages(\"here\")\n# install.packages(\"skimr\")\n\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\npm <- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))\nskimr::skim(pm)\n\n\n\nThings to note:\n\n\ndata are summarized by variable type\nempty/n_missing gives you a sense of how much data are missing for each variable\nn_unique for state indicates that we have data for 49 states\nmany different distributions for continuous data, but many show bimodal distribution\nlarge range of possible values for many variables (i.e. population)"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html",
    "href": "content/lectures/16-cs02-eda.html",
    "title": "16-cs02-eda",
    "section": "",
    "text": "Q: I’m a bit unclear about the direction of case studies. Do just try to visualize data differently to find out the best biomarkers or do we use other methods like modeling to do so?\nA: A little bit of both. If you can rule out a matrix/compound during EDA by looking at visualizations, do that. But, once you have a set of compounds that you want to include for analysis, then you would turn to sensitivity/specificity/ROC curves to help you decide. Modelling is also an option.\n\n\nQ: I saw in the dataset, in some variables I think it’s the log values ( so I assumed it’s already normalized?). Why is it better to use the log data and not the raw values?\nA: Great question! We’ll get to this a bit today!\n\n\nQ: If the monitors are not localized at the granularity of zip code level, how do we assess the accuracy of our predictions? Seems like we can’t know the correct result.\nA: We’ll get to this in the next set of notes!\n\n\n\n\n\nCS01 due Thursday (group work survey due Friday)\nLab07 due Friday\nCS02 due next Friday\nFinal Project due Tues of Finals week (report + presentation)\n\n\nVote:\n\nOption 1: Continue as planned cs01 (12.5) + cs02 (12.5) + final project (18)\nOption 2:\n\nCS01 due Thursday (17)\nComplete either CS02 w/ external data req’d for extension OR Final Project (with final project group; due Finals week) (26)\n\n\n\n\nCS02 and Final Project repos + emails went out yesterday\n\nplease look for your repo/make sure you have access!\nplease get in communication with your groups if you’re not already in communication"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#the-data",
    "href": "content/lectures/16-cs02-eda.html#the-data",
    "title": "16-cs02-eda",
    "section": "The Data",
    "text": "The Data\n\npm <- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#skimr",
    "href": "content/lectures/16-cs02-eda.html#skimr",
    "title": "16-cs02-eda",
    "section": "skimr",
    "text": "skimr\n\nskimr::skim(pm)\n\n\nData summary\n\n\nName\npm\n\n\nNumber of rows\n876\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n47\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n49\n0\n\n\ncounty\n0\n1\n3\n20\n0\n471\n0\n\n\ncity\n0\n1\n4\n48\n0\n607\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n26987.96\n1.578761e+04\n1003.00\n13089.15\n26132.00\n39118.00\n5.603910e+04\n▇▇▆▇▆\n\n\nvalue\n0\n1\n10.81\n2.580000e+00\n3.02\n9.27\n11.15\n12.37\n2.316000e+01\n▂▆▇▁▁\n\n\nfips\n0\n1\n26987.89\n1.578763e+04\n1003.00\n13089.00\n26132.00\n39118.00\n5.603900e+04\n▇▇▆▇▆\n\n\nlat\n0\n1\n38.48\n4.620000e+00\n25.47\n35.03\n39.30\n41.66\n4.840000e+01\n▁▃▅▇▂\n\n\nlon\n0\n1\n-91.74\n1.496000e+01\n-124.18\n-99.16\n-87.47\n-80.69\n-6.804000e+01\n▃▂▃▇▃\n\n\nCMAQ\n0\n1\n8.41\n2.970000e+00\n1.63\n6.53\n8.62\n10.24\n2.313000e+01\n▃▇▃▁▁\n\n\nzcta\n0\n1\n50890.29\n2.778447e+04\n1022.00\n28788.25\n48172.00\n74371.00\n9.920200e+04\n▅▇▇▅▇\n\n\nzcta_area\n0\n1\n183173481.91\n5.425989e+08\n15459.00\n14204601.75\n37653560.50\n160041508.25\n8.164821e+09\n▇▁▁▁▁\n\n\nzcta_pop\n0\n1\n24227.58\n1.777216e+04\n0.00\n9797.00\n22014.00\n35004.75\n9.539700e+04\n▇▇▃▁▁\n\n\nimp_a500\n0\n1\n24.72\n1.934000e+01\n0.00\n3.70\n25.12\n40.22\n6.961000e+01\n▇▅▆▃▂\n\n\nimp_a1000\n0\n1\n24.26\n1.802000e+01\n0.00\n5.32\n24.53\n38.59\n6.750000e+01\n▇▅▆▃▁\n\n\nimp_a5000\n0\n1\n19.93\n1.472000e+01\n0.05\n6.79\n19.07\n30.11\n7.460000e+01\n▇▆▃▁▁\n\n\nimp_a10000\n0\n1\n15.82\n1.381000e+01\n0.09\n4.54\n12.36\n24.17\n7.209000e+01\n▇▃▂▁▁\n\n\nimp_a15000\n0\n1\n13.43\n1.312000e+01\n0.11\n3.24\n9.67\n20.55\n7.110000e+01\n▇▃▁▁▁\n\n\ncounty_area\n0\n1\n3768701992.12\n6.212830e+09\n33703512.00\n1116536297.50\n1690826566.50\n2878192209.00\n5.194723e+10\n▇▁▁▁▁\n\n\ncounty_pop\n0\n1\n687298.44\n1.293489e+06\n783.00\n100948.00\n280730.50\n743159.00\n9.818605e+06\n▇▁▁▁▁\n\n\nlog_dist_to_prisec\n0\n1\n6.19\n1.410000e+00\n-1.46\n5.43\n6.36\n7.15\n1.045000e+01\n▁▁▃▇▁\n\n\nlog_pri_length_5000\n0\n1\n9.82\n1.080000e+00\n8.52\n8.52\n10.05\n10.73\n1.205000e+01\n▇▂▆▅▂\n\n\nlog_pri_length_10000\n0\n1\n10.92\n1.130000e+00\n9.21\n9.80\n11.17\n11.83\n1.302000e+01\n▇▂▇▇▃\n\n\nlog_pri_length_15000\n0\n1\n11.50\n1.150000e+00\n9.62\n10.87\n11.72\n12.40\n1.359000e+01\n▆▂▇▇▃\n\n\nlog_pri_length_25000\n0\n1\n12.24\n1.100000e+00\n10.13\n11.69\n12.46\n13.05\n1.436000e+01\n▅▃▇▇▃\n\n\nlog_prisec_length_500\n0\n1\n6.99\n9.500000e-01\n6.21\n6.21\n6.21\n7.82\n9.400000e+00\n▇▁▂▂▁\n\n\nlog_prisec_length_1000\n0\n1\n8.56\n7.900000e-01\n7.60\n7.60\n8.66\n9.20\n1.047000e+01\n▇▅▆▃▁\n\n\nlog_prisec_length_5000\n0\n1\n11.28\n7.800000e-01\n8.52\n10.91\n11.42\n11.83\n1.278000e+01\n▁▁▃▇▃\n\n\nlog_prisec_length_10000\n0\n1\n12.41\n7.300000e-01\n9.21\n11.99\n12.53\n12.94\n1.385000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_15000\n0\n1\n13.03\n7.200000e-01\n9.62\n12.59\n13.13\n13.57\n1.441000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_25000\n0\n1\n13.82\n7.000000e-01\n10.13\n13.38\n13.92\n14.35\n1.523000e+01\n▁▁▃▇▆\n\n\nlog_nei_2008_pm25_sum_10000\n0\n1\n3.97\n2.350000e+00\n0.00\n2.15\n4.29\n5.69\n9.120000e+00\n▆▅▇▆▂\n\n\nlog_nei_2008_pm25_sum_15000\n0\n1\n4.72\n2.250000e+00\n0.00\n3.47\n5.00\n6.35\n9.420000e+00\n▃▃▇▇▂\n\n\nlog_nei_2008_pm25_sum_25000\n0\n1\n5.67\n2.110000e+00\n0.00\n4.66\n5.91\n7.28\n9.650000e+00\n▂▂▇▇▃\n\n\nlog_nei_2008_pm10_sum_10000\n0\n1\n4.35\n2.320000e+00\n0.00\n2.69\n4.62\n6.07\n9.340000e+00\n▅▅▇▇▂\n\n\nlog_nei_2008_pm10_sum_15000\n0\n1\n5.10\n2.180000e+00\n0.00\n3.87\n5.39\n6.72\n9.710000e+00\n▂▃▇▇▂\n\n\nlog_nei_2008_pm10_sum_25000\n0\n1\n6.07\n2.010000e+00\n0.00\n5.10\n6.37\n7.52\n9.880000e+00\n▁▂▆▇▃\n\n\npopdens_county\n0\n1\n551.76\n1.711510e+03\n0.26\n40.77\n156.67\n510.81\n2.682191e+04\n▇▁▁▁▁\n\n\npopdens_zcta\n0\n1\n1279.66\n2.757490e+03\n0.00\n101.15\n610.35\n1382.52\n3.041884e+04\n▇▁▁▁▁\n\n\nnohs\n0\n1\n6.99\n7.210000e+00\n0.00\n2.70\n5.10\n8.80\n1.000000e+02\n▇▁▁▁▁\n\n\nsomehs\n0\n1\n10.17\n6.200000e+00\n0.00\n5.90\n9.40\n13.90\n7.220000e+01\n▇▂▁▁▁\n\n\nhs\n0\n1\n30.32\n1.140000e+01\n0.00\n23.80\n30.75\n36.10\n1.000000e+02\n▂▇▂▁▁\n\n\nsomecollege\n0\n1\n21.58\n8.600000e+00\n0.00\n17.50\n21.30\n24.70\n1.000000e+02\n▆▇▁▁▁\n\n\nassociate\n0\n1\n7.13\n4.010000e+00\n0.00\n4.90\n7.10\n8.80\n7.140000e+01\n▇▁▁▁▁\n\n\nbachelor\n0\n1\n14.90\n9.710000e+00\n0.00\n8.80\n12.95\n19.22\n1.000000e+02\n▇▂▁▁▁\n\n\ngrad\n0\n1\n8.91\n8.650000e+00\n0.00\n3.90\n6.70\n11.00\n1.000000e+02\n▇▁▁▁▁\n\n\npov\n0\n1\n14.95\n1.133000e+01\n0.00\n6.50\n12.10\n21.22\n6.590000e+01\n▇▅▂▁▁\n\n\nhs_orless\n0\n1\n47.48\n1.675000e+01\n0.00\n37.92\n48.65\n59.10\n1.000000e+02\n▁▃▇▃▁\n\n\nurc2013\n0\n1\n2.92\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\nurc2006\n0\n1\n2.97\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\naod\n0\n1\n43.70\n1.956000e+01\n5.00\n31.66\n40.17\n49.67\n1.430000e+02\n▃▇▁▁▁"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#why-49-states",
    "href": "content/lectures/16-cs02-eda.html#why-49-states",
    "title": "16-cs02-eda",
    "section": "Why 49 “states”?",
    "text": "Why 49 “states”?\n\npm %>% \n  distinct(state) \n\n# A tibble: 49 × 1\n   state               \n   <chr>               \n 1 Alabama             \n 2 Arizona             \n 3 Arkansas            \n 4 California          \n 5 Colorado            \n 6 Connecticut         \n 7 Delaware            \n 8 District Of Columbia\n 9 Florida             \n10 Georgia             \n# ℹ 39 more rows\n\n\n\nDC is included\nAlaska and Hawaii are not"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#number-of-monitors-per-city",
    "href": "content/lectures/16-cs02-eda.html#number-of-monitors-per-city",
    "title": "16-cs02-eda",
    "section": "Number of monitors per city?",
    "text": "Number of monitors per city?\n\npm %>% filter(city == \"San Diego\")\n\n# A tibble: 2 × 50\n     id value  fips   lat   lon state      county    city   CMAQ  zcta zcta_area\n  <dbl> <dbl> <dbl> <dbl> <dbl> <chr>      <chr>     <chr> <dbl> <dbl>     <dbl>\n1 6073.  11.3  6073  32.8 -117. California San Diego San …  9.68 92123  21148247\n2 6073.  13.7  6073  32.7 -117. California San Diego San …  9.68 92113  13647793\n# ℹ 39 more variables: zcta_pop <dbl>, imp_a500 <dbl>, imp_a1000 <dbl>,\n#   imp_a5000 <dbl>, imp_a10000 <dbl>, imp_a15000 <dbl>, county_area <dbl>,\n#   county_pop <dbl>, log_dist_to_prisec <dbl>, log_pri_length_5000 <dbl>,\n#   log_pri_length_10000 <dbl>, log_pri_length_15000 <dbl>,\n#   log_pri_length_25000 <dbl>, log_prisec_length_500 <dbl>,\n#   log_prisec_length_1000 <dbl>, log_prisec_length_5000 <dbl>,\n#   log_prisec_length_10000 <dbl>, log_prisec_length_15000 <dbl>, …\n\n\n\n\npm %>% filter(city == \"Baltimore\")\n\n# A tibble: 5 × 50\n      id value  fips   lat   lon state    county     city   CMAQ  zcta zcta_area\n   <dbl> <dbl> <dbl> <dbl> <dbl> <chr>    <chr>      <chr> <dbl> <dbl>     <dbl>\n1 24510.  12.2 24510  39.3 -76.6 Maryland Baltimore… Balt…  10.9 21251    461424\n2 24510.  12.5 24510  39.3 -76.7 Maryland Baltimore… Balt…  10.9 21215  17645223\n3 24510.  12.8 24510  39.3 -76.5 Maryland Baltimore… Balt…  10.9 21224  24539976\n4 24510.  14.3 24510  39.2 -76.6 Maryland Baltimore… Balt…  10.9 21226  25718732\n5 24510.  13.3 24510  39.3 -76.6 Maryland Baltimore… Balt…  10.9 21202   4111039\n# ℹ 39 more variables: zcta_pop <dbl>, imp_a500 <dbl>, imp_a1000 <dbl>,\n#   imp_a5000 <dbl>, imp_a10000 <dbl>, imp_a15000 <dbl>, county_area <dbl>,\n#   county_pop <dbl>, log_dist_to_prisec <dbl>, log_pri_length_5000 <dbl>,\n#   log_pri_length_10000 <dbl>, log_pri_length_15000 <dbl>,\n#   log_pri_length_25000 <dbl>, log_prisec_length_500 <dbl>,\n#   log_prisec_length_1000 <dbl>, log_prisec_length_5000 <dbl>,\n#   log_prisec_length_10000 <dbl>, log_prisec_length_15000 <dbl>, …\n\n\n\n\nSan Diego has 2, while Baltimore has 5, despite having very similar population densities (popdens_county)…and San Diego having a much larger population (county_pop) and land area (county_area)."
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#feature-correlation",
    "href": "content/lectures/16-cs02-eda.html#feature-correlation",
    "title": "16-cs02-eda",
    "section": "Feature Correlation",
    "text": "Feature Correlation\nWhy do we care if variables in our dataset are correlated?\n\nwe don’t want to include redundant variables\ncan add unnecessary noise to our algorithm causing a reduction in prediction accuracy\ncan cause our algorithm to be slower\ncan also make it difficult to interpret what variables are actually predictive\n\n\nTaking a look at our numeric variables…\n\nPM_cor <- cor(pm %>% dplyr::select_if(is.numeric))\ncorrplot::corrplot(PM_cor, tl.cex = 0.5)\n\n\n\n\n\ndeep blue | strongly, positively correlated\ndeep red | strongly, negatively correlated\n\n\n\nIf we don’t care about direction, but only strength…and using hierarchical clustering:\n\ncorrplot::corrplot(abs(PM_cor), order = \"hclust\", tl.cex = 0.5, cl.lim = c(0, 1))\n\n\n\n\n\n\nObservations:\n\ndevelopment variables (imp), road density (pri), and the emission (nei) variables all seem to be correlated with their group\nnone of the predictors are correlated with value (our outcome)"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#development-imp",
    "href": "content/lectures/16-cs02-eda.html#development-imp",
    "title": "16-cs02-eda",
    "section": "Development (imp)",
    "text": "Development (imp)\n\n# we used GGally in a previous set of notes\n# will need to install if you haven't yet\nselect(pm, contains(\"imp\")) %>%\n  GGally::ggpairs()"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#emmissoins-nei",
    "href": "content/lectures/16-cs02-eda.html#emmissoins-nei",
    "title": "16-cs02-eda",
    "section": "Emmissoins (nei)",
    "text": "Emmissoins (nei)\n\nselect(pm, contains(\"nei\")) %>%\n  GGally::ggpairs()"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#road-density-pri",
    "href": "content/lectures/16-cs02-eda.html#road-density-pri",
    "title": "16-cs02-eda",
    "section": "Road Density (pri)",
    "text": "Road Density (pri)\n\nselect(pm, contains(\"pri\")) %>%\n  GGally::ggcorr(hjust = .85, size = 3,\n       layout.exp=2, label = TRUE)\n\n\n\n\nWarning: colors are reversed from above. If included in final report, you’d want consistency."
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#are-the-categories-correlated-with-one-another",
    "href": "content/lectures/16-cs02-eda.html#are-the-categories-correlated-with-one-another",
    "title": "16-cs02-eda",
    "section": "Are the categories correlated with one another?",
    "text": "Are the categories correlated with one another?\n\npm %>%\nselect(log_nei_2008_pm25_sum_10000, popdens_county, \n       log_pri_length_10000, imp_a10000, county_pop) %>%\n  GGally::ggpairs()\n\n\n\n\n\nReminder:\n\nlog_nei_2008_pm25_sum_10000 | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\npopdens_county | Population density (number of people per kilometer squared area of the county)\nlog_pri_length_10000 | Count of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\nimp_a10000 | Impervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\ncounty_pop | Population of the county of the monitor"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#log-transforming-right-skewed-data",
    "href": "content/lectures/16-cs02-eda.html#log-transforming-right-skewed-data",
    "title": "16-cs02-eda",
    "section": "Log-transforming right-skewed data",
    "text": "Log-transforming right-skewed data\n\npm %>%\n  mutate(log_popdens_county=log(popdens_county),\n         log_pop_county = log(county_pop)) %>%\n  select(log_nei_2008_pm25_sum_10000, log_popdens_county, \n       log_pri_length_10000, imp_a10000, log_pop_county) %>%\n  GGally::ggpairs()"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#your-turn",
    "href": "content/lectures/16-cs02-eda.html#your-turn",
    "title": "16-cs02-eda",
    "section": "Your Turn",
    "text": "Your Turn\n💪 Try to learn at least three things about the data that we haven’t yet discussed now on your own.\n\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  }
]