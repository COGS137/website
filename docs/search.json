[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "COGS 137",
    "section": "",
    "text": "Warning\n\n\n\nThis is the course website for COGS 137 from Fall 2023. While some material may look similar to your current course, be sure you’re on this quarter’s course website as many things have changed.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTitle\nType\n\n\n\n\n0\nTh Sep 28\nWelcome & Tooling\nLecture\n\n\n1\nTu Oct 3\nIntro to R\nLecture\n\n\n1\nTh Oct 5\nData Wrangling: dplyr\nLecture\n\n\n1\nFri Oct 6\nLab 01: Intro to R\nLab\n\n\n2\nTu Oct 10\nData Wrangling: tidyr\nLecture\n\n\n2\nTh Oct 12\nData Visualization: ggplot2 (day 1)\nLecture\n\n\n2\nFri Oct 13\nLab 02: Data Wrangling\nLab\n\n\n3\nMon Oct 16\nHW01 due (11:59 PM)\nHW\n\n\n3\nTu Oct 17\nData Visualization: ggplot2 (day 2)\nLecture\n\n\n3\nTh Oct 19\nData Analysis & Modeling\nLecture\n\n\n3\nFri Oct 20\nLab 03: Data Visualization\nLab\n\n\n4\nTu Oct 24\nLinear Models Review\nLecture\n\n\n4\nTh Oct 26\nEffective Communication\nLecture\n\n\n4\nFri Oct 27\nLab 04: Modeling\nLab\n\n\n5\nMon Oct 30\nHW02 due (11:59 PM)\nHW\n\n\n5\nTu Oct 31\nMultiple Linear Regression*\nLecture\n\n\n5\nTh Nov 2\nCase Study & Final Project Info\nLecture\n\n\n5\nFri Nov 3\nLab used for midterm review\nLab\n\n\n6\nMon Nov 6\nMIDTERM EXAM (due 11:59 PM) \nExam\n\n\n6\nTu Nov 7\nCase Study 01: THC Biomarkers (day 1)\nLecture\n\n\n6\nTh Nov 9\nCase Study 01: THC Biomarkers (day 2)\nLecture\n\n\n6\nFri Nov 10\nLab 05: Multiple Linear Regression\nLab\n\n\n7\nTu Nov 14\nCase Study 01: THC Biomarkers (day 3)\nLecture\n\n\n7\nTh Nov 16\ntidymodels\nLecture\n\n\n7\nSun Nov 19\nLab 06: CS01 [Note: Due date was extended to Sunday]\nLab\n\n\n8\nMon Nov 20\nHW03 due (11:59 PM)\nHW\n\n\n8\nMon Nov 20\nFinal Project Proposal Due\nProject\n\n\n8\nTu Nov 21\nCase Study 02: Air Pollution (day 1)\nLecture\n\n\n8\nTh Nov 23\nNo Class (Thanksgiving)\n--\n\n\n9\nTu Nov 28\nCase Study 02: Air Pollution (day 2)\nLecture\n\n\n9\nTh Nov 30\nCase Study 02: Air Pollution (day 3)\nLecture\n\n\n9\nTh Nov 30\nCS01 Due (11:59 PM)\nCase Study\n\n\n9\nFri Dec 1\nLab 07: CS02\nLab\n\n\n10\nTu Dec 5\nFinal Project Brainstorming\nLecture\n\n\n10\nTh Dec 7\nNext Steps\nLecture\n\n\n10\nFri Dec 8\nLab 08: Final Project\nLab\n\n\nFinals\nTu Dec 12\nFinal Project Due (11:59 PM)\nProject"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COGS 137: Practical Data Science in R",
    "section": "",
    "text": "Warning\n\n\n\nThis is the course website for COGS 137 from Fall 2023. While some material may look similar to your current course, be sure you’re on this quarter’s course website as many things have changed.\n\n\n\nCourse Info\nPractical Data Science in R focuses on teaching students how to think rigorously throughout the data science process. To this end, through interaction with unique data sets and interesting questions, this course helps students 1) gain fluency in the R programming language, 2) effectively explore & visualize data, 3) use statistical thinking to analyze data and rigorously evaluate their conclusions, and 4) effectively communicate their results. Course objectives are accomplished through hands-on practice, using real-world data to learn via case studies, and project-based learning.\n\nDays & Times\n\n\n\n\n\n Lecture: Tu/Th 2-3:20 (MOS 0204)\n Lab: Fri 3-3:50 (Peterson Hall 102)\n\nInstructional Staff & Office Hours\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11-12\nZoom by appt. (see Canvas)\n\n\n\n\n\nTu 3:30-4:30 PM\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTh 3:45-4:45 PM\nZoom (see Canvas)\n\n\nIA\nShenova Davis\n\nTBD\nTBD\n\n\n\n\n\n\n\nCourse Objectives\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports\n\n\n\nTexts\nTexts are freely available online:\n\n\n\nIntroduction to Modern Statistics\nÇetinkaya-Rundel and Hardin\nOpenIntro, 1st Edition, 2021\n\n\nR for Data Science\nGrolemund and Wickham\nO’Reilly, 1st edition, 2016\n\n\n\n\n\nMaterials\nYou should have access to a laptop and bring it to every class, fully charged (as possible).\nNote: If you do not have consistent access to the technology needed, please use this form to request a loaner laptop. (For any issues that you may have, please email vcsa@ucsd.edu, and they will work to assist you.)\n\n\nAcknowledgements\nI want to first recognize Dr. Mine Çetinkaya-Rundeland for her unparalleled efforts in support of education and educators in data science, statistics, and R programming. This course website was adapted from her course website. These course slides/labs/homework…also adapted from Mine’s course and the related datascienceinabox. I am so *very* indebted to Mine! I also want to thank the Open Case Studies team for their tireless work in putting together interesting and topical case studies, a handful of which we use throughout the course. And, finally, thanks to Allison Horst, whose artwork is inspiring, educational, and fun…and is used throughout this course. Further, thanks to the R (education) community generally; planning this course was really fun because I had so many awesome resources to choose from. Having these materials made course prep and planning is just another example of what sets the R community apart!"
  },
  {
    "objectID": "content/lectures/18-brainstorming.html",
    "href": "content/lectures/18-brainstorming.html",
    "title": "18-brainstorming",
    "section": "",
    "text": "Q: How does recipe work? I don’t fully understand it.\nA Recipe allows you to specify the steps you want to carry out on your dataset during modeling. For example, we used it to specify our outcome & predictors and to preprocess the data. You don’t actually DO anything with the recipe…until you bake(), when then carries out your recipe on your data.\n\n\nQ: Why did you specify CMAQ and aod when making the recipe? Why those two?\nA: We wanted to be sure these were not removed from the analysis, given their inportance to predicting the outcome variable (check the data dictionary to see what these variables are!). To see if this was necessary, you could remove their specification and see if/how the results change!\n\n\nQ: Is there any dataset available online so that we can try machine learning on our own?\nA: Yes! In fact there are good/helpful tutorials and datasets on the tidymodels documentation here.\n\n\nQ: What’s a good R^2 to aim for generally?\nA: This is very analysis specific. A model can be useful with a low \\(R^2\\), if prediction is otherwise very difficult. And the reverse can also be true.\n\n\n\n\n\nPlease fill out your SET course evaluations (due Sat 12/9 at 8AM)\nFinal Project due Tues 12/12 at 11:59 PM\n\n.Rmd (report/slides)\nPresentation (recording; submit on Canvas)\nGeneral Communication\n\n\n. . .\n\nHW03 and lab07 scores/feedback now posted\nCS01 Feedback/Scores by Friday\n\n\n\n\n\nHW03 Review\nLab07 Comments\nStorytelling Discussion\nFinal Project Brainstorming/Q&A",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#qa",
    "href": "content/lectures/18-brainstorming.html#qa",
    "title": "18-brainstorming",
    "section": "",
    "text": "Q: How does recipe work? I don’t fully understand it.\nA Recipe allows you to specify the steps you want to carry out on your dataset during modeling. For example, we used it to specify our outcome & predictors and to preprocess the data. You don’t actually DO anything with the recipe…until you bake(), when then carries out your recipe on your data.\n\n\nQ: Why did you specify CMAQ and aod when making the recipe? Why those two?\nA: We wanted to be sure these were not removed from the analysis, given their inportance to predicting the outcome variable (check the data dictionary to see what these variables are!). To see if this was necessary, you could remove their specification and see if/how the results change!\n\n\nQ: Is there any dataset available online so that we can try machine learning on our own?\nA: Yes! In fact there are good/helpful tutorials and datasets on the tidymodels documentation here.\n\n\nQ: What’s a good R^2 to aim for generally?\nA: This is very analysis specific. A model can be useful with a low \\(R^2\\), if prediction is otherwise very difficult. And the reverse can also be true.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#course-announcements",
    "href": "content/lectures/18-brainstorming.html#course-announcements",
    "title": "18-brainstorming",
    "section": "",
    "text": "Please fill out your SET course evaluations (due Sat 12/9 at 8AM)\nFinal Project due Tues 12/12 at 11:59 PM\n\n.Rmd (report/slides)\nPresentation (recording; submit on Canvas)\nGeneral Communication\n\n\n. . .\n\nHW03 and lab07 scores/feedback now posted\nCS01 Feedback/Scores by Friday",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#agenda",
    "href": "content/lectures/18-brainstorming.html#agenda",
    "title": "18-brainstorming",
    "section": "",
    "text": "HW03 Review\nLab07 Comments\nStorytelling Discussion\nFinal Project Brainstorming/Q&A",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#part-i-wrangling",
    "href": "content/lectures/18-brainstorming.html#part-i-wrangling",
    "title": "18-brainstorming",
    "section": "Part I: Wrangling",
    "text": "Part I: Wrangling\n\nrefactoring & getting all the variables of the specified type\npart of this class is retaining from one assignment to the next (lots of questions in logistic regression lab about type)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#part-ii-eda",
    "href": "content/lectures/18-brainstorming.html#part-ii-eda",
    "title": "18-brainstorming",
    "section": "Part II: EDA",
    "text": "Part II: EDA\n\nIncluded Plots\nInterpreted Plots\n\n. . .\n\nMost common mistakes:\n\nforgetting to include interpretation\nusing a barplot when displaying a continuous variable and a categorical variable (Q9)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#hw03-part-iii",
    "href": "content/lectures/18-brainstorming.html#hw03-part-iii",
    "title": "18-brainstorming",
    "section": "HW03: Part III",
    "text": "HW03: Part III\n\nQ9-Q11: fit three different models, building up to a full model\nQ12: backward elimination to settle on a final model\nQ13: interpreting final model\nQ14: contextualizing full model to determine the best day for biking\n\n. . .\n\nComments:\n\n(Q11) Think about what an interaction term actually means in the model\n\ni.e. what would it mean for a holiday and temperature to interact? what would it mean for weathersit and temperature to interact? Which of these makes better sense?\n\n(Q13) if interpreting a categorical variable, must include what you’re comparing against (the baseline)\n\n\n. . .\n\nMost common mistakes:\n\nNot including an interaction term\nNot including season as a factor\nNot considering that all other variables must be held constant in interpretation",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#eda",
    "href": "content/lectures/18-brainstorming.html#eda",
    "title": "18-brainstorming",
    "section": "EDA",
    "text": "EDA\n\nrequired something beyond what was presented in class\nstating what was plotted not enough\n\ni.e. “the relationship between variable X and variable Y”\nDESCRIBE that relationship\nwhat does that MEAN in the context of tehse data? this question?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#possible-extensions",
    "href": "content/lectures/18-brainstorming.html#possible-extensions",
    "title": "18-brainstorming",
    "section": "Possible Extensions",
    "text": "Possible Extensions\n\n\nadditional models\nadditional features/data\nlooking more closely at one aspect of the data (i.e. poverty, education, etc.)\nanalysis over time\nrelated question (weather, demographics, public health)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#feedback-group-work-survey",
    "href": "content/lectures/18-brainstorming.html#feedback-group-work-survey",
    "title": "18-brainstorming",
    "section": "Feedback: Group Work Survey",
    "text": "Feedback: Group Work Survey\nPros:\n\nIt was interesting! I enjoyed the process of figuring out how to work with a group on a bigger project that involved using a lot of GitHub. It did end up taking a lot longer than I thought it would.\n\n\nHard work, but rewarding.\n\n\nGreat thanks to both teammates, the project is going very well and everyone is making a real contribution.\n\n. . .\nCons:\n\nI felt like level of difficulty suddenly leaped…having random group makes it hard to communicate… felt the rubric for this project is all over the place\n\n. . .\n\nThe workload was a lot more than I expected.\n\n\nOverall I think the case study was a lot of work especially during a busy part of the quarter.\n\n. . .\n\nCS01 has too much unexpected workload. (GitHub)\n\n. . .\n\nI actually wished we can have more teammate for this assignment.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#cs01-discussion",
    "href": "content/lectures/18-brainstorming.html#cs01-discussion",
    "title": "18-brainstorming",
    "section": "CS01 Discussion",
    "text": "CS01 Discussion\n\nHow did it go?\nWhat went well? What was difficult?\nGeneral thoughts?\nFeedback?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#general-communication",
    "href": "content/lectures/18-brainstorming.html#general-communication",
    "title": "18-brainstorming",
    "section": "General Communication",
    "text": "General Communication\nExample: email\n\n. . .\nExample: Infographic\n\nGroup: Katie, Andrew, & Sidney\n. . .\nExample: IG slides\nLink here\n. . .\nExample: infographic\n\nGroup: Dhathry, Markus & Linus\n. . .\nExample: Report\nUnable to display PDF file. Download instead.\n. . .\nExample: Infographic\n\nGroup: Sid, Derek, Kushi\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming.html#storytelling-1",
    "href": "content/lectures/18-brainstorming.html#storytelling-1",
    "title": "18-brainstorming",
    "section": "Storytelling",
    "text": "Storytelling\n…so how do you make sure your case study/final project/data analysis tells a story/makes sense from start to finish? How did you all approach it?\n. . .\nWe’re going to work through this document.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#qa",
    "href": "content/lectures/18-brainstorming-slides.html#qa",
    "title": "18-brainstorming",
    "section": "Q&A",
    "text": "Q&A\n\nQ: How does recipe work? I don’t fully understand it.\nA Recipe allows you to specify the steps you want to carry out on your dataset during modeling. For example, we used it to specify our outcome & predictors and to preprocess the data. You don’t actually DO anything with the recipe…until you bake(), when then carries out your recipe on your data.\n\n\nQ: Why did you specify CMAQ and aod when making the recipe? Why those two?\nA: We wanted to be sure these were not removed from the analysis, given their inportance to predicting the outcome variable (check the data dictionary to see what these variables are!). To see if this was necessary, you could remove their specification and see if/how the results change!\n\n\nQ: Is there any dataset available online so that we can try machine learning on our own?\nA: Yes! In fact there are good/helpful tutorials and datasets on the tidymodels documentation here.\n\n\nQ: What’s a good R^2 to aim for generally?\nA: This is very analysis specific. A model can be useful with a low \\(R^2\\), if prediction is otherwise very difficult. And the reverse can also be true.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#course-announcements",
    "href": "content/lectures/18-brainstorming-slides.html#course-announcements",
    "title": "18-brainstorming",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nPlease fill out your SET course evaluations (due Sat 12/9 at 8AM)\nFinal Project due Tues 12/12 at 11:59 PM\n\n.Rmd (report/slides)\nPresentation (recording; submit on Canvas)\nGeneral Communication\n\n\n\n\nHW03 and lab07 scores/feedback now posted\nCS01 Feedback/Scores by Friday",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#agenda",
    "href": "content/lectures/18-brainstorming-slides.html#agenda",
    "title": "18-brainstorming",
    "section": "Agenda",
    "text": "Agenda\n\nHW03 Review\nLab07 Comments\nStorytelling Discussion\nFinal Project Brainstorming/Q&A",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#part-i-wrangling",
    "href": "content/lectures/18-brainstorming-slides.html#part-i-wrangling",
    "title": "18-brainstorming",
    "section": "Part I: Wrangling",
    "text": "Part I: Wrangling\n\nrefactoring & getting all the variables of the specified type\npart of this class is retaining from one assignment to the next (lots of questions in logistic regression lab about type)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#part-ii-eda",
    "href": "content/lectures/18-brainstorming-slides.html#part-ii-eda",
    "title": "18-brainstorming",
    "section": "Part II: EDA",
    "text": "Part II: EDA\n\nIncluded Plots\nInterpreted Plots\n\n\n\nMost common mistakes:\n\nforgetting to include interpretation\nusing a barplot when displaying a continuous variable and a categorical variable (Q9)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#hw03-part-iii",
    "href": "content/lectures/18-brainstorming-slides.html#hw03-part-iii",
    "title": "18-brainstorming",
    "section": "HW03: Part III",
    "text": "HW03: Part III\n\nQ9-Q11: fit three different models, building up to a full model\nQ12: backward elimination to settle on a final model\nQ13: interpreting final model\nQ14: contextualizing full model to determine the best day for biking\n\n\n\nComments:\n\n(Q11) Think about what an interaction term actually means in the model\n\ni.e. what would it mean for a holiday and temperature to interact? what would it mean for weathersit and temperature to interact? Which of these makes better sense?\n\n(Q13) if interpreting a categorical variable, must include what you’re comparing against (the baseline)\n\n\n\n\n\nMost common mistakes:\n\nNot including an interaction term\nNot including season as a factor\nNot considering that all other variables must be held constant in interpretation",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#eda",
    "href": "content/lectures/18-brainstorming-slides.html#eda",
    "title": "18-brainstorming",
    "section": "EDA",
    "text": "EDA\n\nrequired something beyond what was presented in class\nstating what was plotted not enough\n\ni.e. “the relationship between variable X and variable Y”\nDESCRIBE that relationship\nwhat does that MEAN in the context of tehse data? this question?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#possible-extensions",
    "href": "content/lectures/18-brainstorming-slides.html#possible-extensions",
    "title": "18-brainstorming",
    "section": "Possible Extensions",
    "text": "Possible Extensions\n\n\nadditional models\nadditional features/data\nlooking more closely at one aspect of the data (i.e. poverty, education, etc.)\nanalysis over time\nrelated question (weather, demographics, public health)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#feedback-group-work-survey",
    "href": "content/lectures/18-brainstorming-slides.html#feedback-group-work-survey",
    "title": "18-brainstorming",
    "section": "Feedback: Group Work Survey",
    "text": "Feedback: Group Work Survey\nPros:\n\nIt was interesting! I enjoyed the process of figuring out how to work with a group on a bigger project that involved using a lot of GitHub. It did end up taking a lot longer than I thought it would.\n\n\nHard work, but rewarding.\n\n\nGreat thanks to both teammates, the project is going very well and everyone is making a real contribution.\n\n\nCons:\n\nI felt like level of difficulty suddenly leaped…having random group makes it hard to communicate… felt the rubric for this project is all over the place\n\n\n\n\nThe workload was a lot more than I expected.\n\n\nOverall I think the case study was a lot of work especially during a busy part of the quarter.\n\n\n\n\nCS01 has too much unexpected workload. (GitHub)\n\n\n\n\nI actually wished we can have more teammate for this assignment.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#cs01-discussion",
    "href": "content/lectures/18-brainstorming-slides.html#cs01-discussion",
    "title": "18-brainstorming",
    "section": "CS01 Discussion",
    "text": "CS01 Discussion\n\nHow did it go?\nWhat went well? What was difficult?\nGeneral thoughts?\nFeedback?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#general-communication",
    "href": "content/lectures/18-brainstorming-slides.html#general-communication",
    "title": "18-brainstorming",
    "section": "General Communication",
    "text": "General Communication\nExample: email\n\n\nExample: Infographic\n\nGroup: Katie, Andrew, & Sidney\n\n\nExample: IG slides\nLink here\n\n\nExample: infographic\n\nGroup: Dhathry, Markus & Linus\n\n\nExample: Report\nUnable to display PDF file. Download instead.\n\n\nExample: Infographic\n\nGroup: Sid, Derek, Kushi",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/18-brainstorming-slides.html#storytelling-1",
    "href": "content/lectures/18-brainstorming-slides.html#storytelling-1",
    "title": "18-brainstorming",
    "section": "Storytelling",
    "text": "Storytelling\n…so how do you make sure your case study/final project/data analysis tells a story/makes sense from start to finish? How did you all approach it?\n\nWe’re going to work through this document.\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "18-brainstorming"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html",
    "href": "content/lectures/02-dplyr.html",
    "title": "02-dplyr",
    "section": "",
    "text": "Q: How many people are in a group for case studies and final project?\nA: 3-4\n\n\nQ: How to turn in assignments\nA: We’ll discuss this today!\n\n\nQ: Why don’t we have uniform keyboard-shortcut (like run code, new cell) for both R and Python and other coding environment?\nA: Lack of communication? Preferences of developers? I think we’ll get there…\n\n\nQ: Wasn’t clear about the ‘single quotes’ vs “double quotes” thing\nA: When creating a string, or any time you need to use quotes in R, single and double quotes are interchangeable. R doesn’t care which you use. However, your code will be stylistically better if you consistently use one.\n\n\nQ: What are useful libraries that we can use to analyze data?\nA: We’ll be discussing lots, but the tidyverse packages (the first of which we’ll discuss is dplyr) is a great place to start. There are also different packages for basically every statistical analysis out there\n\n\nQ: Is there any way to prevent coercion? / I was wondering if you can types cast a variable when concatenation\nA: Yup. You can explicitly state as._____() when creating a variable (i.e. as.character()) and when reading in data you can specify. You’ll find that R does a pretty good job at guessing, but we can always fix to what we want after the fact.\n\n\nQ: What is the difference between mylist[1] and mylist[[1]]? It looked like class(mylist[1]) returned list and class(mylist[[1]]) returned the class of the element.\nA: Double brackets returns the element directly. Single bracket (for lists) always returns a list.\n\n\nQ: I’m curious about how to handle dataframes in R\nA: Excellent - we’ll start this discussion today and continue throughout the quarter!\n\n\n\n\nDue Dates:\n\nLab 01 due tomorrow (Friday; 11:59 PM)\nStudent survey open until next Thursday\nHW01 and Lab02 will both be released Monday\nLecture Participation survey “due” after class\n\n\n\n\nR4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors\n\n\n\n\n\ndplyr\n\nphilosophy\npipes\ncommon operations\n\n\n\n\n\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#qa",
    "href": "content/lectures/02-dplyr.html#qa",
    "title": "02-dplyr",
    "section": "",
    "text": "Q: How many people are in a group for case studies and final project?\nA: 3-4\n\n\nQ: How to turn in assignments\nA: We’ll discuss this today!\n\n\nQ: Why don’t we have uniform keyboard-shortcut (like run code, new cell) for both R and Python and other coding environment?\nA: Lack of communication? Preferences of developers? I think we’ll get there…\n\n\nQ: Wasn’t clear about the ‘single quotes’ vs “double quotes” thing\nA: When creating a string, or any time you need to use quotes in R, single and double quotes are interchangeable. R doesn’t care which you use. However, your code will be stylistically better if you consistently use one.\n\n\nQ: What are useful libraries that we can use to analyze data?\nA: We’ll be discussing lots, but the tidyverse packages (the first of which we’ll discuss is dplyr) is a great place to start. There are also different packages for basically every statistical analysis out there\n\n\nQ: Is there any way to prevent coercion? / I was wondering if you can types cast a variable when concatenation\nA: Yup. You can explicitly state as._____() when creating a variable (i.e. as.character()) and when reading in data you can specify. You’ll find that R does a pretty good job at guessing, but we can always fix to what we want after the fact.\n\n\nQ: What is the difference between mylist[1] and mylist[[1]]? It looked like class(mylist[1]) returned list and class(mylist[[1]]) returned the class of the element.\nA: Double brackets returns the element directly. Single bracket (for lists) always returns a list.\n\n\nQ: I’m curious about how to handle dataframes in R\nA: Excellent - we’ll start this discussion today and continue throughout the quarter!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#course-announcements",
    "href": "content/lectures/02-dplyr.html#course-announcements",
    "title": "02-dplyr",
    "section": "",
    "text": "Due Dates:\n\nLab 01 due tomorrow (Friday; 11:59 PM)\nStudent survey open until next Thursday\nHW01 and Lab02 will both be released Monday\nLecture Participation survey “due” after class",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#suggested-reading",
    "href": "content/lectures/02-dplyr.html#suggested-reading",
    "title": "02-dplyr",
    "section": "",
    "text": "R4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#agenda",
    "href": "content/lectures/02-dplyr.html#agenda",
    "title": "02-dplyr",
    "section": "",
    "text": "dplyr\n\nphilosophy\npipes\ncommon operations",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#philosophy",
    "href": "content/lectures/02-dplyr.html#philosophy",
    "title": "02-dplyr",
    "section": "",
    "text": "dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "href": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "title": "02-dplyr",
    "section": "The pipe in baseR",
    "text": "The pipe in baseR\n\n\n\n\n|&gt; should be read as “and then”\nfor example “Wake up |&gt; brush teeth” would be read as “wake up and then brush teeth”",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "href": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "title": "02-dplyr",
    "section": "Where does the name come from?",
    "text": "Where does the name come from?\nThe pipe operator was first implemented in the package magrittr.\n\n\n\n\n\n\n\nYou will see this frequently in code online. It’s equivalent to |&gt;.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "href": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "title": "02-dplyr",
    "section": "Review: How does a pipe work?",
    "text": "Review: How does a pipe work?\n\nYou can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.\n\n. . .\n\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"campus\"))\n\n. . .\n\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") |&gt;\n  start_car() |&gt;\n  drive(to = \"campus\") |&gt;\n  park()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "href": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "title": "02-dplyr",
    "section": "NC DOT Fatal Crashes in North Carolina",
    "text": "NC DOT Fatal Crashes in North Carolina\nFrom OpenDurham’s Data Portal\n\nbike &lt;- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#variables",
    "href": "content/lectures/02-dplyr.html#variables",
    "title": "02-dplyr",
    "section": "Variables",
    "text": "Variables\nView the names of variables via\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#viewing-your-data",
    "href": "content/lectures/02-dplyr.html#viewing-your-data",
    "title": "02-dplyr",
    "section": "Viewing your data",
    "text": "Viewing your data\n\nIn the Environment, click on the name of the data frame to view it in the data viewer (or use the View function)\nUse the glimpse function to take a peek\n\n\nglimpse(bike)\n\nRows: 5,716\nColumns: 54\n$ FID        &lt;dbl&gt; 18, 29, 33, 35, 49, 53, 56, 60, 63, 66, 72, 75, 82, 84, 85,…\n$ OBJECTID   &lt;dbl&gt; 19, 30, 34, 36, 50, 54, 57, 61, 64, 67, 73, 76, 83, 85, 86,…\n$ AmbulanceR &lt;chr&gt; \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", …\n$ BikeAge_Gr &lt;chr&gt; NA, \"50-59\", NA, \"16-19\", NA, \"50-59\", \"16-19\", \"40-49\", \"1…\n$ Bike_Age   &lt;dbl&gt; 6, 51, 10, 17, 6, 52, 18, 40, 6, 7, 45, 30, 17, 20, 14, 15,…\n$ Bike_Alc_D &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bike_Dir   &lt;chr&gt; \"Not Applicable\", \"With Traffic\", \"With Traffic\", NA, \"Faci…\n$ Bike_Injur &lt;chr&gt; \"C: Possible Injury\", \"C: Possible Injury\", \"Injury\", \"B: E…\n$ Bike_Pos   &lt;chr&gt; \"Driveway / Alley\", \"Travel Lane\", \"Travel Lane\", \"Travel L…\n$ Bike_Race  &lt;chr&gt; \"Black\", \"Black\", \"Black\", \"White\", \"Black\", \"White\", \"Blac…\n$ Bike_Sex   &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ City       &lt;chr&gt; \"Durham\", \"Greenville\", \"Farmville\", \"Charlotte\", \"Charlott…\n$ County     &lt;chr&gt; \"Durham\", \"Pitt\", \"Pitt\", \"Mecklenburg\", \"Mecklenburg\", \"Du…\n$ CrashAlcoh &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ CrashDay   &lt;chr&gt; \"01-01-06\", \"01-01-02\", \"01-01-07\", \"01-01-05\", NA, NA, NA,…\n$ Crash_Date &lt;date&gt; 2007-01-06, 2007-01-09, 2007-01-14, 2007-01-12, 2007-01-15…\n$ Crash_Grp  &lt;chr&gt; \"Bicyclist Failed to Yield - Midblock\", \"Crossing Paths - O…\n$ Crash_Hour &lt;dbl&gt; 13, 23, 16, 19, 12, 20, 19, 14, 16, 0, 17, 18, 14, 17, 19, …\n$ Crash_Loc  &lt;chr&gt; \"Non-Intersection\", \"Intersection-Related\", \"Intersection\",…\n$ Crash_Mont &lt;chr&gt; NA, NA, NA, NA, NA, \"01-04-01\", \"01-04-01\", NA, \"01-02-01\",…\n$ Crash_Time &lt;dttm&gt; 0001-01-01 13:17:58, 0001-01-01 23:08:58, 0001-01-01 16:44…\n$ Crash_Type &lt;chr&gt; \"Bicyclist Ride Out - Residential Driveway\", \"Crossing Path…\n$ Crash_Ty_1 &lt;dbl&gt; 353311, 211180, 111144, 119139, 112114, 311231, 119144, 132…\n$ Crash_Year &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n$ Crsh_Sevri &lt;chr&gt; \"C: Possible Injury\", \"C: Possible Injury\", \"O: No Injury\",…\n$ Developmen &lt;chr&gt; \"Residential\", \"Commercial\", \"Residential\", \"Residential\", …\n$ DrvrAge_Gr &lt;chr&gt; \"60-69\", \"30-39\", \"50-59\", \"30-39\", NA, \"20-24\", \"40-49\", N…\n$ Drvr_Age   &lt;dbl&gt; 66, 34, 52, 33, NA, 20, 40, NA, 17, 51, NA, 64, 50, 66, 30,…\n$ Drvr_Alc_D &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"Missing\", \"No\", \"No\", \"Missing\", \"…\n$ Drvr_EstSp &lt;chr&gt; \"11-15 mph\", \"0-5 mph\", \"21-25 mph\", \"46-50 mph\", \"16-20 mp…\n$ Drvr_Injur &lt;chr&gt; \"O: No Injury\", \"O: No Injury\", \"O: No Injury\", \"O: No Inju…\n$ Drvr_Race  &lt;chr&gt; \"Black\", \"Black\", \"White\", \"White\", \"/Missing\", \"White\", \"B…\n$ Drvr_Sex   &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", NA, \"Female\", \"Male\", N…\n$ Drvr_VehTy &lt;chr&gt; \"Pickup\", \"Passenger Car\", \"Passenger Car\", \"Sport Utility\"…\n$ ExcsSpdInd &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Hit_Run    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n$ Light_Cond &lt;chr&gt; \"Daylight\", \"Dark - Lighted Roadway\", \"Daylight\", \"Dark - R…\n$ Locality   &lt;chr&gt; \"Mixed (30% To 70% Developed)\", \"Urban (&gt;70% Developed)\", \"…\n$ Num_Lanes  &lt;chr&gt; \"2 lanes\", \"5 lanes\", \"2 lanes\", \"4 lanes\", \"2 lanes\", \"4 l…\n$ Num_Units  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Rd_Charact &lt;chr&gt; \"Straight - Level\", \"Straight - Level\", \"Straight - Level\",…\n$ Rd_Class   &lt;chr&gt; \"Local Street\", \"Local Street\", \"Local Street\", \"NC Route\",…\n$ Rd_Conditi &lt;chr&gt; \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr…\n$ Rd_Config  &lt;chr&gt; \"Two-Way, Not Divided\", \"Two-Way, Divided, Unprotected Medi…\n$ Rd_Defects &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ Rd_Feature &lt;chr&gt; \"No Special Feature\", \"Four-Way Intersection\", \"Four-Way In…\n$ Rd_Surface &lt;chr&gt; \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smoo…\n$ Region     &lt;chr&gt; \"Piedmont\", \"Coastal\", \"Coastal\", \"Piedmont\", \"Piedmont\", \"…\n$ Rural_Urba &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Urban\", \"Urban\", \"Urba…\n$ Speed_Limi &lt;chr&gt; \"20 - 25  MPH\", \"40 - 45  MPH\", \"30 - 35  MPH\", \"40 - 45  M…\n$ Traff_Cntr &lt;chr&gt; \"No Control Present\", \"Stop And Go Signal\", \"Stop Sign\", \"S…\n$ Weather    &lt;chr&gt; \"Clear\", \"Clear\", \"Clear\", \"Cloudy\", \"Clear\", \"Clear\", \"Cle…\n$ Workzone_I &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Location   &lt;chr&gt; \"36.002743, -78.8785\", \"35.612984, -77.39265\", \"35.595676, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "href": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "title": "02-dplyr",
    "section": "A Grammar of Data Manipulation",
    "text": "A Grammar of Data Manipulation\ndplyr is based on the concepts of functions as verbs that manipulate data frames.\nSingle data frame functions / verbs:\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "href": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "title": "02-dplyr",
    "section": "dplyr rules for functions",
    "text": "dplyr rules for functions\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDo not modify in place\nPerformance via lazy evaluation",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "href": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "title": "02-dplyr",
    "section": "Filter rows with filter",
    "text": "Filter rows with filter\n\nSelect a subset of rows in a data frame.\nEasily filter for many conditions at once.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter",
    "href": "content/lectures/02-dplyr.html#filter",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County\n\nbike |&gt;\n  filter(County == \"Durham\")\n\n# A tibble: 253 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     \n 1    18       19 No         &lt;NA&gt;              6 No         Not Appl… C: Possib…\n 2    53       54 Yes        50-59            52 No         With Tra… A: Disabl…\n 3    56       57 Yes        16-19            18 No         &lt;NA&gt;      C: Possib…\n 4   209      210 No         16-19            16 No         Facing T… C: Possib…\n 5   228      229 Yes        40-49            40 No         With Tra… B: Eviden…\n 6   620      621 Yes        50-59            55 No         With Tra… B: Eviden…\n 7   667      668 Yes        60-69            61 No         Not Appl… B: Eviden…\n 8   458      459 Yes        60-69            62 No         With Tra… B: Eviden…\n 9   576      577 No         40-49            49 No         With Tra… C: Possib…\n10   618      619 No         20-24            23 No         With Tra… C: Possib…\n# ℹ 243 more rows\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-1",
    "href": "content/lectures/02-dplyr.html#filter-1",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County where biker was &lt; 10 yrs old\n\nbike |&gt;\n  filter(County == \"Durham\", Bike_Age &lt; 10)\n\n# A tibble: 20 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     \n 1    18       19 No         &lt;NA&gt;              6 No         Not Appl… C: Possib…\n 2    47       48 No         10-Jun            9 No         Not Appl… O: No Inj…\n 3   124      125 Yes        10-Jun            8 No         With Tra… C: Possib…\n 4   531      532 Yes        10-Jun            7 No         With Tra… C: Possib…\n 5   704      705 Yes        10-Jun            9 No         Not Appl… C: Possib…\n 6    42       43 No         10-Jun            8 No         With Tra… O: No Inj…\n 7   392      393 Yes        0-5               2 No         Not Appl… B: Eviden…\n 8   941      942 No         10-Jun            9 No         With Tra… C: Possib…\n 9   436      437 Yes        10-Jun            6 No         Not Appl… O: No Inj…\n10   160      161 Yes        10-Jun            7 No         With Tra… C: Possib…\n11   273      274 Yes        10-Jun            7 No         Facing T… C: Possib…\n12    78       79 Yes        10-Jun            7 No         With Tra… C: Possib…\n13   422      423 No         10-Jun            9 No         Not Appl… O: No Inj…\n14   570      571 No         &lt;NA&gt;              0 Missing    Not Appl… Injury    \n15   683      684 Yes        10-Jun            8 No         Not Appl… C: Possib…\n16    62       63 Yes        10-Jun            7 No         With Tra… C: Possib…\n17   248      249 No         0-5               4 No         Not Appl… O: No Inj…\n18   306      307 Yes        10-Jun            8 No         With Tra… C: Possib…\n19   231      232 Yes        10-Jun            8 No         With Tra… C: Possib…\n20   361      362 Yes        10-Jun            9 No         With Tra… B: Eviden…\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;,\n#   Drvr_Race &lt;chr&gt;, Drvr_Sex &lt;chr&gt;, Drvr_VehTy &lt;chr&gt;, ExcsSpdInd &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "href": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "title": "02-dplyr",
    "section": "Aside: real data is messy!",
    "text": "Aside: real data is messy!\n   What in the world does a BikeAge_gr of 10-Jun or 15-Nov mean?\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   &lt;chr&gt;            &lt;int&gt;\n 1 0-5                 60\n 2 10-Jun             421\n 3 15-Nov             747\n 4 16-19              605\n 5 20-24              680\n 6 25-29              430\n 7 30-39              658\n 8 40-49              920\n 9 50-59              739\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 &lt;NA&gt;               112",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "href": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "title": "02-dplyr",
    "section": "Careful data scientists clean up their data first!",
    "text": "Careful data scientists clean up their data first!\n\nWe’re going to need to do some text parsing to clean up these data\n\n10-Jun should be 6-10\n15-Nov should be 11-15",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "href": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "title": "02-dplyr",
    "section": "Correct and overwrite mutate",
    "text": "Correct and overwrite mutate\n\nRemember we want to do the following in the BikeAge_Gr variable\n\n10-Jun should be 6-10\n15-Nov should be 11-15\n\n\n\nbike &lt;- bike |&gt;\n  mutate(\n    BikeAge_Gr = case_when(\n      BikeAge_Gr == \"10-Jun\" ~ \"6-10\",\n      BikeAge_Gr == \"15-Nov\" ~ \"11-15\",\n      TRUE                   ~ BikeAge_Gr     # everything else\n    )\n  )\n\n\nNote that we’re overwriting existing data and columns, so be careful!\n\nBut remember, it’s easy to revert if you make a mistake since we didn’t touch the raw data, we can always reload it and start over",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr count\n   &lt;chr&gt;      &lt;int&gt;\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 &lt;NA&gt;         112",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "href": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "title": "02-dplyr",
    "section": "mutate to add new variables",
    "text": "mutate to add new variables\n   How is the new alcohol variable determined?\n\nbike |&gt;\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "href": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "title": "02-dplyr",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame.\n\nbike &lt;- bike |&gt;\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "href": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "title": "02-dplyr",
    "section": "transmute to create a new dataset",
    "text": "transmute to create a new dataset\nYou’ll use this much less often than mutate but when you need it, you need it.\n\nbike |&gt; \n  transmute(ID = paste(FID, OBJECTID, sep = \"-\"))\n\n# A tibble: 5,716 × 1\n   ID   \n   &lt;chr&gt;\n 1 18-19\n 2 29-30\n 3 33-34\n 4 35-36\n 5 49-50\n 6 53-54\n 7 56-57\n 8 60-61\n 9 63-64\n10 66-67\n# ℹ 5,706 more rows",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "href": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "title": "02-dplyr",
    "section": "mutate vs. transmute",
    "text": "mutate vs. transmute\n\nmutate adds new and keeps original\ntransmute adds new; drops existing",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn",
    "href": "content/lectures/02-dplyr.html#your-turn",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nHow many accidents in our dataset required an ambulance ride (AmbulanceR) and had the Crash_Type “Bicyclist Lost Control - Mechanical Problems”?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nFirst five\n\nbike |&gt;\n  slice(1:5)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     \n1    18       19 No         &lt;NA&gt;              6 No         Not Appli… C: Possib…\n2    29       30 Yes        50-59            51 No         With Traf… C: Possib…\n3    33       34 No         &lt;NA&gt;             10 No         With Traf… Injury    \n4    35       36 Yes        16-19            17 No         &lt;NA&gt;       B: Eviden…\n5    49       50 No         &lt;NA&gt;              6 No         Facing Tr… O: No Inj…\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;,\n#   Drvr_Race &lt;chr&gt;, Drvr_Sex &lt;chr&gt;, Drvr_VehTy &lt;chr&gt;, ExcsSpdInd &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nLast five\n\nlast_row &lt;- nrow(bike)\nbike |&gt;\n  slice((last_row - 4):last_row)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     \n1   460      461 Yes        6-10              7 No         Not Appli… C: Possib…\n2   474      475 Yes        50-59            50 No         With Traf… B: Eviden…\n3   479      480 Yes        16-19            16 No         Not Appli… C: Possib…\n4   487      488 No         40-49            47 Yes        With Traf… C: Possib…\n5   488      489 Yes        30-39            35 No         Facing Tr… C: Possib…\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;,\n#   Drvr_Race &lt;chr&gt;, Drvr_Sex &lt;chr&gt;, Drvr_VehTy &lt;chr&gt;, ExcsSpdInd &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "href": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "title": "02-dplyr",
    "section": "select to keep only the variables you mention",
    "text": "select to keep only the variables you mention\n\nbike |&gt;\n  select(Crash_Loc, Hit_Run) |&gt;\n  table()\n\n                      Hit_Run\nCrash_Loc                No  Yes\n  Intersection         2223  275\n  Intersection-Related  252   42\n  Location                3    7\n  Non-Intersection     2213  462\n  Non-Roadway           205   30",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "href": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "title": "02-dplyr",
    "section": "or select to exclude variables",
    "text": "or select to exclude variables\n\nbike |&gt;\n  select(-OBJECTID)\n\n# A tibble: 5,716 × 53\n     FID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur Bike_Pos\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;   \n 1    18 No         &lt;NA&gt;              6 No         Not Appl… C: Possib… Drivewa…\n 2    29 Yes        50-59            51 No         With Tra… C: Possib… Travel …\n 3    33 No         &lt;NA&gt;             10 No         With Tra… Injury     Travel …\n 4    35 Yes        16-19            17 No         &lt;NA&gt;      B: Eviden… Travel …\n 5    49 No         &lt;NA&gt;              6 No         Facing T… O: No Inj… Travel …\n 6    53 Yes        50-59            52 No         With Tra… A: Disabl… Travel …\n 7    56 Yes        16-19            18 No         &lt;NA&gt;      C: Possib… Travel …\n 8    60 No         40-49            40 No         Facing T… B: Eviden… Sidewal…\n 9    63 Yes        6-10              6 No         Facing T… B: Eviden… Travel …\n10    66 Yes        6-10              7 No         &lt;NA&gt;      B: Eviden… Non-Roa…\n# ℹ 5,706 more rows\n# ℹ 45 more variables: Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;, City &lt;chr&gt;,\n#   County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;, Crash_Date &lt;date&gt;,\n#   Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;, Crash_Mont &lt;chr&gt;,\n#   Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;, Crash_Year &lt;dbl&gt;,\n#   Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;, Drvr_Age &lt;dbl&gt;,\n#   Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;, Drvr_Race &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "href": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "title": "02-dplyr",
    "section": "or select a range of variables",
    "text": "or select a range of variables\n\nbike |&gt;\n  select(OBJECTID:Bike_Injur)\n\n# A tibble: 5,716 × 7\n   OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir       Bike_Injur \n      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;      \n 1       19 No         &lt;NA&gt;              6 No         Not Applicable C: Possibl…\n 2       30 Yes        50-59            51 No         With Traffic   C: Possibl…\n 3       34 No         &lt;NA&gt;             10 No         With Traffic   Injury     \n 4       36 Yes        16-19            17 No         &lt;NA&gt;           B: Evident…\n 5       50 No         &lt;NA&gt;              6 No         Facing Traffic O: No Inju…\n 6       54 Yes        50-59            52 No         With Traffic   A: Disabli…\n 7       57 Yes        16-19            18 No         &lt;NA&gt;           C: Possibl…\n 8       61 No         40-49            40 No         Facing Traffic B: Evident…\n 9       64 Yes        6-10              6 No         Facing Traffic B: Evident…\n10       67 Yes        6-10              7 No         &lt;NA&gt;           B: Evident…\n# ℹ 5,706 more rows",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "href": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "title": "02-dplyr",
    "section": "pull to extract a column as a vector",
    "text": "pull to extract a column as a vector\n\nbike |&gt;\n  slice(1:6) |&gt;\n  pull(Location)\n\n[1] \"36.002743, -78.8785\"  \"35.612984, -77.39265\" \"35.595676, -77.59074\"\n[4] \"35.076767, -80.7728\"  \"35.19999, -80.75713\"  \"35.966644, -78.96749\"\n\n\n\nbike |&gt;\n  slice(1:6) |&gt;\n  select(Location)\n\n# A tibble: 6 × 1\n  Location            \n  &lt;chr&gt;               \n1 36.002743, -78.8785 \n2 35.612984, -77.39265\n3 35.595676, -77.59074\n4 35.076767, -80.7728 \n5 35.19999, -80.75713 \n6 35.966644, -78.96749",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "href": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "title": "02-dplyr",
    "section": "The two pulls in your lives",
    "text": "The two pulls in your lives\n\n\n\n\n\n\n\n\nDon’t get pull happy when wrangling data! Only extract out variables if you truly need to, otherwise keep in data frame.\nBut always ⬇️ Pull before starting your work when collaborating on GitHub.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\nUseful for correcting typos, and renaming to make variable names shorter and/or more informative\n\nOriginal names:\n\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\n\nRename Speed_Limi to Speed_Limit:\n\n\nbike &lt;- bike |&gt;\n  rename(Speed_Limit = Speed_Limi)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nnames(bike)\n\n [1] \"FID\"         \"OBJECTID\"    \"AmbulanceR\"  \"BikeAge_Gr\"  \"Bike_Age\"   \n [6] \"Bike_Alc_D\"  \"Bike_Dir\"    \"Bike_Injur\"  \"Bike_Pos\"    \"Bike_Race\"  \n[11] \"Bike_Sex\"    \"City\"        \"County\"      \"CrashAlcoh\"  \"CrashDay\"   \n[16] \"Crash_Date\"  \"Crash_Grp\"   \"Crash_Hour\"  \"Crash_Loc\"   \"Crash_Mont\" \n[21] \"Crash_Time\"  \"Crash_Type\"  \"Crash_Ty_1\"  \"Crash_Year\"  \"Crsh_Sevri\" \n[26] \"Developmen\"  \"DrvrAge_Gr\"  \"Drvr_Age\"    \"Drvr_Alc_D\"  \"Drvr_EstSp\" \n[31] \"Drvr_Injur\"  \"Drvr_Race\"   \"Drvr_Sex\"    \"Drvr_VehTy\"  \"ExcsSpdInd\" \n[36] \"Hit_Run\"     \"Light_Cond\"  \"Locality\"    \"Num_Lanes\"   \"Num_Units\"  \n[41] \"Rd_Charact\"  \"Rd_Class\"    \"Rd_Conditi\"  \"Rd_Config\"   \"Rd_Defects\" \n[46] \"Rd_Feature\"  \"Rd_Surface\"  \"Region\"      \"Rural_Urba\"  \"Speed_Limit\"\n[51] \"Traff_Cntr\"  \"Weather\"     \"Workzone_I\"  \"Location\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn-1",
    "href": "content/lectures/02-dplyr.html#your-turn-1",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nYour boss in Cumberland County gets overwhelmed by data easily, but he wants some data from you. He wants all bike accidents from his County, but he only wants to know the road’s speed limit, the age of the biker, and to know if alcohol was involved. If you have time, mine as well make the column names very clear to your boss while you’re at it…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "href": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "title": "02-dplyr",
    "section": "summarize to reduce variables to values",
    "text": "summarize to reduce variables to values\nThe values are summarized in a data frame\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   &lt;chr&gt;            &lt;int&gt;\n 1 0-5                 60\n 2 11-15              747\n 3 16-19              605\n 4 20-24              680\n 5 25-29              430\n 6 30-39              658\n 7 40-49              920\n 8 50-59              739\n 9 6-10               421\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 &lt;NA&gt;               112",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "href": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "title": "02-dplyr",
    "section": "and arrange to order rows",
    "text": "and arrange to order rows\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(crash_count = n()) |&gt;\n  arrange(desc(crash_count))\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   &lt;chr&gt;            &lt;int&gt;\n 1 40-49              920\n 2 11-15              747\n 3 50-59              739\n 4 20-24              680\n 5 30-39              658\n 6 16-19              605\n 7 25-29              430\n 8 6-10               421\n 9 60-69              274\n10 &lt;NA&gt;               112\n11 0-5                 60\n12 70+                 58\n13 70                  12",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "href": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "title": "02-dplyr",
    "section": "count to group by then count",
    "text": "count to group by then count\n\nbike |&gt;\n  count(BikeAge_Gr)\n\n# A tibble: 13 × 2\n   BikeAge_Gr     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 &lt;NA&gt;         112\n\n\n   If you wanted to arrange these in ascending order what would you add to the pipe?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "href": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "title": "02-dplyr",
    "section": "Select rows with sample_n or sample_frac",
    "text": "Select rows with sample_n or sample_frac\n\nsample_n: randomly sample 5 observations\n\n\nbike_n5 &lt;- bike |&gt;\n  sample_n(5, replace = FALSE)\n\ndim(bike_n5)\n\n[1]  5 54\n\n\n\nsample_frac: randomly sample 20% of observations\n\n\nbike_perc20 &lt;- bike |&gt;\n  sample_frac(0.2, replace = FALSE)\n\ndim(bike_perc20)\n\n[1] 1143   54",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "href": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "title": "02-dplyr",
    "section": "distinct to filter for unique rows",
    "text": "distinct to filter for unique rows\n\nbike |&gt; \n  select(County, City) |&gt; \n  distinct() |&gt; \n  arrange(County, City)\n\n# A tibble: 360 × 2\n   County    City              \n   &lt;chr&gt;     &lt;chr&gt;             \n 1 Alamance  Alamance          \n 2 Alamance  Burlington        \n 3 Alamance  Elon College      \n 4 Alamance  Gibsonville       \n 5 Alamance  Graham            \n 6 Alamance  Green Level       \n 7 Alamance  Mebane            \n 8 Alamance  None - Rural Crash\n 9 Alexander None - Rural Crash\n10 Alleghany None - Rural Crash\n# ℹ 350 more rows",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "href": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "title": "02-dplyr",
    "section": "distinct has a .keep_all parameter",
    "text": "distinct has a .keep_all parameter\n\nbike |&gt; \n  distinct(County, City, .keep_all = TRUE) |&gt; \n  arrange(County, City)\n\n# A tibble: 360 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     \n 1   524      525 Yes        11-15            12 No         &lt;NA&gt;      B: Eviden…\n 2    84       85 Yes        20-24            20 No         With Tra… B: Eviden…\n 3   571      572 Yes        16-19            16 No         Not Appl… B: Eviden…\n 4   509      510 Yes        40-49            43 Yes        With Tra… K: Killed \n 5   855      856 Yes        30-39            30 No         With Tra… A: Disabl…\n 6     5        6 Yes        40-49            44 Yes        With Tra… C: Possib…\n 7   163      164 Yes        30-39            35 No         Not Appl… C: Possib…\n 8    96       97 Yes        30-39            36 No         With Tra… C: Possib…\n 9    46       47 Yes        50-59            53 No         With Tra… B: Eviden…\n10   485      486 Yes        60-69            62 No         With Tra… C: Possib…\n# ℹ 350 more rows\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#factors-1",
    "href": "content/lectures/02-dplyr.html#factors-1",
    "title": "02-dplyr",
    "section": "Factors",
    "text": "Factors\nFactor objects are how R stores data for categorical variables (fixed numbers of discrete values).\n\n(x = factor(c(\"BS\", \"MS\", \"PhD\", \"MS\")))\n\n[1] BS  MS  PhD MS \nLevels: BS MS PhD\n\n\n\nglimpse(x)\n\n Factor w/ 3 levels \"BS\",\"MS\",\"PhD\": 1 2 3 2\n\n\n\ntypeof(x)\n\n[1] \"integer\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "href": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "title": "02-dplyr",
    "section": "Returning to: Cat lovers",
    "text": "Returning to: Cat lovers\nReading in the cat-lovers data…\n\ncat_lovers &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "href": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "title": "02-dplyr",
    "section": "Read data in as character strings",
    "text": "Read data in as character strings\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           &lt;chr&gt; \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats &lt;chr&gt; \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     &lt;chr&gt; \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "href": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "title": "02-dplyr",
    "section": "But coerce when plotting",
    "text": "But coerce when plotting\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "href": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "title": "02-dplyr",
    "section": "Use forcats to manipulate factors",
    "text": "Use forcats to manipulate factors\n\ncat_lovers &lt;- cat_lovers |&gt;\n  mutate(handedness = fct_relevel(handedness, \n                                  \"right\", \"left\", \"ambidextrous\"))\n\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr.html#forcats-functionality",
    "href": "content/lectures/02-dplyr.html#forcats-functionality",
    "title": "02-dplyr",
    "section": "forcats functionality ",
    "text": "forcats functionality \n\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values. Historically, factors were much easier to work with than character vectors, so many base R functions automatically convert character vectors to factors.\nfactors are still useful when you have true categorical data, and when you want to override the ordering of character vectors to improve display. The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors.\n\n\n\nSource: forcats.tidyverse.org",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#qa",
    "href": "content/lectures/02-dplyr-slides.html#qa",
    "title": "02-dplyr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: How many people are in a group for case studies and final project?\nA: 3-4\n\n\nQ: How to turn in assignments\nA: We’ll discuss this today!\n\n\nQ: Why don’t we have uniform keyboard-shortcut (like run code, new cell) for both R and Python and other coding environment?\nA: Lack of communication? Preferences of developers? I think we’ll get there…\n\n\nQ: Wasn’t clear about the ‘single quotes’ vs “double quotes” thing\nA: When creating a string, or any time you need to use quotes in R, single and double quotes are interchangeable. R doesn’t care which you use. However, your code will be stylistically better if you consistently use one.\n\n\nQ: What are useful libraries that we can use to analyze data?\nA: We’ll be discussing lots, but the tidyverse packages (the first of which we’ll discuss is dplyr) is a great place to start. There are also different packages for basically every statistical analysis out there\n\n\nQ: Is there any way to prevent coercion? / I was wondering if you can types cast a variable when concatenation\nA: Yup. You can explicitly state as._____() when creating a variable (i.e. as.character()) and when reading in data you can specify. You’ll find that R does a pretty good job at guessing, but we can always fix to what we want after the fact.\n\n\nQ: What is the difference between mylist[1] and mylist[[1]]? It looked like class(mylist[1]) returned list and class(mylist[[1]]) returned the class of the element.\nA: Double brackets returns the element directly. Single bracket (for lists) always returns a list.\n\n\nQ: I’m curious about how to handle dataframes in R\nA: Excellent - we’ll start this discussion today and continue throughout the quarter!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#course-announcements",
    "href": "content/lectures/02-dplyr-slides.html#course-announcements",
    "title": "02-dplyr",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 01 due tomorrow (Friday; 11:59 PM)\nStudent survey open until next Thursday\nHW01 and Lab02 will both be released Monday\nLecture Participation survey “due” after class",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#suggested-reading",
    "href": "content/lectures/02-dplyr-slides.html#suggested-reading",
    "title": "02-dplyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#agenda",
    "href": "content/lectures/02-dplyr-slides.html#agenda",
    "title": "02-dplyr",
    "section": "Agenda",
    "text": "Agenda\n\ndplyr\n\nphilosophy\npipes\ncommon operations",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#philosophy",
    "href": "content/lectures/02-dplyr-slides.html#philosophy",
    "title": "02-dplyr",
    "section": "Philosophy",
    "text": "Philosophy\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#the-pipe-in-baser",
    "href": "content/lectures/02-dplyr-slides.html#the-pipe-in-baser",
    "title": "02-dplyr",
    "section": "The pipe in baseR",
    "text": "The pipe in baseR\n\n\n\n\n|&gt; should be read as “and then”\nfor example “Wake up |&gt; brush teeth” would be read as “wake up and then brush teeth”",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#where-does-the-name-come-from",
    "href": "content/lectures/02-dplyr-slides.html#where-does-the-name-come-from",
    "title": "02-dplyr",
    "section": "Where does the name come from?",
    "text": "Where does the name come from?\nThe pipe operator was first implemented in the package magrittr.\n\n\n\n\n\n\n\nYou will see this frequently in code online. It’s equivalent to |&gt;.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#review-how-does-a-pipe-work",
    "href": "content/lectures/02-dplyr-slides.html#review-how-does-a-pipe-work",
    "title": "02-dplyr",
    "section": "Review: How does a pipe work?",
    "text": "Review: How does a pipe work?\n\nYou can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.\n\n\n\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"campus\"))\n\n\n\n\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") |&gt;\n  start_car() |&gt;\n  drive(to = \"campus\") |&gt;\n  park()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#nc-dot-fatal-crashes-in-north-carolina",
    "href": "content/lectures/02-dplyr-slides.html#nc-dot-fatal-crashes-in-north-carolina",
    "title": "02-dplyr",
    "section": "NC DOT Fatal Crashes in North Carolina",
    "text": "NC DOT Fatal Crashes in North Carolina\nFrom OpenDurham’s Data Portal\n\nbike &lt;- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#variables",
    "href": "content/lectures/02-dplyr-slides.html#variables",
    "title": "02-dplyr",
    "section": "Variables",
    "text": "Variables\nView the names of variables via\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#viewing-your-data",
    "href": "content/lectures/02-dplyr-slides.html#viewing-your-data",
    "title": "02-dplyr",
    "section": "Viewing your data",
    "text": "Viewing your data\n\nIn the Environment, click on the name of the data frame to view it in the data viewer (or use the View function)\nUse the glimpse function to take a peek\n\n\nglimpse(bike)\n\nRows: 5,716\nColumns: 54\n$ FID        &lt;dbl&gt; 18, 29, 33, 35, 49, 53, 56, 60, 63, 66, 72, 75, 82, 84, 85,…\n$ OBJECTID   &lt;dbl&gt; 19, 30, 34, 36, 50, 54, 57, 61, 64, 67, 73, 76, 83, 85, 86,…\n$ AmbulanceR &lt;chr&gt; \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", …\n$ BikeAge_Gr &lt;chr&gt; NA, \"50-59\", NA, \"16-19\", NA, \"50-59\", \"16-19\", \"40-49\", \"1…\n$ Bike_Age   &lt;dbl&gt; 6, 51, 10, 17, 6, 52, 18, 40, 6, 7, 45, 30, 17, 20, 14, 15,…\n$ Bike_Alc_D &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bike_Dir   &lt;chr&gt; \"Not Applicable\", \"With Traffic\", \"With Traffic\", NA, \"Faci…\n$ Bike_Injur &lt;chr&gt; \"C: Possible Injury\", \"C: Possible Injury\", \"Injury\", \"B: E…\n$ Bike_Pos   &lt;chr&gt; \"Driveway / Alley\", \"Travel Lane\", \"Travel Lane\", \"Travel L…\n$ Bike_Race  &lt;chr&gt; \"Black\", \"Black\", \"Black\", \"White\", \"Black\", \"White\", \"Blac…\n$ Bike_Sex   &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ City       &lt;chr&gt; \"Durham\", \"Greenville\", \"Farmville\", \"Charlotte\", \"Charlott…\n$ County     &lt;chr&gt; \"Durham\", \"Pitt\", \"Pitt\", \"Mecklenburg\", \"Mecklenburg\", \"Du…\n$ CrashAlcoh &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ CrashDay   &lt;chr&gt; \"01-01-06\", \"01-01-02\", \"01-01-07\", \"01-01-05\", NA, NA, NA,…\n$ Crash_Date &lt;date&gt; 2007-01-06, 2007-01-09, 2007-01-14, 2007-01-12, 2007-01-15…\n$ Crash_Grp  &lt;chr&gt; \"Bicyclist Failed to Yield - Midblock\", \"Crossing Paths - O…\n$ Crash_Hour &lt;dbl&gt; 13, 23, 16, 19, 12, 20, 19, 14, 16, 0, 17, 18, 14, 17, 19, …\n$ Crash_Loc  &lt;chr&gt; \"Non-Intersection\", \"Intersection-Related\", \"Intersection\",…\n$ Crash_Mont &lt;chr&gt; NA, NA, NA, NA, NA, \"01-04-01\", \"01-04-01\", NA, \"01-02-01\",…\n$ Crash_Time &lt;dttm&gt; 0001-01-01 13:17:58, 0001-01-01 23:08:58, 0001-01-01 16:44…\n$ Crash_Type &lt;chr&gt; \"Bicyclist Ride Out - Residential Driveway\", \"Crossing Path…\n$ Crash_Ty_1 &lt;dbl&gt; 353311, 211180, 111144, 119139, 112114, 311231, 119144, 132…\n$ Crash_Year &lt;dbl&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n$ Crsh_Sevri &lt;chr&gt; \"C: Possible Injury\", \"C: Possible Injury\", \"O: No Injury\",…\n$ Developmen &lt;chr&gt; \"Residential\", \"Commercial\", \"Residential\", \"Residential\", …\n$ DrvrAge_Gr &lt;chr&gt; \"60-69\", \"30-39\", \"50-59\", \"30-39\", NA, \"20-24\", \"40-49\", N…\n$ Drvr_Age   &lt;dbl&gt; 66, 34, 52, 33, NA, 20, 40, NA, 17, 51, NA, 64, 50, 66, 30,…\n$ Drvr_Alc_D &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"Missing\", \"No\", \"No\", \"Missing\", \"…\n$ Drvr_EstSp &lt;chr&gt; \"11-15 mph\", \"0-5 mph\", \"21-25 mph\", \"46-50 mph\", \"16-20 mp…\n$ Drvr_Injur &lt;chr&gt; \"O: No Injury\", \"O: No Injury\", \"O: No Injury\", \"O: No Inju…\n$ Drvr_Race  &lt;chr&gt; \"Black\", \"Black\", \"White\", \"White\", \"/Missing\", \"White\", \"B…\n$ Drvr_Sex   &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", NA, \"Female\", \"Male\", N…\n$ Drvr_VehTy &lt;chr&gt; \"Pickup\", \"Passenger Car\", \"Passenger Car\", \"Sport Utility\"…\n$ ExcsSpdInd &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Hit_Run    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n$ Light_Cond &lt;chr&gt; \"Daylight\", \"Dark - Lighted Roadway\", \"Daylight\", \"Dark - R…\n$ Locality   &lt;chr&gt; \"Mixed (30% To 70% Developed)\", \"Urban (&gt;70% Developed)\", \"…\n$ Num_Lanes  &lt;chr&gt; \"2 lanes\", \"5 lanes\", \"2 lanes\", \"4 lanes\", \"2 lanes\", \"4 l…\n$ Num_Units  &lt;dbl&gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Rd_Charact &lt;chr&gt; \"Straight - Level\", \"Straight - Level\", \"Straight - Level\",…\n$ Rd_Class   &lt;chr&gt; \"Local Street\", \"Local Street\", \"Local Street\", \"NC Route\",…\n$ Rd_Conditi &lt;chr&gt; \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr…\n$ Rd_Config  &lt;chr&gt; \"Two-Way, Not Divided\", \"Two-Way, Divided, Unprotected Medi…\n$ Rd_Defects &lt;chr&gt; \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ Rd_Feature &lt;chr&gt; \"No Special Feature\", \"Four-Way Intersection\", \"Four-Way In…\n$ Rd_Surface &lt;chr&gt; \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smoo…\n$ Region     &lt;chr&gt; \"Piedmont\", \"Coastal\", \"Coastal\", \"Piedmont\", \"Piedmont\", \"…\n$ Rural_Urba &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Urban\", \"Urban\", \"Urba…\n$ Speed_Limi &lt;chr&gt; \"20 - 25  MPH\", \"40 - 45  MPH\", \"30 - 35  MPH\", \"40 - 45  M…\n$ Traff_Cntr &lt;chr&gt; \"No Control Present\", \"Stop And Go Signal\", \"Stop Sign\", \"S…\n$ Weather    &lt;chr&gt; \"Clear\", \"Clear\", \"Clear\", \"Cloudy\", \"Clear\", \"Clear\", \"Cle…\n$ Workzone_I &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Location   &lt;chr&gt; \"36.002743, -78.8785\", \"35.612984, -77.39265\", \"35.595676, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#a-grammar-of-data-manipulation",
    "href": "content/lectures/02-dplyr-slides.html#a-grammar-of-data-manipulation",
    "title": "02-dplyr",
    "section": "A Grammar of Data Manipulation",
    "text": "A Grammar of Data Manipulation\ndplyr is based on the concepts of functions as verbs that manipulate data frames.\nSingle data frame functions / verbs:\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#dplyr-rules-for-functions",
    "href": "content/lectures/02-dplyr-slides.html#dplyr-rules-for-functions",
    "title": "02-dplyr",
    "section": "dplyr rules for functions",
    "text": "dplyr rules for functions\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDo not modify in place\nPerformance via lazy evaluation",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter-rows-with-filter",
    "href": "content/lectures/02-dplyr-slides.html#filter-rows-with-filter",
    "title": "02-dplyr",
    "section": "Filter rows with filter",
    "text": "Filter rows with filter\n\nSelect a subset of rows in a data frame.\nEasily filter for many conditions at once.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter",
    "href": "content/lectures/02-dplyr-slides.html#filter",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County\n\nbike |&gt;\n  filter(County == \"Durham\")\n\n# A tibble: 253 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     \n 1    18       19 No         &lt;NA&gt;              6 No         Not Appl… C: Possib…\n 2    53       54 Yes        50-59            52 No         With Tra… A: Disabl…\n 3    56       57 Yes        16-19            18 No         &lt;NA&gt;      C: Possib…\n 4   209      210 No         16-19            16 No         Facing T… C: Possib…\n 5   228      229 Yes        40-49            40 No         With Tra… B: Eviden…\n 6   620      621 Yes        50-59            55 No         With Tra… B: Eviden…\n 7   667      668 Yes        60-69            61 No         Not Appl… B: Eviden…\n 8   458      459 Yes        60-69            62 No         With Tra… B: Eviden…\n 9   576      577 No         40-49            49 No         With Tra… C: Possib…\n10   618      619 No         20-24            23 No         With Tra… C: Possib…\n# ℹ 243 more rows\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter-1",
    "href": "content/lectures/02-dplyr-slides.html#filter-1",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County where biker was &lt; 10 yrs old\n\nbike |&gt;\n  filter(County == \"Durham\", Bike_Age &lt; 10)\n\n# A tibble: 20 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     \n 1    18       19 No         &lt;NA&gt;              6 No         Not Appl… C: Possib…\n 2    47       48 No         10-Jun            9 No         Not Appl… O: No Inj…\n 3   124      125 Yes        10-Jun            8 No         With Tra… C: Possib…\n 4   531      532 Yes        10-Jun            7 No         With Tra… C: Possib…\n 5   704      705 Yes        10-Jun            9 No         Not Appl… C: Possib…\n 6    42       43 No         10-Jun            8 No         With Tra… O: No Inj…\n 7   392      393 Yes        0-5               2 No         Not Appl… B: Eviden…\n 8   941      942 No         10-Jun            9 No         With Tra… C: Possib…\n 9   436      437 Yes        10-Jun            6 No         Not Appl… O: No Inj…\n10   160      161 Yes        10-Jun            7 No         With Tra… C: Possib…\n11   273      274 Yes        10-Jun            7 No         Facing T… C: Possib…\n12    78       79 Yes        10-Jun            7 No         With Tra… C: Possib…\n13   422      423 No         10-Jun            9 No         Not Appl… O: No Inj…\n14   570      571 No         &lt;NA&gt;              0 Missing    Not Appl… Injury    \n15   683      684 Yes        10-Jun            8 No         Not Appl… C: Possib…\n16    62       63 Yes        10-Jun            7 No         With Tra… C: Possib…\n17   248      249 No         0-5               4 No         Not Appl… O: No Inj…\n18   306      307 Yes        10-Jun            8 No         With Tra… C: Possib…\n19   231      232 Yes        10-Jun            8 No         With Tra… C: Possib…\n20   361      362 Yes        10-Jun            9 No         With Tra… B: Eviden…\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;,\n#   Drvr_Race &lt;chr&gt;, Drvr_Sex &lt;chr&gt;, Drvr_VehTy &lt;chr&gt;, ExcsSpdInd &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#aside-real-data-is-messy",
    "href": "content/lectures/02-dplyr-slides.html#aside-real-data-is-messy",
    "title": "02-dplyr",
    "section": "Aside: real data is messy!",
    "text": "Aside: real data is messy!\n   What in the world does a BikeAge_gr of 10-Jun or 15-Nov mean?\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   &lt;chr&gt;            &lt;int&gt;\n 1 0-5                 60\n 2 10-Jun             421\n 3 15-Nov             747\n 4 16-19              605\n 5 20-24              680\n 6 25-29              430\n 7 30-39              658\n 8 40-49              920\n 9 50-59              739\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 &lt;NA&gt;               112",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#careful-data-scientists-clean-up-their-data-first",
    "href": "content/lectures/02-dplyr-slides.html#careful-data-scientists-clean-up-their-data-first",
    "title": "02-dplyr",
    "section": "Careful data scientists clean up their data first!",
    "text": "Careful data scientists clean up their data first!\n\nWe’re going to need to do some text parsing to clean up these data\n\n10-Jun should be 6-10\n15-Nov should be 11-15",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#correct-and-overwrite-mutate",
    "href": "content/lectures/02-dplyr-slides.html#correct-and-overwrite-mutate",
    "title": "02-dplyr",
    "section": "Correct and overwrite mutate",
    "text": "Correct and overwrite mutate\n\nRemember we want to do the following in the BikeAge_Gr variable\n\n10-Jun should be 6-10\n15-Nov should be 11-15\n\n\n\nbike &lt;- bike |&gt;\n  mutate(\n    BikeAge_Gr = case_when(\n      BikeAge_Gr == \"10-Jun\" ~ \"6-10\",\n      BikeAge_Gr == \"15-Nov\" ~ \"11-15\",\n      TRUE                   ~ BikeAge_Gr     # everything else\n    )\n  )\n\n\nNote that we’re overwriting existing data and columns, so be careful!\n\nBut remember, it’s easy to revert if you make a mistake since we didn’t touch the raw data, we can always reload it and start over",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#check-before-you-move-on",
    "href": "content/lectures/02-dplyr-slides.html#check-before-you-move-on",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr count\n   &lt;chr&gt;      &lt;int&gt;\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 &lt;NA&gt;         112",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#mutate-to-add-new-variables",
    "href": "content/lectures/02-dplyr-slides.html#mutate-to-add-new-variables",
    "title": "02-dplyr",
    "section": "mutate to add new variables",
    "text": "mutate to add new variables\n   How is the new alcohol variable determined?\n\nbike |&gt;\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#save-when-you-mutate",
    "href": "content/lectures/02-dplyr-slides.html#save-when-you-mutate",
    "title": "02-dplyr",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame.\n\nbike &lt;- bike |&gt;\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#transmute-to-create-a-new-dataset",
    "href": "content/lectures/02-dplyr-slides.html#transmute-to-create-a-new-dataset",
    "title": "02-dplyr",
    "section": "transmute to create a new dataset",
    "text": "transmute to create a new dataset\nYou’ll use this much less often than mutate but when you need it, you need it.\n\nbike |&gt; \n  transmute(ID = paste(FID, OBJECTID, sep = \"-\"))\n\n# A tibble: 5,716 × 1\n   ID   \n   &lt;chr&gt;\n 1 18-19\n 2 29-30\n 3 33-34\n 4 35-36\n 5 49-50\n 6 53-54\n 7 56-57\n 8 60-61\n 9 63-64\n10 66-67\n# ℹ 5,706 more rows",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#mutate-vs.-transmute",
    "href": "content/lectures/02-dplyr-slides.html#mutate-vs.-transmute",
    "title": "02-dplyr",
    "section": "mutate vs. transmute",
    "text": "mutate vs. transmute\n\nmutate adds new and keeps original\ntransmute adds new; drops existing",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#your-turn",
    "href": "content/lectures/02-dplyr-slides.html#your-turn",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nHow many accidents in our dataset required an ambulance ride (AmbulanceR) and had the Crash_Type “Bicyclist Lost Control - Mechanical Problems”?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers",
    "href": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nFirst five\n\nbike |&gt;\n  slice(1:5)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     \n1    18       19 No         &lt;NA&gt;              6 No         Not Appli… C: Possib…\n2    29       30 Yes        50-59            51 No         With Traf… C: Possib…\n3    33       34 No         &lt;NA&gt;             10 No         With Traf… Injury    \n4    35       36 Yes        16-19            17 No         &lt;NA&gt;       B: Eviden…\n5    49       50 No         &lt;NA&gt;              6 No         Facing Tr… O: No Inj…\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;,\n#   Drvr_Race &lt;chr&gt;, Drvr_Sex &lt;chr&gt;, Drvr_VehTy &lt;chr&gt;, ExcsSpdInd &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers-1",
    "href": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers-1",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nLast five\n\nlast_row &lt;- nrow(bike)\nbike |&gt;\n  slice((last_row - 4):last_row)\n\n# A tibble: 5 × 54\n    FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir   Bike_Injur\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     \n1   460      461 Yes        6-10              7 No         Not Appli… C: Possib…\n2   474      475 Yes        50-59            50 No         With Traf… B: Eviden…\n3   479      480 Yes        16-19            16 No         Not Appli… C: Possib…\n4   487      488 No         40-49            47 Yes        With Traf… C: Possib…\n5   488      489 Yes        30-39            35 No         Facing Tr… C: Possib…\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;,\n#   Drvr_Race &lt;chr&gt;, Drvr_Sex &lt;chr&gt;, Drvr_VehTy &lt;chr&gt;, ExcsSpdInd &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#select-to-keep-only-the-variables-you-mention",
    "href": "content/lectures/02-dplyr-slides.html#select-to-keep-only-the-variables-you-mention",
    "title": "02-dplyr",
    "section": "select to keep only the variables you mention",
    "text": "select to keep only the variables you mention\n\nbike |&gt;\n  select(Crash_Loc, Hit_Run) |&gt;\n  table()\n\n                      Hit_Run\nCrash_Loc                No  Yes\n  Intersection         2223  275\n  Intersection-Related  252   42\n  Location                3    7\n  Non-Intersection     2213  462\n  Non-Roadway           205   30",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#or-select-to-exclude-variables",
    "href": "content/lectures/02-dplyr-slides.html#or-select-to-exclude-variables",
    "title": "02-dplyr",
    "section": "or select to exclude variables",
    "text": "or select to exclude variables\n\nbike |&gt;\n  select(-OBJECTID)\n\n# A tibble: 5,716 × 53\n     FID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur Bike_Pos\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;   \n 1    18 No         &lt;NA&gt;              6 No         Not Appl… C: Possib… Drivewa…\n 2    29 Yes        50-59            51 No         With Tra… C: Possib… Travel …\n 3    33 No         &lt;NA&gt;             10 No         With Tra… Injury     Travel …\n 4    35 Yes        16-19            17 No         &lt;NA&gt;      B: Eviden… Travel …\n 5    49 No         &lt;NA&gt;              6 No         Facing T… O: No Inj… Travel …\n 6    53 Yes        50-59            52 No         With Tra… A: Disabl… Travel …\n 7    56 Yes        16-19            18 No         &lt;NA&gt;      C: Possib… Travel …\n 8    60 No         40-49            40 No         Facing T… B: Eviden… Sidewal…\n 9    63 Yes        6-10              6 No         Facing T… B: Eviden… Travel …\n10    66 Yes        6-10              7 No         &lt;NA&gt;      B: Eviden… Non-Roa…\n# ℹ 5,706 more rows\n# ℹ 45 more variables: Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;, City &lt;chr&gt;,\n#   County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;, Crash_Date &lt;date&gt;,\n#   Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;, Crash_Mont &lt;chr&gt;,\n#   Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;, Crash_Year &lt;dbl&gt;,\n#   Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;, Drvr_Age &lt;dbl&gt;,\n#   Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;, Drvr_Race &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#or-select-a-range-of-variables",
    "href": "content/lectures/02-dplyr-slides.html#or-select-a-range-of-variables",
    "title": "02-dplyr",
    "section": "or select a range of variables",
    "text": "or select a range of variables\n\nbike |&gt;\n  select(OBJECTID:Bike_Injur)\n\n# A tibble: 5,716 × 7\n   OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir       Bike_Injur \n      &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;      \n 1       19 No         &lt;NA&gt;              6 No         Not Applicable C: Possibl…\n 2       30 Yes        50-59            51 No         With Traffic   C: Possibl…\n 3       34 No         &lt;NA&gt;             10 No         With Traffic   Injury     \n 4       36 Yes        16-19            17 No         &lt;NA&gt;           B: Evident…\n 5       50 No         &lt;NA&gt;              6 No         Facing Traffic O: No Inju…\n 6       54 Yes        50-59            52 No         With Traffic   A: Disabli…\n 7       57 Yes        16-19            18 No         &lt;NA&gt;           C: Possibl…\n 8       61 No         40-49            40 No         Facing Traffic B: Evident…\n 9       64 Yes        6-10              6 No         Facing Traffic B: Evident…\n10       67 Yes        6-10              7 No         &lt;NA&gt;           B: Evident…\n# ℹ 5,706 more rows",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#pull-to-extract-a-column-as-a-vector",
    "href": "content/lectures/02-dplyr-slides.html#pull-to-extract-a-column-as-a-vector",
    "title": "02-dplyr",
    "section": "pull to extract a column as a vector",
    "text": "pull to extract a column as a vector\n\nbike |&gt;\n  slice(1:6) |&gt;\n  pull(Location)\n\n[1] \"36.002743, -78.8785\"  \"35.612984, -77.39265\" \"35.595676, -77.59074\"\n[4] \"35.076767, -80.7728\"  \"35.19999, -80.75713\"  \"35.966644, -78.96749\"\n\n\n\nbike |&gt;\n  slice(1:6) |&gt;\n  select(Location)\n\n# A tibble: 6 × 1\n  Location            \n  &lt;chr&gt;               \n1 36.002743, -78.8785 \n2 35.612984, -77.39265\n3 35.595676, -77.59074\n4 35.076767, -80.7728 \n5 35.19999, -80.75713 \n6 35.966644, -78.96749",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#the-two-pulls-in-your-lives",
    "href": "content/lectures/02-dplyr-slides.html#the-two-pulls-in-your-lives",
    "title": "02-dplyr",
    "section": "The two pulls in your lives",
    "text": "The two pulls in your lives\n\n\n\n\n\n\n\n\nDon’t get pull happy when wrangling data! Only extract out variables if you truly need to, otherwise keep in data frame.\nBut always ⬇️ Pull before starting your work when collaborating on GitHub.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#rename-specific-columns",
    "href": "content/lectures/02-dplyr-slides.html#rename-specific-columns",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\nUseful for correcting typos, and renaming to make variable names shorter and/or more informative\n\nOriginal names:\n\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#rename-specific-columns-1",
    "href": "content/lectures/02-dplyr-slides.html#rename-specific-columns-1",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\n\nRename Speed_Limi to Speed_Limit:\n\n\nbike &lt;- bike |&gt;\n  rename(Speed_Limit = Speed_Limi)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#check-before-you-move-on-1",
    "href": "content/lectures/02-dplyr-slides.html#check-before-you-move-on-1",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nnames(bike)\n\n [1] \"FID\"         \"OBJECTID\"    \"AmbulanceR\"  \"BikeAge_Gr\"  \"Bike_Age\"   \n [6] \"Bike_Alc_D\"  \"Bike_Dir\"    \"Bike_Injur\"  \"Bike_Pos\"    \"Bike_Race\"  \n[11] \"Bike_Sex\"    \"City\"        \"County\"      \"CrashAlcoh\"  \"CrashDay\"   \n[16] \"Crash_Date\"  \"Crash_Grp\"   \"Crash_Hour\"  \"Crash_Loc\"   \"Crash_Mont\" \n[21] \"Crash_Time\"  \"Crash_Type\"  \"Crash_Ty_1\"  \"Crash_Year\"  \"Crsh_Sevri\" \n[26] \"Developmen\"  \"DrvrAge_Gr\"  \"Drvr_Age\"    \"Drvr_Alc_D\"  \"Drvr_EstSp\" \n[31] \"Drvr_Injur\"  \"Drvr_Race\"   \"Drvr_Sex\"    \"Drvr_VehTy\"  \"ExcsSpdInd\" \n[36] \"Hit_Run\"     \"Light_Cond\"  \"Locality\"    \"Num_Lanes\"   \"Num_Units\"  \n[41] \"Rd_Charact\"  \"Rd_Class\"    \"Rd_Conditi\"  \"Rd_Config\"   \"Rd_Defects\" \n[46] \"Rd_Feature\"  \"Rd_Surface\"  \"Region\"      \"Rural_Urba\"  \"Speed_Limit\"\n[51] \"Traff_Cntr\"  \"Weather\"     \"Workzone_I\"  \"Location\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#your-turn-1",
    "href": "content/lectures/02-dplyr-slides.html#your-turn-1",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nYour boss in Cumberland County gets overwhelmed by data easily, but he wants some data from you. He wants all bike accidents from his County, but he only wants to know the road’s speed limit, the age of the biker, and to know if alcohol was involved. If you have time, mine as well make the column names very clear to your boss while you’re at it…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#summarize-to-reduce-variables-to-values",
    "href": "content/lectures/02-dplyr-slides.html#summarize-to-reduce-variables-to-values",
    "title": "02-dplyr",
    "section": "summarize to reduce variables to values",
    "text": "summarize to reduce variables to values\nThe values are summarized in a data frame\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   &lt;chr&gt;            &lt;int&gt;\n 1 0-5                 60\n 2 11-15              747\n 3 16-19              605\n 4 20-24              680\n 5 25-29              430\n 6 30-39              658\n 7 40-49              920\n 8 50-59              739\n 9 6-10               421\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 &lt;NA&gt;               112",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#and-arrange-to-order-rows",
    "href": "content/lectures/02-dplyr-slides.html#and-arrange-to-order-rows",
    "title": "02-dplyr",
    "section": "and arrange to order rows",
    "text": "and arrange to order rows\n\nbike |&gt;\n  group_by(BikeAge_Gr) |&gt;\n  summarize(crash_count = n()) |&gt;\n  arrange(desc(crash_count))\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   &lt;chr&gt;            &lt;int&gt;\n 1 40-49              920\n 2 11-15              747\n 3 50-59              739\n 4 20-24              680\n 5 30-39              658\n 6 16-19              605\n 7 25-29              430\n 8 6-10               421\n 9 60-69              274\n10 &lt;NA&gt;               112\n11 0-5                 60\n12 70+                 58\n13 70                  12",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#count-to-group-by-then-count",
    "href": "content/lectures/02-dplyr-slides.html#count-to-group-by-then-count",
    "title": "02-dplyr",
    "section": "count to group by then count",
    "text": "count to group by then count\n\nbike |&gt;\n  count(BikeAge_Gr)\n\n# A tibble: 13 × 2\n   BikeAge_Gr     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 &lt;NA&gt;         112\n\n\n   If you wanted to arrange these in ascending order what would you add to the pipe?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#select-rows-with-sample_n-or-sample_frac",
    "href": "content/lectures/02-dplyr-slides.html#select-rows-with-sample_n-or-sample_frac",
    "title": "02-dplyr",
    "section": "Select rows with sample_n or sample_frac",
    "text": "Select rows with sample_n or sample_frac\n\nsample_n: randomly sample 5 observations\n\n\nbike_n5 &lt;- bike |&gt;\n  sample_n(5, replace = FALSE)\n\ndim(bike_n5)\n\n[1]  5 54\n\n\n\nsample_frac: randomly sample 20% of observations\n\n\nbike_perc20 &lt;- bike |&gt;\n  sample_frac(0.2, replace = FALSE)\n\ndim(bike_perc20)\n\n[1] 1143   54",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#distinct-to-filter-for-unique-rows",
    "href": "content/lectures/02-dplyr-slides.html#distinct-to-filter-for-unique-rows",
    "title": "02-dplyr",
    "section": "distinct to filter for unique rows",
    "text": "distinct to filter for unique rows\n\nbike |&gt; \n  select(County, City) |&gt; \n  distinct() |&gt; \n  arrange(County, City)\n\n# A tibble: 360 × 2\n   County    City              \n   &lt;chr&gt;     &lt;chr&gt;             \n 1 Alamance  Alamance          \n 2 Alamance  Burlington        \n 3 Alamance  Elon College      \n 4 Alamance  Gibsonville       \n 5 Alamance  Graham            \n 6 Alamance  Green Level       \n 7 Alamance  Mebane            \n 8 Alamance  None - Rural Crash\n 9 Alexander None - Rural Crash\n10 Alleghany None - Rural Crash\n# ℹ 350 more rows",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#distinct-has-a-.keep_all-parameter",
    "href": "content/lectures/02-dplyr-slides.html#distinct-has-a-.keep_all-parameter",
    "title": "02-dplyr",
    "section": "distinct has a .keep_all parameter",
    "text": "distinct has a .keep_all parameter\n\nbike |&gt; \n  distinct(County, City, .keep_all = TRUE) |&gt; \n  arrange(County, City)\n\n# A tibble: 360 × 54\n     FID OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir  Bike_Injur\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     \n 1   524      525 Yes        11-15            12 No         &lt;NA&gt;      B: Eviden…\n 2    84       85 Yes        20-24            20 No         With Tra… B: Eviden…\n 3   571      572 Yes        16-19            16 No         Not Appl… B: Eviden…\n 4   509      510 Yes        40-49            43 Yes        With Tra… K: Killed \n 5   855      856 Yes        30-39            30 No         With Tra… A: Disabl…\n 6     5        6 Yes        40-49            44 Yes        With Tra… C: Possib…\n 7   163      164 Yes        30-39            35 No         Not Appl… C: Possib…\n 8    96       97 Yes        30-39            36 No         With Tra… C: Possib…\n 9    46       47 Yes        50-59            53 No         With Tra… B: Eviden…\n10   485      486 Yes        60-69            62 No         With Tra… C: Possib…\n# ℹ 350 more rows\n# ℹ 46 more variables: Bike_Pos &lt;chr&gt;, Bike_Race &lt;chr&gt;, Bike_Sex &lt;chr&gt;,\n#   City &lt;chr&gt;, County &lt;chr&gt;, CrashAlcoh &lt;chr&gt;, CrashDay &lt;chr&gt;,\n#   Crash_Date &lt;date&gt;, Crash_Grp &lt;chr&gt;, Crash_Hour &lt;dbl&gt;, Crash_Loc &lt;chr&gt;,\n#   Crash_Mont &lt;chr&gt;, Crash_Time &lt;dttm&gt;, Crash_Type &lt;chr&gt;, Crash_Ty_1 &lt;dbl&gt;,\n#   Crash_Year &lt;dbl&gt;, Crsh_Sevri &lt;chr&gt;, Developmen &lt;chr&gt;, DrvrAge_Gr &lt;chr&gt;,\n#   Drvr_Age &lt;dbl&gt;, Drvr_Alc_D &lt;chr&gt;, Drvr_EstSp &lt;chr&gt;, Drvr_Injur &lt;chr&gt;, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#factors-1",
    "href": "content/lectures/02-dplyr-slides.html#factors-1",
    "title": "02-dplyr",
    "section": "Factors",
    "text": "Factors\nFactor objects are how R stores data for categorical variables (fixed numbers of discrete values).\n\n(x = factor(c(\"BS\", \"MS\", \"PhD\", \"MS\")))\n\n[1] BS  MS  PhD MS \nLevels: BS MS PhD\n\n\n\nglimpse(x)\n\n Factor w/ 3 levels \"BS\",\"MS\",\"PhD\": 1 2 3 2\n\n\n\ntypeof(x)\n\n[1] \"integer\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#returning-to-cat-lovers",
    "href": "content/lectures/02-dplyr-slides.html#returning-to-cat-lovers",
    "title": "02-dplyr",
    "section": "Returning to: Cat lovers",
    "text": "Returning to: Cat lovers\nReading in the cat-lovers data…\n\ncat_lovers &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#read-data-in-as-character-strings",
    "href": "content/lectures/02-dplyr-slides.html#read-data-in-as-character-strings",
    "title": "02-dplyr",
    "section": "Read data in as character strings",
    "text": "Read data in as character strings\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           &lt;chr&gt; \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats &lt;chr&gt; \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     &lt;chr&gt; \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#but-coerce-when-plotting",
    "href": "content/lectures/02-dplyr-slides.html#but-coerce-when-plotting",
    "title": "02-dplyr",
    "section": "But coerce when plotting",
    "text": "But coerce when plotting\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#use-forcats-to-manipulate-factors",
    "href": "content/lectures/02-dplyr-slides.html#use-forcats-to-manipulate-factors",
    "title": "02-dplyr",
    "section": "Use forcats to manipulate factors",
    "text": "Use forcats to manipulate factors\n\ncat_lovers &lt;- cat_lovers |&gt;\n  mutate(handedness = fct_relevel(handedness, \n                                  \"right\", \"left\", \"ambidextrous\"))\n\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#forcats-functionality",
    "href": "content/lectures/02-dplyr-slides.html#forcats-functionality",
    "title": "02-dplyr",
    "section": "forcats functionality ",
    "text": "forcats functionality \n\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values. Historically, factors were much easier to work with than character vectors, so many base R functions automatically convert character vectors to factors.\nfactors are still useful when you have true categorical data, and when you want to override the ordering of character vectors to improve display. The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors.\n\n\n\nSource: forcats.tidyverse.org",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "02-dplyr"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html",
    "href": "content/lectures/05-viz.html",
    "title": "05-viz",
    "section": "",
    "text": "Slides modified from datascienceinabox.org\n\n\n\nQ: I just think it’s a bit fast and I would appreciate if you made sure to show like how to import and stuff rather then jumping to the final product\nA: That’s really valuable feedback. I don’t think you’re alone. I’m going to work to better get everyone on the same page going forward.\n\nNote: a handful of comments on needing more time on joins & pivots…which makes sense! They’re new :)\n\n\n\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHW01 can be submitted by tonight for full credit (datahub issues)\nLab02 Ans Posted\nHW02 Now Available\nMy OH will now be at the tables along the back of this building\n\n\n\n\n\nR4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit: Angela Zoss and Eric Monson, Duke DVS",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#qa",
    "href": "content/lectures/05-viz.html#qa",
    "title": "05-viz",
    "section": "",
    "text": "Q: I just think it’s a bit fast and I would appreciate if you made sure to show like how to import and stuff rather then jumping to the final product\nA: That’s really valuable feedback. I don’t think you’re alone. I’m going to work to better get everyone on the same page going forward.\n\nNote: a handful of comments on needing more time on joins & pivots…which makes sense! They’re new :)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#course-announcements",
    "href": "content/lectures/05-viz.html#course-announcements",
    "title": "05-viz",
    "section": "",
    "text": "Due Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHW01 can be submitted by tonight for full credit (datahub issues)\nLab02 Ans Posted\nHW02 Now Available\nMy OH will now be at the tables along the back of this building",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#suggested-reading",
    "href": "content/lectures/05-viz.html#suggested-reading",
    "title": "05-viz",
    "section": "",
    "text": "R4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#tell-a-story",
    "href": "content/lectures/05-viz.html#tell-a-story",
    "title": "05-viz",
    "section": "",
    "text": "Credit: Angela Zoss and Eric Monson, Duke DVS",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "href": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "title": "05-viz",
    "section": "Principles for effective visualizations",
    "text": "Principles for effective visualizations\n\nOrder matters\nPut long categories on the y-axis\nKeep scales consistent\nSelect meaningful colors\nUse meaningful and nonredundant labels",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#data",
    "href": "content/lectures/05-viz.html#data",
    "title": "05-viz",
    "section": "Data",
    "text": "Data\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\n\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: YouGov Survey Results, retrieved Oct 7, 2019",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#the-data-code",
    "href": "content/lectures/05-viz.html#the-data-code",
    "title": "05-viz",
    "section": "The Data: Code",
    "text": "The Data: Code\n\nbrexit &lt;- tibble(\n  opinion = c(\n    rep(\"Right\", 664), rep(\"Wrong\", 787), rep(\"Don't know\", 188)\n  ),\n  region = c(\n    rep(\"london\", 63), rep(\"rest_of_south\", 241), rep(\"midlands_wales\", 145), rep(\"north\", 176), rep(\"scot\", 39),\n    rep(\"london\", 110), rep(\"rest_of_south\", 257), rep(\"midlands_wales\", 152), rep(\"north\", 176), rep(\"scot\", 92),\n    rep(\"london\", 24), rep(\"rest_of_south\", 49), rep(\"midlands_wales\", 57), rep(\"north\", 48), rep(\"scot\", 10)\n  )\n)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#order-matters",
    "href": "content/lectures/05-viz.html#order-matters",
    "title": "05-viz",
    "section": "Order matters",
    "text": "Order matters\nAlphabetical is rarely ideal\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#order-by-frequency",
    "href": "content/lectures/05-viz.html#order-by-frequency",
    "title": "05-viz",
    "section": "Order by frequency",
    "text": "Order by frequency\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nfct_infreq: Reorder factors’ levels by frequency\n\nggplot(brexit, aes(x = fct_infreq(opinion))) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels",
    "href": "content/lectures/05-viz.html#clean-up-labels",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar() +\n  labs( \n    x = \"Opinion\", \n    y = \"Count\" \n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "href": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "title": "05-viz",
    "section": "Avoiding Alphabetical Order",
    "text": "Avoiding Alphabetical Order\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = region)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#use-inherent-level-order",
    "href": "content/lectures/05-viz.html#use-inherent-level-order",
    "title": "05-viz",
    "section": "Use inherent level order",
    "text": "Use inherent level order\n\nRelevelPlot\n\n\nfct_relevel: Reorder factor levels using a custom order\n\nbrexit &lt;- brexit |&gt;\n  mutate(\n    region = fct_relevel( \n      region,\n      \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"\n    )\n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-1",
    "href": "content/lectures/05-viz.html#clean-up-labels-1",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nRecodePlot\n\n\nfct_recode: Change factor levels by hand\n\nbrexit &lt;- brexit |&gt;\n  mutate(\n    region = fct_recode( \n      region,\n      London = \"london\",\n      `Rest of South` = \"rest_of_south\",\n      `Midlands / Wales` = \"midlands_wales\",\n      North = \"north\",\n      Scotland = \"scot\"\n    )\n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "href": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "title": "05-viz",
    "section": "Put long categories on the y-axis",
    "text": "Put long categories on the y-axis\nLong categories can be hard to read",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "href": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "title": "05-viz",
    "section": "Move them to the y-axis",
    "text": "Move them to the y-axis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region)) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "href": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "title": "05-viz",
    "section": "And reverse the order of levels",
    "text": "And reverse the order of levels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nfct_rev: Reverse order of factor levels\n\nggplot(brexit, aes(y = fct_rev(region))) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-2",
    "href": "content/lectures/05-viz.html#clean-up-labels-2",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = fct_rev(region))) +\n  geom_bar() +\n  labs( \n    x = \"Count\", \n    y = \"Region\" \n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "href": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "title": "05-viz",
    "section": "Segmented bar plots can be hard to read",
    "text": "Segmented bar plots can be hard to read\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region, fill = opinion)) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#use-facets",
    "href": "content/lectures/05-viz.html#use-facets",
    "title": "05-viz",
    "section": "Use facets",
    "text": "Use facets\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = region)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#avoid-redundancy",
    "href": "content/lectures/05-viz.html#avoid-redundancy",
    "title": "05-viz",
    "section": "Avoid redundancy?",
    "text": "Avoid redundancy?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "href": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "title": "05-viz",
    "section": "Redundancy can help tell a story",
    "text": "Redundancy can help tell a story\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "href": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "title": "05-viz",
    "section": "Be selective with redundancy",
    "text": "Be selective with redundancy\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#use-informative-labels",
    "href": "content/lectures/05-viz.html#use-informative-labels",
    "title": "05-viz",
    "section": "Use informative labels",
    "text": "Use informative labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\", \n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#a-bit-more-info",
    "href": "content/lectures/05-viz.html#a-bit-more-info",
    "title": "05-viz",
    "section": "A bit more info",
    "text": "A bit more info\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\", \n    caption = \"Source: https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf\", \n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#lets-do-better",
    "href": "content/lectures/05-viz.html#lets-do-better",
    "title": "05-viz",
    "section": "Let’s do better",
    "text": "Let’s do better\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\", \n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#fix-up-facet-labels",
    "href": "content/lectures/05-viz.html#fix-up-facet-labels",
    "title": "05-viz",
    "section": "Fix up facet labels",
    "text": "Fix up facet labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1,\n    labeller = label_wrap_gen(width = 12) \n  ) + \n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "href": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "title": "05-viz",
    "section": "Rainbow colors not always best",
    "text": "Rainbow colors not always best",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "href": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "title": "05-viz",
    "section": "Manually choose colors when needed",
    "text": "Manually choose colors when needed\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c( \n    \"Wrong\" = \"red\", \n    \"Right\" = \"green\", \n    \"Don't know\" = \"gray\" \n  ))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#choosing-better-colors",
    "href": "content/lectures/05-viz.html#choosing-better-colors",
    "title": "05-viz",
    "section": "Choosing better colors",
    "text": "Choosing better colors\ncolorbrewer2.org",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#use-better-colors",
    "href": "content/lectures/05-viz.html#use-better-colors",
    "title": "05-viz",
    "section": "Use better colors",
    "text": "Use better colors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\", \n    \"Right\" = \"#67a9cf\", \n    \"Don't know\" = \"gray\" \n  ))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#select-theme",
    "href": "content/lectures/05-viz.html#select-theme",
    "title": "05-viz",
    "section": "Select theme",
    "text": "Select theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal() \n\n\n\n\n\n\nggthemes described here",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#customize-theme",
    "href": "content/lectures/05-viz.html#customize-theme",
    "title": "05-viz",
    "section": "Customize theme",
    "text": "Customize theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal(base_size = 16) + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank())",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz.html#your-turn",
    "href": "content/lectures/05-viz.html#your-turn",
    "title": "05-viz",
    "section": "Your Turn",
    "text": "Your Turn\n\nRead in the data (Data slide)\nThink of at least three different ways to tell slightly different stories with these data\nTry to implement at least one of these ideas!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#qa",
    "href": "content/lectures/05-viz-slides.html#qa",
    "title": "05-viz",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I just think it’s a bit fast and I would appreciate if you made sure to show like how to import and stuff rather then jumping to the final product\nA: That’s really valuable feedback. I don’t think you’re alone. I’m going to work to better get everyone on the same page going forward.\n\nNote: a handful of comments on needing more time on joins & pivots…which makes sense! They’re new :)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#course-announcements",
    "href": "content/lectures/05-viz-slides.html#course-announcements",
    "title": "05-viz",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHW01 can be submitted by tonight for full credit (datahub issues)\nLab02 Ans Posted\nHW02 Now Available\nMy OH will now be at the tables along the back of this building",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#suggested-reading",
    "href": "content/lectures/05-viz-slides.html#suggested-reading",
    "title": "05-viz",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#keep-it-simple",
    "href": "content/lectures/05-viz-slides.html#keep-it-simple",
    "title": "05-viz",
    "section": "Keep it simple",
    "text": "Keep it simple",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-color-to-draw-attention",
    "href": "content/lectures/05-viz-slides.html#use-color-to-draw-attention",
    "title": "05-viz",
    "section": "Use color to draw attention",
    "text": "Use color to draw attention",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#tell-a-story",
    "href": "content/lectures/05-viz-slides.html#tell-a-story",
    "title": "05-viz",
    "section": "Tell a story",
    "text": "Tell a story\n\n\n\n\n\n\n\n\n\n\n\nCredit: Angela Zoss and Eric Monson, Duke DVS",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#principles-for-effective-visualizations-1",
    "href": "content/lectures/05-viz-slides.html#principles-for-effective-visualizations-1",
    "title": "05-viz",
    "section": "Principles for effective visualizations",
    "text": "Principles for effective visualizations\n\nOrder matters\nPut long categories on the y-axis\nKeep scales consistent\nSelect meaningful colors\nUse meaningful and nonredundant labels",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#data",
    "href": "content/lectures/05-viz-slides.html#data",
    "title": "05-viz",
    "section": "Data",
    "text": "Data\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\n\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: YouGov Survey Results, retrieved Oct 7, 2019",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#the-data-code",
    "href": "content/lectures/05-viz-slides.html#the-data-code",
    "title": "05-viz",
    "section": "The Data: Code",
    "text": "The Data: Code\n\nbrexit &lt;- tibble(\n  opinion = c(\n    rep(\"Right\", 664), rep(\"Wrong\", 787), rep(\"Don't know\", 188)\n  ),\n  region = c(\n    rep(\"london\", 63), rep(\"rest_of_south\", 241), rep(\"midlands_wales\", 145), rep(\"north\", 176), rep(\"scot\", 39),\n    rep(\"london\", 110), rep(\"rest_of_south\", 257), rep(\"midlands_wales\", 152), rep(\"north\", 176), rep(\"scot\", 92),\n    rep(\"london\", 24), rep(\"rest_of_south\", 49), rep(\"midlands_wales\", 57), rep(\"north\", 48), rep(\"scot\", 10)\n  )\n)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#order-matters",
    "href": "content/lectures/05-viz-slides.html#order-matters",
    "title": "05-viz",
    "section": "Order matters",
    "text": "Order matters\nAlphabetical is rarely ideal\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#order-by-frequency",
    "href": "content/lectures/05-viz-slides.html#order-by-frequency",
    "title": "05-viz",
    "section": "Order by frequency",
    "text": "Order by frequency\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nfct_infreq: Reorder factors’ levels by frequency\n\nggplot(brexit, aes(x = fct_infreq(opinion))) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar() +\n  labs( \n    x = \"Opinion\", \n    y = \"Count\" \n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#avoiding-alphabetical-order",
    "href": "content/lectures/05-viz-slides.html#avoiding-alphabetical-order",
    "title": "05-viz",
    "section": "Avoiding Alphabetical Order",
    "text": "Avoiding Alphabetical Order\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = region)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-inherent-level-order",
    "href": "content/lectures/05-viz-slides.html#use-inherent-level-order",
    "title": "05-viz",
    "section": "Use inherent level order",
    "text": "Use inherent level order\n\nRelevelPlot\n\n\nfct_relevel: Reorder factor levels using a custom order\n\nbrexit &lt;- brexit |&gt;\n  mutate(\n    region = fct_relevel( \n      region,\n      \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"\n    )\n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels-1",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels-1",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nRecodePlot\n\n\nfct_recode: Change factor levels by hand\n\nbrexit &lt;- brexit |&gt;\n  mutate(\n    region = fct_recode( \n      region,\n      London = \"london\",\n      `Rest of South` = \"rest_of_south\",\n      `Midlands / Wales` = \"midlands_wales\",\n      North = \"north\",\n      Scotland = \"scot\"\n    )\n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#put-long-categories-on-the-y-axis",
    "href": "content/lectures/05-viz-slides.html#put-long-categories-on-the-y-axis",
    "title": "05-viz",
    "section": "Put long categories on the y-axis",
    "text": "Put long categories on the y-axis\nLong categories can be hard to read",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#move-them-to-the-y-axis",
    "href": "content/lectures/05-viz-slides.html#move-them-to-the-y-axis",
    "title": "05-viz",
    "section": "Move them to the y-axis",
    "text": "Move them to the y-axis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region)) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#and-reverse-the-order-of-levels",
    "href": "content/lectures/05-viz-slides.html#and-reverse-the-order-of-levels",
    "title": "05-viz",
    "section": "And reverse the order of levels",
    "text": "And reverse the order of levels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nfct_rev: Reverse order of factor levels\n\nggplot(brexit, aes(y = fct_rev(region))) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels-2",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels-2",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = fct_rev(region))) +\n  geom_bar() +\n  labs( \n    x = \"Count\", \n    y = \"Region\" \n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#segmented-bar-plots-can-be-hard-to-read",
    "href": "content/lectures/05-viz-slides.html#segmented-bar-plots-can-be-hard-to-read",
    "title": "05-viz",
    "section": "Segmented bar plots can be hard to read",
    "text": "Segmented bar plots can be hard to read\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region, fill = opinion)) + \n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-facets",
    "href": "content/lectures/05-viz-slides.html#use-facets",
    "title": "05-viz",
    "section": "Use facets",
    "text": "Use facets\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = region)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#avoid-redundancy",
    "href": "content/lectures/05-viz-slides.html#avoid-redundancy",
    "title": "05-viz",
    "section": "Avoid redundancy?",
    "text": "Avoid redundancy?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#redundancy-can-help-tell-a-story",
    "href": "content/lectures/05-viz-slides.html#redundancy-can-help-tell-a-story",
    "title": "05-viz",
    "section": "Redundancy can help tell a story",
    "text": "Redundancy can help tell a story\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#be-selective-with-redundancy",
    "href": "content/lectures/05-viz-slides.html#be-selective-with-redundancy",
    "title": "05-viz",
    "section": "Be selective with redundancy",
    "text": "Be selective with redundancy\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-informative-labels",
    "href": "content/lectures/05-viz-slides.html#use-informative-labels",
    "title": "05-viz",
    "section": "Use informative labels",
    "text": "Use informative labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\", \n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#a-bit-more-info",
    "href": "content/lectures/05-viz-slides.html#a-bit-more-info",
    "title": "05-viz",
    "section": "A bit more info",
    "text": "A bit more info\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\", \n    caption = \"Source: https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf\", \n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#lets-do-better",
    "href": "content/lectures/05-viz-slides.html#lets-do-better",
    "title": "05-viz",
    "section": "Let’s do better",
    "text": "Let’s do better\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\", \n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#fix-up-facet-labels",
    "href": "content/lectures/05-viz-slides.html#fix-up-facet-labels",
    "title": "05-viz",
    "section": "Fix up facet labels",
    "text": "Fix up facet labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1,\n    labeller = label_wrap_gen(width = 12) \n  ) + \n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#rainbow-colors-not-always-best",
    "href": "content/lectures/05-viz-slides.html#rainbow-colors-not-always-best",
    "title": "05-viz",
    "section": "Rainbow colors not always best",
    "text": "Rainbow colors not always best",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#manually-choose-colors-when-needed",
    "href": "content/lectures/05-viz-slides.html#manually-choose-colors-when-needed",
    "title": "05-viz",
    "section": "Manually choose colors when needed",
    "text": "Manually choose colors when needed\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c( \n    \"Wrong\" = \"red\", \n    \"Right\" = \"green\", \n    \"Don't know\" = \"gray\" \n  ))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#choosing-better-colors",
    "href": "content/lectures/05-viz-slides.html#choosing-better-colors",
    "title": "05-viz",
    "section": "Choosing better colors",
    "text": "Choosing better colors\ncolorbrewer2.org",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-better-colors",
    "href": "content/lectures/05-viz-slides.html#use-better-colors",
    "title": "05-viz",
    "section": "Use better colors",
    "text": "Use better colors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\", \n    \"Right\" = \"#67a9cf\", \n    \"Don't know\" = \"gray\" \n  ))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#select-theme",
    "href": "content/lectures/05-viz-slides.html#select-theme",
    "title": "05-viz",
    "section": "Select theme",
    "text": "Select theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal() \n\n\n\n\n\n\nggthemes described here",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#customize-theme",
    "href": "content/lectures/05-viz-slides.html#customize-theme",
    "title": "05-viz",
    "section": "Customize theme",
    "text": "Customize theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal(base_size = 16) + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank())",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#your-turn",
    "href": "content/lectures/05-viz-slides.html#your-turn",
    "title": "05-viz",
    "section": "Your Turn",
    "text": "Your Turn\n\nRead in the data (Data slide)\nThink of at least three different ways to tell slightly different stories with these data\nTry to implement at least one of these ideas!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "05-viz"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html",
    "href": "content/lectures/08-effective-communication.html",
    "title": "08-effective-communication",
    "section": "",
    "text": "Q: for the last part of lecture 07, I tried glance(m_ht_wt) but it didn’t work because m_ht_wt doesn’t exist. Is “m_ht_wt” supposed be a model?\nA: Yup, this model was the height by weight model:\n\n\nm_ht_wt &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ Width_in, data = pp)\n\n\nQ: I was not too sure what was going on when talking about the relationship between painting height and school.\nA: I don’t think you were the only one confused! Briefly here (and I’m happy to chat more before/after class and in OH), we were looking to determine/quantify the relationship between the size (height) of a painting and the school from which the painting originated. This was an example of having more than two categories for a categorical (factor) predictor. The important points were undersatnding that each level is compared to the baseline and the linear model that results from multiple categories. Part 4 of the lab gets into this a bit more too. Definitely follow up if you’re unsure after doing that part of the lab!\n\n\nQ: How do you calculate the linear regression model when you have non-numeric values? For example, on lab 04, when it asks to calculate the linear regression model by gender, the gender appears only as male and female. Suppose male is 1 and female is 0 (interpreted by the function), then male linear regression model is y =ax + 1?\nA: Close! the “1” would be plugged in as the value of x (in what you suggested)m not for the intercept. So the function would be \\(y=\\beta_1*1 + \\beta_0\\)\n\n\n\n\n\nLab04 due Friday\nHW02 due Monday\n\n. . .\n\nPractice Midterms Now Available\n\nanswers posted next week\n\nMidterm Exam\n\nwill cover material through “Multiple Linear Regression”\nwill be released/posted next Friday after lab\nwill be due Monday Nov 6th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be completed individually (open Notes; open Internet)\n\n\n. . .\n\nLink for Later\n\n. . .\n\n\n\n\n\nCommunicating for your audience\nOral Communication\nWritten Communication\nVisual Communication\n\n\n\n\n\nBookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#qa",
    "href": "content/lectures/08-effective-communication.html#qa",
    "title": "08-effective-communication",
    "section": "",
    "text": "Q: for the last part of lecture 07, I tried glance(m_ht_wt) but it didn’t work because m_ht_wt doesn’t exist. Is “m_ht_wt” supposed be a model?\nA: Yup, this model was the height by weight model:\n\n\nm_ht_wt &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ Width_in, data = pp)\n\n\nQ: I was not too sure what was going on when talking about the relationship between painting height and school.\nA: I don’t think you were the only one confused! Briefly here (and I’m happy to chat more before/after class and in OH), we were looking to determine/quantify the relationship between the size (height) of a painting and the school from which the painting originated. This was an example of having more than two categories for a categorical (factor) predictor. The important points were undersatnding that each level is compared to the baseline and the linear model that results from multiple categories. Part 4 of the lab gets into this a bit more too. Definitely follow up if you’re unsure after doing that part of the lab!\n\n\nQ: How do you calculate the linear regression model when you have non-numeric values? For example, on lab 04, when it asks to calculate the linear regression model by gender, the gender appears only as male and female. Suppose male is 1 and female is 0 (interpreted by the function), then male linear regression model is y =ax + 1?\nA: Close! the “1” would be plugged in as the value of x (in what you suggested)m not for the intercept. So the function would be \\(y=\\beta_1*1 + \\beta_0\\)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#course-announcements",
    "href": "content/lectures/08-effective-communication.html#course-announcements",
    "title": "08-effective-communication",
    "section": "",
    "text": "Lab04 due Friday\nHW02 due Monday\n\n. . .\n\nPractice Midterms Now Available\n\nanswers posted next week\n\nMidterm Exam\n\nwill cover material through “Multiple Linear Regression”\nwill be released/posted next Friday after lab\nwill be due Monday Nov 6th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be completed individually (open Notes; open Internet)\n\n\n. . .\n\nLink for Later\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#agenda",
    "href": "content/lectures/08-effective-communication.html#agenda",
    "title": "08-effective-communication",
    "section": "",
    "text": "Communicating for your audience\nOral Communication\nWritten Communication\nVisual Communication",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#suggested-reading",
    "href": "content/lectures/08-effective-communication.html#suggested-reading",
    "title": "08-effective-communication",
    "section": "",
    "text": "Bookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "href": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "title": "08-effective-communication",
    "section": "What does this mean?",
    "text": "What does this mean?\n❓ What does it mean to “consider your audience?”\n. . .\nSimply: You do the work so they don’t have to.\n. . .\n…also the aesthetic-usability effect exists.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "href": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "title": "08-effective-communication",
    "section": "What’s the right level?",
    "text": "What’s the right level?\n\n\nGeneral Audience\n✔ background\n🚫 limit technical details\n🎉 emphasize take-home\n\n\n\nTechnical Audience\n⬇ limit background\n💻 all-the-details\n🎉 emphasize take-home",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#considerations",
    "href": "content/lectures/08-effective-communication.html#considerations",
    "title": "08-effective-communication",
    "section": "Considerations",
    "text": "Considerations\n\nPlatform: written? oral?\n\n. . .\n\nSetting: informal? formal?\n\n. . .\n\nTiming: never go over your time limit!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#storytelling",
    "href": "content/lectures/08-effective-communication.html#storytelling",
    "title": "08-effective-communication",
    "section": "Storytelling",
    "text": "Storytelling\n\nStories have a beginning, a middle, and an end.\n\n. . .\n\nStories do not need every detail of what you’ve tried\n\n. . .\n\nReports and presentations should tell a story\nPlanning out your report/presentation can help\n\n. . .\n\nHold the audience’s attention with what needs to be said; do so effectively\nTell your audience why they should care; why it matters\nYou should explain your choices and the “why”",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "href": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "title": "08-effective-communication",
    "section": "Choose informative titles",
    "text": "Choose informative titles\nOn presentations: Balance b/w short and informative (goal: concise)\n. . .\n\nAvoid: “Analyzing NHANES”\n\nBetter: “Data from the NHANES study shows that diet is related to overall health”\n. . .\nOn visualizations: emphasize the take-home! (what’s learned or what action to take)\n. . .\n\nAvoid: “Boxplot of gender”\n\nBetter: “Twice as many females as males included for analysis”\n. . .\n\nAvoid: “Tickets vs. Time”\n\nBetter: “Staff unable to respond to incoming tickets; need to hire 2 FTEs”",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses",
    "href": "content/lectures/08-effective-communication.html#student-responses",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nconsider your audience\n\n\nspeak organizedly and logically\n\n\nA narrative format is preferable to an enumeration or a nonlinear presentation such as what would arise from reading off an infographic, for example.\n\n\nBe clear and direct!\n\n\nDon't say filler words like \"uhm\" \"like\". Take a pause instead\n\n\nConsciously speak slowly than you normally do (for fast talkers)\n\n\nspeak confidently and know material well enough to sound natural/not just memorize material\n\n\ninteract with the audience\n\n\nTalk slowly and clearly\n\n\nPut yourself in the shoes of your audience\n\n\nSpeak clearly at a good pace (not too fast or slow), make eye contact and engage with your audience\n\n\nEnunciation, proper volume, etc.\n\n\nI tend to speak in long sentences which can confuse the audience.\n\n\nTalk slower and clearer. Enunciation. Eye contact while talking. Avoid filler words.\n\n\nspeak clearly, slow down if you need to, don't just read off slides when presenting\n\n\nbe sure to point out areas of interest on your plots and explain them\n\n\nUse simply words if possible\n\n\nSpeaking slowly at someone\n\n\nDon't assume the audience know the same thing (like the research background or the research design) as you do. Another thing is: try to make sentence as simple as possible.\n\n\nTake moments to pause in between you sentences if you get lost.\n\n\nSpeak slowly and clearly\n\n\nKeep it engaging, involve audience participation, make eye contact, be confident\n\n\nTalk clearly and stick with the theme\n\n\nCater to your audience. Be conscious of what they know and don't know.\n\n\nUse appropriate font.\n\n\nSpeak clearly and slow down when you're picking up a fast pace.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "href": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "title": "08-effective-communication",
    "section": "Presentations are for listening",
    "text": "Presentations are for listening\n\nAdvantage: words to explain out loud what you’re showing\n\n. . .\n\nYou are presenting for the person in the back of the room.\n\n. . .\nTo accomplish:\n\ndon’t read directly off slides\nrepetition is ok: tell what you’re going to tell them, tell them, tell them what you told them\nuse animation to build your story (not to distract)\nintroduce your axes\ntext/labels larger\nwatch your speech speed\npractice!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "href": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "title": "08-effective-communication",
    "section": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood",
    "text": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood\n\nRed Riding Hood (RRH) has to walk 0.54 mi from Point A (home) to Point B (Grandma’s)\nRRH meets Wolf who (1) runs ahead to Grandma’s, (2) eats her, and (3) dresses in her clothes\nRRH arrives at Grandmas at 2PM, asks her three questions\nIdentified problem: after third question, Wolf eats RRH\nSolution: vendor (Woodsman) employs tool (ax)\nExpected outcome: Grandma and RRH alive, wolf is not",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-1",
    "href": "content/lectures/08-effective-communication.html#student-responses-1",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nWhen sending a status update on a project to people in my team, I often had a habit of over explaining things such as specific terms or what a specific p-value indicates, etc. and it was redundant to my team, who all knew what these terms mean and how they are defined. On the flip side, during a meeting with non-technical people, a lot of my team's work didn't make sense to some people in the meeting and they requested info in \"layman terms\". My mentor advised me to not over-explain terms in depth to technical people, but keep things simple, clear and concise to those without a technical background.\n\n\nNA\n\n\nBe as concise as possible while getting your point across\n\n\nno need to write full sentences for bullet points\n\n\nwrite in a concise manner, don’t use big words unless it’s relevant\n\n\nkeep things succinct and write in a neutral tone\n\n\nBold or italicize important ideas/ key words in long writing\n\n\nMain idea sentence in the beginning of your text (report, essay, email).\n\n\nRefrain from using the first person. Talk in the past-tense\n\n\nUse grammarly\n\n\nWatch repetition of certain words. Occasionally change the structure of sentences. Know your audience.\n\n\nDont be too repetitive and Don’t have run on long sentences and get caught up in the details too much - I do that a lot ://\n\n\nAvoid ambiguity, have someone else proof read to double check what you've written, try not to make your sentences too wordy.\n\n\nWrite for your audience, avoid overuse of jargon and if necessary be sure to define the terms in a way appropriate for how you’re actually using them.\n\n\nBe clear and concise with the points you're trying to make and don't lose them with sentences that run on for too long\n\n\nBe concise and use as few words to effectively get point across. Don't go off on tangents.\n\n\nOrganize using subheadings, highlight main points using bold or colors if appropriate, vary sentence structure\n\n\nmake your sentences simpler to understand\n\n\nWhen giving a status report to a technical team, no need to over-explain terms. It is a lot of times effective to make a concise bullet point list such as p value= x, correlation coefficinet = y, instead of overexplaining what each value means b/c a technical team probably would know the signifance anyways\n\n\nWrite in words that the readers will understand, and do not assume that the readers will know what you mean.\n\n\nuse an outline to help organize the order of your paper. it helps you figure out where to place images, plots, and text\n\n\nOrganize arguments, don't be overly repetitive",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "href": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "title": "08-effective-communication",
    "section": "Benefits of written communciation",
    "text": "Benefits of written communciation\nYour audience has time to process…but the explanation has to be there!\n. . .\nVisually: more on a single visualization\n. . .\nYes, often there are different visualizations for reports/papers than for presentations/lectures.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "href": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "title": "08-effective-communication",
    "section": "When you have time to digest (read)",
    "text": "When you have time to digest (read)\n\n. . .\n❓ What makes this an effective visualization for a written communication?”\nSource: Storytelling wtih data by cole nussbaumer knaflic",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#written-explanations",
    "href": "content/lectures/08-effective-communication.html#written-explanations",
    "title": "08-effective-communication",
    "section": "Written Explanations",
    "text": "Written Explanations\n\nVisualizations should be explained/interpreted\nModels should be explained\n\nshould be clear what question is being answered\nwhat conclusions is being drawn\nand what numbers were used to draw that conclusion",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "title": "08-effective-communication",
    "section": "Data Science Reports in .Rmd",
    "text": "Data Science Reports in .Rmd\n\nAs concise as possible\nNecessary details (for your audience); nothing more\n\nBe sure that the knit output contains what you intended (plots displayed; headers etc.)\n…and does NOT display stuff that doesn’t need to be there (messages/warnings suppressed, brainstorming, etc.)\n\nTypical Sections: Introduction/Background, Setup, Data, Analysis, Conclusion, References",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "href": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "title": "08-effective-communication",
    "section": "Controlling HTML document settings",
    "text": "Controlling HTML document settings\n\nTable of Contents\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n---\n. . .\n\nTheme\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    theme: united\n    highlight: tango\n---\n. . .\n\nFigure Options\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    fig_width: 7\n    fig_height: 6\n    fig_caption: true\n---\n. . .\n\nCode Folding\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    code_folding: hide\n---",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "href": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "title": "08-effective-communication",
    "section": "Controlling code chunk output",
    "text": "Controlling code chunk output\n\nSpecified in the curly braces, separated by commas\n\n. . .\n\neval: whether to execute the code chunk\necho: whether to include the code in the output\nwarning, message, and error: whether to show warnings, messages, or errors in the knit document\nfig.width and fig.height: control the width/height of plots\n\n. . .\n\nControlling for the whole document:\n\nknitr::opts_chunk$set(fig.width = 8, collapse = TRUE)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#editing-proofreading",
    "href": "content/lectures/08-effective-communication.html#editing-proofreading",
    "title": "08-effective-communication",
    "section": "Editing & Proofreading",
    "text": "Editing & Proofreading\n\nDid you end up telling a story?\n\nThings missing?\nThings to delete?\n\n\n. . .\n\nDo not fall in love with your words/code/plots\n\n. . .\n\nDo spell check\nDo read it over before sending/presenting/submitting",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "href": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "title": "08-effective-communication",
    "section": "Aside: Citing Sources",
    "text": "Aside: Citing Sources\nWhen are citations needed?\n. . .\n\n\n“We will be doing our analysis using two different data sets created by two different groups: Donohue and Mustard + Lott, or simply Lott”\n\n\n. . .\n\n\n“What turned from the idea of carrying firearms to protect oneself from enemies such as the British monarchy and the unknown frontier of North America has now become a nationwide issue.”\n\n\n. . .\n\n\n“Right to Carry Laws refer to laws that specify how citizens are allowed to carry concealed handguns when they’re away from home without a permit”\n\n\n. . .\n\n\n“In this case study, we are examining the relationship between unemployment rate, poverty rate, police staffing, and violent crime rate.”\n\n\n. . .\n\n\n“In the United States, the second amendment permits the right to bear arms, and this law has not been changed since its creation in 1791.”\n\n\n. . .\n\n\n“The Right to Carry Laws (RTC) is defined as”a law that specifies if and how citizens are allowed to have a firearm on their person or nearby in public.””\n\n\n. . .\nReminder: You do NOT get docked points for citing others’ work. You can be at risk of AI Violation if you don’t. When in doubt, give credit.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "title": "08-effective-communication",
    "section": "Footnotes in .Rmd",
    "text": "Footnotes in .Rmd\nHow to specify a footnote in text:\nHere is some body text.[^1]\nHow to include the footnote’s reference:\n[^1]: This footnote will appear at the bottom of the page.\n\n\nNote: .bib files can be included with BibTeX references using the bibliography parameter in your YAML",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-2",
    "href": "content/lectures/08-effective-communication.html#student-responses-2",
    "title": "08-effective-communication",
    "section": "Student Responses",
    "text": "Student Responses\n\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nDon’t make slides overly colorful\n\n\nno borders on plots, graphs.don't write the whole info on one slide. take advantage of white space\n\n\nuse images to help audience understand\n\n\nHighlight important points in visualization.\n\n\ndon’t write everything on slides, just main points, try to use pictures that model/reflect/support talking points\n\n\nuse a legend for graphs\n\n\nmake sure your plot is relevant to the point you are trying to make\n\n\nreduce the number of words on the slide\n\n\nIt should be easy to understand/digest relatively quickly, only put absolutely necessary/relevant things\n\n\nPick a font and size for body+headings and commit to it\n\n\nTry to keep the design minimalistic and aesthetic, no cognitive overload that way.\n\n\nTitle your plots & graphs\n\n\nImages/visuals should help strengthen your presentation/story, not distracting from it\n\n\nUse lots of pictures!\n\n\nIt's better to have meaningful and intuitive color selection.\n\n\nSpecific graphs are more beneficial to a technical audience, while others are better for a non-technical one. My coworkers like graphs such as boxplots, but when presenting to partners, I have found that they prefer more intuitive/popular graphs like histograms or line plots.\n\n\nComplementary colors, appropriate graphs for the type of information you have and want to get across, neat and not cluttered\n\n\nConcise and clear, use colors and space effectively\n\n\nUse color responsibly in graphs/tables, make text large enough for everyone in the room to see, don't overload slides with information\n\n\ndont put too much words\n\n\nDon’t put too many animations (if any)\n\n\nLess is more. Too much can distract and detract from the main point\n\n\ngood contrast color between background and text\n\n\nmake clear visual guide, don’t make it too complicated\n\n\nAvoid neon colors\n\n\nKeep accessibility in mind when presenting visuals. (e.g. using texture instead of color, image descriptions, etc.)\n\n\nMake presentations look cleaner. Seems like you know what youre talking about.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "href": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "title": "08-effective-communication",
    "section": "The Glamour of Graphics",
    "text": "The Glamour of Graphics\n\nbuilds on top of the grammar (components) of a graphic\nconsiderations for the design of a graphic\ncolor, typography, layout\ngoing from accurate to 😍effective\n\n\n\nThese ideas and slides are all modified from Will Chase’s rstudio::conf2020 slides/talk",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "href": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "title": "08-effective-communication",
    "section": "Left-align titles at top-left",
    "text": "Left-align titles at top-left\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "href": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "title": "08-effective-communication",
    "section": "Avoid head-tilting",
    "text": "Avoid head-tilting\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "href": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "title": "08-effective-communication",
    "section": "Borders & Backgrounds: 👎",
    "text": "Borders & Backgrounds: 👎\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_bw() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "href": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "title": "08-effective-communication",
    "section": "Organize & Remove/Lighten as much as possible",
    "text": "Organize & Remove/Lighten as much as possible\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#legends-suck",
    "href": "content/lectures/08-effective-communication.html#legends-suck",
    "title": "08-effective-communication",
    "section": "Legends suck",
    "text": "Legends suck\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 7) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 20) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#additional-guidance",
    "href": "content/lectures/08-effective-communication.html#additional-guidance",
    "title": "08-effective-communication",
    "section": "Additional Guidance",
    "text": "Additional Guidance\n\nWhite space is like garlic - take the amount you need and triple it\nFonts Matter\nUse Color Effectively",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#qa",
    "href": "content/lectures/08-effective-communication-slides.html#qa",
    "title": "08-effective-communication",
    "section": "Q&A",
    "text": "Q&A\n\nQ: for the last part of lecture 07, I tried glance(m_ht_wt) but it didn’t work because m_ht_wt doesn’t exist. Is “m_ht_wt” supposed be a model?\nA: Yup, this model was the height by weight model:\n\n\nm_ht_wt &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ Width_in, data = pp)\n\n\nQ: I was not too sure what was going on when talking about the relationship between painting height and school.\nA: I don’t think you were the only one confused! Briefly here (and I’m happy to chat more before/after class and in OH), we were looking to determine/quantify the relationship between the size (height) of a painting and the school from which the painting originated. This was an example of having more than two categories for a categorical (factor) predictor. The important points were undersatnding that each level is compared to the baseline and the linear model that results from multiple categories. Part 4 of the lab gets into this a bit more too. Definitely follow up if you’re unsure after doing that part of the lab!\n\n\nQ: How do you calculate the linear regression model when you have non-numeric values? For example, on lab 04, when it asks to calculate the linear regression model by gender, the gender appears only as male and female. Suppose male is 1 and female is 0 (interpreted by the function), then male linear regression model is y =ax + 1?\nA: Close! the “1” would be plugged in as the value of x (in what you suggested)m not for the intercept. So the function would be \\(y=\\beta_1*1 + \\beta_0\\)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#course-announcements",
    "href": "content/lectures/08-effective-communication-slides.html#course-announcements",
    "title": "08-effective-communication",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nLab04 due Friday\nHW02 due Monday\n\n\n\nPractice Midterms Now Available\n\nanswers posted next week\n\nMidterm Exam\n\nwill cover material through “Multiple Linear Regression”\nwill be released/posted next Friday after lab\nwill be due Monday Nov 6th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be completed individually (open Notes; open Internet)\n\n\n\n\n\nLink for Later",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#agenda",
    "href": "content/lectures/08-effective-communication-slides.html#agenda",
    "title": "08-effective-communication",
    "section": "Agenda",
    "text": "Agenda\n\nCommunicating for your audience\nOral Communication\nWritten Communication\nVisual Communication",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#suggested-reading",
    "href": "content/lectures/08-effective-communication-slides.html#suggested-reading",
    "title": "08-effective-communication",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nBookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#what-does-this-mean",
    "href": "content/lectures/08-effective-communication-slides.html#what-does-this-mean",
    "title": "08-effective-communication",
    "section": "What does this mean?",
    "text": "What does this mean?\n❓ What does it mean to “consider your audience?”\n\nSimply: You do the work so they don’t have to.\n\n\n…also the aesthetic-usability effect exists.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#whats-the-right-level",
    "href": "content/lectures/08-effective-communication-slides.html#whats-the-right-level",
    "title": "08-effective-communication",
    "section": "What’s the right level?",
    "text": "What’s the right level?\n\n\nGeneral Audience\n✔ background\n🚫 limit technical details\n🎉 emphasize take-home\n\n\n\nTechnical Audience\n⬇ limit background\n💻 all-the-details\n🎉 emphasize take-home",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#considerations",
    "href": "content/lectures/08-effective-communication-slides.html#considerations",
    "title": "08-effective-communication",
    "section": "Considerations",
    "text": "Considerations\n\nPlatform: written? oral?\n\n\n\nSetting: informal? formal?\n\n\n\n\nTiming: never go over your time limit!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#storytelling",
    "href": "content/lectures/08-effective-communication-slides.html#storytelling",
    "title": "08-effective-communication",
    "section": "Storytelling",
    "text": "Storytelling\n\nStories have a beginning, a middle, and an end.\n\n\n\nStories do not need every detail of what you’ve tried\n\n\n\n\nReports and presentations should tell a story\nPlanning out your report/presentation can help\n\n\n\n\nHold the audience’s attention with what needs to be said; do so effectively\nTell your audience why they should care; why it matters\nYou should explain your choices and the “why”",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#choose-informative-titles",
    "href": "content/lectures/08-effective-communication-slides.html#choose-informative-titles",
    "title": "08-effective-communication",
    "section": "Choose informative titles",
    "text": "Choose informative titles\nOn presentations: Balance b/w short and informative (goal: concise)\n\n\nAvoid: “Analyzing NHANES”\n\nBetter: “Data from the NHANES study shows that diet is related to overall health”\n\n\nOn visualizations: emphasize the take-home! (what’s learned or what action to take)\n\n\n\nAvoid: “Boxplot of gender”\n\nBetter: “Twice as many females as males included for analysis”\n\n\n\nAvoid: “Tickets vs. Time”\n\nBetter: “Staff unable to respond to incoming tickets; need to hire 2 FTEs”",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nconsider your audience\n\n\nspeak organizedly and logically\n\n\nA narrative format is preferable to an enumeration or a nonlinear presentation such as what would arise from reading off an infographic, for example.\n\n\nBe clear and direct!\n\n\nDon't say filler words like \"uhm\" \"like\". Take a pause instead\n\n\nConsciously speak slowly than you normally do (for fast talkers)\n\n\nspeak confidently and know material well enough to sound natural/not just memorize material\n\n\ninteract with the audience\n\n\nTalk slowly and clearly\n\n\nPut yourself in the shoes of your audience\n\n\nSpeak clearly at a good pace (not too fast or slow), make eye contact and engage with your audience\n\n\nEnunciation, proper volume, etc.\n\n\nI tend to speak in long sentences which can confuse the audience.\n\n\nTalk slower and clearer. Enunciation. Eye contact while talking. Avoid filler words.\n\n\nspeak clearly, slow down if you need to, don't just read off slides when presenting\n\n\nbe sure to point out areas of interest on your plots and explain them\n\n\nUse simply words if possible\n\n\nSpeaking slowly at someone\n\n\nDon't assume the audience know the same thing (like the research background or the research design) as you do. Another thing is: try to make sentence as simple as possible.\n\n\nTake moments to pause in between you sentences if you get lost.\n\n\nSpeak slowly and clearly\n\n\nKeep it engaging, involve audience participation, make eye contact, be confident\n\n\nTalk clearly and stick with the theme\n\n\nCater to your audience. Be conscious of what they know and don't know.\n\n\nUse appropriate font.\n\n\nSpeak clearly and slow down when you're picking up a fast pace.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#presentations-are-for-listening",
    "href": "content/lectures/08-effective-communication-slides.html#presentations-are-for-listening",
    "title": "08-effective-communication",
    "section": "Presentations are for listening",
    "text": "Presentations are for listening\n\nAdvantage: words to explain out loud what you’re showing\n\n\n\nYou are presenting for the person in the back of the room.\n\n\n\nTo accomplish:\n\ndon’t read directly off slides\nrepetition is ok: tell what you’re going to tell them, tell them, tell them what you told them\nuse animation to build your story (not to distract)\nintroduce your axes\ntext/labels larger\nwatch your speech speed\npractice!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "href": "content/lectures/08-effective-communication-slides.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "title": "08-effective-communication",
    "section": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood",
    "text": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood\n\nRed Riding Hood (RRH) has to walk 0.54 mi from Point A (home) to Point B (Grandma’s)\nRRH meets Wolf who (1) runs ahead to Grandma’s, (2) eats her, and (3) dresses in her clothes\nRRH arrives at Grandmas at 2PM, asks her three questions\nIdentified problem: after third question, Wolf eats RRH\nSolution: vendor (Woodsman) employs tool (ax)\nExpected outcome: Grandma and RRH alive, wolf is not",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses-1",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses-1",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nWhen sending a status update on a project to people in my team, I often had a habit of over explaining things such as specific terms or what a specific p-value indicates, etc. and it was redundant to my team, who all knew what these terms mean and how they are defined. On the flip side, during a meeting with non-technical people, a lot of my team's work didn't make sense to some people in the meeting and they requested info in \"layman terms\". My mentor advised me to not over-explain terms in depth to technical people, but keep things simple, clear and concise to those without a technical background.\n\n\nNA\n\n\nBe as concise as possible while getting your point across\n\n\nno need to write full sentences for bullet points\n\n\nwrite in a concise manner, don’t use big words unless it’s relevant\n\n\nkeep things succinct and write in a neutral tone\n\n\nBold or italicize important ideas/ key words in long writing\n\n\nMain idea sentence in the beginning of your text (report, essay, email).\n\n\nRefrain from using the first person. Talk in the past-tense\n\n\nUse grammarly\n\n\nWatch repetition of certain words. Occasionally change the structure of sentences. Know your audience.\n\n\nDont be too repetitive and Don’t have run on long sentences and get caught up in the details too much - I do that a lot ://\n\n\nAvoid ambiguity, have someone else proof read to double check what you've written, try not to make your sentences too wordy.\n\n\nWrite for your audience, avoid overuse of jargon and if necessary be sure to define the terms in a way appropriate for how you’re actually using them.\n\n\nBe clear and concise with the points you're trying to make and don't lose them with sentences that run on for too long\n\n\nBe concise and use as few words to effectively get point across. Don't go off on tangents.\n\n\nOrganize using subheadings, highlight main points using bold or colors if appropriate, vary sentence structure\n\n\nmake your sentences simpler to understand\n\n\nWhen giving a status report to a technical team, no need to over-explain terms. It is a lot of times effective to make a concise bullet point list such as p value= x, correlation coefficinet = y, instead of overexplaining what each value means b/c a technical team probably would know the signifance anyways\n\n\nWrite in words that the readers will understand, and do not assume that the readers will know what you mean.\n\n\nuse an outline to help organize the order of your paper. it helps you figure out where to place images, plots, and text\n\n\nOrganize arguments, don't be overly repetitive",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#benefits-of-written-communciation",
    "href": "content/lectures/08-effective-communication-slides.html#benefits-of-written-communciation",
    "title": "08-effective-communication",
    "section": "Benefits of written communciation",
    "text": "Benefits of written communciation\nYour audience has time to process…but the explanation has to be there!\n\nVisually: more on a single visualization\n\n\nYes, often there are different visualizations for reports/papers than for presentations/lectures.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#when-you-have-time-to-digest-read",
    "href": "content/lectures/08-effective-communication-slides.html#when-you-have-time-to-digest-read",
    "title": "08-effective-communication",
    "section": "When you have time to digest (read)",
    "text": "When you have time to digest (read)\n\n\n❓ What makes this an effective visualization for a written communication?”\nSource: Storytelling wtih data by cole nussbaumer knaflic",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#written-explanations",
    "href": "content/lectures/08-effective-communication-slides.html#written-explanations",
    "title": "08-effective-communication",
    "section": "Written Explanations",
    "text": "Written Explanations\n\nVisualizations should be explained/interpreted\nModels should be explained\n\nshould be clear what question is being answered\nwhat conclusions is being drawn\nand what numbers were used to draw that conclusion",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#data-science-reports-in-.rmd",
    "href": "content/lectures/08-effective-communication-slides.html#data-science-reports-in-.rmd",
    "title": "08-effective-communication",
    "section": "Data Science Reports in .Rmd",
    "text": "Data Science Reports in .Rmd\n\nAs concise as possible\nNecessary details (for your audience); nothing more\n\nBe sure that the knit output contains what you intended (plots displayed; headers etc.)\n…and does NOT display stuff that doesn’t need to be there (messages/warnings suppressed, brainstorming, etc.)\n\nTypical Sections: Introduction/Background, Setup, Data, Analysis, Conclusion, References",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#controlling-html-document-settings",
    "href": "content/lectures/08-effective-communication-slides.html#controlling-html-document-settings",
    "title": "08-effective-communication",
    "section": "Controlling HTML document settings",
    "text": "Controlling HTML document settings\n\nTable of Contents\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\nTheme\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    theme: united\n    highlight: tango\n---\n\n\n\nFigure Options\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    fig_width: 7\n    fig_height: 6\n    fig_caption: true\n---\n\n\n\nCode Folding\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    code_folding: hide\n---",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#controlling-code-chunk-output",
    "href": "content/lectures/08-effective-communication-slides.html#controlling-code-chunk-output",
    "title": "08-effective-communication",
    "section": "Controlling code chunk output",
    "text": "Controlling code chunk output\n\nSpecified in the curly braces, separated by commas\n\n\n\neval: whether to execute the code chunk\necho: whether to include the code in the output\nwarning, message, and error: whether to show warnings, messages, or errors in the knit document\nfig.width and fig.height: control the width/height of plots\n\n\n\n\nControlling for the whole document:\n\nknitr::opts_chunk$set(fig.width = 8, collapse = TRUE)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#editing-proofreading",
    "href": "content/lectures/08-effective-communication-slides.html#editing-proofreading",
    "title": "08-effective-communication",
    "section": "Editing & Proofreading",
    "text": "Editing & Proofreading\n\nDid you end up telling a story?\n\nThings missing?\nThings to delete?\n\n\n\n\nDo not fall in love with your words/code/plots\n\n\n\n\nDo spell check\nDo read it over before sending/presenting/submitting",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#aside-citing-sources",
    "href": "content/lectures/08-effective-communication-slides.html#aside-citing-sources",
    "title": "08-effective-communication",
    "section": "Aside: Citing Sources",
    "text": "Aside: Citing Sources\nWhen are citations needed?\n\n\n\n“We will be doing our analysis using two different data sets created by two different groups: Donohue and Mustard + Lott, or simply Lott”\n\n\n\n\n\n\n“What turned from the idea of carrying firearms to protect oneself from enemies such as the British monarchy and the unknown frontier of North America has now become a nationwide issue.”\n\n\n\n\n\n\n“Right to Carry Laws refer to laws that specify how citizens are allowed to carry concealed handguns when they’re away from home without a permit”\n\n\n\n\n\n\n“In this case study, we are examining the relationship between unemployment rate, poverty rate, police staffing, and violent crime rate.”\n\n\n\n\n\n\n“In the United States, the second amendment permits the right to bear arms, and this law has not been changed since its creation in 1791.”\n\n\n\n\n\n\n“The Right to Carry Laws (RTC) is defined as”a law that specifies if and how citizens are allowed to have a firearm on their person or nearby in public.””\n\n\n\n\nReminder: You do NOT get docked points for citing others’ work. You can be at risk of AI Violation if you don’t. When in doubt, give credit.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#footnotes-in-.rmd",
    "href": "content/lectures/08-effective-communication-slides.html#footnotes-in-.rmd",
    "title": "08-effective-communication",
    "section": "Footnotes in .Rmd",
    "text": "Footnotes in .Rmd\nHow to specify a footnote in text:\nHere is some body text.[^1]\nHow to include the footnote’s reference:\n[^1]: This footnote will appear at the bottom of the page.\n\n\nNote: .bib files can be included with BibTeX references using the bibliography parameter in your YAML",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses-2",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses-2",
    "title": "08-effective-communication",
    "section": "Student Responses",
    "text": "Student Responses\n\n\n\n\n\n\n\nAdvice you've received\n\n\n\n\nDon’t make slides overly colorful\n\n\nno borders on plots, graphs.don't write the whole info on one slide. take advantage of white space\n\n\nuse images to help audience understand\n\n\nHighlight important points in visualization.\n\n\ndon’t write everything on slides, just main points, try to use pictures that model/reflect/support talking points\n\n\nuse a legend for graphs\n\n\nmake sure your plot is relevant to the point you are trying to make\n\n\nreduce the number of words on the slide\n\n\nIt should be easy to understand/digest relatively quickly, only put absolutely necessary/relevant things\n\n\nPick a font and size for body+headings and commit to it\n\n\nTry to keep the design minimalistic and aesthetic, no cognitive overload that way.\n\n\nTitle your plots & graphs\n\n\nImages/visuals should help strengthen your presentation/story, not distracting from it\n\n\nUse lots of pictures!\n\n\nIt's better to have meaningful and intuitive color selection.\n\n\nSpecific graphs are more beneficial to a technical audience, while others are better for a non-technical one. My coworkers like graphs such as boxplots, but when presenting to partners, I have found that they prefer more intuitive/popular graphs like histograms or line plots.\n\n\nComplementary colors, appropriate graphs for the type of information you have and want to get across, neat and not cluttered\n\n\nConcise and clear, use colors and space effectively\n\n\nUse color responsibly in graphs/tables, make text large enough for everyone in the room to see, don't overload slides with information\n\n\ndont put too much words\n\n\nDon’t put too many animations (if any)\n\n\nLess is more. Too much can distract and detract from the main point\n\n\ngood contrast color between background and text\n\n\nmake clear visual guide, don’t make it too complicated\n\n\nAvoid neon colors\n\n\nKeep accessibility in mind when presenting visuals. (e.g. using texture instead of color, image descriptions, etc.)\n\n\nMake presentations look cleaner. Seems like you know what youre talking about.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#the-glamour-of-graphics",
    "href": "content/lectures/08-effective-communication-slides.html#the-glamour-of-graphics",
    "title": "08-effective-communication",
    "section": "The Glamour of Graphics",
    "text": "The Glamour of Graphics\n\nbuilds on top of the grammar (components) of a graphic\nconsiderations for the design of a graphic\ncolor, typography, layout\ngoing from accurate to 😍effective\n\n\n\nThese ideas and slides are all modified from Will Chase’s rstudio::conf2020 slides/talk",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#left-align-titles-at-top-left",
    "href": "content/lectures/08-effective-communication-slides.html#left-align-titles-at-top-left",
    "title": "08-effective-communication",
    "section": "Left-align titles at top-left",
    "text": "Left-align titles at top-left\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#avoid-head-tilting",
    "href": "content/lectures/08-effective-communication-slides.html#avoid-head-tilting",
    "title": "08-effective-communication",
    "section": "Avoid head-tilting",
    "text": "Avoid head-tilting\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#borders-backgrounds",
    "href": "content/lectures/08-effective-communication-slides.html#borders-backgrounds",
    "title": "08-effective-communication",
    "section": "Borders & Backgrounds: 👎",
    "text": "Borders & Backgrounds: 👎\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_bw() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#organize-removelighten-as-much-as-possible",
    "href": "content/lectures/08-effective-communication-slides.html#organize-removelighten-as-much-as-possible",
    "title": "08-effective-communication",
    "section": "Organize & Remove/Lighten as much as possible",
    "text": "Organize & Remove/Lighten as much as possible\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#legends-suck",
    "href": "content/lectures/08-effective-communication-slides.html#legends-suck",
    "title": "08-effective-communication",
    "section": "Legends suck",
    "text": "Legends suck\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 7) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 20) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#additional-guidance",
    "href": "content/lectures/08-effective-communication-slides.html#additional-guidance",
    "title": "08-effective-communication",
    "section": "Additional Guidance",
    "text": "Additional Guidance\n\nWhite space is like garlic - take the amount you need and triple it\nFonts Matter\nUse Color Effectively\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "08-effective-communication"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html",
    "href": "content/lectures/10-projects.html",
    "title": "10-projects",
    "section": "",
    "text": "Coming Soon\n\n\n\nComing Soon\n\n\n\n\nCase Studies\nFinal Project",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#qa",
    "href": "content/lectures/10-projects.html#qa",
    "title": "10-projects",
    "section": "",
    "text": "Coming Soon",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#course-announcements",
    "href": "content/lectures/10-projects.html#course-announcements",
    "title": "10-projects",
    "section": "",
    "text": "Coming Soon",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#agenda",
    "href": "content/lectures/10-projects.html#agenda",
    "title": "10-projects",
    "section": "",
    "text": "Case Studies\nFinal Project",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#biomarkers-of-recent-use",
    "href": "content/lectures/10-projects.html#biomarkers-of-recent-use",
    "title": "10-projects",
    "section": "Biomarkers of Recent Use",
    "text": "Biomarkers of Recent Use\n\nWe’ll use data from this paper:\n\n\nHubbard et al. Biomarkers of Recent Cannabis Use in Blood, Oral Fluid and Breath. Journal of Analytical Toxicology. 2021. Link to paper.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#opencasestudies",
    "href": "content/lectures/10-projects.html#opencasestudies",
    "title": "10-projects",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\n\nOpenCaseStudies\nUses R/the tidyverse\nasks public health-centric questions\ngoal: to teach statistical analysis/data science through case studies",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#what-well-do",
    "href": "content/lectures/10-projects.html#what-well-do",
    "title": "10-projects",
    "section": "What We’ll Do",
    "text": "What We’ll Do\nFor each case study (2), during lecture:\n\nStats: (1-2d)\nBackground, Data & Wrangling (1-2d)\nEDA & Analysis (1-2d)\n\n. . .\nFor each case study:\n\nyou’ll also work with case study data in lab.\nyou’ll work in assigned groups of ~3 students to complete a data science report\n\n. . .\nI will share previous student examples and we’ll discuss pros and cons in coming lectures.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#data-science-reports",
    "href": "content/lectures/10-projects.html#data-science-reports",
    "title": "10-projects",
    "section": "Data Science Reports",
    "text": "Data Science Reports\nWith your group, you will:\n\ncarry out all steps of the analysis\n\nsome code will be taken directly from lecture\n\nadd text/organize into a report\n\n. . .\n\nhave to extend the case study\n\n. . .\nThis should be written at the level of a data science-knowledgeable undergrad.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#general-communication-submission",
    "href": "content/lectures/10-projects.html#general-communication-submission",
    "title": "10-projects",
    "section": "General Communication Submission",
    "text": "General Communication Submission\nThis is (intentionally) very open-ended.\nYou need to communicate the most important aspect/finding/part(s) of your case study to a general audience (any undergrad).\n. . .\nWhat might this look like?\n\nshort TikTok like video\nbrief Youtube video\nslides for an Instagram post\nX (Twitter) thread\nposter to be displayed next to an elevator\nposter to be put on public bulletin boards\neffective email communication",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#what-does-extend-the-case-study-mean",
    "href": "content/lectures/10-projects.html#what-does-extend-the-case-study-mean",
    "title": "10-projects",
    "section": "What does extend the case study mean?",
    "text": "What does extend the case study mean?\nYou’ll need to do something more on the topic beyond what is presented in class.\n. . .\nExamples:\n\nAsking an additional question and answering it from the data provided\nFinding an additional dataset and using it to add to the case study\nGenerating a handful of additional and very informative visualizations (beyond what’s presented in class)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#grading",
    "href": "content/lectures/10-projects.html#grading",
    "title": "10-projects",
    "section": "Grading",
    "text": "Grading\nGraded on:\n\ncontent (code, text, viz)\nreport: effective written communication (clarity/content &gt; grammar/spelling)\n\nextension carried out\n\neffective general communication (effectively conveys message to a general audience)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#final-project-logistics",
    "href": "content/lectures/10-projects.html#final-project-logistics",
    "title": "10-projects",
    "section": "Final Project Logistics",
    "text": "Final Project Logistics\n\nwill be completed in groups of 3-4 students\nyou get to choose the group\nI will ask Monday week 7 for your final project groups (If you are not in one, I will help)\nYou will submit a proposal week 8.\nFinal projects are due during Finals week",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#what-is-the-final-project-proposal",
    "href": "content/lectures/10-projects.html#what-is-the-final-project-proposal",
    "title": "10-projects",
    "section": "What is the final project proposal?",
    "text": "What is the final project proposal?\n\nA short Google Form\nyou’ll submit your topic and a few details about that topic (depending upon which option you choose)\nYour idea can change after you submit your proposal\nThis has been added to help you start your project before finals week.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#final-project-details",
    "href": "content/lectures/10-projects.html#final-project-details",
    "title": "10-projects",
    "section": "Final Project Details",
    "text": "Final Project Details\nTwo possible paths:\n\nCreate a technical presentation on a statistics topic and/or an R package.\nCarry out a data analysis",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#option-1-technical-presentation",
    "href": "content/lectures/10-projects.html#option-1-technical-presentation",
    "title": "10-projects",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\n\n.Rmd document used to make slides\n“Teaches” the details of the R package/statistics topic\nDemonstrates how to use the package and/or carry out the statistical analysis in R\nTopic/Package must go beyond what was taught in this course or what you should have learned in an intro stats course\nPresentation Length: 10-15min",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#option-2-data-analysis",
    "href": "content/lectures/10-projects.html#option-2-data-analysis",
    "title": "10-projects",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\n\n.Rmd document used for data science report\nAsks a question, finds data, analyzes data (basically: a mini case report, but you find the data and formulate the question)\nPresentation Length: 3-5min (brief summary of the full report)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#wherewhen-for-this-presentation",
    "href": "content/lectures/10-projects.html#wherewhen-for-this-presentation",
    "title": "10-projects",
    "section": "Where/when for this presentation?",
    "text": "Where/when for this presentation?\n\nSubmit by Tues of finals week at 11:59 PM",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#should-i-be-working-on-my-final-project-now",
    "href": "content/lectures/10-projects.html#should-i-be-working-on-my-final-project-now",
    "title": "10-projects",
    "section": "Should I be working on my final project now?",
    "text": "Should I be working on my final project now?\n…probably not\n. . .\nBut, you should start thinking about/getting a group of 3-4 people together. You’ll need to submit who’s in your final project group Monday of week 7.\n. . .\nYou’ll need to have a general plan for your final project around wk 8. You’ll submit a “proposal” Monday of week 8.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects.html#what-is-the-general-audience-communication",
    "href": "content/lectures/10-projects.html#what-is-the-general-audience-communication",
    "title": "10-projects",
    "section": "What is the “general audience” communication?",
    "text": "What is the “general audience” communication?\nConsider who the audience would be -&gt; design for them\n. . .\nFor example, if you present on an R package, who would benefit from knowing about this package? How would you reach them? What can you design to inform them of what it is and get them to use it?\n. . .\nOr if you do a data analysis on a particular topic, what would you want others to know? How would you communicate that?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#qa",
    "href": "content/lectures/10-projects-slides.html#qa",
    "title": "10-projects",
    "section": "Q&A",
    "text": "Q&A\nComing Soon",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#course-announcements",
    "href": "content/lectures/10-projects-slides.html#course-announcements",
    "title": "10-projects",
    "section": "Course Announcements",
    "text": "Course Announcements\nComing Soon",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#agenda",
    "href": "content/lectures/10-projects-slides.html#agenda",
    "title": "10-projects",
    "section": "Agenda",
    "text": "Agenda\n\nCase Studies\nFinal Project",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#biomarkers-of-recent-use",
    "href": "content/lectures/10-projects-slides.html#biomarkers-of-recent-use",
    "title": "10-projects",
    "section": "Biomarkers of Recent Use",
    "text": "Biomarkers of Recent Use\n\nWe’ll use data from this paper:\n\n\nHubbard et al. Biomarkers of Recent Cannabis Use in Blood, Oral Fluid and Breath. Journal of Analytical Toxicology. 2021. Link to paper.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#opencasestudies",
    "href": "content/lectures/10-projects-slides.html#opencasestudies",
    "title": "10-projects",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\n\nOpenCaseStudies\nUses R/the tidyverse\nasks public health-centric questions\ngoal: to teach statistical analysis/data science through case studies",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-well-do",
    "href": "content/lectures/10-projects-slides.html#what-well-do",
    "title": "10-projects",
    "section": "What We’ll Do",
    "text": "What We’ll Do\nFor each case study (2), during lecture:\n\nStats: (1-2d)\nBackground, Data & Wrangling (1-2d)\nEDA & Analysis (1-2d)\n\n\nFor each case study:\n\nyou’ll also work with case study data in lab.\nyou’ll work in assigned groups of ~3 students to complete a data science report\n\n\n\nI will share previous student examples and we’ll discuss pros and cons in coming lectures.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#data-science-reports",
    "href": "content/lectures/10-projects-slides.html#data-science-reports",
    "title": "10-projects",
    "section": "Data Science Reports",
    "text": "Data Science Reports\nWith your group, you will:\n\ncarry out all steps of the analysis\n\nsome code will be taken directly from lecture\n\nadd text/organize into a report\n\n\n\nhave to extend the case study\n\n\n\nThis should be written at the level of a data science-knowledgeable undergrad.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#general-communication-submission",
    "href": "content/lectures/10-projects-slides.html#general-communication-submission",
    "title": "10-projects",
    "section": "General Communication Submission",
    "text": "General Communication Submission\nThis is (intentionally) very open-ended.\nYou need to communicate the most important aspect/finding/part(s) of your case study to a general audience (any undergrad).\n\nWhat might this look like?\n\nshort TikTok like video\nbrief Youtube video\nslides for an Instagram post\nX (Twitter) thread\nposter to be displayed next to an elevator\nposter to be put on public bulletin boards\neffective email communication",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-does-extend-the-case-study-mean",
    "href": "content/lectures/10-projects-slides.html#what-does-extend-the-case-study-mean",
    "title": "10-projects",
    "section": "What does extend the case study mean?",
    "text": "What does extend the case study mean?\nYou’ll need to do something more on the topic beyond what is presented in class.\n\nExamples:\n\nAsking an additional question and answering it from the data provided\nFinding an additional dataset and using it to add to the case study\nGenerating a handful of additional and very informative visualizations (beyond what’s presented in class)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#grading",
    "href": "content/lectures/10-projects-slides.html#grading",
    "title": "10-projects",
    "section": "Grading",
    "text": "Grading\nGraded on:\n\ncontent (code, text, viz)\nreport: effective written communication (clarity/content &gt; grammar/spelling)\n\nextension carried out\n\neffective general communication (effectively conveys message to a general audience)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#final-project-logistics",
    "href": "content/lectures/10-projects-slides.html#final-project-logistics",
    "title": "10-projects",
    "section": "Final Project Logistics",
    "text": "Final Project Logistics\n\nwill be completed in groups of 3-4 students\nyou get to choose the group\nI will ask Monday week 7 for your final project groups (If you are not in one, I will help)\nYou will submit a proposal week 8.\nFinal projects are due during Finals week",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-is-the-final-project-proposal",
    "href": "content/lectures/10-projects-slides.html#what-is-the-final-project-proposal",
    "title": "10-projects",
    "section": "What is the final project proposal?",
    "text": "What is the final project proposal?\n\nA short Google Form\nyou’ll submit your topic and a few details about that topic (depending upon which option you choose)\nYour idea can change after you submit your proposal\nThis has been added to help you start your project before finals week.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#final-project-details",
    "href": "content/lectures/10-projects-slides.html#final-project-details",
    "title": "10-projects",
    "section": "Final Project Details",
    "text": "Final Project Details\nTwo possible paths:\n\nCreate a technical presentation on a statistics topic and/or an R package.\nCarry out a data analysis",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#option-1-technical-presentation",
    "href": "content/lectures/10-projects-slides.html#option-1-technical-presentation",
    "title": "10-projects",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\n\n.Rmd document used to make slides\n“Teaches” the details of the R package/statistics topic\nDemonstrates how to use the package and/or carry out the statistical analysis in R\nTopic/Package must go beyond what was taught in this course or what you should have learned in an intro stats course\nPresentation Length: 10-15min",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#option-2-data-analysis",
    "href": "content/lectures/10-projects-slides.html#option-2-data-analysis",
    "title": "10-projects",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\n\n.Rmd document used for data science report\nAsks a question, finds data, analyzes data (basically: a mini case report, but you find the data and formulate the question)\nPresentation Length: 3-5min (brief summary of the full report)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#wherewhen-for-this-presentation",
    "href": "content/lectures/10-projects-slides.html#wherewhen-for-this-presentation",
    "title": "10-projects",
    "section": "Where/when for this presentation?",
    "text": "Where/when for this presentation?\n\nSubmit by Tues of finals week at 11:59 PM",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#should-i-be-working-on-my-final-project-now",
    "href": "content/lectures/10-projects-slides.html#should-i-be-working-on-my-final-project-now",
    "title": "10-projects",
    "section": "Should I be working on my final project now?",
    "text": "Should I be working on my final project now?\n…probably not\n\nBut, you should start thinking about/getting a group of 3-4 people together. You’ll need to submit who’s in your final project group Monday of week 7.\n\n\nYou’ll need to have a general plan for your final project around wk 8. You’ll submit a “proposal” Monday of week 8.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/10-projects-slides.html#what-is-the-general-audience-communication",
    "href": "content/lectures/10-projects-slides.html#what-is-the-general-audience-communication",
    "title": "10-projects",
    "section": "What is the “general audience” communication?",
    "text": "What is the “general audience” communication?\nConsider who the audience would be -&gt; design for them\n\nFor example, if you present on an R package, who would benefit from knowing about this package? How would you reach them? What can you design to inform them of what it is and get them to use it?\n\n\nOr if you do a data analysis on a particular topic, what would you want others to know? How would you communicate that?\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "10-projects"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html",
    "href": "content/lectures/07-linear-models.html",
    "title": "07-linear-models",
    "section": "",
    "text": "Q: I was just wondering if you could still provide those videos you talked about with like syntax stuff so that we can follow along. I would also appreciate if it was really through with like a walk through of how you got to each point\nA: I like this suggestion, and I’ll try to make these. No promises though. Just have to find magical time in the schedule to make them. Note that lectures will be the more detailed walk through with more time on your own to figure it out (rather than detailed walk through outside of class), so you will get the information…the time/location will just differ. But, I’ll try to make some supplemental videos for those interested.\n\n\nQ: Had a question about the modeling and the last bit showing how decreasing the alpha showed greater clustering at the bottom left corner. If the bottom left corner looks like 0 height and 0 width, how does that translate into the dimensions of an actual painting?\nA: Good observation. The follow-up to this is…are there any paintings with zero width or zero height? And, if you dig in the data (i.e. min(pp$Height_in, na.rm=TRUE)), you’ll see that there are some very small paintings, but that none are zero.\n\n\nQ: If we want to built our own model, can we plot them with ggplot2?\nA: Yup! This post starts to get at that. It does so for a linear model, but the logic follows for other models.\n\n\nQ: For the paintings dataset, how could we perform EDA on specific subject matter, like seeing how many portraits include Jesus as part of the subject?\nA: Love this question. There is a whole field of natural language processing that would have sophisticated ways to analyze this. A simple first pass would be to, for example, filter for paintings that include “Jesus” in the subject column.\n\n\nQ: I think the segmented bar plots based on proportion seem difficult to read. I’m not sure why we should be using this instead of the stacked plots?\nA: Grouped bar blots are typically most quickly understood. Proportion stacked plots are then easiest to understand proportion across categories. Stacked plots of raw numbers take longer (for most) to understand and thus are often avoided, but like all viz, it depends on context and audience.\n\n\nQ: In the last lecture, we talked about how segmented bar plots might not be the ideal choice for data visualization, but we still demonstrated them in today’s lecture. So, specifically in what cases should we choose segmented bar plots as a data analysis tool?\nA: They can be helpful when the audience is familiar with them, but typically are most helpful when you want to display relative proportions rather than counts\n\n\n\n\nDue Dates:\n\nLab 04 due Friday\n\nModel Interpretations\nText, code, & viz all matter\n\nLecture Participation survey “due” after class\nHW02 due Monday (10/30; 11:59 PM)\ndiscuss displaying image in Markdown\n\n\n\n\n\nLinear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & &gt;2 levels)\nresiduals\ndata transformations\n\n\n\n\n\n\nR4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#qa",
    "href": "content/lectures/07-linear-models.html#qa",
    "title": "07-linear-models",
    "section": "",
    "text": "Q: I was just wondering if you could still provide those videos you talked about with like syntax stuff so that we can follow along. I would also appreciate if it was really through with like a walk through of how you got to each point\nA: I like this suggestion, and I’ll try to make these. No promises though. Just have to find magical time in the schedule to make them. Note that lectures will be the more detailed walk through with more time on your own to figure it out (rather than detailed walk through outside of class), so you will get the information…the time/location will just differ. But, I’ll try to make some supplemental videos for those interested.\n\n\nQ: Had a question about the modeling and the last bit showing how decreasing the alpha showed greater clustering at the bottom left corner. If the bottom left corner looks like 0 height and 0 width, how does that translate into the dimensions of an actual painting?\nA: Good observation. The follow-up to this is…are there any paintings with zero width or zero height? And, if you dig in the data (i.e. min(pp$Height_in, na.rm=TRUE)), you’ll see that there are some very small paintings, but that none are zero.\n\n\nQ: If we want to built our own model, can we plot them with ggplot2?\nA: Yup! This post starts to get at that. It does so for a linear model, but the logic follows for other models.\n\n\nQ: For the paintings dataset, how could we perform EDA on specific subject matter, like seeing how many portraits include Jesus as part of the subject?\nA: Love this question. There is a whole field of natural language processing that would have sophisticated ways to analyze this. A simple first pass would be to, for example, filter for paintings that include “Jesus” in the subject column.\n\n\nQ: I think the segmented bar plots based on proportion seem difficult to read. I’m not sure why we should be using this instead of the stacked plots?\nA: Grouped bar blots are typically most quickly understood. Proportion stacked plots are then easiest to understand proportion across categories. Stacked plots of raw numbers take longer (for most) to understand and thus are often avoided, but like all viz, it depends on context and audience.\n\n\nQ: In the last lecture, we talked about how segmented bar plots might not be the ideal choice for data visualization, but we still demonstrated them in today’s lecture. So, specifically in what cases should we choose segmented bar plots as a data analysis tool?\nA: They can be helpful when the audience is familiar with them, but typically are most helpful when you want to display relative proportions rather than counts",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#course-announcements",
    "href": "content/lectures/07-linear-models.html#course-announcements",
    "title": "07-linear-models",
    "section": "",
    "text": "Due Dates:\n\nLab 04 due Friday\n\nModel Interpretations\nText, code, & viz all matter\n\nLecture Participation survey “due” after class\nHW02 due Monday (10/30; 11:59 PM)\ndiscuss displaying image in Markdown",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#agenda",
    "href": "content/lectures/07-linear-models.html#agenda",
    "title": "07-linear-models",
    "section": "",
    "text": "Linear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & &gt;2 levels)\nresiduals\ndata transformations",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#suggested-reading",
    "href": "content/lectures/07-linear-models.html#suggested-reading",
    "title": "07-linear-models",
    "section": "",
    "text": "R4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings\n\npp &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nNumber of observations: 3393\nNumber of variables: 61",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "title": "07-linear-models",
    "section": "Goal: Predict height from width",
    "text": "Goal: Predict height from width\n\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#tidymodels-1",
    "href": "content/lectures/07-linear-models.html#tidymodels-1",
    "title": "07-linear-models",
    "section": "tidymodels",
    "text": "tidymodels\n\nNOT a core tidyverse package\nfollows the structure of a tidyverse package\n\n\n\n\n\n\n\n\n\n\n\n# should already be installed for you on datahub\nlibrary(tidymodels)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-1-specify-model",
    "href": "content/lectures/07-linear-models.html#step-1-specify-model",
    "title": "07-linear-models",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "href": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "title": "07-linear-models",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "href": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "title": "07-linear-models",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\n… using formula syntax\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ Width_in, data = pp)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "title": "07-linear-models",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n\n\n\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "title": "07-linear-models",
    "section": "A tidy look at model output",
    "text": "A tidy look at model output\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ Width_in, data = pp) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n\n\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#slope-and-intercept",
    "href": "content/lectures/07-linear-models.html#slope-and-intercept",
    "title": "07-linear-models",
    "section": "Slope and intercept",
    "text": "Slope and intercept\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]\n. . .\n\nSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n. . .\n\nIntercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "href": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "title": "07-linear-models",
    "section": "Correlation does not imply causation",
    "text": "Correlation does not imply causation\nRemember this when interpreting model coefficients\n\n\n\n\n\n\n\n\n\n\n\nSource: XKCD, Cell phones",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "href": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "title": "07-linear-models",
    "section": "Linear model with a single predictor",
    "text": "Linear model with a single predictor\n\nWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\n\n\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n. . .\n\nTough luck, you can’t have them…\n\n. . .\n\nSo we use sample statistics to estimate them:\n\n\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#least-squares-regression",
    "href": "content/lectures/07-linear-models.html#least-squares-regression",
    "title": "07-linear-models",
    "section": "Least squares regression",
    "text": "Least squares regression\n\nThe regression line minimizes the sum of squared residuals.\n\n. . .\n\nIf \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals",
    "title": "07-linear-models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "href": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "title": "07-linear-models",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\n\n\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n. . .\n\nThe slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\n\n. . .\n\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\n\n. . .\n\nThe residuals and \\(x\\) values are uncorrelated",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 2 levels",
    "text": "Categorical predictor with 2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# ℹ 3,373 more rows\n\n\n\n\nlandsALL = 0: No landscape features\nlandsALL = 1: Some landscape features",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features",
    "href": "content/lectures/07-linear-models.html#height-landscape-features",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\nm_ht_lands &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |&gt; tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "href": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\nSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n\nCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\n\nIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "title": "07-linear-models",
    "section": "Categorical predictor with >2 levels",
    "text": "Categorical predictor with &gt;2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows\n\n\n\n\nschool from which painting came (details in a few slides)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship between height and school",
    "text": "Relationship between height and school\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ school_pntg, data = pp) |&gt;\n  tidy()\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#dummy-variables",
    "href": "content/lectures/07-linear-models.html#dummy-variables",
    "title": "07-linear-models",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nEach coefficient describes the expected difference between heights in that particular school compared to the baseline level",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 3+ levels",
    "text": "Categorical predictor with 3+ levels\n\n\n\n\n\n\n\n\nschool_pntg\nD_FL\nF\nG\nI\nS\nX\n\n\n\n\nA\n0\n0\n0\n0\n0\n0\n\n\nD/FL\n1\n0\n0\n0\n0\n0\n\n\nF\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n1\n0\n0\n0\n\n\nI\n0\n0\n0\n1\n0\n0\n\n\nS\n0\n0\n0\n0\n1\n0\n\n\nX\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "href": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "title": "07-linear-models",
    "section": "The linear model with multiple predictors",
    "text": "The linear model with multiple predictors\n\nPopulation model:\n\n\\[ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k \\]\n. . .\n\nSample model that we use to estimate the population model:\n\n\\[ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k \\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship b/w height and school",
    "text": "Relationship b/w height and school\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nAustrian school (A) paintings are expected, on average, to be 14 inches tall.\nDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\nFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\nGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\nItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\nSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\nPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings. ]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#predict-height-from-width",
    "title": "07-linear-models",
    "section": "Predict height from width",
    "text": "Predict height from width\n❓ On average, how tall are paintings that are 60 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]\n. . .\n\n3.62 + 0.78 * 60\n\n[1] 50.42\n\n\n“On average, we expect paintings that are 60 inches wide to be 50.42 inches high.”\nWarning: We “expect” this to happen, but there will be some variability. (We’ll learn about measuring the variability around the prediction later.)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "href": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "title": "07-linear-models",
    "section": "Prediction vs. extrapolation",
    "text": "Prediction vs. extrapolation\n❓ On average, how tall are paintings that are 400 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "href": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "title": "07-linear-models",
    "section": "Watch out for extrapolation!",
    "text": "Watch out for extrapolation!\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”1  Stephen Colbert, April 6th, 2010",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "href": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "title": "07-linear-models",
    "section": "Measuring the strength of the fit",
    "text": "Measuring the strength of the fit\n\nThe strength of the fit of a linear model is most commonly evaluated using \\(R^2\\).\nIt tells us what percent of variability in the response variable is explained by the model.\nThe remainder of the variability is explained by variables not included in the model.\n\\(R^2\\) is sometimes called the coefficient of determination.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "href": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "title": "07-linear-models",
    "section": "Obtaining \\(R^2\\) in R",
    "text": "Obtaining \\(R^2\\) in R\n\nHeight vs. width\n\n\nglance(m_ht_wt)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(m_ht_wt)$r.squared # extract R-squared\n\n[1] 0.6829468\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n. . .\n\nHeight vs. landscape features\n\n\nglance(m_ht_lands)$r.squared\n\n[1] 0.03456724",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width",
    "href": "content/lectures/07-linear-models.html#price-vs.-width",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Describe the relationship between price and width of paintings whose width is less than 100in.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Which plot shows a more linear relationship?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "title": "07-linear-models",
    "section": "Price vs. width, residuals",
    "text": "Price vs. width, residuals\n❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n❓What’s the unit of residuals?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#transforming-the-data",
    "href": "content/lectures/07-linear-models.html#transforming-the-data",
    "title": "07-linear-models",
    "section": "Transforming the data",
    "text": "Transforming the data\n\nWe saw that price has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n. . .\n\nIn these situations a transformation applied to the response variable may be useful.\n\n. . .\n\nIn order to decide which transformation to use, we should examine the distribution of the response variable.\n\n. . .\n\nThe extremely right skewed distribution suggests that a log transformation may be useful.\n\nlog = natural log, \\(ln\\)\nDefault base of the log function in R is the natural log:  log(x, base = exp(1))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "href": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "title": "07-linear-models",
    "section": "Logged price vs. width",
    "text": "Logged price vs. width\n❓ How do we interpret the slope of this model?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\n\nm_lprice_wt &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(log(price) ~ Width_in, data = pp_wt_lt_100)\n\nm_lprice_wt |&gt;\n  tidy() |&gt;\n  select(term, estimate) |&gt;\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    4.67 \n2 Width_in       0.019",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "title": "07-linear-models",
    "section": "Linear model with log transformation",
    "text": "Linear model with log transformation\n\\[ \\widehat{log(price)} = 4.67 + 0.02 Width \\]\n. . .\n\nFor each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n. . .\n\nwhich is not a very useful statement…",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#working-with-logs",
    "href": "content/lectures/07-linear-models.html#working-with-logs",
    "title": "07-linear-models",
    "section": "Working with logs",
    "text": "Working with logs\n\nSubtraction and logs: \\(log(a) − log(b) = log(a / b)\\)\n\n. . .\n\nNatural logarithm: \\(e^{log(x)} = x\\)\n\n. . .\n\nWe can use these identities to “undo” the log transformation",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n. . .\n\\[ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 \\]\n. . .\n\\[ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 \\]\n. . .\n\\[ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} \\]\n. . .\n\\[ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 \\]\n. . .\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "href": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "title": "07-linear-models",
    "section": "Shortcuts in R",
    "text": "Shortcuts in R\n\nm_lprice_wt |&gt;\n  tidy() |&gt;\n  select(term, estimate) |&gt;\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    4.67 \n2 Width_in       0.019\n\n\n\nm_lprice_wt |&gt;\n  tidy() |&gt;\n  select(term, estimate) |&gt;\n  mutate(estimate = round(exp(estimate), 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)   107.  \n2 Width_in        1.02",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap-log-transformations",
    "href": "content/lectures/07-linear-models.html#recap-log-transformations",
    "title": "07-linear-models",
    "section": "Recap: Log Transformations",
    "text": "Recap: Log Transformations\n\nNon-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n. . .\n\nThe most common transformation when the response variable is right skewed is the log transform: \\(log(y)\\), especially useful when the response variable is (extremely) right skewed.\n\n. . .\n\nThis transformation is also useful for variance stabilization.\n\n. . .\n\nWhen using a log transformation on the response variable the interpretation of the slope changes: *“For each unit increase in x, y is expected on average to be higher/lower*  by a factor of \\(e^{b_1}\\).”\n\n. . .\n\nAnother useful transformation is the square root: \\(\\sqrt{y}\\), especially useful when the response variable is counts.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#aside-when-y-0",
    "href": "content/lectures/07-linear-models.html#aside-when-y-0",
    "title": "07-linear-models",
    "section": "Aside: when \\(y = 0\\)",
    "text": "Aside: when \\(y = 0\\)\nIn some cases the value of the response variable might be 0, and\n\nlog(0)\n\n[1] -Inf\n\n\n. . .\nThe trick is to add a very small number to the value of the response variable for these cases so that the log function can still be applied:\n\nlog(0 + 0.00001)\n\n[1] -11.51293",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap",
    "href": "content/lectures/07-linear-models.html#recap",
    "title": "07-linear-models",
    "section": "Recap",
    "text": "Recap\n\nCan I carry out linear regression using the tidymodels approach?\nCan I interpret and explain the results from a linear model with a single predictor?\nDo I understand the limitations of modelling data w/ linear regression?\nCan I describe and implement the use of a dummy variable in linear regression?\nCan I determine when logistic transformation may be appropriate? Can I interpret these results?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models.html#footnotes",
    "href": "content/lectures/07-linear-models.html#footnotes",
    "title": "07-linear-models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIntroduction to Modern Statistics. “Extrapolation is treacherous.”↩︎",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#qa",
    "href": "content/lectures/07-linear-models-slides.html#qa",
    "title": "07-linear-models",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I was just wondering if you could still provide those videos you talked about with like syntax stuff so that we can follow along. I would also appreciate if it was really through with like a walk through of how you got to each point\nA: I like this suggestion, and I’ll try to make these. No promises though. Just have to find magical time in the schedule to make them. Note that lectures will be the more detailed walk through with more time on your own to figure it out (rather than detailed walk through outside of class), so you will get the information…the time/location will just differ. But, I’ll try to make some supplemental videos for those interested.\n\n\nQ: Had a question about the modeling and the last bit showing how decreasing the alpha showed greater clustering at the bottom left corner. If the bottom left corner looks like 0 height and 0 width, how does that translate into the dimensions of an actual painting?\nA: Good observation. The follow-up to this is…are there any paintings with zero width or zero height? And, if you dig in the data (i.e. min(pp$Height_in, na.rm=TRUE)), you’ll see that there are some very small paintings, but that none are zero.\n\n\nQ: If we want to built our own model, can we plot them with ggplot2?\nA: Yup! This post starts to get at that. It does so for a linear model, but the logic follows for other models.\n\n\nQ: For the paintings dataset, how could we perform EDA on specific subject matter, like seeing how many portraits include Jesus as part of the subject?\nA: Love this question. There is a whole field of natural language processing that would have sophisticated ways to analyze this. A simple first pass would be to, for example, filter for paintings that include “Jesus” in the subject column.\n\n\nQ: I think the segmented bar plots based on proportion seem difficult to read. I’m not sure why we should be using this instead of the stacked plots?\nA: Grouped bar blots are typically most quickly understood. Proportion stacked plots are then easiest to understand proportion across categories. Stacked plots of raw numbers take longer (for most) to understand and thus are often avoided, but like all viz, it depends on context and audience.\n\n\nQ: In the last lecture, we talked about how segmented bar plots might not be the ideal choice for data visualization, but we still demonstrated them in today’s lecture. So, specifically in what cases should we choose segmented bar plots as a data analysis tool?\nA: They can be helpful when the audience is familiar with them, but typically are most helpful when you want to display relative proportions rather than counts",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#course-announcements",
    "href": "content/lectures/07-linear-models-slides.html#course-announcements",
    "title": "07-linear-models",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 04 due Friday\n\nModel Interpretations\nText, code, & viz all matter\n\nLecture Participation survey “due” after class\nHW02 due Monday (10/30; 11:59 PM)\ndiscuss displaying image in Markdown",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#agenda",
    "href": "content/lectures/07-linear-models-slides.html#agenda",
    "title": "07-linear-models",
    "section": "Agenda",
    "text": "Agenda\n\nLinear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & &gt;2 levels)\nresiduals\ndata transformations",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#suggested-reading",
    "href": "content/lectures/07-linear-models-slides.html#suggested-reading",
    "title": "07-linear-models",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#data-paris-paintings",
    "href": "content/lectures/07-linear-models-slides.html#data-paris-paintings",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings\n\npp &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nNumber of observations: 3393\nNumber of variables: 61",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#goal-predict-height-from-width",
    "href": "content/lectures/07-linear-models-slides.html#goal-predict-height-from-width",
    "title": "07-linear-models",
    "section": "Goal: Predict height from width",
    "text": "Goal: Predict height from width\n\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#tidymodels-1",
    "href": "content/lectures/07-linear-models-slides.html#tidymodels-1",
    "title": "07-linear-models",
    "section": "tidymodels",
    "text": "tidymodels\n\nNOT a core tidyverse package\nfollows the structure of a tidyverse package\n\n\n\n# should already be installed for you on datahub\nlibrary(tidymodels)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-1-specify-model",
    "href": "content/lectures/07-linear-models-slides.html#step-1-specify-model",
    "title": "07-linear-models",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-2-set-model-fitting-engine",
    "href": "content/lectures/07-linear-models-slides.html#step-2-set-model-fitting-engine",
    "title": "07-linear-models",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-3-fit-model-estimate-parameters",
    "href": "content/lectures/07-linear-models-slides.html#step-3-fit-model-estimate-parameters",
    "title": "07-linear-models",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\n… using formula syntax\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ Width_in, data = pp)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#a-closer-look-at-model-output",
    "href": "content/lectures/07-linear-models-slides.html#a-closer-look-at-model-output",
    "title": "07-linear-models",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n\n\n\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#a-tidy-look-at-model-output",
    "href": "content/lectures/07-linear-models-slides.html#a-tidy-look-at-model-output",
    "title": "07-linear-models",
    "section": "A tidy look at model output",
    "text": "A tidy look at model output\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ Width_in, data = pp) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n\n\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#slope-and-intercept",
    "href": "content/lectures/07-linear-models-slides.html#slope-and-intercept",
    "title": "07-linear-models",
    "section": "Slope and intercept",
    "text": "Slope and intercept\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]\n\n\nSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n\n\n\nIntercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#correlation-does-not-imply-causation",
    "href": "content/lectures/07-linear-models-slides.html#correlation-does-not-imply-causation",
    "title": "07-linear-models",
    "section": "Correlation does not imply causation",
    "text": "Correlation does not imply causation\nRemember this when interpreting model coefficients\n\n\n\n\n\n\n\n\n\n\n\nSource: XKCD, Cell phones",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#linear-model-with-a-single-predictor",
    "href": "content/lectures/07-linear-models-slides.html#linear-model-with-a-single-predictor",
    "title": "07-linear-models",
    "section": "Linear model with a single predictor",
    "text": "Linear model with a single predictor\n\nWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\n\n\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n\n\nTough luck, you can’t have them…\n\n\n\n\nSo we use sample statistics to estimate them:\n\n\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#least-squares-regression",
    "href": "content/lectures/07-linear-models-slides.html#least-squares-regression",
    "title": "07-linear-models",
    "section": "Least squares regression",
    "text": "Least squares regression\n\nThe regression line minimizes the sum of squared residuals.\n\n\n\nIf \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals",
    "title": "07-linear-models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.-1",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.-1",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#properties-of-least-squares-regression",
    "href": "content/lectures/07-linear-models-slides.html#properties-of-least-squares-regression",
    "title": "07-linear-models",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\n\n\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\nThe slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\n\n\n\n\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\n\n\n\n\nThe residuals and \\(x\\) values are uncorrelated",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 2 levels",
    "text": "Categorical predictor with 2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# ℹ 3,373 more rows\n\n\n\n\nlandsALL = 0: No landscape features\nlandsALL = 1: Some landscape features",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#height-landscape-features",
    "href": "content/lectures/07-linear-models-slides.html#height-landscape-features",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\nm_ht_lands &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |&gt; tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#height-landscape-features-1",
    "href": "content/lectures/07-linear-models-slides.html#height-landscape-features-1",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\nSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n\nCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\n\nIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels-1",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels-1",
    "title": "07-linear-models",
    "section": "Categorical predictor with >2 levels",
    "text": "Categorical predictor with &gt;2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows\n\n\n\n\nschool from which painting came (details in a few slides)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#relationship-between-height-and-school",
    "href": "content/lectures/07-linear-models-slides.html#relationship-between-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship between height and school",
    "text": "Relationship between height and school\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Height_in ~ school_pntg, data = pp) |&gt;\n  tidy()\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#dummy-variables",
    "href": "content/lectures/07-linear-models-slides.html#dummy-variables",
    "title": "07-linear-models",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nEach coefficient describes the expected difference between heights in that particular school compared to the baseline level",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-3-levels",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-3-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 3+ levels",
    "text": "Categorical predictor with 3+ levels\n\n\n\n\n\n\n\n\nschool_pntg\nD_FL\nF\nG\nI\nS\nX\n\n\n\n\nA\n0\n0\n0\n0\n0\n0\n\n\nD/FL\n1\n0\n0\n0\n0\n0\n\n\nF\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n1\n0\n0\n0\n\n\nI\n0\n0\n0\n1\n0\n0\n\n\nS\n0\n0\n0\n0\n1\n0\n\n\nX\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#the-linear-model-with-multiple-predictors",
    "href": "content/lectures/07-linear-models-slides.html#the-linear-model-with-multiple-predictors",
    "title": "07-linear-models",
    "section": "The linear model with multiple predictors",
    "text": "The linear model with multiple predictors\n\nPopulation model:\n\n\\[ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k \\]\n\n\nSample model that we use to estimate the population model:\n\n\\[ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k \\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#relationship-bw-height-and-school",
    "href": "content/lectures/07-linear-models-slides.html#relationship-bw-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship b/w height and school",
    "text": "Relationship b/w height and school\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nAustrian school (A) paintings are expected, on average, to be 14 inches tall.\nDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\nFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\nGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\nItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\nSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\nPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings. ]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#predict-height-from-width",
    "href": "content/lectures/07-linear-models-slides.html#predict-height-from-width",
    "title": "07-linear-models",
    "section": "Predict height from width",
    "text": "Predict height from width\n❓ On average, how tall are paintings that are 60 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]\n\n\n3.62 + 0.78 * 60\n\n[1] 50.42\n\n\n“On average, we expect paintings that are 60 inches wide to be 50.42 inches high.”\nWarning: We “expect” this to happen, but there will be some variability. (We’ll learn about measuring the variability around the prediction later.)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#prediction-vs.-extrapolation",
    "href": "content/lectures/07-linear-models-slides.html#prediction-vs.-extrapolation",
    "title": "07-linear-models",
    "section": "Prediction vs. extrapolation",
    "text": "Prediction vs. extrapolation\n❓ On average, how tall are paintings that are 400 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#watch-out-for-extrapolation",
    "href": "content/lectures/07-linear-models-slides.html#watch-out-for-extrapolation",
    "title": "07-linear-models",
    "section": "Watch out for extrapolation!",
    "text": "Watch out for extrapolation!\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”1  Stephen Colbert, April 6th, 2010\n\nIntroduction to Modern Statistics. “Extrapolation is treacherous.”",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#measuring-the-strength-of-the-fit",
    "href": "content/lectures/07-linear-models-slides.html#measuring-the-strength-of-the-fit",
    "title": "07-linear-models",
    "section": "Measuring the strength of the fit",
    "text": "Measuring the strength of the fit\n\nThe strength of the fit of a linear model is most commonly evaluated using \\(R^2\\).\nIt tells us what percent of variability in the response variable is explained by the model.\nThe remainder of the variability is explained by variables not included in the model.\n\\(R^2\\) is sometimes called the coefficient of determination.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#obtaining-r2-in-r",
    "href": "content/lectures/07-linear-models-slides.html#obtaining-r2-in-r",
    "title": "07-linear-models",
    "section": "Obtaining \\(R^2\\) in R",
    "text": "Obtaining \\(R^2\\) in R\n\nHeight vs. width\n\n\nglance(m_ht_wt)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(m_ht_wt)$r.squared # extract R-squared\n\n[1] 0.6829468\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n\n\nHeight vs. landscape features\n\n\nglance(m_ht_lands)$r.squared\n\n[1] 0.03456724",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#data-paris-paintings-1",
    "href": "content/lectures/07-linear-models-slides.html#data-paris-paintings-1",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Describe the relationship between price and width of paintings whose width is less than 100in.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width-1",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width-1",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Which plot shows a more linear relationship?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width-residuals",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width-residuals",
    "title": "07-linear-models",
    "section": "Price vs. width, residuals",
    "text": "Price vs. width, residuals\n❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❓What’s the unit of residuals?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#transforming-the-data",
    "href": "content/lectures/07-linear-models-slides.html#transforming-the-data",
    "title": "07-linear-models",
    "section": "Transforming the data",
    "text": "Transforming the data\n\nWe saw that price has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n\n\nIn these situations a transformation applied to the response variable may be useful.\n\n\n\n\nIn order to decide which transformation to use, we should examine the distribution of the response variable.\n\n\n\n\nThe extremely right skewed distribution suggests that a log transformation may be useful.\n\nlog = natural log, \\(ln\\)\nDefault base of the log function in R is the natural log:  log(x, base = exp(1))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#logged-price-vs.-width",
    "href": "content/lectures/07-linear-models-slides.html#logged-price-vs.-width",
    "title": "07-linear-models",
    "section": "Logged price vs. width",
    "text": "Logged price vs. width\n❓ How do we interpret the slope of this model?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation",
    "href": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\n\nm_lprice_wt &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(log(price) ~ Width_in, data = pp_wt_lt_100)\n\nm_lprice_wt |&gt;\n  tidy() |&gt;\n  select(term, estimate) |&gt;\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    4.67 \n2 Width_in       0.019",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#linear-model-with-log-transformation",
    "href": "content/lectures/07-linear-models-slides.html#linear-model-with-log-transformation",
    "title": "07-linear-models",
    "section": "Linear model with log transformation",
    "text": "Linear model with log transformation\n\\[ \\widehat{log(price)} = 4.67 + 0.02 Width \\]\n\n\nFor each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n\n\n\nwhich is not a very useful statement…",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#working-with-logs",
    "href": "content/lectures/07-linear-models-slides.html#working-with-logs",
    "title": "07-linear-models",
    "section": "Working with logs",
    "text": "Working with logs\n\nSubtraction and logs: \\(log(a) − log(b) = log(a / b)\\)\n\n\n\nNatural logarithm: \\(e^{log(x)} = x\\)\n\n\n\n\nWe can use these identities to “undo” the log transformation",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation-1",
    "href": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation-1",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n\n\\[ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 \\]\n\n\n\\[ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 \\]\n\n\n\\[ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} \\]\n\n\n\\[ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 \\]\n\n\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#shortcuts-in-r",
    "href": "content/lectures/07-linear-models-slides.html#shortcuts-in-r",
    "title": "07-linear-models",
    "section": "Shortcuts in R",
    "text": "Shortcuts in R\n\nm_lprice_wt |&gt;\n  tidy() |&gt;\n  select(term, estimate) |&gt;\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    4.67 \n2 Width_in       0.019\n\n\n\nm_lprice_wt |&gt;\n  tidy() |&gt;\n  select(term, estimate) |&gt;\n  mutate(estimate = round(exp(estimate), 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)   107.  \n2 Width_in        1.02",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#recap-log-transformations",
    "href": "content/lectures/07-linear-models-slides.html#recap-log-transformations",
    "title": "07-linear-models",
    "section": "Recap: Log Transformations",
    "text": "Recap: Log Transformations\n\nNon-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n\n\nThe most common transformation when the response variable is right skewed is the log transform: \\(log(y)\\), especially useful when the response variable is (extremely) right skewed.\n\n\n\n\nThis transformation is also useful for variance stabilization.\n\n\n\n\nWhen using a log transformation on the response variable the interpretation of the slope changes: *“For each unit increase in x, y is expected on average to be higher/lower*  by a factor of \\(e^{b_1}\\).”\n\n\n\n\nAnother useful transformation is the square root: \\(\\sqrt{y}\\), especially useful when the response variable is counts.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#aside-when-y-0",
    "href": "content/lectures/07-linear-models-slides.html#aside-when-y-0",
    "title": "07-linear-models",
    "section": "Aside: when \\(y = 0\\)",
    "text": "Aside: when \\(y = 0\\)\nIn some cases the value of the response variable might be 0, and\n\nlog(0)\n\n[1] -Inf\n\n\n\nThe trick is to add a very small number to the value of the response variable for these cases so that the log function can still be applied:\n\nlog(0 + 0.00001)\n\n[1] -11.51293",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#recap",
    "href": "content/lectures/07-linear-models-slides.html#recap",
    "title": "07-linear-models",
    "section": "Recap",
    "text": "Recap\n\nCan I carry out linear regression using the tidymodels approach?\nCan I interpret and explain the results from a linear model with a single predictor?\nDo I understand the limitations of modelling data w/ linear regression?\nCan I describe and implement the use of a dummy variable in linear regression?\nCan I determine when logistic transformation may be appropriate? Can I interpret these results?\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "07-linear-models"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html",
    "href": "content/lectures/17-cs02-analysis.html",
    "title": "17-cs02-analysis",
    "section": "",
    "text": "Q: Just to confirm, if we choose to do the CS02 do we still do it with our final group or the assigned CS02 group?\nA: Final group!\n\n\nQ: If we choose to do the final project instead of CS2, will everything be the same as our originally planned final projects in terms of contents except the grade weighting?\nA: Yup!\n\n\nQ: Understanding some of the variable’s meaning, maybe would have been good for us to recode them to make them more intuitive.\nA: A good suggestion if you decide to go this route for the final!\n\n\nQ: For the distribution on the diagonal line of the correlation graphs, what does x/y axis represent?\nA: It’s a densityplot (shows the distribution) for each individual variable - the one that’s in that row.\n\n\n\n\n\nCS01 due tonight (group work survey due Friday)\nLab07 due tomorrow (Friday)\nFinal Project due Tues of Finals week (report + presentation + general communication)\n\n. . .\n\nAny CS01 questions?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#qa",
    "href": "content/lectures/17-cs02-analysis.html#qa",
    "title": "17-cs02-analysis",
    "section": "",
    "text": "Q: Just to confirm, if we choose to do the CS02 do we still do it with our final group or the assigned CS02 group?\nA: Final group!\n\n\nQ: If we choose to do the final project instead of CS2, will everything be the same as our originally planned final projects in terms of contents except the grade weighting?\nA: Yup!\n\n\nQ: Understanding some of the variable’s meaning, maybe would have been good for us to recode them to make them more intuitive.\nA: A good suggestion if you decide to go this route for the final!\n\n\nQ: For the distribution on the diagonal line of the correlation graphs, what does x/y axis represent?\nA: It’s a densityplot (shows the distribution) for each individual variable - the one that’s in that row.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#course-announcements",
    "href": "content/lectures/17-cs02-analysis.html#course-announcements",
    "title": "17-cs02-analysis",
    "section": "",
    "text": "CS01 due tonight (group work survey due Friday)\nLab07 due tomorrow (Friday)\nFinal Project due Tues of Finals week (report + presentation + general communication)\n\n. . .\n\nAny CS01 questions?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#recall",
    "href": "content/lectures/17-cs02-analysis.html#recall",
    "title": "17-cs02-analysis",
    "section": "Recall:",
    "text": "Recall:",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#the-data",
    "href": "content/lectures/17-cs02-analysis.html#the-data",
    "title": "17-cs02-analysis",
    "section": "The Data",
    "text": "The Data\n\npm &lt;- read_csv(here::here(\"OCS_data\", \"data\", \"raw\", \"pm25_data.csv\"))\n\nRows: 876 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): state, county, city\ndbl (47): id, value, fips, lat, lon, CMAQ, zcta, zcta_area, zcta_pop, imp_a5...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Converting to factors as discussed last class\npm &lt;- pm %&gt;%\n  dplyr::mutate(across(c(id, fips, zcta), as.factor))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#data-splitting",
    "href": "content/lectures/17-cs02-analysis.html#data-splitting",
    "title": "17-cs02-analysis",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n\n\n. . .\nSpecify the split:\n\nset.seed(1234)\npm_split &lt;- rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n&lt;Training/Testing/Total&gt;\n&lt;584/292/876&gt;\n\n\n\nset.seed &lt;- ensures we all get the exact same random split\noutput displayed: &lt;training data sample number, testing data sample number, original sample number&gt;\n\nMore on how people decide what proportions to use for data splitting here\n. . .\nSplit the Data\n\ntrain_pm &lt;- rsample::training(pm_split)\ntest_pm &lt;- rsample::testing(pm_split)\n \n# Scroll through the output!\ncount(train_pm, state)\n\n# A tibble: 49 × 2\n   state                    n\n   &lt;chr&gt;                &lt;int&gt;\n 1 Alabama                 13\n 2 Arizona                 12\n 3 Arkansas                 8\n 4 California              55\n 5 Colorado                10\n 6 Connecticut             12\n 7 Delaware                 3\n 8 District Of Columbia     2\n 9 Florida                 22\n10 Georgia                 20\n# ℹ 39 more rows\n\n\n❓ What do you observe about the output?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#pre-processing-recipe-bake",
    "href": "content/lectures/17-cs02-analysis.html#pre-processing-recipe-bake",
    "title": "17-cs02-analysis",
    "section": "Pre-processing: recipe() + bake()",
    "text": "Pre-processing: recipe() + bake()\nNeed to:\n\nspecify predictors vs. outcome\nscale variables\nremove redundant variables (feature engineering)\n\n. . .\nrecipe provides a standardized format for a sequence of steps for pre-processing the data\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-1-specify-variable-roles",
    "href": "content/lectures/17-cs02-analysis.html#step-1-specify-variable-roles",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify variable roles",
    "text": "Step 1: Specify variable roles\nThe simplest approach…\n\nsimple_rec &lt;- train_pm %&gt;%\n  recipes::recipe(value ~ .)\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 49\n\n\n. . .\n…but we need to specify which column includes ID information\n\nsimple_rec &lt;- train_pm %&gt;%\n  recipes::recipe(value ~ .) %&gt;%\n  recipes::update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n. . .\n…and which are our predictors and which is our outcome\n\nsimple_rec &lt;- recipe(train_pm) %&gt;%\n    update_role(everything(), new_role = \"predictor\") %&gt;%\n    update_role(value, new_role = \"outcome\") %&gt;%\n    update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n❓ Can someone summarize what this code is specifying?\n. . .\nSummarizing our recipe thus far:\n\nsummary(simple_rec)\n\n# A tibble: 50 × 4\n   variable type      role        source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;       &lt;chr&gt;   \n 1 id       &lt;chr [3]&gt; id variable original\n 2 value    &lt;chr [2]&gt; outcome     original\n 3 fips     &lt;chr [3]&gt; predictor   original\n 4 lat      &lt;chr [2]&gt; predictor   original\n 5 lon      &lt;chr [2]&gt; predictor   original\n 6 state    &lt;chr [3]&gt; predictor   original\n 7 county   &lt;chr [3]&gt; predictor   original\n 8 city     &lt;chr [3]&gt; predictor   original\n 9 CMAQ     &lt;chr [2]&gt; predictor   original\n10 zcta     &lt;chr [3]&gt; predictor   original\n# ℹ 40 more rows",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-2-specify-pre-processing-with-step",
    "href": "content/lectures/17-cs02-analysis.html#step-2-specify-pre-processing-with-step",
    "title": "17-cs02-analysis",
    "section": "Step 2: Specify pre-processing with step*()",
    "text": "Step 2: Specify pre-processing with step*()\n\n\n\n. . .\nThere are step functions for a variety of purposes:\n\nImputation – filling in missing values based on the existing data\nTransformation – changing all values of a variable in the same way, typically to make it more normal or easier to interpret\nDiscretization – converting continuous values into discrete or nominal values - binning for example to reduce the number of possible levels (However this is generally not advisable!)\nEncoding / Creating Dummy Variables – creating a numeric code for categorical variables (More on one-hot and Dummy Variables encoding)\nData type conversions – which means changing from integer to factor or numeric to date etc.\nInteraction term addition to the model – which means that we would be modeling for predictors that would influence the capacity of each other to predict the outcome\nNormalization – centering and scaling the data to a similar range of values\nDimensionality Reduction/ Signal Extraction – reducing the space of features or predictors to a smaller set of variables that capture the variation or signal in the original variables (ex. Principal Component Analysis and Independent Component Analysis)\nFiltering – filtering options for removing variables (ex. remove variables that are highly correlated to others or remove variables with very little variance and therefore likely little predictive capacity)\nRow operations – performing functions on the values within the rows (ex. rearranging, filtering, imputing)\nChecking functions – Gut checks to look for missing values, to look at the variable classes etc.\n\nThis link and this link show the many options for recipe step functions.\n. . .\nThere are several ways to select what variables to apply steps to:\n\nUsing tidyselect methods: contains(), matches(), starts_with(), ends_with(), everything(), num_range()\n\nUsing the type: all_nominal(), all_numeric() , has_type()\nUsing the role: all_predictors(), all_outcomes(), has_role()\nUsing the name - use the actual name of the variable/variables of interest\n\n. . .\nOne-hot encoding categorical variables:\n\nsimple_rec %&gt;%\n  step_dummy(state, county, city, zcta, one_hot = TRUE)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: state, county, city, zcta\n\n\n❓ Can anyone remind us what one-hot encoding does?\n. . .\n\nfips includes numeric code for state and county, so it’s another way to specify county\nso, we’ll change fips’ role\nwe get to decide what to call it (\"county id\")\n\n\nsimple_rec %&gt;%\n  update_role(\"fips\", new_role = \"county id\")\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   47\ncounty id:    1\nid variable:  1\n\n\n. . .\nRemoving highly correlated variables:\n\nsimple_rec %&gt;%\n  step_corr(all_predictors(), - CMAQ, - aod)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: all_predictors(), -CMAQ, -aod\n\n\n\nspecifying to KEEP CMAQ and aod\n\n. . .\nRemoving variables with non-zero variance:\n\nsimple_rec %&gt;%\n  step_nzv(all_predictors(), - CMAQ, - aod)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Sparse, unbalanced variable filter on: all_predictors(), -CMAQ, -aod",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#putting-our-recipe-together",
    "href": "content/lectures/17-cs02-analysis.html#putting-our-recipe-together",
    "title": "17-cs02-analysis",
    "section": "Putting our recipe together",
    "text": "Putting our recipe together\n\nsimple_rec &lt;- simple_rec %&gt;% \n  update_role(\"fips\", new_role = \"county id\") %&gt;%\n  step_dummy(state, county, city, zcta, one_hot = TRUE) %&gt;%\n  step_corr(all_predictors(), - CMAQ, - aod)%&gt;%\n  step_nzv(all_predictors(), - CMAQ, - aod)\n  \nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   47\ncounty id:    1\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: state, county, city, zcta\n\n\n• Correlation filter on: all_predictors(), -CMAQ, -aod\n\n\n• Sparse, unbalanced variable filter on: all_predictors(), -CMAQ, -aod\n\n\nNote: order of steps matters",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-3-running-the-pre-processing-prep",
    "href": "content/lectures/17-cs02-analysis.html#step-3-running-the-pre-processing-prep",
    "title": "17-cs02-analysis",
    "section": "Step 3: Running the pre-processing (prep)",
    "text": "Step 3: Running the pre-processing (prep)\nThere are some important arguments to know about:\n\ntraining - you must supply a training data set to estimate parameters for pre-processing operations (recipe steps) - this may already be included in your recipe - as is the case for us\nfresh - if fresh=TRUE, will retrain and estimate parameters for any previous steps that were already prepped if you add more steps to the recipe (default is FALSE)\nverbose - if verbose=TRUE, shows the progress as the steps are evaluated and the size of the pre-processed training set (default is FALSE)\nretain - if retain=TRUE, then the pre-processed training set will be saved within the recipe (as template). This is good if you are likely to add more steps and do not want to rerun the prep() on the previous steps. However this can make the recipe size large. This is necessary if you want to actually look at the pre-processed data (default is TRUE)\n\n\nprepped_rec &lt;- prep(simple_rec, verbose = TRUE, retain = TRUE )\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.26 Mb  in memory.\n\nnames(prepped_rec)\n\n [1] \"var_info\"       \"term_info\"      \"steps\"          \"template\"      \n [5] \"levels\"         \"retained\"       \"requirements\"   \"tr_info\"       \n [9] \"orig_lvls\"      \"last_term_info\"\n\n\n. . .\nThis output includes a lot of information:\n\nthe steps that were run\n\nthe original variable info (var_info)\n\nthe updated variable info after pre-processing (term_info)\nthe new levels of the variables\nthe original levels of the variables (orig_lvls)\ninfo about the training data set size and completeness (tr_info)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-4-extract-pre-processed-training-data-using-bake",
    "href": "content/lectures/17-cs02-analysis.html#step-4-extract-pre-processed-training-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 4: Extract pre-processed training data using bake()",
    "text": "Step 4: Extract pre-processed training data using bake()\n\n\n\nbake(): apply our modeling steps (in this case just pre-processing on the training data) and see what it would do the data\n. . .\n\nbaked_train &lt;- bake(prepped_rec, new_data = NULL)\nglimpse(baked_train)\n\nRows: 584\nColumns: 37\n$ id                          &lt;fct&gt; 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       &lt;dbl&gt; 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        &lt;fct&gt; 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         &lt;dbl&gt; 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         &lt;dbl&gt; -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        &lt;dbl&gt; 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   &lt;dbl&gt; 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    &lt;dbl&gt; 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    &lt;dbl&gt; 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  &lt;dbl&gt; 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 &lt;dbl&gt; 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  &lt;dbl&gt; 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          &lt;dbl&gt; 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        &lt;dbl&gt; 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       &lt;dbl&gt; 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              &lt;dbl&gt; 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                &lt;dbl&gt; 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        &lt;dbl&gt; 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      &lt;dbl&gt; 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          &lt;dbl&gt; 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 &lt;dbl&gt; 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   &lt;dbl&gt; 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    &lt;dbl&gt; 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        &lt;dbl&gt; 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         &lt;dbl&gt; 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   &lt;dbl&gt; 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     &lt;dbl&gt; 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         &lt;dbl&gt; 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n\nnew_data = NULL specifies that we’re not (yet) looking at our testing data\nWe only have 36 variables (33 predictors + 2 id variables + outcome)\ncategorical variables (state) are gone (one-hot encoding)\nstate_California remains - only state with nonzero variance (largest # of monitors)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-5-extract-pre-processed-testing-data-using-bake",
    "href": "content/lectures/17-cs02-analysis.html#step-5-extract-pre-processed-testing-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 5: Extract pre-processed testing data using bake()",
    "text": "Step 5: Extract pre-processed testing data using bake()\n\nbake() takes a trained recipe and applies the operations to a data set to create a design matrix. For example: it applies the centering to new data sets using these means used to create the recipe. - tidymodels documentation\n\n. . .\nTypically, you want to avoid using your testing data…but our data set is not that large and NA values in our testing dataset could cause issues later on.\n\n\n\n. . .\n\nbaked_test_pm &lt;- recipes::bake(prepped_rec, new_data = test_pm)\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 37\n$ id                          &lt;fct&gt; 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       &lt;dbl&gt; 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        &lt;fct&gt; 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         &lt;dbl&gt; 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         &lt;dbl&gt; -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        &lt;dbl&gt; 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   &lt;dbl&gt; 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    &lt;dbl&gt; 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    &lt;dbl&gt; 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  &lt;dbl&gt; 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 &lt;dbl&gt; 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  &lt;dbl&gt; 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          &lt;dbl&gt; 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        &lt;dbl&gt; 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       &lt;dbl&gt; 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              &lt;dbl&gt; 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                &lt;dbl&gt; 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        &lt;dbl&gt; 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      &lt;dbl&gt; 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          &lt;dbl&gt; 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 &lt;dbl&gt; 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   &lt;dbl&gt; 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    &lt;dbl&gt; 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        &lt;dbl&gt; 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         &lt;dbl&gt; 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   &lt;dbl&gt; 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     &lt;dbl&gt; 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         &lt;dbl&gt; 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; NA, NA, NA, 0, 1, 1, 1, NA, NA, NA, 0, NA,…\n\n\n. . .\nHmm….lots of NAs now in city_Not.in.a.city\nLikely b/c there are cities in our testing dataset that were not in our training dataset…\n\ntraincities &lt;- train_pm %&gt;% distinct(city)\ntestcities &lt;- test_pm %&gt;% distinct(city)\n\n#get the number of cities that were different\ndim(dplyr::setdiff(traincities, testcities))\n\n[1] 381   1\n\n#get the number of cities that overlapped\ndim(dplyr::intersect(traincities, testcities))\n\n[1] 55  1",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#aside-return-to-wrangling",
    "href": "content/lectures/17-cs02-analysis.html#aside-return-to-wrangling",
    "title": "17-cs02-analysis",
    "section": "Aside: return to wrangling",
    "text": "Aside: return to wrangling\nA quick return to wrangling…and re-splitting our data\n\npm &lt;- pm %&gt;%\n  mutate(city = case_when(city == \"Not in a city\" ~ \"Not in a city\",\n                          city != \"Not in a city\" ~ \"In a city\"))\n\nset.seed(1234) # same seed as before\npm_split &lt;-rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n&lt;Training/Testing/Total&gt;\n&lt;584/292/876&gt;\n\n train_pm &lt;-rsample::training(pm_split)\n test_pm &lt;-rsample::testing(pm_split)\n\n. . .\nAnd a recipe update…(putting it all together)\n\nnovel_rec &lt;- recipe(train_pm) %&gt;%\n    update_role(everything(), new_role = \"predictor\") %&gt;%\n    update_role(value, new_role = \"outcome\") %&gt;%\n    update_role(id, new_role = \"id variable\") %&gt;%\n    update_role(\"fips\", new_role = \"county id\") %&gt;%\n    step_dummy(state, county, city, zcta, one_hot = TRUE) %&gt;%\n    step_corr(all_numeric()) %&gt;%\n    step_nzv(all_numeric()) \n\n. . .\nre-bake()\n\nprepped_rec &lt;- prep(novel_rec, verbose = TRUE, retain = TRUE)\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.27 Mb  in memory.\n\nbaked_train &lt;- bake(prepped_rec, new_data = NULL)\n\n. . .\nLooking at the output\n\nglimpse(baked_train)\n\nRows: 584\nColumns: 38\n$ id                          &lt;fct&gt; 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       &lt;dbl&gt; 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        &lt;fct&gt; 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         &lt;dbl&gt; 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         &lt;dbl&gt; -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        &lt;dbl&gt; 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   &lt;dbl&gt; 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    &lt;dbl&gt; 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    &lt;dbl&gt; 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  &lt;dbl&gt; 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 &lt;dbl&gt; 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  &lt;dbl&gt; 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          &lt;dbl&gt; 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        &lt;dbl&gt; 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       &lt;dbl&gt; 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_prisec_length_25000     &lt;dbl&gt; 13.98749, 13.15082, 13.44293, 13.58697, 14…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              &lt;dbl&gt; 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                &lt;dbl&gt; 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        &lt;dbl&gt; 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      &lt;dbl&gt; 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          &lt;dbl&gt; 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 &lt;dbl&gt; 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   &lt;dbl&gt; 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    &lt;dbl&gt; 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        &lt;dbl&gt; 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         &lt;dbl&gt; 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   &lt;dbl&gt; 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     &lt;dbl&gt; 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         &lt;dbl&gt; 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n. . .\nMaking sure the NA issue is taken are of:\n\nbaked_test_pm &lt;- bake(prepped_rec, new_data = test_pm)\n\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 38\n$ id                          &lt;fct&gt; 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       &lt;dbl&gt; 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        &lt;fct&gt; 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         &lt;dbl&gt; 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         &lt;dbl&gt; -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        &lt;dbl&gt; 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   &lt;dbl&gt; 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    &lt;dbl&gt; 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    &lt;dbl&gt; 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  &lt;dbl&gt; 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 &lt;dbl&gt; 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  &lt;dbl&gt; 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          &lt;dbl&gt; 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        &lt;dbl&gt; 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       &lt;dbl&gt; 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_prisec_length_25000     &lt;dbl&gt; 13.79973, 13.70026, 13.85550, 14.45221, 13…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              &lt;dbl&gt; 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                &lt;dbl&gt; 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        &lt;dbl&gt; 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      &lt;dbl&gt; 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          &lt;dbl&gt; 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 &lt;dbl&gt; 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   &lt;dbl&gt; 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    &lt;dbl&gt; 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        &lt;dbl&gt; 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         &lt;dbl&gt; 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   &lt;dbl&gt; 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     &lt;dbl&gt; 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         &lt;dbl&gt; 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, …",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#specifying-our-model-parsnip",
    "href": "content/lectures/17-cs02-analysis.html#specifying-our-model-parsnip",
    "title": "17-cs02-analysis",
    "section": "Specifying our model (parsnip)",
    "text": "Specifying our model (parsnip)\nThere are four things we need to define about our model:\n\n\nThe type of model (using specific functions in parsnip like rand_forest(), logistic_reg() etc.)\n\nThe package or engine that we will use to implement the type of model selected (using the set_engine() function)\nThe mode of learning - classification or regression (using the set_mode() function)\nAny arguments necessary for the model/package selected (using the set_args()function - for example the mtry = argument for random forest which is the number of variables to be used as options for splitting at each tree node)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-1-specify-the-model",
    "href": "content/lectures/17-cs02-analysis.html#step-1-specify-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify the model",
    "text": "Step 1: Specify the model\n\nWe’ll start with linear regression, but move to random forest\nSee here for modeling options in parsnip.\n\n\nlm_PM_model &lt;- parsnip::linear_reg() %&gt;%\n  parsnip::set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nlm_PM_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-2-fit-the-model",
    "href": "content/lectures/17-cs02-analysis.html#step-2-fit-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 2: Fit the model",
    "text": "Step 2: Fit the model\n\nworkflows package allows us to keep track of both our pre-processing steps and our model specification\nIt also allows us to implement fancier optimizations in an automated way and it can also handle post-processing operations.\n\n\nPM_wflow &lt;- workflows::workflow() %&gt;%\n            workflows::add_recipe(novel_rec) %&gt;%\n            workflows::add_model(lm_PM_model)\nPM_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n❓ Who can explain the difference between a recipe, baking, and a workflow?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "href": "content/lectures/17-cs02-analysis.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "title": "17-cs02-analysis",
    "section": "Step 3: Prepare the recipe (estimate the parameters)",
    "text": "Step 3: Prepare the recipe (estimate the parameters)\n\nPM_wflow_fit &lt;- parsnip::fit(PM_wflow, data = train_pm)\nPM_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                (Intercept)                          lat  \n                  2.936e+02                    3.261e-02  \n                        lon                         CMAQ  \n                  1.586e-02                    2.463e-01  \n                  zcta_area                     zcta_pop  \n                 -3.433e-10                    1.013e-05  \n                   imp_a500                   imp_a15000  \n                  5.064e-03                   -3.066e-03  \n                county_area                   county_pop  \n                 -2.324e-11                   -7.576e-08  \n         log_dist_to_prisec          log_pri_length_5000  \n                  6.214e-02                   -2.006e-01  \n       log_pri_length_25000        log_prisec_length_500  \n                 -5.411e-02                    2.204e-01  \n     log_prisec_length_1000       log_prisec_length_5000  \n                  1.154e-01                    2.374e-01  \n    log_prisec_length_10000      log_prisec_length_25000  \n                 -3.436e-02                    5.224e-01  \nlog_nei_2008_pm10_sum_10000  log_nei_2008_pm10_sum_15000  \n                  1.829e-01                   -2.355e-02  \nlog_nei_2008_pm10_sum_25000               popdens_county  \n                  2.403e-02                    2.203e-05  \n               popdens_zcta                         nohs  \n                 -2.132e-06                   -2.983e+00  \n                     somehs                           hs  \n                 -2.956e+00                   -2.962e+00  \n                somecollege                    associate  \n                 -2.967e+00                   -2.999e+00  \n                   bachelor                         grad  \n                 -2.979e+00                   -2.978e+00  \n                        pov                    hs_orless  \n                  1.859e-03                           NA  \n                    urc2006                          aod  \n                  2.577e-01                    1.535e-02  \n           state_California           city_Not.in.a.city  \n                  3.114e+00                   -4.250e-02",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#step-4-assess-model-fit",
    "href": "content/lectures/17-cs02-analysis.html#step-4-assess-model-fit",
    "title": "17-cs02-analysis",
    "section": "Step 4: Assess model fit",
    "text": "Step 4: Assess model fit\n\nwflowoutput &lt;- PM_wflow_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  broom::tidy() \n\nwflowoutput\n\n# A tibble: 36 × 5\n   term         estimate std.error statistic       p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 (Intercept)  2.94e+ 2  1.18e+ 2     2.49  0.0130       \n 2 lat          3.26e- 2  2.28e- 2     1.43  0.153        \n 3 lon          1.59e- 2  1.01e- 2     1.58  0.115        \n 4 CMAQ         2.46e- 1  3.97e- 2     6.20  0.00000000108\n 5 zcta_area   -3.43e-10  1.60e-10    -2.15  0.0320       \n 6 zcta_pop     1.01e- 5  5.33e- 6     1.90  0.0578       \n 7 imp_a500     5.06e- 3  7.42e- 3     0.683 0.495        \n 8 imp_a15000  -3.07e- 3  1.16e- 2    -0.263 0.792        \n 9 county_area -2.32e-11  1.97e-11    -1.18  0.238        \n10 county_pop  -7.58e- 8  9.29e- 8    -0.815 0.415        \n# ℹ 26 more rows\n\n\n\nWe have fit our model on our training data\nWe have created a model to predict values of air pollution based on the predictors that we have included\n\n. . .\nUnderstanding what variables are most important in our model…\n\nPM_wflow_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip::vip(num_features = 10)\n\n\n\n\n\n\n\n\n. . .\nA closer look at monitors in CA:\n\nbaked_train %&gt;% \n  mutate(state_California = as.factor(state_California)) %&gt;%\n  mutate(state_California = recode(state_California, \n                                   \"0\" = \"Not California\", \n                                   \"1\" = \"California\")) %&gt;%\n  ggplot(aes(x = state_California, y = value)) + \n  geom_boxplot() +\n  geom_jitter(width = .05) + \n  xlab(\"Location of Monitor\")\n\n\n\n\n\n\n\n\n. . .\nRemember: machine learning (ML) as an optimization problem that tries to minimize the distance between our predicted outcome \\(\\hat{Y} = f(X)\\) and actual outcome \\(Y\\) using our features (or predictor variables) \\(X\\) as input to a function \\(f\\) that we want to estimate.\n\\[d(Y - \\hat{Y})\\]\n. . .\nLet’s pull out our predicted outcome values \\(\\hat{Y} = f(X)\\) from the models we fit (using different approaches).\n\nwf_fit &lt;- PM_wflow_fit %&gt;% \n  extract_fit_parsnip()\n\n\nwf_fitted_values &lt;- \n  broom::augment(wf_fit[[\"fit\"]], data = baked_train) %&gt;% \n  select(value, .fitted:.std.resid)\n\nhead(wf_fitted_values)\n\n# A tibble: 6 × 6\n  value .fitted   .hat .sigma   .cooksd .std.resid\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 11.7    12.2  0.0370   2.05 0.0000648     -0.243\n2  6.96    9.14 0.0496   2.05 0.00179       -1.09 \n3 13.3    12.6  0.0484   2.05 0.000151       0.322\n4 10.7    10.4  0.0502   2.05 0.0000504      0.183\n5 14.5    11.9  0.0243   2.05 0.00113        1.26 \n6 12.2     9.52 0.476    2.04 0.0850         1.81",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#visualizing-model-performance",
    "href": "content/lectures/17-cs02-analysis.html#visualizing-model-performance",
    "title": "17-cs02-analysis",
    "section": "Visualizing Model Performance",
    "text": "Visualizing Model Performance\n\nwf_fitted_values %&gt;% \n  ggplot(aes(x =  value, y = .fitted)) + \n  geom_point() + \n  xlab(\"actual outcome values\") + \n  ylab(\"predicted outcome values\")\n\n\n\n\n\n\n\n\n❓ What do you notice about/learn from these results?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#quantifying-model-performance",
    "href": "content/lectures/17-cs02-analysis.html#quantifying-model-performance",
    "title": "17-cs02-analysis",
    "section": "Quantifying Model Performance",
    "text": "Quantifying Model Performance\n\\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}{(\\hat{y_t}- y_t)}^2}{n}}\\]\n. . .\nCan use the yardstick package using the rmse()` function to calculate:\n\nyardstick::metrics(wf_fitted_values,\n                   truth = value, estimate = .fitted)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       1.98 \n2 rsq     standard       0.392\n3 mae     standard       1.47 \n\n\n\nRMSE isn’t too bad\n\\(R^2\\) suggests model is only explaining 39% of the variance in the data\nThe MAE value suggests that the average difference between the value predicted and the real value was 1.47 ug/m3. The range of the values was 3-22 in the training data, so this is a relatively small amount",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#cross-validation",
    "href": "content/lectures/17-cs02-analysis.html#cross-validation",
    "title": "17-cs02-analysis",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nResampling + Re-partitioning:\n\n\n\n. . .\nPreparing the data for cross-validation:\n\n\n\nNote: this is called v-fold or k-fold CV",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#implementing-in-rsample",
    "href": "content/lectures/17-cs02-analysis.html#implementing-in-rsample",
    "title": "17-cs02-analysis",
    "section": "Implementing in rsample()",
    "text": "Implementing in rsample()\n\nset.seed(1234)\nvfold_pm &lt;- rsample::vfold_cv(data = train_pm, v = 4)\nvfold_pm\n\n#  4-fold cross-validation \n# A tibble: 4 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [438/146]&gt; Fold1\n2 &lt;split [438/146]&gt; Fold2\n3 &lt;split [438/146]&gt; Fold3\n4 &lt;split [438/146]&gt; Fold4\n\n\n. . .\n\npull(vfold_pm, splits)\n\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n[[2]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n[[3]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n[[4]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n\n. . .\nVisualizing this process:",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#model-assessment-on-v-folds",
    "href": "content/lectures/17-cs02-analysis.html#model-assessment-on-v-folds",
    "title": "17-cs02-analysis",
    "section": "Model Assessment on v-folds",
    "text": "Model Assessment on v-folds\nWhere this workflow thing really shines…\n\nresample_fit &lt;- tune::fit_resamples(PM_wflow, vfold_pm)\n\n→ A | warning: the standard deviation is zero, The correlation matrix has missing values. 415 columns were excluded from the filter.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: There are new levels in a factor: Maine, There are new levels in a factor: Forest, Mecklenburg, Clermont, Camden, Trumbull, Yellowstone, Caddo, Hinds, Codington, Preble, Broward, Rowan, Beaver, Dauphin, Buncombe, LaPorte, Ashley, Clayton, Talladega, Queens, Jones, Mitchell, Kalamazoo, Seminole, Henderson, Sussex, Ingham, Sangamon, Aroostook, Muscogee, Plumas, Dodge, Bennington, Sumner, Butler, Butte, Passaic, Page, Custer, Sainte Genevieve, Bullitt, Palo Alto, Rapides, Faulkner, San Francisco, Ravalli, San Mateo, Delaware, Davis, Fremont, Santa Clara, White, Carter, DeSoto, Wilkinson, Muscatine, Hampden, Yakima, Solano, Mendocino, Mobile, Roanoke City, Wake, Gwinnett, Alamance, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n→ C | warning: the standard deviation is zero, The correlation matrix has missing values. 408 columns were excluded from the filter.\nThere were issues with some computations   A: x1\n→ D | warning: There are new levels in a factor: Athens, Kent, Linn, Stark, Cabell, Arlington, St. Lucie, Grafton, Champaign, Brewster, Morgan, Lawrence, Tarrant, Yolo, Weber, Mille Lacs, Clarke, Harrison, Will, Grant, Morris, Santa Cruz, Taylor, Klamath, Prince George's, Howard, Buchanan, Cedar, Ventura, Monongalia, Bolivar, Medina, Dona Ana, Hancock, Missoula, Chittenden, Monroe, Knox, Essex, Pierce, Tuscaloosa, Ellis, Contra Costa, Apache, Harris, Edgecombe, Stearns, Outagamie, Escambia, Hidalgo, Teton, Loudoun, Belknap, Sauk, Pittsburg, Charles, Gibson, Marshall, Chester, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1\n→ E | warning: the standard deviation is zero, The correlation matrix has missing values. 417 columns were excluded from the filter.\nThere were issues with some computations   A: x1\n→ F | warning: There are new levels in a factor: Nevada, North Dakota, There are new levels in a factor: Placer, Bay, Niagara, DeKalb, Hampton City, Oconee, Spencer, Sutter, Cobb, Randolph, Anne Arundel, Houston, Kane, Genesee, Dane, Yavapai, Lenawee, Washtenaw, Durham, Scioto, Henry, Spartanburg, Harney, Converse, Portage, St. Croix, Colusa, Berkshire, Lenoir, Lancaster, Haywood, Iberville, Adams, Catawba, St. Clair, Lynchburg City, Nassau, Brookings, Raleigh, Summit, Sebastian, Ouachita, Westmoreland, Rock Island, Duplin, Erie, Burleigh, Vilas, Kanawha, Rutland, San Joaquin, Washoe, Sandoval, Josephine, Kenosha, Plymouth, Stanislaus, Caswell, Cameron, Lucas, Sarpy, West Baton Rouge, Mayes, Cass, Chautauqua, Terrebonne, Sweetwater, Glynn, Harford, Spokane, La Salle, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1   E: x…\n→ G | warning: There are new levels in a factor: Allen, McDowell, Macon, Chaves, Yuma, Dougherty, Flathead, Ashland, Manistee, Hartford, Park, Box Elder, East Baton Rouge, Chesterfield, Woodbury, Bell, Citrus, New London, Cumberland, Fairfax, Forrest, Allegan, Ohio, Pueblo, Gaston, Bernalillo, Sullivan, Nevada, McLean, McCracken, Potter, Mahoning, Porter, Albemarle, Manitowoc, Shawnee, Ocean, Ottawa, El Paso, Baldwin, Bannock, Cheshire, Clay, Jersey, Brown, Lexington, Clinton, Peoria, Macomb, Davidson, Tooele, Dubois, Robeson, St. Lawrence, Lincoln, Virginia Beach City, Shelby, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1   E: x…\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1   E: x…\n\n\n. . .\nGives us a sense of the RMSE across the four folds:\n\ntune::show_best(resample_fit, metric = \"rmse\")\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2.12     4  0.0444 Preprocessor1_Model1",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#random-forest",
    "href": "content/lectures/17-cs02-analysis.html#random-forest",
    "title": "17-cs02-analysis",
    "section": "Random Forest",
    "text": "Random Forest\nFitting a different model\n. . .\nBased on a decision tree:\n\n\n\n\n\n\n\n\n\n\n[source]\n. . .\nBut…in the case of random forest:\n\n\nmultiple decision trees are created (hence: forest),\neach tree is built using a random subset of the training data (with replacement) (hence: random)\nhelps to keep the algorithm from overfitting the data\nThe mean of the predictions from each of the trees is used in the final output.\n\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#updating-our-recipe",
    "href": "content/lectures/17-cs02-analysis.html#updating-our-recipe",
    "title": "17-cs02-analysis",
    "section": "Updating our recipe()",
    "text": "Updating our recipe()\n\nRF_rec &lt;- recipe(train_pm) %&gt;%\n    update_role(everything(), new_role = \"predictor\")%&gt;%\n    update_role(value, new_role = \"outcome\")%&gt;%\n    update_role(id, new_role = \"id variable\") %&gt;%\n    update_role(\"fips\", new_role = \"county id\") %&gt;%\n    step_novel(\"state\") %&gt;%\n    step_string2factor(\"state\", \"county\", \"city\") %&gt;%\n    step_rm(\"county\") %&gt;%\n    step_rm(\"zcta\") %&gt;%\n    step_corr(all_numeric())%&gt;%\n    step_nzv(all_numeric())\n\n\ncan use our categorical data as is (no dummy coding)\nstep_novel()necessary here for the state variable to get all cross validation folds to work, (b/c there will be different levels included in each fold test and training sets. The new levels for some of the test sets would otherwise result in an error.; “step_novel creates a specification of a recipe step that will assign a previously unseen factor level to a new value.”",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#model-specification",
    "href": "content/lectures/17-cs02-analysis.html#model-specification",
    "title": "17-cs02-analysis",
    "section": "Model Specification",
    "text": "Model Specification\nModel parameters:\n\nmtry - The number of predictor variables (or features) that will be randomly sampled at each split when creating the tree models. The default number for regression analyses is the number of predictors divided by 3.\nmin_n - The minimum number of data points in a node that are required for the node to be split further.\ntrees - the number of trees in the ensemble\n\n. . .\n\n# install.packages(\"randomForest\")\nRF_PM_model &lt;- parsnip::rand_forest(mtry = 10, min_n = 3) %&gt;% \n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"regression\")\n\nRF_PM_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#workflow",
    "href": "content/lectures/17-cs02-analysis.html#workflow",
    "title": "17-cs02-analysis",
    "section": "Workflow",
    "text": "Workflow\n\nRF_wflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(RF_rec) %&gt;%\n  workflows::add_model(RF_PM_model)\n\nRF_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#fit-the-data",
    "href": "content/lectures/17-cs02-analysis.html#fit-the-data",
    "title": "17-cs02-analysis",
    "section": "Fit the Data",
    "text": "Fit the Data\n\nRF_wflow_fit &lt;- parsnip::fit(RF_wflow, data = train_pm)\n\nRF_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, mtry = min_cols(~10,      x), nodesize = min_rows(~3, x)) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 10\n\n          Mean of squared residuals: 2.633639\n                    % Var explained: 59.29",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#assess-feature-importance",
    "href": "content/lectures/17-cs02-analysis.html#assess-feature-importance",
    "title": "17-cs02-analysis",
    "section": "Assess Feature Importance",
    "text": "Assess Feature Importance\n\nRF_wflow_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip::vip(num_features = 10)\n\n\n\n\n\n\n\n\n❓ What’s your interpretation of these results?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#assess-model-performance",
    "href": "content/lectures/17-cs02-analysis.html#assess-model-performance",
    "title": "17-cs02-analysis",
    "section": "Assess Model Performance",
    "text": "Assess Model Performance\n\nset.seed(456)\nresample_RF_fit &lt;- tune::fit_resamples(RF_wflow, vfold_pm)\ncollect_metrics(resample_RF_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   1.67      4  0.101  Preprocessor1_Model1\n2 rsq     standard   0.591     4  0.0514 Preprocessor1_Model1\n\n\n. . .\nFor comparison:\n\ncollect_metrics(resample_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.12      4  0.0444 Preprocessor1_Model1\n2 rsq     standard   0.307     4  0.0263 Preprocessor1_Model1",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#model-tuning",
    "href": "content/lectures/17-cs02-analysis.html#model-tuning",
    "title": "17-cs02-analysis",
    "section": "Model Tuning",
    "text": "Model Tuning\nHyperparameters are often things that we need to specify about a model. Instead of arbitrarily specifying this, we can try to determine the best option for model performance by a process called tuning.\n. . .\nRather than specifying values, we can use tune():\n\ntune_RF_model &lt;- rand_forest(mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"regression\")\n    \ntune_RF_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\n. . .\nCreate Workflow:\n\nRF_tune_wflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(RF_rec) %&gt;%\n  workflows::add_model(tune_RF_model)\n\nRF_tune_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\nDetect how many cores you have access to:\n\nn_cores &lt;- parallel::detectCores()\nn_cores\n\n[1] 10\n\n\n. . .\nThis code will take some time to run:\n\n# install.packages(\"doParallel\")\ndoParallel::registerDoParallel(cores = n_cores)\n\nset.seed(123)\ntune_RF_results &lt;- tune_grid(object = RF_tune_wflow, resamples = vfold_pm, grid = 20)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntune_RF_results\n\n# Tuning results\n# 4-fold cross-validation \n# A tibble: 4 × 4\n  splits            id    .metrics          .notes          \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [438/146]&gt; Fold1 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [438/146]&gt; Fold2 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [438/146]&gt; Fold3 &lt;tibble [40 × 6]&gt; &lt;tibble [1 × 3]&gt;\n4 &lt;split [438/146]&gt; Fold4 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x1: 36 columns were requested but there were 35 predictors in the dat...\n\nRun `show_notes(.Last.tune.result)` for more information.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#check-metrics",
    "href": "content/lectures/17-cs02-analysis.html#check-metrics",
    "title": "17-cs02-analysis",
    "section": "Check Metrics:",
    "text": "Check Metrics:\n\ntune_RF_results %&gt;%\n  collect_metrics()\n\n# A tibble: 40 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    12    33 rmse    standard   1.72      4  0.0866 Preprocessor1_Model01\n 2    12    33 rsq     standard   0.562     4  0.0466 Preprocessor1_Model01\n 3    27    35 rmse    standard   1.69      4  0.102  Preprocessor1_Model02\n 4    27    35 rsq     standard   0.563     4  0.0511 Preprocessor1_Model02\n 5    22    40 rmse    standard   1.71      4  0.106  Preprocessor1_Model03\n 6    22    40 rsq     standard   0.556     4  0.0543 Preprocessor1_Model03\n 7     1    27 rmse    standard   2.03      4  0.0501 Preprocessor1_Model04\n 8     1    27 rsq     standard   0.440     4  0.0245 Preprocessor1_Model04\n 9     6    32 rmse    standard   1.77      4  0.0756 Preprocessor1_Model05\n10     6    32 rsq     standard   0.552     4  0.0435 Preprocessor1_Model05\n# ℹ 30 more rows\n\n\n. . .\n\nshow_best(tune_RF_results, metric = \"rmse\", n = 1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    32    11 rmse    standard    1.65     4   0.113 Preprocessor1_Model10",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#final-model-evaluation",
    "href": "content/lectures/17-cs02-analysis.html#final-model-evaluation",
    "title": "17-cs02-analysis",
    "section": "Final Model Evaluation",
    "text": "Final Model Evaluation\n\ntuned_RF_values&lt;- select_best(tune_RF_results, \"rmse\")\ntuned_RF_values\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    32    11 Preprocessor1_Model10\n\n\n. . .\nThe testing data!\n\n# specify best combination from tune in workflow\nRF_tuned_wflow &lt;-RF_tune_wflow %&gt;%\n  tune::finalize_workflow(tuned_RF_values)\n\n# fit model with those parameters on train AND test\noverallfit &lt;- RF_wflow %&gt;%\n  tune::last_fit(pm_split)\n\ncollect_metrics(overallfit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.72  Preprocessor1_Model1\n2 rsq     standard       0.608 Preprocessor1_Model1\n\n\nResults are similar to what we saw in training (RMSE: 1.65)\n. . .\nGetting the predictions for the test data:\n\ntest_predictions &lt;- collect_predictions(overallfit)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#a-map-of-the-us",
    "href": "content/lectures/17-cs02-analysis.html#a-map-of-the-us",
    "title": "17-cs02-analysis",
    "section": "A map of the US",
    "text": "A map of the US\nPackages needed:\n\nsf - the simple features package helps to convert geographical coordinates into geometry variables which are useful for making 2D plots\nmaps - this package contains geographical outlines and plotting functions to create plots with maps\nrnaturalearth- this allows for easy interaction with map data from Natural Earth which is a public domain map dataset\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(rnaturalearth)\n\nSupport for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#outline-of-the-us",
    "href": "content/lectures/17-cs02-analysis.html#outline-of-the-us",
    "title": "17-cs02-analysis",
    "section": "Outline of the US",
    "text": "Outline of the US\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nglimpse(world)\n\nRows: 241\nColumns: 64\n$ scalerank  &lt;int&gt; 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 5, 3, 1, 1, 1, 1, 1, 1,…\n$ featurecla &lt;chr&gt; \"Admin-0 country\", \"Admin-0 country\", \"Admin-0 country\", \"A…\n$ labelrank  &lt;dbl&gt; 5, 3, 3, 6, 6, 6, 6, 4, 2, 6, 4, 4, 5, 6, 6, 2, 4, 5, 6, 2,…\n$ sovereignt &lt;chr&gt; \"Netherlands\", \"Afghanistan\", \"Angola\", \"United Kingdom\", \"…\n$ sov_a3     &lt;chr&gt; \"NL1\", \"AFG\", \"AGO\", \"GB1\", \"ALB\", \"FI1\", \"AND\", \"ARE\", \"AR…\n$ adm0_dif   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,…\n$ level      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ type       &lt;chr&gt; \"Country\", \"Sovereign country\", \"Sovereign country\", \"Depen…\n$ admin      &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ adm0_a3    &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ geou_dif   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ geounit    &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ gu_a3      &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ su_dif     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ subunit    &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ su_a3      &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_diff   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ name       &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_long  &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_a3     &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_name   &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_group  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ abbrev     &lt;chr&gt; \"Aruba\", \"Afg.\", \"Ang.\", \"Ang.\", \"Alb.\", \"Aland\", \"And.\", \"…\n$ postal     &lt;chr&gt; \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AI\", \"AND\", \"AE\", \"AR\", \"ARM…\n$ formal_en  &lt;chr&gt; \"Aruba\", \"Islamic State of Afghanistan\", \"People's Republic…\n$ formal_fr  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ note_adm0  &lt;chr&gt; \"Neth.\", NA, NA, \"U.K.\", NA, \"Fin.\", NA, NA, NA, NA, \"U.S.A…\n$ note_brk   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Multiple claim…\n$ name_sort  &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_alt   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ mapcolor7  &lt;dbl&gt; 4, 5, 3, 6, 1, 4, 1, 2, 3, 3, 4, 4, 1, 7, 2, 1, 3, 1, 2, 3,…\n$ mapcolor8  &lt;dbl&gt; 2, 6, 2, 6, 4, 1, 4, 1, 1, 1, 5, 5, 2, 5, 2, 2, 1, 6, 2, 2,…\n$ mapcolor9  &lt;dbl&gt; 2, 8, 6, 6, 1, 4, 1, 3, 3, 2, 1, 1, 2, 9, 5, 2, 3, 5, 5, 1,…\n$ mapcolor13 &lt;dbl&gt; 9, 7, 1, 3, 6, 6, 8, 3, 13, 10, 1, NA, 7, 11, 5, 7, 4, 8, 8…\n$ pop_est    &lt;dbl&gt; 103065, 28400000, 12799293, 14436, 3639453, 27153, 83888, 4…\n$ gdp_md_est &lt;dbl&gt; 2258.0, 22270.0, 110300.0, 108.9, 21810.0, 1563.0, 3660.0, …\n$ pop_year   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ lastcensus &lt;dbl&gt; 2010, 1979, 1970, NA, 2001, NA, 1989, 2010, 2010, 2001, 201…\n$ gdp_year   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ economy    &lt;chr&gt; \"6. Developing region\", \"7. Least developed region\", \"7. Le…\n$ income_grp &lt;chr&gt; \"2. High income: nonOECD\", \"5. Low income\", \"3. Upper middl…\n$ wikipedia  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fips_10    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso_a2     &lt;chr&gt; \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AX\", \"AD\", \"AE\", \"AR\", \"AM\",…\n$ iso_a3     &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ iso_n3     &lt;chr&gt; \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ un_a3      &lt;chr&gt; \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ wb_a2      &lt;chr&gt; \"AW\", \"AF\", \"AO\", NA, \"AL\", NA, \"AD\", \"AE\", \"AR\", \"AM\", \"AS…\n$ wb_a3      &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", NA, \"ALB\", NA, \"ADO\", \"ARE\", \"ARG\", \"A…\n$ woe_id     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_is &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_us &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_un &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_wb &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ continent  &lt;chr&gt; \"North America\", \"Asia\", \"Africa\", \"North America\", \"Europe…\n$ region_un  &lt;chr&gt; \"Americas\", \"Asia\", \"Africa\", \"Americas\", \"Europe\", \"Europe…\n$ subregion  &lt;chr&gt; \"Caribbean\", \"Southern Asia\", \"Middle Africa\", \"Caribbean\",…\n$ region_wb  &lt;chr&gt; \"Latin America & Caribbean\", \"South Asia\", \"Sub-Saharan Afr…\n$ name_len   &lt;dbl&gt; 5, 11, 6, 8, 7, 5, 7, 20, 9, 7, 14, 10, 23, 22, 17, 9, 7, 1…\n$ long_len   &lt;dbl&gt; 5, 11, 6, 8, 7, 13, 7, 20, 9, 7, 14, 10, 27, 35, 19, 9, 7, …\n$ abbrev_len &lt;dbl&gt; 5, 4, 4, 4, 4, 5, 4, 6, 4, 4, 9, 4, 7, 10, 6, 4, 5, 4, 4, 5…\n$ tiny       &lt;dbl&gt; 4, NA, NA, NA, NA, 5, 5, NA, NA, NA, 3, NA, NA, 2, 4, NA, N…\n$ homepart   &lt;dbl&gt; NA, 1, 1, NA, 1, NA, 1, 1, 1, 1, NA, 1, NA, NA, 1, 1, 1, 1,…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-69.89912 1..., MULTIPOLYGON (…\n\n\n. . .\nWorld map:\n\nggplot(data = world) +\n    geom_sf() \n\n\n\n\n\n\n\n\n. . .\nJust the US\nAccording to this link, these are the latitude and longitude bounds of the continental US:\n\ntop = 49.3457868 # north lat\nleft = -124.7844079 # west long\nright = -66.9513812 # east long\nbottom = 24.7433195 # south lat\n\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)\n\n\n\n\n\n\n\n\n. . .\nAdding in our monitors…\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)+\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\")\n\n\n\n\n\n\n\n\n. . .\nAdding in county lines\n\ncounties &lt;- sf::st_as_sf(maps::map(\"county\", plot = FALSE,\n                                   fill = TRUE))\n\ncounties\n\nSimple feature collection with 3076 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.6813 ymin: 25.12993 xmax: -67.00742 ymax: 49.38323\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\nFirst 10 features:\n                               ID                           geom\nalabama,autauga   alabama,autauga MULTIPOLYGON (((-86.50517 3...\nalabama,baldwin   alabama,baldwin MULTIPOLYGON (((-87.93757 3...\nalabama,barbour   alabama,barbour MULTIPOLYGON (((-85.42801 3...\nalabama,bibb         alabama,bibb MULTIPOLYGON (((-87.02083 3...\nalabama,blount     alabama,blount MULTIPOLYGON (((-86.9578 33...\nalabama,bullock   alabama,bullock MULTIPOLYGON (((-85.66866 3...\nalabama,butler     alabama,butler MULTIPOLYGON (((-86.8604 31...\nalabama,calhoun   alabama,calhoun MULTIPOLYGON (((-85.74313 3...\nalabama,chambers alabama,chambers MULTIPOLYGON (((-85.59416 3...\nalabama,cherokee alabama,cherokee MULTIPOLYGON (((-85.46812 3...\n\n\n. . .\nAnd now onto the map…\n\nmonitors &lt;- ggplot(data = world) +\n    geom_sf(data = counties, fill = NA, color = gray(.5))+\n      coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE) +\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\") +\n    ggtitle(\"Monitor Locations\") +\n    theme(axis.title.x=element_blank(),\n          axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\nmonitors\n\n\n\n\n\n\n\n\n. . .\nWrangle counties:\n\nseparate county and state into separate columns\nmake title case\ncombine with PM data\n\n\ncounties &lt;- counties %&gt;% \n  tidyr::separate(ID, into = c(\"state\", \"county\"), sep = \",\") %&gt;% \n  dplyr::mutate(county = stringr::str_to_title(county))\n\nmap_data &lt;- dplyr::inner_join(counties, pm, by = \"county\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#map-truth",
    "href": "content/lectures/17-cs02-analysis.html#map-truth",
    "title": "17-cs02-analysis",
    "section": "Map: Truth",
    "text": "Map: Truth\n\nCodePlot\n\n\n\ntruth &lt;- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = value)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"True PM 2.5 levels\") +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#map-predictions",
    "href": "content/lectures/17-cs02-analysis.html#map-predictions",
    "title": "17-cs02-analysis",
    "section": "Map: Predictions",
    "text": "Map: Predictions\n\nDataCodePlot\n\n\n\n# fit data\nRF_final_train_fit &lt;- parsnip::fit(RF_tuned_wflow, data = train_pm)\nRF_final_test_fit &lt;- parsnip::fit(RF_tuned_wflow, data = test_pm)\n\n# get predictions on training data\nvalues_pred_train &lt;- predict(RF_final_train_fit, train_pm) %&gt;% \n  bind_cols(train_pm %&gt;% select(value, fips, county, id)) \n\n# get predictions on testing data\nvalues_pred_test &lt;- predict(RF_final_test_fit, test_pm) %&gt;% \n  bind_cols(test_pm %&gt;% select(value, fips, county, id)) \nvalues_pred_test\n\n# A tibble: 292 × 5\n   .pred value fips  county     id       \n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;      &lt;fct&gt;    \n 1  11.6  11.2 1033  Colbert    1033.1002\n 2  11.9  12.4 1055  Etowah     1055.001 \n 3  11.1  10.5 1069  Houston    1069.0003\n 4  13.9  15.6 1073  Jefferson  1073.0023\n 5  12.0  12.4 1073  Jefferson  1073.1005\n 6  11.3  11.1 1073  Jefferson  1073.1009\n 7  11.5  11.8 1073  Jefferson  1073.5003\n 8  11.0  10.0 1097  Mobile     1097.0003\n 9  11.9  12.0 1101  Montgomery 1101.0007\n10  12.9  13.2 1113  Russell    1113.0001\n# ℹ 282 more rows\n\n# combine\nall_pred &lt;- bind_rows(values_pred_test, values_pred_train)\n\n\n\n\nmap_data &lt;- inner_join(counties, all_pred, by = \"county\")\n\npred &lt;- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = .pred)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"Predicted PM 2.5 levels\") +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis.html#final-plot",
    "href": "content/lectures/17-cs02-analysis.html#final-plot",
    "title": "17-cs02-analysis",
    "section": "Final Plot",
    "text": "Final Plot\n\nlibrary(patchwork)\n\n(truth/pred) + \n  plot_annotation(title = \"Machine Learning Methods Allow for Prediction of Air Pollution\", subtitle = \"A random forest model predicts true monitored levels of fine particulate matter (PM 2.5) air pollution based on\\ndata about population density and other predictors reasonably well, thus suggesting that we can use similar methods to predict levels\\nof pollution in places with poor monitoring\",\n                  theme = theme(plot.title = element_text(size =12, face = \"bold\"), \n                                plot.subtitle = element_text(size = 8)))\n\n\n\n\n\n\n\n\n❓ What do you learn from these results?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#qa",
    "href": "content/lectures/17-cs02-analysis-slides.html#qa",
    "title": "17-cs02-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Just to confirm, if we choose to do the CS02 do we still do it with our final group or the assigned CS02 group?\nA: Final group!\n\n\nQ: If we choose to do the final project instead of CS2, will everything be the same as our originally planned final projects in terms of contents except the grade weighting?\nA: Yup!\n\n\nQ: Understanding some of the variable’s meaning, maybe would have been good for us to recode them to make them more intuitive.\nA: A good suggestion if you decide to go this route for the final!\n\n\nQ: For the distribution on the diagonal line of the correlation graphs, what does x/y axis represent?\nA: It’s a densityplot (shows the distribution) for each individual variable - the one that’s in that row.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#course-announcements",
    "href": "content/lectures/17-cs02-analysis-slides.html#course-announcements",
    "title": "17-cs02-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nCS01 due tonight (group work survey due Friday)\nLab07 due tomorrow (Friday)\nFinal Project due Tues of Finals week (report + presentation + general communication)\n\n\n\nAny CS01 questions?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#recall",
    "href": "content/lectures/17-cs02-analysis-slides.html#recall",
    "title": "17-cs02-analysis",
    "section": "Recall:",
    "text": "Recall:",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#the-data",
    "href": "content/lectures/17-cs02-analysis-slides.html#the-data",
    "title": "17-cs02-analysis",
    "section": "The Data",
    "text": "The Data\n\npm &lt;- read_csv(here::here(\"OCS_data\", \"data\", \"raw\", \"pm25_data.csv\"))\n\nRows: 876 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): state, county, city\ndbl (47): id, value, fips, lat, lon, CMAQ, zcta, zcta_area, zcta_pop, imp_a5...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Converting to factors as discussed last class\npm &lt;- pm %&gt;%\n  dplyr::mutate(across(c(id, fips, zcta), as.factor))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#data-splitting",
    "href": "content/lectures/17-cs02-analysis-slides.html#data-splitting",
    "title": "17-cs02-analysis",
    "section": "Data Splitting",
    "text": "Data Splitting\n\n\n\n\nSpecify the split:\n\nset.seed(1234)\npm_split &lt;- rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n&lt;Training/Testing/Total&gt;\n&lt;584/292/876&gt;\n\n\n\nset.seed &lt;- ensures we all get the exact same random split\noutput displayed: &lt;training data sample number, testing data sample number, original sample number&gt;\n\nMore on how people decide what proportions to use for data splitting here\n\n\nSplit the Data\n\ntrain_pm &lt;- rsample::training(pm_split)\ntest_pm &lt;- rsample::testing(pm_split)\n \n# Scroll through the output!\ncount(train_pm, state)\n\n# A tibble: 49 × 2\n   state                    n\n   &lt;chr&gt;                &lt;int&gt;\n 1 Alabama                 13\n 2 Arizona                 12\n 3 Arkansas                 8\n 4 California              55\n 5 Colorado                10\n 6 Connecticut             12\n 7 Delaware                 3\n 8 District Of Columbia     2\n 9 Florida                 22\n10 Georgia                 20\n# ℹ 39 more rows\n\n\n❓ What do you observe about the output?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#pre-processing-recipe-bake",
    "href": "content/lectures/17-cs02-analysis-slides.html#pre-processing-recipe-bake",
    "title": "17-cs02-analysis",
    "section": "Pre-processing: recipe() + bake()",
    "text": "Pre-processing: recipe() + bake()\nNeed to:\n\nspecify predictors vs. outcome\nscale variables\nremove redundant variables (feature engineering)\n\n\nrecipe provides a standardized format for a sequence of steps for pre-processing the data",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-variable-roles",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-variable-roles",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify variable roles",
    "text": "Step 1: Specify variable roles\nThe simplest approach…\n\nsimple_rec &lt;- train_pm %&gt;%\n  recipes::recipe(value ~ .)\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 49\n\n\n\n…but we need to specify which column includes ID information\n\nsimple_rec &lt;- train_pm %&gt;%\n  recipes::recipe(value ~ .) %&gt;%\n  recipes::update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n…and which are our predictors and which is our outcome\n\nsimple_rec &lt;- recipe(train_pm) %&gt;%\n    update_role(everything(), new_role = \"predictor\") %&gt;%\n    update_role(value, new_role = \"outcome\") %&gt;%\n    update_role(id, new_role = \"id variable\")\n\nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n❓ Can someone summarize what this code is specifying?\n\n\nSummarizing our recipe thus far:\n\nsummary(simple_rec)\n\n# A tibble: 50 × 4\n   variable type      role        source  \n   &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;       &lt;chr&gt;   \n 1 id       &lt;chr [3]&gt; id variable original\n 2 value    &lt;chr [2]&gt; outcome     original\n 3 fips     &lt;chr [3]&gt; predictor   original\n 4 lat      &lt;chr [2]&gt; predictor   original\n 5 lon      &lt;chr [2]&gt; predictor   original\n 6 state    &lt;chr [3]&gt; predictor   original\n 7 county   &lt;chr [3]&gt; predictor   original\n 8 city     &lt;chr [3]&gt; predictor   original\n 9 CMAQ     &lt;chr [2]&gt; predictor   original\n10 zcta     &lt;chr [3]&gt; predictor   original\n# ℹ 40 more rows",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-2-specify-pre-processing-with-step",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-2-specify-pre-processing-with-step",
    "title": "17-cs02-analysis",
    "section": "Step 2: Specify pre-processing with step*()",
    "text": "Step 2: Specify pre-processing with step*()\n\n\n\n\nThere are step functions for a variety of purposes:\n\nImputation – filling in missing values based on the existing data\nTransformation – changing all values of a variable in the same way, typically to make it more normal or easier to interpret\nDiscretization – converting continuous values into discrete or nominal values - binning for example to reduce the number of possible levels (However this is generally not advisable!)\nEncoding / Creating Dummy Variables – creating a numeric code for categorical variables (More on one-hot and Dummy Variables encoding)\nData type conversions – which means changing from integer to factor or numeric to date etc.\nInteraction term addition to the model – which means that we would be modeling for predictors that would influence the capacity of each other to predict the outcome\nNormalization – centering and scaling the data to a similar range of values\nDimensionality Reduction/ Signal Extraction – reducing the space of features or predictors to a smaller set of variables that capture the variation or signal in the original variables (ex. Principal Component Analysis and Independent Component Analysis)\nFiltering – filtering options for removing variables (ex. remove variables that are highly correlated to others or remove variables with very little variance and therefore likely little predictive capacity)\nRow operations – performing functions on the values within the rows (ex. rearranging, filtering, imputing)\nChecking functions – Gut checks to look for missing values, to look at the variable classes etc.\n\nThis link and this link show the many options for recipe step functions.\n\n\nThere are several ways to select what variables to apply steps to:\n\nUsing tidyselect methods: contains(), matches(), starts_with(), ends_with(), everything(), num_range()\n\nUsing the type: all_nominal(), all_numeric() , has_type()\nUsing the role: all_predictors(), all_outcomes(), has_role()\nUsing the name - use the actual name of the variable/variables of interest\n\n\n\nOne-hot encoding categorical variables:\n\nsimple_rec %&gt;%\n  step_dummy(state, county, city, zcta, one_hot = TRUE)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: state, county, city, zcta\n\n\n❓ Can anyone remind us what one-hot encoding does?\n\n\n\nfips includes numeric code for state and county, so it’s another way to specify county\nso, we’ll change fips’ role\nwe get to decide what to call it (\"county id\")\n\n\nsimple_rec %&gt;%\n  update_role(\"fips\", new_role = \"county id\")\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   47\ncounty id:    1\nid variable:  1\n\n\n\n\nRemoving highly correlated variables:\n\nsimple_rec %&gt;%\n  step_corr(all_predictors(), - CMAQ, - aod)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: all_predictors(), -CMAQ, -aod\n\n\n\nspecifying to KEEP CMAQ and aod\n\n\n\nRemoving variables with non-zero variance:\n\nsimple_rec %&gt;%\n  step_nzv(all_predictors(), - CMAQ, - aod)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   48\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Sparse, unbalanced variable filter on: all_predictors(), -CMAQ, -aod",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#putting-our-recipe-together",
    "href": "content/lectures/17-cs02-analysis-slides.html#putting-our-recipe-together",
    "title": "17-cs02-analysis",
    "section": "Putting our recipe together",
    "text": "Putting our recipe together\n\nsimple_rec &lt;- simple_rec %&gt;% \n  update_role(\"fips\", new_role = \"county id\") %&gt;%\n  step_dummy(state, county, city, zcta, one_hot = TRUE) %&gt;%\n  step_corr(all_predictors(), - CMAQ, - aod)%&gt;%\n  step_nzv(all_predictors(), - CMAQ, - aod)\n  \nsimple_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor:   47\ncounty id:    1\nid variable:  1\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: state, county, city, zcta\n\n\n• Correlation filter on: all_predictors(), -CMAQ, -aod\n\n\n• Sparse, unbalanced variable filter on: all_predictors(), -CMAQ, -aod\n\n\nNote: order of steps matters",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-3-running-the-pre-processing-prep",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-3-running-the-pre-processing-prep",
    "title": "17-cs02-analysis",
    "section": "Step 3: Running the pre-processing (prep)",
    "text": "Step 3: Running the pre-processing (prep)\nThere are some important arguments to know about:\n\ntraining - you must supply a training data set to estimate parameters for pre-processing operations (recipe steps) - this may already be included in your recipe - as is the case for us\nfresh - if fresh=TRUE, will retrain and estimate parameters for any previous steps that were already prepped if you add more steps to the recipe (default is FALSE)\nverbose - if verbose=TRUE, shows the progress as the steps are evaluated and the size of the pre-processed training set (default is FALSE)\nretain - if retain=TRUE, then the pre-processed training set will be saved within the recipe (as template). This is good if you are likely to add more steps and do not want to rerun the prep() on the previous steps. However this can make the recipe size large. This is necessary if you want to actually look at the pre-processed data (default is TRUE)\n\n\nprepped_rec &lt;- prep(simple_rec, verbose = TRUE, retain = TRUE )\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.26 Mb  in memory.\n\nnames(prepped_rec)\n\n [1] \"var_info\"       \"term_info\"      \"steps\"          \"template\"      \n [5] \"levels\"         \"retained\"       \"requirements\"   \"tr_info\"       \n [9] \"orig_lvls\"      \"last_term_info\"\n\n\n\nThis output includes a lot of information:\n\nthe steps that were run\n\nthe original variable info (var_info)\n\nthe updated variable info after pre-processing (term_info)\nthe new levels of the variables\nthe original levels of the variables (orig_lvls)\ninfo about the training data set size and completeness (tr_info)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-4-extract-pre-processed-training-data-using-bake",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-4-extract-pre-processed-training-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 4: Extract pre-processed training data using bake()",
    "text": "Step 4: Extract pre-processed training data using bake()\n\n\n\nbake(): apply our modeling steps (in this case just pre-processing on the training data) and see what it would do the data\n\n\nbaked_train &lt;- bake(prepped_rec, new_data = NULL)\nglimpse(baked_train)\n\nRows: 584\nColumns: 37\n$ id                          &lt;fct&gt; 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       &lt;dbl&gt; 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        &lt;fct&gt; 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         &lt;dbl&gt; 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         &lt;dbl&gt; -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        &lt;dbl&gt; 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   &lt;dbl&gt; 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    &lt;dbl&gt; 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    &lt;dbl&gt; 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  &lt;dbl&gt; 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 &lt;dbl&gt; 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  &lt;dbl&gt; 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          &lt;dbl&gt; 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        &lt;dbl&gt; 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       &lt;dbl&gt; 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              &lt;dbl&gt; 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                &lt;dbl&gt; 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        &lt;dbl&gt; 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      &lt;dbl&gt; 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          &lt;dbl&gt; 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 &lt;dbl&gt; 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   &lt;dbl&gt; 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    &lt;dbl&gt; 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        &lt;dbl&gt; 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         &lt;dbl&gt; 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   &lt;dbl&gt; 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     &lt;dbl&gt; 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         &lt;dbl&gt; 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n\nnew_data = NULL specifies that we’re not (yet) looking at our testing data\nWe only have 36 variables (33 predictors + 2 id variables + outcome)\ncategorical variables (state) are gone (one-hot encoding)\nstate_California remains - only state with nonzero variance (largest # of monitors)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-5-extract-pre-processed-testing-data-using-bake",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-5-extract-pre-processed-testing-data-using-bake",
    "title": "17-cs02-analysis",
    "section": "Step 5: Extract pre-processed testing data using bake()",
    "text": "Step 5: Extract pre-processed testing data using bake()\n\nbake() takes a trained recipe and applies the operations to a data set to create a design matrix. For example: it applies the centering to new data sets using these means used to create the recipe. - tidymodels documentation\n\n\nTypically, you want to avoid using your testing data…but our data set is not that large and NA values in our testing dataset could cause issues later on.\n\n\n\n\n\n\nbaked_test_pm &lt;- recipes::bake(prepped_rec, new_data = test_pm)\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 37\n$ id                          &lt;fct&gt; 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       &lt;dbl&gt; 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        &lt;fct&gt; 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         &lt;dbl&gt; 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         &lt;dbl&gt; -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        &lt;dbl&gt; 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   &lt;dbl&gt; 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    &lt;dbl&gt; 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    &lt;dbl&gt; 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  &lt;dbl&gt; 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 &lt;dbl&gt; 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  &lt;dbl&gt; 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          &lt;dbl&gt; 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        &lt;dbl&gt; 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       &lt;dbl&gt; 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              &lt;dbl&gt; 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                &lt;dbl&gt; 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        &lt;dbl&gt; 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      &lt;dbl&gt; 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          &lt;dbl&gt; 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 &lt;dbl&gt; 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   &lt;dbl&gt; 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    &lt;dbl&gt; 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        &lt;dbl&gt; 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         &lt;dbl&gt; 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   &lt;dbl&gt; 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     &lt;dbl&gt; 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         &lt;dbl&gt; 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; NA, NA, NA, 0, 1, 1, 1, NA, NA, NA, 0, NA,…\n\n\n\n\nHmm….lots of NAs now in city_Not.in.a.city\nLikely b/c there are cities in our testing dataset that were not in our training dataset…\n\ntraincities &lt;- train_pm %&gt;% distinct(city)\ntestcities &lt;- test_pm %&gt;% distinct(city)\n\n#get the number of cities that were different\ndim(dplyr::setdiff(traincities, testcities))\n\n[1] 381   1\n\n#get the number of cities that overlapped\ndim(dplyr::intersect(traincities, testcities))\n\n[1] 55  1",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#aside-return-to-wrangling",
    "href": "content/lectures/17-cs02-analysis-slides.html#aside-return-to-wrangling",
    "title": "17-cs02-analysis",
    "section": "Aside: return to wrangling",
    "text": "Aside: return to wrangling\nA quick return to wrangling…and re-splitting our data\n\npm &lt;- pm %&gt;%\n  mutate(city = case_when(city == \"Not in a city\" ~ \"Not in a city\",\n                          city != \"Not in a city\" ~ \"In a city\"))\n\nset.seed(1234) # same seed as before\npm_split &lt;-rsample::initial_split(data = pm, prop = 2/3)\npm_split\n\n&lt;Training/Testing/Total&gt;\n&lt;584/292/876&gt;\n\n train_pm &lt;-rsample::training(pm_split)\n test_pm &lt;-rsample::testing(pm_split)\n\n\nAnd a recipe update…(putting it all together)\n\nnovel_rec &lt;- recipe(train_pm) %&gt;%\n    update_role(everything(), new_role = \"predictor\") %&gt;%\n    update_role(value, new_role = \"outcome\") %&gt;%\n    update_role(id, new_role = \"id variable\") %&gt;%\n    update_role(\"fips\", new_role = \"county id\") %&gt;%\n    step_dummy(state, county, city, zcta, one_hot = TRUE) %&gt;%\n    step_corr(all_numeric()) %&gt;%\n    step_nzv(all_numeric()) \n\n\n\nre-bake()\n\nprepped_rec &lt;- prep(novel_rec, verbose = TRUE, retain = TRUE)\n\noper 1 step dummy [training] \noper 2 step corr [training] \noper 3 step nzv [training] \nThe retained training set is ~ 0.27 Mb  in memory.\n\nbaked_train &lt;- bake(prepped_rec, new_data = NULL)\n\n\n\nLooking at the output\n\nglimpse(baked_train)\n\nRows: 584\nColumns: 38\n$ id                          &lt;fct&gt; 18003.0004, 55041.0007, 6065.1003, 39009.0…\n$ value                       &lt;dbl&gt; 11.699065, 6.956780, 13.289744, 10.742000,…\n$ fips                        &lt;fct&gt; 18003, 55041, 6065, 39009, 39061, 24510, 6…\n$ lat                         &lt;dbl&gt; 41.09497, 45.56300, 33.94603, 39.44217, 39…\n$ lon                         &lt;dbl&gt; -85.10182, -88.80880, -117.40063, -81.9088…\n$ CMAQ                        &lt;dbl&gt; 10.383231, 3.411247, 11.404085, 7.971165, …\n$ zcta_area                   &lt;dbl&gt; 16696709, 370280916, 41957182, 132383592, …\n$ zcta_pop                    &lt;dbl&gt; 21306, 4141, 44001, 1115, 6566, 934, 41192…\n$ imp_a500                    &lt;dbl&gt; 28.9783737, 0.0000000, 30.3901384, 0.00000…\n$ imp_a15000                  &lt;dbl&gt; 13.0547959, 0.3676404, 23.7457506, 0.33079…\n$ county_area                 &lt;dbl&gt; 1702419942, 2626421270, 18664696661, 13043…\n$ county_pop                  &lt;dbl&gt; 355329, 9304, 2189641, 64757, 802374, 6209…\n$ log_dist_to_prisec          &lt;dbl&gt; 6.621891, 8.415468, 7.419762, 6.344681, 5.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 8.517193, 10.150514, 8.517193, 9…\n$ log_pri_length_25000        &lt;dbl&gt; 12.77378, 10.16440, 13.14450, 10.12663, 13…\n$ log_prisec_length_500       &lt;dbl&gt; 6.214608, 6.214608, 6.214608, 6.214608, 7.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.240294, 7.600902, 7.600902, 8.793450, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.485093, 9.425537, 10.155961, 10.562382,…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.75582, 11.44833, 11.59563, 11.69093, 12…\n$ log_prisec_length_25000     &lt;dbl&gt; 13.98749, 13.15082, 13.44293, 13.58697, 14…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 4.91110140, 3.86982666, 4.03184660, 0.0000…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 5.399131, 3.883689, 5.459257, 0.000000, 6.…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 5.816047, 3.887264, 6.884537, 3.765635, 6.…\n$ popdens_county              &lt;dbl&gt; 208.719947, 3.542463, 117.314577, 49.64834…\n$ popdens_zcta                &lt;dbl&gt; 1276.059851, 11.183401, 1048.711994, 8.422…\n$ nohs                        &lt;dbl&gt; 4.3, 5.1, 3.7, 4.8, 2.1, 0.0, 2.5, 7.7, 0.…\n$ somehs                      &lt;dbl&gt; 6.7, 10.4, 5.9, 11.5, 10.5, 0.0, 4.3, 7.5,…\n$ hs                          &lt;dbl&gt; 31.7, 40.3, 17.9, 47.3, 30.0, 0.0, 17.8, 2…\n$ somecollege                 &lt;dbl&gt; 27.2, 24.1, 26.3, 20.0, 27.1, 0.0, 26.1, 2…\n$ associate                   &lt;dbl&gt; 8.2, 7.4, 8.3, 3.1, 8.5, 71.4, 13.2, 7.6, …\n$ bachelor                    &lt;dbl&gt; 15.0, 8.6, 20.2, 9.8, 14.2, 0.0, 23.4, 17.…\n$ grad                        &lt;dbl&gt; 6.8, 4.2, 17.7, 3.5, 7.6, 28.6, 12.6, 12.3…\n$ pov                         &lt;dbl&gt; 13.500, 18.900, 6.700, 14.400, 12.500, 3.5…\n$ hs_orless                   &lt;dbl&gt; 42.7, 55.8, 27.5, 63.6, 42.6, 0.0, 24.6, 3…\n$ urc2006                     &lt;dbl&gt; 3, 6, 1, 5, 1, 1, 2, 1, 2, 6, 4, 4, 4, 4, …\n$ aod                         &lt;dbl&gt; 54.11111, 31.16667, 83.12500, 33.36364, 50…\n$ state_California            &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n\n\n\n\nMaking sure the NA issue is taken are of:\n\nbaked_test_pm &lt;- bake(prepped_rec, new_data = test_pm)\n\nglimpse(baked_test_pm)\n\nRows: 292\nColumns: 38\n$ id                          &lt;fct&gt; 1033.1002, 1055.001, 1069.0003, 1073.0023,…\n$ value                       &lt;dbl&gt; 11.212174, 12.375394, 10.508850, 15.591017…\n$ fips                        &lt;fct&gt; 1033, 1055, 1069, 1073, 1073, 1073, 1073, …\n$ lat                         &lt;dbl&gt; 34.75878, 33.99375, 31.22636, 33.55306, 33…\n$ lon                         &lt;dbl&gt; -87.65056, -85.99107, -85.39077, -86.81500…\n$ CMAQ                        &lt;dbl&gt; 9.402679, 9.241744, 9.121892, 10.235612, 1…\n$ zcta_area                   &lt;dbl&gt; 16716984, 154069359, 162685124, 26929603, …\n$ zcta_pop                    &lt;dbl&gt; 9042, 20045, 30217, 9010, 16140, 3699, 137…\n$ imp_a500                    &lt;dbl&gt; 19.17301038, 16.49307958, 19.13927336, 41.…\n$ imp_a15000                  &lt;dbl&gt; 5.2472094, 5.1612102, 4.7401296, 17.452484…\n$ county_area                 &lt;dbl&gt; 1534877333, 1385618994, 1501737720, 287819…\n$ county_pop                  &lt;dbl&gt; 54428, 104430, 101547, 658466, 658466, 194…\n$ log_dist_to_prisec          &lt;dbl&gt; 5.760131, 5.261457, 7.112373, 6.600958, 6.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 9.066563, 8.517193, 11.156977, 1…\n$ log_pri_length_25000        &lt;dbl&gt; 10.15769, 12.01356, 10.12663, 12.98762, 12…\n$ log_prisec_length_500       &lt;dbl&gt; 8.611945, 8.740680, 6.214608, 6.214608, 6.…\n$ log_prisec_length_1000      &lt;dbl&gt; 9.735569, 9.627898, 7.600902, 9.075921, 8.…\n$ log_prisec_length_5000      &lt;dbl&gt; 11.770407, 11.728889, 12.298627, 12.281645…\n$ log_prisec_length_10000     &lt;dbl&gt; 12.840663, 12.768279, 12.994141, 13.278416…\n$ log_prisec_length_25000     &lt;dbl&gt; 13.79973, 13.70026, 13.85550, 14.45221, 13…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 6.69187313, 4.43719884, 0.92888890, 8.2097…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 6.70127741, 4.46267932, 3.67473904, 8.6488…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 7.148858, 4.678311, 3.744629, 8.858019, 8.…\n$ popdens_county              &lt;dbl&gt; 35.460814, 75.367038, 67.619664, 228.77763…\n$ popdens_zcta                &lt;dbl&gt; 540.8870404, 130.1037411, 185.7391706, 334…\n$ nohs                        &lt;dbl&gt; 7.3, 4.3, 5.8, 7.1, 2.7, 11.1, 9.7, 3.0, 8…\n$ somehs                      &lt;dbl&gt; 15.8, 13.3, 11.6, 17.1, 6.6, 11.6, 21.6, 1…\n$ hs                          &lt;dbl&gt; 30.6, 27.8, 29.8, 37.2, 30.7, 46.0, 39.3, …\n$ somecollege                 &lt;dbl&gt; 20.9, 29.2, 21.4, 23.5, 25.7, 17.2, 21.6, …\n$ associate                   &lt;dbl&gt; 7.6, 10.1, 7.9, 7.3, 8.0, 4.1, 5.2, 6.6, 4…\n$ bachelor                    &lt;dbl&gt; 12.7, 10.0, 13.7, 5.9, 17.6, 7.1, 2.2, 7.8…\n$ grad                        &lt;dbl&gt; 5.1, 5.4, 9.8, 2.0, 8.7, 2.9, 0.4, 4.2, 3.…\n$ pov                         &lt;dbl&gt; 19.0, 8.8, 15.6, 25.5, 7.3, 8.1, 13.3, 23.…\n$ hs_orless                   &lt;dbl&gt; 53.7, 45.4, 47.2, 61.4, 40.0, 68.7, 70.6, …\n$ urc2006                     &lt;dbl&gt; 4, 4, 4, 1, 1, 1, 2, 3, 3, 3, 2, 5, 4, 1, …\n$ aod                         &lt;dbl&gt; 36.000000, 43.416667, 33.000000, 39.583333…\n$ state_California            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ city_Not.in.a.city          &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, …",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#specifying-our-model-parsnip",
    "href": "content/lectures/17-cs02-analysis-slides.html#specifying-our-model-parsnip",
    "title": "17-cs02-analysis",
    "section": "Specifying our model (parsnip)",
    "text": "Specifying our model (parsnip)\nThere are four things we need to define about our model:\n\n\nThe type of model (using specific functions in parsnip like rand_forest(), logistic_reg() etc.)\n\nThe package or engine that we will use to implement the type of model selected (using the set_engine() function)\nThe mode of learning - classification or regression (using the set_mode() function)\nAny arguments necessary for the model/package selected (using the set_args()function - for example the mtry = argument for random forest which is the number of variables to be used as options for splitting at each tree node)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-the-model",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-1-specify-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 1: Specify the model",
    "text": "Step 1: Specify the model\n\nWe’ll start with linear regression, but move to random forest\nSee here for modeling options in parsnip.\n\n\nlm_PM_model &lt;- parsnip::linear_reg() %&gt;%\n  parsnip::set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nlm_PM_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-2-fit-the-model",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-2-fit-the-model",
    "title": "17-cs02-analysis",
    "section": "Step 2: Fit the model",
    "text": "Step 2: Fit the model\n\nworkflows package allows us to keep track of both our pre-processing steps and our model specification\nIt also allows us to implement fancier optimizations in an automated way and it can also handle post-processing operations.\n\n\nPM_wflow &lt;- workflows::workflow() %&gt;%\n            workflows::add_recipe(novel_rec) %&gt;%\n            workflows::add_model(lm_PM_model)\nPM_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n❓ Who can explain the difference between a recipe, baking, and a workflow?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-3-prepare-the-recipe-estimate-the-parameters",
    "title": "17-cs02-analysis",
    "section": "Step 3: Prepare the recipe (estimate the parameters)",
    "text": "Step 3: Prepare the recipe (estimate the parameters)\n\nPM_wflow_fit &lt;- parsnip::fit(PM_wflow, data = train_pm)\nPM_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_dummy()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n                (Intercept)                          lat  \n                  2.936e+02                    3.261e-02  \n                        lon                         CMAQ  \n                  1.586e-02                    2.463e-01  \n                  zcta_area                     zcta_pop  \n                 -3.433e-10                    1.013e-05  \n                   imp_a500                   imp_a15000  \n                  5.064e-03                   -3.066e-03  \n                county_area                   county_pop  \n                 -2.324e-11                   -7.576e-08  \n         log_dist_to_prisec          log_pri_length_5000  \n                  6.214e-02                   -2.006e-01  \n       log_pri_length_25000        log_prisec_length_500  \n                 -5.411e-02                    2.204e-01  \n     log_prisec_length_1000       log_prisec_length_5000  \n                  1.154e-01                    2.374e-01  \n    log_prisec_length_10000      log_prisec_length_25000  \n                 -3.436e-02                    5.224e-01  \nlog_nei_2008_pm10_sum_10000  log_nei_2008_pm10_sum_15000  \n                  1.829e-01                   -2.355e-02  \nlog_nei_2008_pm10_sum_25000               popdens_county  \n                  2.403e-02                    2.203e-05  \n               popdens_zcta                         nohs  \n                 -2.132e-06                   -2.983e+00  \n                     somehs                           hs  \n                 -2.956e+00                   -2.962e+00  \n                somecollege                    associate  \n                 -2.967e+00                   -2.999e+00  \n                   bachelor                         grad  \n                 -2.979e+00                   -2.978e+00  \n                        pov                    hs_orless  \n                  1.859e-03                           NA  \n                    urc2006                          aod  \n                  2.577e-01                    1.535e-02  \n           state_California           city_Not.in.a.city  \n                  3.114e+00                   -4.250e-02",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#step-4-assess-model-fit",
    "href": "content/lectures/17-cs02-analysis-slides.html#step-4-assess-model-fit",
    "title": "17-cs02-analysis",
    "section": "Step 4: Assess model fit",
    "text": "Step 4: Assess model fit\n\nwflowoutput &lt;- PM_wflow_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  broom::tidy() \n\nwflowoutput\n\n# A tibble: 36 × 5\n   term         estimate std.error statistic       p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 (Intercept)  2.94e+ 2  1.18e+ 2     2.49  0.0130       \n 2 lat          3.26e- 2  2.28e- 2     1.43  0.153        \n 3 lon          1.59e- 2  1.01e- 2     1.58  0.115        \n 4 CMAQ         2.46e- 1  3.97e- 2     6.20  0.00000000108\n 5 zcta_area   -3.43e-10  1.60e-10    -2.15  0.0320       \n 6 zcta_pop     1.01e- 5  5.33e- 6     1.90  0.0578       \n 7 imp_a500     5.06e- 3  7.42e- 3     0.683 0.495        \n 8 imp_a15000  -3.07e- 3  1.16e- 2    -0.263 0.792        \n 9 county_area -2.32e-11  1.97e-11    -1.18  0.238        \n10 county_pop  -7.58e- 8  9.29e- 8    -0.815 0.415        \n# ℹ 26 more rows\n\n\n\nWe have fit our model on our training data\nWe have created a model to predict values of air pollution based on the predictors that we have included\n\n\nUnderstanding what variables are most important in our model…\n\nPM_wflow_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip::vip(num_features = 10)\n\n\n\n\n\n\n\n\n\n\nA closer look at monitors in CA:\n\nbaked_train %&gt;% \n  mutate(state_California = as.factor(state_California)) %&gt;%\n  mutate(state_California = recode(state_California, \n                                   \"0\" = \"Not California\", \n                                   \"1\" = \"California\")) %&gt;%\n  ggplot(aes(x = state_California, y = value)) + \n  geom_boxplot() +\n  geom_jitter(width = .05) + \n  xlab(\"Location of Monitor\")\n\n\n\n\n\n\n\n\n\n\nRemember: machine learning (ML) as an optimization problem that tries to minimize the distance between our predicted outcome \\(\\hat{Y} = f(X)\\) and actual outcome \\(Y\\) using our features (or predictor variables) \\(X\\) as input to a function \\(f\\) that we want to estimate.\n\\[d(Y - \\hat{Y})\\]\n\n\nLet’s pull out our predicted outcome values \\(\\hat{Y} = f(X)\\) from the models we fit (using different approaches).\n\nwf_fit &lt;- PM_wflow_fit %&gt;% \n  extract_fit_parsnip()\n\n\nwf_fitted_values &lt;- \n  broom::augment(wf_fit[[\"fit\"]], data = baked_train) %&gt;% \n  select(value, .fitted:.std.resid)\n\nhead(wf_fitted_values)\n\n# A tibble: 6 × 6\n  value .fitted   .hat .sigma   .cooksd .std.resid\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 11.7    12.2  0.0370   2.05 0.0000648     -0.243\n2  6.96    9.14 0.0496   2.05 0.00179       -1.09 \n3 13.3    12.6  0.0484   2.05 0.000151       0.322\n4 10.7    10.4  0.0502   2.05 0.0000504      0.183\n5 14.5    11.9  0.0243   2.05 0.00113        1.26 \n6 12.2     9.52 0.476    2.04 0.0850         1.81",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#visualizing-model-performance",
    "href": "content/lectures/17-cs02-analysis-slides.html#visualizing-model-performance",
    "title": "17-cs02-analysis",
    "section": "Visualizing Model Performance",
    "text": "Visualizing Model Performance\n\nwf_fitted_values %&gt;% \n  ggplot(aes(x =  value, y = .fitted)) + \n  geom_point() + \n  xlab(\"actual outcome values\") + \n  ylab(\"predicted outcome values\")\n\n\n❓ What do you notice about/learn from these results?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#quantifying-model-performance",
    "href": "content/lectures/17-cs02-analysis-slides.html#quantifying-model-performance",
    "title": "17-cs02-analysis",
    "section": "Quantifying Model Performance",
    "text": "Quantifying Model Performance\n\\[RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}{(\\hat{y_t}- y_t)}^2}{n}}\\]\n\nCan use the yardstick package using the rmse()` function to calculate:\n\nyardstick::metrics(wf_fitted_values,\n                   truth = value, estimate = .fitted)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       1.98 \n2 rsq     standard       0.392\n3 mae     standard       1.47 \n\n\n\nRMSE isn’t too bad\n\\(R^2\\) suggests model is only explaining 39% of the variance in the data\nThe MAE value suggests that the average difference between the value predicted and the real value was 1.47 ug/m3. The range of the values was 3-22 in the training data, so this is a relatively small amount",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#cross-validation",
    "href": "content/lectures/17-cs02-analysis-slides.html#cross-validation",
    "title": "17-cs02-analysis",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nResampling + Re-partitioning:\n\n\n\n\nPreparing the data for cross-validation:\n\n\n\nNote: this is called v-fold or k-fold CV",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#implementing-in-rsample",
    "href": "content/lectures/17-cs02-analysis-slides.html#implementing-in-rsample",
    "title": "17-cs02-analysis",
    "section": "Implementing in rsample()",
    "text": "Implementing in rsample()\n\nset.seed(1234)\nvfold_pm &lt;- rsample::vfold_cv(data = train_pm, v = 4)\nvfold_pm\n\n#  4-fold cross-validation \n# A tibble: 4 × 2\n  splits            id   \n  &lt;list&gt;            &lt;chr&gt;\n1 &lt;split [438/146]&gt; Fold1\n2 &lt;split [438/146]&gt; Fold2\n3 &lt;split [438/146]&gt; Fold3\n4 &lt;split [438/146]&gt; Fold4\n\n\n\n\npull(vfold_pm, splits)\n\n[[1]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n[[2]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n[[3]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n[[4]]\n&lt;Analysis/Assess/Total&gt;\n&lt;438/146/584&gt;\n\n\n\n\nVisualizing this process:",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#model-assessment-on-v-folds",
    "href": "content/lectures/17-cs02-analysis-slides.html#model-assessment-on-v-folds",
    "title": "17-cs02-analysis",
    "section": "Model Assessment on v-folds",
    "text": "Model Assessment on v-folds\nWhere this workflow thing really shines…\n\nresample_fit &lt;- tune::fit_resamples(PM_wflow, vfold_pm)\n\n→ A | warning: the standard deviation is zero, The correlation matrix has missing values. 415 columns were excluded from the filter.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: There are new levels in a factor: Maine, There are new levels in a factor: Forest, Mecklenburg, Clermont, Camden, Trumbull, Yellowstone, Caddo, Hinds, Codington, Preble, Broward, Rowan, Beaver, Dauphin, Buncombe, LaPorte, Ashley, Clayton, Talladega, Queens, Jones, Mitchell, Kalamazoo, Seminole, Henderson, Sussex, Ingham, Sangamon, Aroostook, Muscogee, Plumas, Dodge, Bennington, Sumner, Butler, Butte, Passaic, Page, Custer, Sainte Genevieve, Bullitt, Palo Alto, Rapides, Faulkner, San Francisco, Ravalli, San Mateo, Delaware, Davis, Fremont, Santa Clara, White, Carter, DeSoto, Wilkinson, Muscatine, Hampden, Yakima, Solano, Mendocino, Mobile, Roanoke City, Wake, Gwinnett, Alamance, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n→ C | warning: the standard deviation is zero, The correlation matrix has missing values. 408 columns were excluded from the filter.\nThere were issues with some computations   A: x1\n→ D | warning: There are new levels in a factor: Athens, Kent, Linn, Stark, Cabell, Arlington, St. Lucie, Grafton, Champaign, Brewster, Morgan, Lawrence, Tarrant, Yolo, Weber, Mille Lacs, Clarke, Harrison, Will, Grant, Morris, Santa Cruz, Taylor, Klamath, Prince George's, Howard, Buchanan, Cedar, Ventura, Monongalia, Bolivar, Medina, Dona Ana, Hancock, Missoula, Chittenden, Monroe, Knox, Essex, Pierce, Tuscaloosa, Ellis, Contra Costa, Apache, Harris, Edgecombe, Stearns, Outagamie, Escambia, Hidalgo, Teton, Loudoun, Belknap, Sauk, Pittsburg, Charles, Gibson, Marshall, Chester, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1\n→ E | warning: the standard deviation is zero, The correlation matrix has missing values. 417 columns were excluded from the filter.\nThere were issues with some computations   A: x1\n→ F | warning: There are new levels in a factor: Nevada, North Dakota, There are new levels in a factor: Placer, Bay, Niagara, DeKalb, Hampton City, Oconee, Spencer, Sutter, Cobb, Randolph, Anne Arundel, Houston, Kane, Genesee, Dane, Yavapai, Lenawee, Washtenaw, Durham, Scioto, Henry, Spartanburg, Harney, Converse, Portage, St. Croix, Colusa, Berkshire, Lenoir, Lancaster, Haywood, Iberville, Adams, Catawba, St. Clair, Lynchburg City, Nassau, Brookings, Raleigh, Summit, Sebastian, Ouachita, Westmoreland, Rock Island, Duplin, Erie, Burleigh, Vilas, Kanawha, Rutland, San Joaquin, Washoe, Sandoval, Josephine, Kenosha, Plymouth, Stanislaus, Caswell, Cameron, Lucas, Sarpy, West Baton Rouge, Mayes, Cass, Chautauqua, Terrebonne, Sweetwater, Glynn, Harford, Spokane, La Salle, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1   E: x…\n→ G | warning: There are new levels in a factor: Allen, McDowell, Macon, Chaves, Yuma, Dougherty, Flathead, Ashland, Manistee, Hartford, Park, Box Elder, East Baton Rouge, Chesterfield, Woodbury, Bell, Citrus, New London, Cumberland, Fairfax, Forrest, Allegan, Ohio, Pueblo, Gaston, Bernalillo, Sullivan, Nevada, McLean, McCracken, Potter, Mahoning, Porter, Albemarle, Manitowoc, Shawnee, Ocean, Ottawa, El Paso, Baldwin, Bannock, Cheshire, Clay, Jersey, Brown, Lexington, Clinton, Peoria, Macomb, Davidson, Tooele, Dubois, Robeson, St. Lawrence, Lincoln, Virginia Beach City, Shelby, prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1   E: x…\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1   E: x…\n\n\n\nGives us a sense of the RMSE across the four folds:\n\ntune::show_best(resample_fit, metric = \"rmse\")\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2.12     4  0.0444 Preprocessor1_Model1",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#random-forest",
    "href": "content/lectures/17-cs02-analysis-slides.html#random-forest",
    "title": "17-cs02-analysis",
    "section": "Random Forest",
    "text": "Random Forest\nFitting a different model\n\nBased on a decision tree:\n\n\n\n\n\n\n\n\n\n[source]\n. . .\nBut…in the case of random forest:\n\n\nmultiple decision trees are created (hence: forest),\neach tree is built using a random subset of the training data (with replacement) (hence: random)\nhelps to keep the algorithm from overfitting the data\nThe mean of the predictions from each of the trees is used in the final output.\n\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#updating-our-recipe",
    "href": "content/lectures/17-cs02-analysis-slides.html#updating-our-recipe",
    "title": "17-cs02-analysis",
    "section": "Updating our recipe()",
    "text": "Updating our recipe()\n\nRF_rec &lt;- recipe(train_pm) %&gt;%\n    update_role(everything(), new_role = \"predictor\")%&gt;%\n    update_role(value, new_role = \"outcome\")%&gt;%\n    update_role(id, new_role = \"id variable\") %&gt;%\n    update_role(\"fips\", new_role = \"county id\") %&gt;%\n    step_novel(\"state\") %&gt;%\n    step_string2factor(\"state\", \"county\", \"city\") %&gt;%\n    step_rm(\"county\") %&gt;%\n    step_rm(\"zcta\") %&gt;%\n    step_corr(all_numeric())%&gt;%\n    step_nzv(all_numeric())\n\n\ncan use our categorical data as is (no dummy coding)\nstep_novel()necessary here for the state variable to get all cross validation folds to work, (b/c there will be different levels included in each fold test and training sets. The new levels for some of the test sets would otherwise result in an error.; “step_novel creates a specification of a recipe step that will assign a previously unseen factor level to a new value.”",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#model-specification",
    "href": "content/lectures/17-cs02-analysis-slides.html#model-specification",
    "title": "17-cs02-analysis",
    "section": "Model Specification",
    "text": "Model Specification\nModel parameters:\n\nmtry - The number of predictor variables (or features) that will be randomly sampled at each split when creating the tree models. The default number for regression analyses is the number of predictors divided by 3.\nmin_n - The minimum number of data points in a node that are required for the node to be split further.\ntrees - the number of trees in the ensemble\n\n\n\n# install.packages(\"randomForest\")\nRF_PM_model &lt;- parsnip::rand_forest(mtry = 10, min_n = 3) %&gt;% \n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"regression\")\n\nRF_PM_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#workflow",
    "href": "content/lectures/17-cs02-analysis-slides.html#workflow",
    "title": "17-cs02-analysis",
    "section": "Workflow",
    "text": "Workflow\n\nRF_wflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(RF_rec) %&gt;%\n  workflows::add_model(RF_PM_model)\n\nRF_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 10\n  min_n = 3\n\nComputational engine: randomForest",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#fit-the-data",
    "href": "content/lectures/17-cs02-analysis-slides.html#fit-the-data",
    "title": "17-cs02-analysis",
    "section": "Fit the Data",
    "text": "Fit the Data\n\nRF_wflow_fit &lt;- parsnip::fit(RF_wflow, data = train_pm)\n\nRF_wflow_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\n randomForest(x = maybe_data_frame(x), y = y, mtry = min_cols(~10,      x), nodesize = min_rows(~3, x)) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 10\n\n          Mean of squared residuals: 2.633639\n                    % Var explained: 59.29",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#assess-feature-importance",
    "href": "content/lectures/17-cs02-analysis-slides.html#assess-feature-importance",
    "title": "17-cs02-analysis",
    "section": "Assess Feature Importance",
    "text": "Assess Feature Importance\n\nRF_wflow_fit %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip::vip(num_features = 10)\n\n\n❓ What’s your interpretation of these results?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#assess-model-performance",
    "href": "content/lectures/17-cs02-analysis-slides.html#assess-model-performance",
    "title": "17-cs02-analysis",
    "section": "Assess Model Performance",
    "text": "Assess Model Performance\n\nset.seed(456)\nresample_RF_fit &lt;- tune::fit_resamples(RF_wflow, vfold_pm)\ncollect_metrics(resample_RF_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   1.67      4  0.101  Preprocessor1_Model1\n2 rsq     standard   0.591     4  0.0514 Preprocessor1_Model1\n\n\n\nFor comparison:\n\ncollect_metrics(resample_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.12      4  0.0444 Preprocessor1_Model1\n2 rsq     standard   0.307     4  0.0263 Preprocessor1_Model1",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#model-tuning",
    "href": "content/lectures/17-cs02-analysis-slides.html#model-tuning",
    "title": "17-cs02-analysis",
    "section": "Model Tuning",
    "text": "Model Tuning\nHyperparameters are often things that we need to specify about a model. Instead of arbitrarily specifying this, we can try to determine the best option for model performance by a process called tuning.\n\nRather than specifying values, we can use tune():\n\ntune_RF_model &lt;- rand_forest(mtry = tune(), min_n = tune()) %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"regression\")\n    \ntune_RF_model\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\n\n\nCreate Workflow:\n\nRF_tune_wflow &lt;- workflows::workflow() %&gt;%\n  workflows::add_recipe(RF_rec) %&gt;%\n  workflows::add_model(tune_RF_model)\n\nRF_tune_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_novel()\n• step_string2factor()\n• step_rm()\n• step_rm()\n• step_corr()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  min_n = tune()\n\nComputational engine: randomForest \n\n\nDetect how many cores you have access to:\n\nn_cores &lt;- parallel::detectCores()\nn_cores\n\n[1] 10\n\n\n\n\nThis code will take some time to run:\n\n# install.packages(\"doParallel\")\ndoParallel::registerDoParallel(cores = n_cores)\n\nset.seed(123)\ntune_RF_results &lt;- tune_grid(object = RF_tune_wflow, resamples = vfold_pm, grid = 20)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntune_RF_results\n\n# Tuning results\n# 4-fold cross-validation \n# A tibble: 4 × 4\n  splits            id    .metrics          .notes          \n  &lt;list&gt;            &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [438/146]&gt; Fold1 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [438/146]&gt; Fold2 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [438/146]&gt; Fold3 &lt;tibble [40 × 6]&gt; &lt;tibble [1 × 3]&gt;\n4 &lt;split [438/146]&gt; Fold4 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x1: 36 columns were requested but there were 35 predictors in the dat...\n\nRun `show_notes(.Last.tune.result)` for more information.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#check-metrics",
    "href": "content/lectures/17-cs02-analysis-slides.html#check-metrics",
    "title": "17-cs02-analysis",
    "section": "Check Metrics:",
    "text": "Check Metrics:\n\ntune_RF_results %&gt;%\n  collect_metrics()\n\n# A tibble: 40 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    12    33 rmse    standard   1.72      4  0.0866 Preprocessor1_Model01\n 2    12    33 rsq     standard   0.562     4  0.0466 Preprocessor1_Model01\n 3    27    35 rmse    standard   1.69      4  0.102  Preprocessor1_Model02\n 4    27    35 rsq     standard   0.563     4  0.0511 Preprocessor1_Model02\n 5    22    40 rmse    standard   1.71      4  0.106  Preprocessor1_Model03\n 6    22    40 rsq     standard   0.556     4  0.0543 Preprocessor1_Model03\n 7     1    27 rmse    standard   2.03      4  0.0501 Preprocessor1_Model04\n 8     1    27 rsq     standard   0.440     4  0.0245 Preprocessor1_Model04\n 9     6    32 rmse    standard   1.77      4  0.0756 Preprocessor1_Model05\n10     6    32 rsq     standard   0.552     4  0.0435 Preprocessor1_Model05\n# ℹ 30 more rows\n\n\n\n\nshow_best(tune_RF_results, metric = \"rmse\", n = 1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    32    11 rmse    standard    1.65     4   0.113 Preprocessor1_Model10",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#final-model-evaluation",
    "href": "content/lectures/17-cs02-analysis-slides.html#final-model-evaluation",
    "title": "17-cs02-analysis",
    "section": "Final Model Evaluation",
    "text": "Final Model Evaluation\n\ntuned_RF_values&lt;- select_best(tune_RF_results, \"rmse\")\ntuned_RF_values\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    32    11 Preprocessor1_Model10\n\n\n\nThe testing data!\n\n# specify best combination from tune in workflow\nRF_tuned_wflow &lt;-RF_tune_wflow %&gt;%\n  tune::finalize_workflow(tuned_RF_values)\n\n# fit model with those parameters on train AND test\noverallfit &lt;- RF_wflow %&gt;%\n  tune::last_fit(pm_split)\n\ncollect_metrics(overallfit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.72  Preprocessor1_Model1\n2 rsq     standard       0.608 Preprocessor1_Model1\n\n\nResults are similar to what we saw in training (RMSE: 1.65)\n\n\nGetting the predictions for the test data:\n\ntest_predictions &lt;- collect_predictions(overallfit)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#a-map-of-the-us",
    "href": "content/lectures/17-cs02-analysis-slides.html#a-map-of-the-us",
    "title": "17-cs02-analysis",
    "section": "A map of the US",
    "text": "A map of the US\nPackages needed:\n\nsf - the simple features package helps to convert geographical coordinates into geometry variables which are useful for making 2D plots\nmaps - this package contains geographical outlines and plotting functions to create plots with maps\nrnaturalearth- this allows for easy interaction with map data from Natural Earth which is a public domain map dataset\n\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(rnaturalearth)\n\nSupport for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#outline-of-the-us",
    "href": "content/lectures/17-cs02-analysis-slides.html#outline-of-the-us",
    "title": "17-cs02-analysis",
    "section": "Outline of the US",
    "text": "Outline of the US\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nglimpse(world)\n\nRows: 241\nColumns: 64\n$ scalerank  &lt;int&gt; 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 5, 3, 1, 1, 1, 1, 1, 1,…\n$ featurecla &lt;chr&gt; \"Admin-0 country\", \"Admin-0 country\", \"Admin-0 country\", \"A…\n$ labelrank  &lt;dbl&gt; 5, 3, 3, 6, 6, 6, 6, 4, 2, 6, 4, 4, 5, 6, 6, 2, 4, 5, 6, 2,…\n$ sovereignt &lt;chr&gt; \"Netherlands\", \"Afghanistan\", \"Angola\", \"United Kingdom\", \"…\n$ sov_a3     &lt;chr&gt; \"NL1\", \"AFG\", \"AGO\", \"GB1\", \"ALB\", \"FI1\", \"AND\", \"ARE\", \"AR…\n$ adm0_dif   &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,…\n$ level      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ type       &lt;chr&gt; \"Country\", \"Sovereign country\", \"Sovereign country\", \"Depen…\n$ admin      &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ adm0_a3    &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ geou_dif   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ geounit    &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ gu_a3      &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ su_dif     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ subunit    &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ su_a3      &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_diff   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ name       &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_long  &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_a3     &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ brk_name   &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ brk_group  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ abbrev     &lt;chr&gt; \"Aruba\", \"Afg.\", \"Ang.\", \"Ang.\", \"Alb.\", \"Aland\", \"And.\", \"…\n$ postal     &lt;chr&gt; \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AI\", \"AND\", \"AE\", \"AR\", \"ARM…\n$ formal_en  &lt;chr&gt; \"Aruba\", \"Islamic State of Afghanistan\", \"People's Republic…\n$ formal_fr  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ note_adm0  &lt;chr&gt; \"Neth.\", NA, NA, \"U.K.\", NA, \"Fin.\", NA, NA, NA, NA, \"U.S.A…\n$ note_brk   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Multiple claim…\n$ name_sort  &lt;chr&gt; \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"A…\n$ name_alt   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ mapcolor7  &lt;dbl&gt; 4, 5, 3, 6, 1, 4, 1, 2, 3, 3, 4, 4, 1, 7, 2, 1, 3, 1, 2, 3,…\n$ mapcolor8  &lt;dbl&gt; 2, 6, 2, 6, 4, 1, 4, 1, 1, 1, 5, 5, 2, 5, 2, 2, 1, 6, 2, 2,…\n$ mapcolor9  &lt;dbl&gt; 2, 8, 6, 6, 1, 4, 1, 3, 3, 2, 1, 1, 2, 9, 5, 2, 3, 5, 5, 1,…\n$ mapcolor13 &lt;dbl&gt; 9, 7, 1, 3, 6, 6, 8, 3, 13, 10, 1, NA, 7, 11, 5, 7, 4, 8, 8…\n$ pop_est    &lt;dbl&gt; 103065, 28400000, 12799293, 14436, 3639453, 27153, 83888, 4…\n$ gdp_md_est &lt;dbl&gt; 2258.0, 22270.0, 110300.0, 108.9, 21810.0, 1563.0, 3660.0, …\n$ pop_year   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ lastcensus &lt;dbl&gt; 2010, 1979, 1970, NA, 2001, NA, 1989, 2010, 2010, 2001, 201…\n$ gdp_year   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ economy    &lt;chr&gt; \"6. Developing region\", \"7. Least developed region\", \"7. Le…\n$ income_grp &lt;chr&gt; \"2. High income: nonOECD\", \"5. Low income\", \"3. Upper middl…\n$ wikipedia  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fips_10    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ iso_a2     &lt;chr&gt; \"AW\", \"AF\", \"AO\", \"AI\", \"AL\", \"AX\", \"AD\", \"AE\", \"AR\", \"AM\",…\n$ iso_a3     &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ iso_n3     &lt;chr&gt; \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ un_a3      &lt;chr&gt; \"533\", \"004\", \"024\", \"660\", \"008\", \"248\", \"020\", \"784\", \"03…\n$ wb_a2      &lt;chr&gt; \"AW\", \"AF\", \"AO\", NA, \"AL\", NA, \"AD\", \"AE\", \"AR\", \"AM\", \"AS…\n$ wb_a3      &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", NA, \"ALB\", NA, \"ADO\", \"ARE\", \"ARG\", \"A…\n$ woe_id     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_is &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALA\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_us &lt;chr&gt; \"ABW\", \"AFG\", \"AGO\", \"AIA\", \"ALB\", \"ALD\", \"AND\", \"ARE\", \"AR…\n$ adm0_a3_un &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ adm0_a3_wb &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ continent  &lt;chr&gt; \"North America\", \"Asia\", \"Africa\", \"North America\", \"Europe…\n$ region_un  &lt;chr&gt; \"Americas\", \"Asia\", \"Africa\", \"Americas\", \"Europe\", \"Europe…\n$ subregion  &lt;chr&gt; \"Caribbean\", \"Southern Asia\", \"Middle Africa\", \"Caribbean\",…\n$ region_wb  &lt;chr&gt; \"Latin America & Caribbean\", \"South Asia\", \"Sub-Saharan Afr…\n$ name_len   &lt;dbl&gt; 5, 11, 6, 8, 7, 5, 7, 20, 9, 7, 14, 10, 23, 22, 17, 9, 7, 1…\n$ long_len   &lt;dbl&gt; 5, 11, 6, 8, 7, 13, 7, 20, 9, 7, 14, 10, 27, 35, 19, 9, 7, …\n$ abbrev_len &lt;dbl&gt; 5, 4, 4, 4, 4, 5, 4, 6, 4, 4, 9, 4, 7, 10, 6, 4, 5, 4, 4, 5…\n$ tiny       &lt;dbl&gt; 4, NA, NA, NA, NA, 5, 5, NA, NA, NA, 3, NA, NA, 2, 4, NA, N…\n$ homepart   &lt;dbl&gt; NA, 1, 1, NA, 1, NA, 1, 1, 1, 1, NA, 1, NA, NA, 1, 1, 1, 1,…\n$ geometry   &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-69.89912 1..., MULTIPOLYGON (…\n\n\n\nWorld map:\n\nggplot(data = world) +\n    geom_sf() \n\n\n\n\n\n\n\n\n\n\nJust the US\nAccording to this link, these are the latitude and longitude bounds of the continental US:\n\ntop = 49.3457868 # north lat\nleft = -124.7844079 # west long\nright = -66.9513812 # east long\nbottom = 24.7433195 # south lat\n\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)\n\n\n\n\n\n\n\n\n\n\nAdding in our monitors…\n\nggplot(data = world) +\n    geom_sf() +\n    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE)+\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\")\n\n\n\n\n\n\n\n\n\n\nAdding in county lines\n\ncounties &lt;- sf::st_as_sf(maps::map(\"county\", plot = FALSE,\n                                   fill = TRUE))\n\ncounties\n\nSimple feature collection with 3076 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.6813 ymin: 25.12993 xmax: -67.00742 ymax: 49.38323\nGeodetic CRS:  +proj=longlat +ellps=clrk66 +no_defs +type=crs\nFirst 10 features:\n                               ID                           geom\nalabama,autauga   alabama,autauga MULTIPOLYGON (((-86.50517 3...\nalabama,baldwin   alabama,baldwin MULTIPOLYGON (((-87.93757 3...\nalabama,barbour   alabama,barbour MULTIPOLYGON (((-85.42801 3...\nalabama,bibb         alabama,bibb MULTIPOLYGON (((-87.02083 3...\nalabama,blount     alabama,blount MULTIPOLYGON (((-86.9578 33...\nalabama,bullock   alabama,bullock MULTIPOLYGON (((-85.66866 3...\nalabama,butler     alabama,butler MULTIPOLYGON (((-86.8604 31...\nalabama,calhoun   alabama,calhoun MULTIPOLYGON (((-85.74313 3...\nalabama,chambers alabama,chambers MULTIPOLYGON (((-85.59416 3...\nalabama,cherokee alabama,cherokee MULTIPOLYGON (((-85.46812 3...\n\n\n\n\nAnd now onto the map…\n\nmonitors &lt;- ggplot(data = world) +\n    geom_sf(data = counties, fill = NA, color = gray(.5))+\n      coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), \n             expand = FALSE) +\n    geom_point(data = pm, aes(x = lon, y = lat), size = 2, \n               shape = 23, fill = \"darkred\") +\n    ggtitle(\"Monitor Locations\") +\n    theme(axis.title.x=element_blank(),\n          axis.text.x = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.title.y = element_blank(),\n          axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n\nmonitors\n\n\n\n\n\n\n\n\n\n\nWrangle counties:\n\nseparate county and state into separate columns\nmake title case\ncombine with PM data\n\n\ncounties &lt;- counties %&gt;% \n  tidyr::separate(ID, into = c(\"state\", \"county\"), sep = \",\") %&gt;% \n  dplyr::mutate(county = stringr::str_to_title(county))\n\nmap_data &lt;- dplyr::inner_join(counties, pm, by = \"county\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#map-truth",
    "href": "content/lectures/17-cs02-analysis-slides.html#map-truth",
    "title": "17-cs02-analysis",
    "section": "Map: Truth",
    "text": "Map: Truth\n\nCodePlot\n\n\n\ntruth &lt;- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = value)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"True PM 2.5 levels\") +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#map-predictions",
    "href": "content/lectures/17-cs02-analysis-slides.html#map-predictions",
    "title": "17-cs02-analysis",
    "section": "Map: Predictions",
    "text": "Map: Predictions\n\nDataCodePlot\n\n\n\n# fit data\nRF_final_train_fit &lt;- parsnip::fit(RF_tuned_wflow, data = train_pm)\nRF_final_test_fit &lt;- parsnip::fit(RF_tuned_wflow, data = test_pm)\n\n# get predictions on training data\nvalues_pred_train &lt;- predict(RF_final_train_fit, train_pm) %&gt;% \n  bind_cols(train_pm %&gt;% select(value, fips, county, id)) \n\n# get predictions on testing data\nvalues_pred_test &lt;- predict(RF_final_test_fit, test_pm) %&gt;% \n  bind_cols(test_pm %&gt;% select(value, fips, county, id)) \nvalues_pred_test\n\n# A tibble: 292 × 5\n   .pred value fips  county     id       \n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt;      &lt;fct&gt;    \n 1  11.6  11.2 1033  Colbert    1033.1002\n 2  11.9  12.4 1055  Etowah     1055.001 \n 3  11.1  10.5 1069  Houston    1069.0003\n 4  13.9  15.6 1073  Jefferson  1073.0023\n 5  12.0  12.4 1073  Jefferson  1073.1005\n 6  11.3  11.1 1073  Jefferson  1073.1009\n 7  11.5  11.8 1073  Jefferson  1073.5003\n 8  11.0  10.0 1097  Mobile     1097.0003\n 9  11.9  12.0 1101  Montgomery 1101.0007\n10  12.9  13.2 1113  Russell    1113.0001\n# ℹ 282 more rows\n\n# combine\nall_pred &lt;- bind_rows(values_pred_test, values_pred_train)\n\n\n\n\nmap_data &lt;- inner_join(counties, all_pred, by = \"county\")\n\npred &lt;- ggplot(data = world) +\n  coord_sf(xlim = c(-125,-66),\n           ylim = c(24.5, 50),\n           expand = FALSE) +\n  geom_sf(data = map_data, aes(fill = .pred)) +\n  scale_fill_gradientn(colours = topo.colors(7),\n                       na.value = \"transparent\",\n                       breaks = c(0, 10, 20),\n                       labels = c(0, 10, 20),\n                       limits = c(0, 23.5),\n                       name = \"PM ug/m3\") +\n  ggtitle(\"Predicted PM 2.5 levels\") +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        axis.ticks.y=element_blank())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/17-cs02-analysis-slides.html#final-plot",
    "href": "content/lectures/17-cs02-analysis-slides.html#final-plot",
    "title": "17-cs02-analysis",
    "section": "Final Plot",
    "text": "Final Plot\n\nlibrary(patchwork)\n\n(truth/pred) + \n  plot_annotation(title = \"Machine Learning Methods Allow for Prediction of Air Pollution\", subtitle = \"A random forest model predicts true monitored levels of fine particulate matter (PM 2.5) air pollution based on\\ndata about population density and other predictors reasonably well, thus suggesting that we can use similar methods to predict levels\\nof pollution in places with poor monitoring\",\n                  theme = theme(plot.title = element_text(size =12, face = \"bold\"), \n                                plot.subtitle = element_text(size = 8)))\n\n\n\n\n\n\n\n\n❓ What do you learn from these results?\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "17-cs02-analysis"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html",
    "href": "content/lectures/16-cs02-eda.html",
    "title": "16-cs02-eda",
    "section": "",
    "text": "Q: I’m a bit unclear about the direction of case studies. Do just try to visualize data differently to find out the best biomarkers or do we use other methods like modeling to do so?\nA: A little bit of both. If you can rule out a matrix/compound during EDA by looking at visualizations, do that. But, once you have a set of compounds that you want to include for analysis, then you would turn to sensitivity/specificity/ROC curves to help you decide. Modelling is also an option.\n\n\nQ: I saw in the dataset, in some variables I think it’s the log values ( so I assumed it’s already normalized?). Why is it better to use the log data and not the raw values?\nA: Great question! We’ll get to this a bit today!\n\n\nQ: If the monitors are not localized at the granularity of zip code level, how do we assess the accuracy of our predictions? Seems like we can’t know the correct result.\nA: We’ll get to this in the next set of notes!\n\n\n\n\n\nCS01 due Thursday (group work survey due Friday)\nLab07 due Friday\nCS02 due next Friday\nFinal Project due Tues of Finals week (report + presentation)\n\n. . .\nVote:\n\nOption 1: Continue as planned cs01 (12.5) + cs02 (12.5) + final project (18)\nOption 2:\n\nCS01 due Thursday (17)\nComplete either CS02 w/ external data req’d for extension OR Final Project (with final project group; due Finals week) (26)\n\n\n. . .\nCS02 and Final Project repos + emails went out yesterday\n\nplease look for your repo/make sure you have access!\nplease get in communication with your groups if you’re not already in communication",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#qa",
    "href": "content/lectures/16-cs02-eda.html#qa",
    "title": "16-cs02-eda",
    "section": "",
    "text": "Q: I’m a bit unclear about the direction of case studies. Do just try to visualize data differently to find out the best biomarkers or do we use other methods like modeling to do so?\nA: A little bit of both. If you can rule out a matrix/compound during EDA by looking at visualizations, do that. But, once you have a set of compounds that you want to include for analysis, then you would turn to sensitivity/specificity/ROC curves to help you decide. Modelling is also an option.\n\n\nQ: I saw in the dataset, in some variables I think it’s the log values ( so I assumed it’s already normalized?). Why is it better to use the log data and not the raw values?\nA: Great question! We’ll get to this a bit today!\n\n\nQ: If the monitors are not localized at the granularity of zip code level, how do we assess the accuracy of our predictions? Seems like we can’t know the correct result.\nA: We’ll get to this in the next set of notes!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#course-announcements",
    "href": "content/lectures/16-cs02-eda.html#course-announcements",
    "title": "16-cs02-eda",
    "section": "",
    "text": "CS01 due Thursday (group work survey due Friday)\nLab07 due Friday\nCS02 due next Friday\nFinal Project due Tues of Finals week (report + presentation)\n\n. . .\nVote:\n\nOption 1: Continue as planned cs01 (12.5) + cs02 (12.5) + final project (18)\nOption 2:\n\nCS01 due Thursday (17)\nComplete either CS02 w/ external data req’d for extension OR Final Project (with final project group; due Finals week) (26)\n\n\n. . .\nCS02 and Final Project repos + emails went out yesterday\n\nplease look for your repo/make sure you have access!\nplease get in communication with your groups if you’re not already in communication",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#the-data",
    "href": "content/lectures/16-cs02-eda.html#the-data",
    "title": "16-cs02-eda",
    "section": "The Data",
    "text": "The Data\n\npm &lt;- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#skimr",
    "href": "content/lectures/16-cs02-eda.html#skimr",
    "title": "16-cs02-eda",
    "section": "skimr",
    "text": "skimr\n\nskimr::skim(pm)\n\n\nData summary\n\n\nName\npm\n\n\nNumber of rows\n876\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n47\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n49\n0\n\n\ncounty\n0\n1\n3\n20\n0\n471\n0\n\n\ncity\n0\n1\n4\n48\n0\n607\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n26987.96\n1.578761e+04\n1003.00\n13089.15\n26132.00\n39118.00\n5.603910e+04\n▇▇▆▇▆\n\n\nvalue\n0\n1\n10.81\n2.580000e+00\n3.02\n9.27\n11.15\n12.37\n2.316000e+01\n▂▆▇▁▁\n\n\nfips\n0\n1\n26987.89\n1.578763e+04\n1003.00\n13089.00\n26132.00\n39118.00\n5.603900e+04\n▇▇▆▇▆\n\n\nlat\n0\n1\n38.48\n4.620000e+00\n25.47\n35.03\n39.30\n41.66\n4.840000e+01\n▁▃▅▇▂\n\n\nlon\n0\n1\n-91.74\n1.496000e+01\n-124.18\n-99.16\n-87.47\n-80.69\n-6.804000e+01\n▃▂▃▇▃\n\n\nCMAQ\n0\n1\n8.41\n2.970000e+00\n1.63\n6.53\n8.62\n10.24\n2.313000e+01\n▃▇▃▁▁\n\n\nzcta\n0\n1\n50890.29\n2.778447e+04\n1022.00\n28788.25\n48172.00\n74371.00\n9.920200e+04\n▅▇▇▅▇\n\n\nzcta_area\n0\n1\n183173481.91\n5.425989e+08\n15459.00\n14204601.75\n37653560.50\n160041508.25\n8.164821e+09\n▇▁▁▁▁\n\n\nzcta_pop\n0\n1\n24227.58\n1.777216e+04\n0.00\n9797.00\n22014.00\n35004.75\n9.539700e+04\n▇▇▃▁▁\n\n\nimp_a500\n0\n1\n24.72\n1.934000e+01\n0.00\n3.70\n25.12\n40.22\n6.961000e+01\n▇▅▆▃▂\n\n\nimp_a1000\n0\n1\n24.26\n1.802000e+01\n0.00\n5.32\n24.53\n38.59\n6.750000e+01\n▇▅▆▃▁\n\n\nimp_a5000\n0\n1\n19.93\n1.472000e+01\n0.05\n6.79\n19.07\n30.11\n7.460000e+01\n▇▆▃▁▁\n\n\nimp_a10000\n0\n1\n15.82\n1.381000e+01\n0.09\n4.54\n12.36\n24.17\n7.209000e+01\n▇▃▂▁▁\n\n\nimp_a15000\n0\n1\n13.43\n1.312000e+01\n0.11\n3.24\n9.67\n20.55\n7.110000e+01\n▇▃▁▁▁\n\n\ncounty_area\n0\n1\n3768701992.12\n6.212830e+09\n33703512.00\n1116536297.50\n1690826566.50\n2878192209.00\n5.194723e+10\n▇▁▁▁▁\n\n\ncounty_pop\n0\n1\n687298.44\n1.293489e+06\n783.00\n100948.00\n280730.50\n743159.00\n9.818605e+06\n▇▁▁▁▁\n\n\nlog_dist_to_prisec\n0\n1\n6.19\n1.410000e+00\n-1.46\n5.43\n6.36\n7.15\n1.045000e+01\n▁▁▃▇▁\n\n\nlog_pri_length_5000\n0\n1\n9.82\n1.080000e+00\n8.52\n8.52\n10.05\n10.73\n1.205000e+01\n▇▂▆▅▂\n\n\nlog_pri_length_10000\n0\n1\n10.92\n1.130000e+00\n9.21\n9.80\n11.17\n11.83\n1.302000e+01\n▇▂▇▇▃\n\n\nlog_pri_length_15000\n0\n1\n11.50\n1.150000e+00\n9.62\n10.87\n11.72\n12.40\n1.359000e+01\n▆▂▇▇▃\n\n\nlog_pri_length_25000\n0\n1\n12.24\n1.100000e+00\n10.13\n11.69\n12.46\n13.05\n1.436000e+01\n▅▃▇▇▃\n\n\nlog_prisec_length_500\n0\n1\n6.99\n9.500000e-01\n6.21\n6.21\n6.21\n7.82\n9.400000e+00\n▇▁▂▂▁\n\n\nlog_prisec_length_1000\n0\n1\n8.56\n7.900000e-01\n7.60\n7.60\n8.66\n9.20\n1.047000e+01\n▇▅▆▃▁\n\n\nlog_prisec_length_5000\n0\n1\n11.28\n7.800000e-01\n8.52\n10.91\n11.42\n11.83\n1.278000e+01\n▁▁▃▇▃\n\n\nlog_prisec_length_10000\n0\n1\n12.41\n7.300000e-01\n9.21\n11.99\n12.53\n12.94\n1.385000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_15000\n0\n1\n13.03\n7.200000e-01\n9.62\n12.59\n13.13\n13.57\n1.441000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_25000\n0\n1\n13.82\n7.000000e-01\n10.13\n13.38\n13.92\n14.35\n1.523000e+01\n▁▁▃▇▆\n\n\nlog_nei_2008_pm25_sum_10000\n0\n1\n3.97\n2.350000e+00\n0.00\n2.15\n4.29\n5.69\n9.120000e+00\n▆▅▇▆▂\n\n\nlog_nei_2008_pm25_sum_15000\n0\n1\n4.72\n2.250000e+00\n0.00\n3.47\n5.00\n6.35\n9.420000e+00\n▃▃▇▇▂\n\n\nlog_nei_2008_pm25_sum_25000\n0\n1\n5.67\n2.110000e+00\n0.00\n4.66\n5.91\n7.28\n9.650000e+00\n▂▂▇▇▃\n\n\nlog_nei_2008_pm10_sum_10000\n0\n1\n4.35\n2.320000e+00\n0.00\n2.69\n4.62\n6.07\n9.340000e+00\n▅▅▇▇▂\n\n\nlog_nei_2008_pm10_sum_15000\n0\n1\n5.10\n2.180000e+00\n0.00\n3.87\n5.39\n6.72\n9.710000e+00\n▂▃▇▇▂\n\n\nlog_nei_2008_pm10_sum_25000\n0\n1\n6.07\n2.010000e+00\n0.00\n5.10\n6.37\n7.52\n9.880000e+00\n▁▂▆▇▃\n\n\npopdens_county\n0\n1\n551.76\n1.711510e+03\n0.26\n40.77\n156.67\n510.81\n2.682191e+04\n▇▁▁▁▁\n\n\npopdens_zcta\n0\n1\n1279.66\n2.757490e+03\n0.00\n101.15\n610.35\n1382.52\n3.041884e+04\n▇▁▁▁▁\n\n\nnohs\n0\n1\n6.99\n7.210000e+00\n0.00\n2.70\n5.10\n8.80\n1.000000e+02\n▇▁▁▁▁\n\n\nsomehs\n0\n1\n10.17\n6.200000e+00\n0.00\n5.90\n9.40\n13.90\n7.220000e+01\n▇▂▁▁▁\n\n\nhs\n0\n1\n30.32\n1.140000e+01\n0.00\n23.80\n30.75\n36.10\n1.000000e+02\n▂▇▂▁▁\n\n\nsomecollege\n0\n1\n21.58\n8.600000e+00\n0.00\n17.50\n21.30\n24.70\n1.000000e+02\n▆▇▁▁▁\n\n\nassociate\n0\n1\n7.13\n4.010000e+00\n0.00\n4.90\n7.10\n8.80\n7.140000e+01\n▇▁▁▁▁\n\n\nbachelor\n0\n1\n14.90\n9.710000e+00\n0.00\n8.80\n12.95\n19.22\n1.000000e+02\n▇▂▁▁▁\n\n\ngrad\n0\n1\n8.91\n8.650000e+00\n0.00\n3.90\n6.70\n11.00\n1.000000e+02\n▇▁▁▁▁\n\n\npov\n0\n1\n14.95\n1.133000e+01\n0.00\n6.50\n12.10\n21.22\n6.590000e+01\n▇▅▂▁▁\n\n\nhs_orless\n0\n1\n47.48\n1.675000e+01\n0.00\n37.92\n48.65\n59.10\n1.000000e+02\n▁▃▇▃▁\n\n\nurc2013\n0\n1\n2.92\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\nurc2006\n0\n1\n2.97\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\naod\n0\n1\n43.70\n1.956000e+01\n5.00\n31.66\n40.17\n49.67\n1.430000e+02\n▃▇▁▁▁",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#why-49-states",
    "href": "content/lectures/16-cs02-eda.html#why-49-states",
    "title": "16-cs02-eda",
    "section": "Why 49 “states”?",
    "text": "Why 49 “states”?\n\npm %&gt;% \n  distinct(state) \n\n# A tibble: 49 × 1\n   state               \n   &lt;chr&gt;               \n 1 Alabama             \n 2 Arizona             \n 3 Arkansas            \n 4 California          \n 5 Colorado            \n 6 Connecticut         \n 7 Delaware            \n 8 District Of Columbia\n 9 Florida             \n10 Georgia             \n# ℹ 39 more rows\n\n\n\nDC is included\nAlaska and Hawaii are not",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#number-of-monitors-per-city",
    "href": "content/lectures/16-cs02-eda.html#number-of-monitors-per-city",
    "title": "16-cs02-eda",
    "section": "Number of monitors per city?",
    "text": "Number of monitors per city?\n\npm %&gt;% filter(city == \"San Diego\")\n\n# A tibble: 2 × 50\n     id value  fips   lat   lon state      county    city   CMAQ  zcta zcta_area\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 6073.  11.3  6073  32.8 -117. California San Diego San …  9.68 92123  21148247\n2 6073.  13.7  6073  32.7 -117. California San Diego San …  9.68 92113  13647793\n# ℹ 39 more variables: zcta_pop &lt;dbl&gt;, imp_a500 &lt;dbl&gt;, imp_a1000 &lt;dbl&gt;,\n#   imp_a5000 &lt;dbl&gt;, imp_a10000 &lt;dbl&gt;, imp_a15000 &lt;dbl&gt;, county_area &lt;dbl&gt;,\n#   county_pop &lt;dbl&gt;, log_dist_to_prisec &lt;dbl&gt;, log_pri_length_5000 &lt;dbl&gt;,\n#   log_pri_length_10000 &lt;dbl&gt;, log_pri_length_15000 &lt;dbl&gt;,\n#   log_pri_length_25000 &lt;dbl&gt;, log_prisec_length_500 &lt;dbl&gt;,\n#   log_prisec_length_1000 &lt;dbl&gt;, log_prisec_length_5000 &lt;dbl&gt;,\n#   log_prisec_length_10000 &lt;dbl&gt;, log_prisec_length_15000 &lt;dbl&gt;, …\n\n\n. . .\n\npm %&gt;% filter(city == \"Baltimore\")\n\n# A tibble: 5 × 50\n      id value  fips   lat   lon state    county     city   CMAQ  zcta zcta_area\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 24510.  12.2 24510  39.3 -76.6 Maryland Baltimore… Balt…  10.9 21251    461424\n2 24510.  12.5 24510  39.3 -76.7 Maryland Baltimore… Balt…  10.9 21215  17645223\n3 24510.  12.8 24510  39.3 -76.5 Maryland Baltimore… Balt…  10.9 21224  24539976\n4 24510.  14.3 24510  39.2 -76.6 Maryland Baltimore… Balt…  10.9 21226  25718732\n5 24510.  13.3 24510  39.3 -76.6 Maryland Baltimore… Balt…  10.9 21202   4111039\n# ℹ 39 more variables: zcta_pop &lt;dbl&gt;, imp_a500 &lt;dbl&gt;, imp_a1000 &lt;dbl&gt;,\n#   imp_a5000 &lt;dbl&gt;, imp_a10000 &lt;dbl&gt;, imp_a15000 &lt;dbl&gt;, county_area &lt;dbl&gt;,\n#   county_pop &lt;dbl&gt;, log_dist_to_prisec &lt;dbl&gt;, log_pri_length_5000 &lt;dbl&gt;,\n#   log_pri_length_10000 &lt;dbl&gt;, log_pri_length_15000 &lt;dbl&gt;,\n#   log_pri_length_25000 &lt;dbl&gt;, log_prisec_length_500 &lt;dbl&gt;,\n#   log_prisec_length_1000 &lt;dbl&gt;, log_prisec_length_5000 &lt;dbl&gt;,\n#   log_prisec_length_10000 &lt;dbl&gt;, log_prisec_length_15000 &lt;dbl&gt;, …\n\n\n. . .\nSan Diego has 2, while Baltimore has 5, despite having very similar population densities (popdens_county)…and San Diego having a much larger population (county_pop) and land area (county_area).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#feature-correlation",
    "href": "content/lectures/16-cs02-eda.html#feature-correlation",
    "title": "16-cs02-eda",
    "section": "Feature Correlation",
    "text": "Feature Correlation\nWhy do we care if variables in our dataset are correlated?\n\nwe don’t want to include redundant variables\ncan add unnecessary noise to our algorithm causing a reduction in prediction accuracy\ncan cause our algorithm to be slower\ncan also make it difficult to interpret what variables are actually predictive\n\n. . .\nTaking a look at our numeric variables…\n\nPM_cor &lt;- cor(pm %&gt;% dplyr::select_if(is.numeric))\ncorrplot::corrplot(PM_cor, tl.cex = 0.5)\n\n\n\n\n\n\n\n\n\ndeep blue | strongly, positively correlated\ndeep red | strongly, negatively correlated\n\n. . .\nIf we don’t care about direction, but only strength…and using hierarchical clustering:\n\ncorrplot::corrplot(abs(PM_cor), order = \"hclust\", tl.cex = 0.5, cl.lim = c(0, 1))\n\n\n\n\n\n\n\n\n. . .\nObservations:\n\ndevelopment variables (imp), road density (pri), and the emission (nei) variables all seem to be correlated with their group\nnone of the predictors are correlated with value (our outcome)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#development-imp",
    "href": "content/lectures/16-cs02-eda.html#development-imp",
    "title": "16-cs02-eda",
    "section": "Development (imp)",
    "text": "Development (imp)\n\n# we used GGally in a previous set of notes\n# will need to install if you haven't yet\nselect(pm, contains(\"imp\")) %&gt;%\n  GGally::ggpairs()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#emmissoins-nei",
    "href": "content/lectures/16-cs02-eda.html#emmissoins-nei",
    "title": "16-cs02-eda",
    "section": "Emmissoins (nei)",
    "text": "Emmissoins (nei)\n\nselect(pm, contains(\"nei\")) %&gt;%\n  GGally::ggpairs()\n\n\n\n\n\n\n\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#road-density-pri",
    "href": "content/lectures/16-cs02-eda.html#road-density-pri",
    "title": "16-cs02-eda",
    "section": "Road Density (pri)",
    "text": "Road Density (pri)\n\nselect(pm, contains(\"pri\")) %&gt;%\n  GGally::ggcorr(hjust = .85, size = 3,\n       layout.exp=2, label = TRUE)\n\n\n\n\n\n\n\n\nWarning: colors are reversed from above. If included in final report, you’d want consistency.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#are-the-categories-correlated-with-one-another",
    "href": "content/lectures/16-cs02-eda.html#are-the-categories-correlated-with-one-another",
    "title": "16-cs02-eda",
    "section": "Are the categories correlated with one another?",
    "text": "Are the categories correlated with one another?\n\npm %&gt;%\nselect(log_nei_2008_pm25_sum_10000, popdens_county, \n       log_pri_length_10000, imp_a10000, county_pop) %&gt;%\n  GGally::ggpairs()\n\n\n\n\n\n\n\n\n. . .\nReminder:\n\nlog_nei_2008_pm25_sum_10000 | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\npopdens_county | Population density (number of people per kilometer squared area of the county)\nlog_pri_length_10000 | Count of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\nimp_a10000 | Impervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\ncounty_pop | Population of the county of the monitor",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#log-transforming-right-skewed-data",
    "href": "content/lectures/16-cs02-eda.html#log-transforming-right-skewed-data",
    "title": "16-cs02-eda",
    "section": "Log-transforming right-skewed data",
    "text": "Log-transforming right-skewed data\n\npm %&gt;%\n  mutate(log_popdens_county=log(popdens_county),\n         log_pop_county = log(county_pop)) %&gt;%\n  select(log_nei_2008_pm25_sum_10000, log_popdens_county, \n       log_pri_length_10000, imp_a10000, log_pop_county) %&gt;%\n  GGally::ggpairs()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#your-turn",
    "href": "content/lectures/16-cs02-eda.html#your-turn",
    "title": "16-cs02-eda",
    "section": "Your Turn",
    "text": "Your Turn\n💪 Try to learn at least three things about the data that we haven’t yet discussed now on your own.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\nOutcome variable\n\nsummary(pm$value)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.024   9.268  11.153  10.808  12.369  23.161 \n\nggplot(pm, aes(value)) + geom_histogram()\n\n\n\n\n\n\n\n\nWhat do we know about the monitors that have very high values?\n\npm |&gt; \n  filter(value &gt; 20) |&gt;\n  select(state) \n\n# A tibble: 6 × 1\n  state     \n  &lt;chr&gt;     \n1 California\n2 California\n3 California\n4 California\n5 California\n6 California",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#qa",
    "href": "content/lectures/16-cs02-eda-slides.html#qa",
    "title": "16-cs02-eda",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I’m a bit unclear about the direction of case studies. Do just try to visualize data differently to find out the best biomarkers or do we use other methods like modeling to do so?\nA: A little bit of both. If you can rule out a matrix/compound during EDA by looking at visualizations, do that. But, once you have a set of compounds that you want to include for analysis, then you would turn to sensitivity/specificity/ROC curves to help you decide. Modelling is also an option.\n\n\nQ: I saw in the dataset, in some variables I think it’s the log values ( so I assumed it’s already normalized?). Why is it better to use the log data and not the raw values?\nA: Great question! We’ll get to this a bit today!\n\n\nQ: If the monitors are not localized at the granularity of zip code level, how do we assess the accuracy of our predictions? Seems like we can’t know the correct result.\nA: We’ll get to this in the next set of notes!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#course-announcements",
    "href": "content/lectures/16-cs02-eda-slides.html#course-announcements",
    "title": "16-cs02-eda",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nCS01 due Thursday (group work survey due Friday)\nLab07 due Friday\nCS02 due next Friday\nFinal Project due Tues of Finals week (report + presentation)\n\n\nVote:\n\nOption 1: Continue as planned cs01 (12.5) + cs02 (12.5) + final project (18)\nOption 2:\n\nCS01 due Thursday (17)\nComplete either CS02 w/ external data req’d for extension OR Final Project (with final project group; due Finals week) (26)\n\n\n\n\nCS02 and Final Project repos + emails went out yesterday\n\nplease look for your repo/make sure you have access!\nplease get in communication with your groups if you’re not already in communication",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#the-data",
    "href": "content/lectures/16-cs02-eda-slides.html#the-data",
    "title": "16-cs02-eda",
    "section": "The Data",
    "text": "The Data\n\npm &lt;- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#skimr",
    "href": "content/lectures/16-cs02-eda-slides.html#skimr",
    "title": "16-cs02-eda",
    "section": "skimr",
    "text": "skimr\n\nskimr::skim(pm)\n\n\nData summary\n\n\nName\npm\n\n\nNumber of rows\n876\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n47\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n49\n0\n\n\ncounty\n0\n1\n3\n20\n0\n471\n0\n\n\ncity\n0\n1\n4\n48\n0\n607\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n26987.96\n1.578761e+04\n1003.00\n13089.15\n26132.00\n39118.00\n5.603910e+04\n▇▇▆▇▆\n\n\nvalue\n0\n1\n10.81\n2.580000e+00\n3.02\n9.27\n11.15\n12.37\n2.316000e+01\n▂▆▇▁▁\n\n\nfips\n0\n1\n26987.89\n1.578763e+04\n1003.00\n13089.00\n26132.00\n39118.00\n5.603900e+04\n▇▇▆▇▆\n\n\nlat\n0\n1\n38.48\n4.620000e+00\n25.47\n35.03\n39.30\n41.66\n4.840000e+01\n▁▃▅▇▂\n\n\nlon\n0\n1\n-91.74\n1.496000e+01\n-124.18\n-99.16\n-87.47\n-80.69\n-6.804000e+01\n▃▂▃▇▃\n\n\nCMAQ\n0\n1\n8.41\n2.970000e+00\n1.63\n6.53\n8.62\n10.24\n2.313000e+01\n▃▇▃▁▁\n\n\nzcta\n0\n1\n50890.29\n2.778447e+04\n1022.00\n28788.25\n48172.00\n74371.00\n9.920200e+04\n▅▇▇▅▇\n\n\nzcta_area\n0\n1\n183173481.91\n5.425989e+08\n15459.00\n14204601.75\n37653560.50\n160041508.25\n8.164821e+09\n▇▁▁▁▁\n\n\nzcta_pop\n0\n1\n24227.58\n1.777216e+04\n0.00\n9797.00\n22014.00\n35004.75\n9.539700e+04\n▇▇▃▁▁\n\n\nimp_a500\n0\n1\n24.72\n1.934000e+01\n0.00\n3.70\n25.12\n40.22\n6.961000e+01\n▇▅▆▃▂\n\n\nimp_a1000\n0\n1\n24.26\n1.802000e+01\n0.00\n5.32\n24.53\n38.59\n6.750000e+01\n▇▅▆▃▁\n\n\nimp_a5000\n0\n1\n19.93\n1.472000e+01\n0.05\n6.79\n19.07\n30.11\n7.460000e+01\n▇▆▃▁▁\n\n\nimp_a10000\n0\n1\n15.82\n1.381000e+01\n0.09\n4.54\n12.36\n24.17\n7.209000e+01\n▇▃▂▁▁\n\n\nimp_a15000\n0\n1\n13.43\n1.312000e+01\n0.11\n3.24\n9.67\n20.55\n7.110000e+01\n▇▃▁▁▁\n\n\ncounty_area\n0\n1\n3768701992.12\n6.212830e+09\n33703512.00\n1116536297.50\n1690826566.50\n2878192209.00\n5.194723e+10\n▇▁▁▁▁\n\n\ncounty_pop\n0\n1\n687298.44\n1.293489e+06\n783.00\n100948.00\n280730.50\n743159.00\n9.818605e+06\n▇▁▁▁▁\n\n\nlog_dist_to_prisec\n0\n1\n6.19\n1.410000e+00\n-1.46\n5.43\n6.36\n7.15\n1.045000e+01\n▁▁▃▇▁\n\n\nlog_pri_length_5000\n0\n1\n9.82\n1.080000e+00\n8.52\n8.52\n10.05\n10.73\n1.205000e+01\n▇▂▆▅▂\n\n\nlog_pri_length_10000\n0\n1\n10.92\n1.130000e+00\n9.21\n9.80\n11.17\n11.83\n1.302000e+01\n▇▂▇▇▃\n\n\nlog_pri_length_15000\n0\n1\n11.50\n1.150000e+00\n9.62\n10.87\n11.72\n12.40\n1.359000e+01\n▆▂▇▇▃\n\n\nlog_pri_length_25000\n0\n1\n12.24\n1.100000e+00\n10.13\n11.69\n12.46\n13.05\n1.436000e+01\n▅▃▇▇▃\n\n\nlog_prisec_length_500\n0\n1\n6.99\n9.500000e-01\n6.21\n6.21\n6.21\n7.82\n9.400000e+00\n▇▁▂▂▁\n\n\nlog_prisec_length_1000\n0\n1\n8.56\n7.900000e-01\n7.60\n7.60\n8.66\n9.20\n1.047000e+01\n▇▅▆▃▁\n\n\nlog_prisec_length_5000\n0\n1\n11.28\n7.800000e-01\n8.52\n10.91\n11.42\n11.83\n1.278000e+01\n▁▁▃▇▃\n\n\nlog_prisec_length_10000\n0\n1\n12.41\n7.300000e-01\n9.21\n11.99\n12.53\n12.94\n1.385000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_15000\n0\n1\n13.03\n7.200000e-01\n9.62\n12.59\n13.13\n13.57\n1.441000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_25000\n0\n1\n13.82\n7.000000e-01\n10.13\n13.38\n13.92\n14.35\n1.523000e+01\n▁▁▃▇▆\n\n\nlog_nei_2008_pm25_sum_10000\n0\n1\n3.97\n2.350000e+00\n0.00\n2.15\n4.29\n5.69\n9.120000e+00\n▆▅▇▆▂\n\n\nlog_nei_2008_pm25_sum_15000\n0\n1\n4.72\n2.250000e+00\n0.00\n3.47\n5.00\n6.35\n9.420000e+00\n▃▃▇▇▂\n\n\nlog_nei_2008_pm25_sum_25000\n0\n1\n5.67\n2.110000e+00\n0.00\n4.66\n5.91\n7.28\n9.650000e+00\n▂▂▇▇▃\n\n\nlog_nei_2008_pm10_sum_10000\n0\n1\n4.35\n2.320000e+00\n0.00\n2.69\n4.62\n6.07\n9.340000e+00\n▅▅▇▇▂\n\n\nlog_nei_2008_pm10_sum_15000\n0\n1\n5.10\n2.180000e+00\n0.00\n3.87\n5.39\n6.72\n9.710000e+00\n▂▃▇▇▂\n\n\nlog_nei_2008_pm10_sum_25000\n0\n1\n6.07\n2.010000e+00\n0.00\n5.10\n6.37\n7.52\n9.880000e+00\n▁▂▆▇▃\n\n\npopdens_county\n0\n1\n551.76\n1.711510e+03\n0.26\n40.77\n156.67\n510.81\n2.682191e+04\n▇▁▁▁▁\n\n\npopdens_zcta\n0\n1\n1279.66\n2.757490e+03\n0.00\n101.15\n610.35\n1382.52\n3.041884e+04\n▇▁▁▁▁\n\n\nnohs\n0\n1\n6.99\n7.210000e+00\n0.00\n2.70\n5.10\n8.80\n1.000000e+02\n▇▁▁▁▁\n\n\nsomehs\n0\n1\n10.17\n6.200000e+00\n0.00\n5.90\n9.40\n13.90\n7.220000e+01\n▇▂▁▁▁\n\n\nhs\n0\n1\n30.32\n1.140000e+01\n0.00\n23.80\n30.75\n36.10\n1.000000e+02\n▂▇▂▁▁\n\n\nsomecollege\n0\n1\n21.58\n8.600000e+00\n0.00\n17.50\n21.30\n24.70\n1.000000e+02\n▆▇▁▁▁\n\n\nassociate\n0\n1\n7.13\n4.010000e+00\n0.00\n4.90\n7.10\n8.80\n7.140000e+01\n▇▁▁▁▁\n\n\nbachelor\n0\n1\n14.90\n9.710000e+00\n0.00\n8.80\n12.95\n19.22\n1.000000e+02\n▇▂▁▁▁\n\n\ngrad\n0\n1\n8.91\n8.650000e+00\n0.00\n3.90\n6.70\n11.00\n1.000000e+02\n▇▁▁▁▁\n\n\npov\n0\n1\n14.95\n1.133000e+01\n0.00\n6.50\n12.10\n21.22\n6.590000e+01\n▇▅▂▁▁\n\n\nhs_orless\n0\n1\n47.48\n1.675000e+01\n0.00\n37.92\n48.65\n59.10\n1.000000e+02\n▁▃▇▃▁\n\n\nurc2013\n0\n1\n2.92\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\nurc2006\n0\n1\n2.97\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\naod\n0\n1\n43.70\n1.956000e+01\n5.00\n31.66\n40.17\n49.67\n1.430000e+02\n▃▇▁▁▁",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#why-49-states",
    "href": "content/lectures/16-cs02-eda-slides.html#why-49-states",
    "title": "16-cs02-eda",
    "section": "Why 49 “states”?",
    "text": "Why 49 “states”?\n\npm %&gt;% \n  distinct(state) \n\n# A tibble: 49 × 1\n   state               \n   &lt;chr&gt;               \n 1 Alabama             \n 2 Arizona             \n 3 Arkansas            \n 4 California          \n 5 Colorado            \n 6 Connecticut         \n 7 Delaware            \n 8 District Of Columbia\n 9 Florida             \n10 Georgia             \n# ℹ 39 more rows\n\n\n\nDC is included\nAlaska and Hawaii are not",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#number-of-monitors-per-city",
    "href": "content/lectures/16-cs02-eda-slides.html#number-of-monitors-per-city",
    "title": "16-cs02-eda",
    "section": "Number of monitors per city?",
    "text": "Number of monitors per city?\n\npm %&gt;% filter(city == \"San Diego\")\n\n# A tibble: 2 × 50\n     id value  fips   lat   lon state      county    city   CMAQ  zcta zcta_area\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 6073.  11.3  6073  32.8 -117. California San Diego San …  9.68 92123  21148247\n2 6073.  13.7  6073  32.7 -117. California San Diego San …  9.68 92113  13647793\n# ℹ 39 more variables: zcta_pop &lt;dbl&gt;, imp_a500 &lt;dbl&gt;, imp_a1000 &lt;dbl&gt;,\n#   imp_a5000 &lt;dbl&gt;, imp_a10000 &lt;dbl&gt;, imp_a15000 &lt;dbl&gt;, county_area &lt;dbl&gt;,\n#   county_pop &lt;dbl&gt;, log_dist_to_prisec &lt;dbl&gt;, log_pri_length_5000 &lt;dbl&gt;,\n#   log_pri_length_10000 &lt;dbl&gt;, log_pri_length_15000 &lt;dbl&gt;,\n#   log_pri_length_25000 &lt;dbl&gt;, log_prisec_length_500 &lt;dbl&gt;,\n#   log_prisec_length_1000 &lt;dbl&gt;, log_prisec_length_5000 &lt;dbl&gt;,\n#   log_prisec_length_10000 &lt;dbl&gt;, log_prisec_length_15000 &lt;dbl&gt;, …\n\n\n\n\npm %&gt;% filter(city == \"Baltimore\")\n\n# A tibble: 5 × 50\n      id value  fips   lat   lon state    county     city   CMAQ  zcta zcta_area\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 24510.  12.2 24510  39.3 -76.6 Maryland Baltimore… Balt…  10.9 21251    461424\n2 24510.  12.5 24510  39.3 -76.7 Maryland Baltimore… Balt…  10.9 21215  17645223\n3 24510.  12.8 24510  39.3 -76.5 Maryland Baltimore… Balt…  10.9 21224  24539976\n4 24510.  14.3 24510  39.2 -76.6 Maryland Baltimore… Balt…  10.9 21226  25718732\n5 24510.  13.3 24510  39.3 -76.6 Maryland Baltimore… Balt…  10.9 21202   4111039\n# ℹ 39 more variables: zcta_pop &lt;dbl&gt;, imp_a500 &lt;dbl&gt;, imp_a1000 &lt;dbl&gt;,\n#   imp_a5000 &lt;dbl&gt;, imp_a10000 &lt;dbl&gt;, imp_a15000 &lt;dbl&gt;, county_area &lt;dbl&gt;,\n#   county_pop &lt;dbl&gt;, log_dist_to_prisec &lt;dbl&gt;, log_pri_length_5000 &lt;dbl&gt;,\n#   log_pri_length_10000 &lt;dbl&gt;, log_pri_length_15000 &lt;dbl&gt;,\n#   log_pri_length_25000 &lt;dbl&gt;, log_prisec_length_500 &lt;dbl&gt;,\n#   log_prisec_length_1000 &lt;dbl&gt;, log_prisec_length_5000 &lt;dbl&gt;,\n#   log_prisec_length_10000 &lt;dbl&gt;, log_prisec_length_15000 &lt;dbl&gt;, …\n\n\n\n\nSan Diego has 2, while Baltimore has 5, despite having very similar population densities (popdens_county)…and San Diego having a much larger population (county_pop) and land area (county_area).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#feature-correlation",
    "href": "content/lectures/16-cs02-eda-slides.html#feature-correlation",
    "title": "16-cs02-eda",
    "section": "Feature Correlation",
    "text": "Feature Correlation\nWhy do we care if variables in our dataset are correlated?\n\nwe don’t want to include redundant variables\ncan add unnecessary noise to our algorithm causing a reduction in prediction accuracy\ncan cause our algorithm to be slower\ncan also make it difficult to interpret what variables are actually predictive\n\n\nTaking a look at our numeric variables…\n\nPM_cor &lt;- cor(pm %&gt;% dplyr::select_if(is.numeric))\ncorrplot::corrplot(PM_cor, tl.cex = 0.5)\n\n\n\n\n\n\n\n\n\ndeep blue | strongly, positively correlated\ndeep red | strongly, negatively correlated\n\n\n\nIf we don’t care about direction, but only strength…and using hierarchical clustering:\n\ncorrplot::corrplot(abs(PM_cor), order = \"hclust\", tl.cex = 0.5, cl.lim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\nObservations:\n\ndevelopment variables (imp), road density (pri), and the emission (nei) variables all seem to be correlated with their group\nnone of the predictors are correlated with value (our outcome)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#development-imp",
    "href": "content/lectures/16-cs02-eda-slides.html#development-imp",
    "title": "16-cs02-eda",
    "section": "Development (imp)",
    "text": "Development (imp)\n\n# we used GGally in a previous set of notes\n# will need to install if you haven't yet\nselect(pm, contains(\"imp\")) %&gt;%\n  GGally::ggpairs()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#emmissoins-nei",
    "href": "content/lectures/16-cs02-eda-slides.html#emmissoins-nei",
    "title": "16-cs02-eda",
    "section": "Emmissoins (nei)",
    "text": "Emmissoins (nei)\n\nselect(pm, contains(\"nei\")) %&gt;%\n  GGally::ggpairs()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#road-density-pri",
    "href": "content/lectures/16-cs02-eda-slides.html#road-density-pri",
    "title": "16-cs02-eda",
    "section": "Road Density (pri)",
    "text": "Road Density (pri)\n\nselect(pm, contains(\"pri\")) %&gt;%\n  GGally::ggcorr(hjust = .85, size = 3,\n       layout.exp=2, label = TRUE)\n\n\nWarning: colors are reversed from above. If included in final report, you’d want consistency.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#are-the-categories-correlated-with-one-another",
    "href": "content/lectures/16-cs02-eda-slides.html#are-the-categories-correlated-with-one-another",
    "title": "16-cs02-eda",
    "section": "Are the categories correlated with one another?",
    "text": "Are the categories correlated with one another?\n\npm %&gt;%\nselect(log_nei_2008_pm25_sum_10000, popdens_county, \n       log_pri_length_10000, imp_a10000, county_pop) %&gt;%\n  GGally::ggpairs()\n\n\n\nReminder:\n\nlog_nei_2008_pm25_sum_10000 | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\npopdens_county | Population density (number of people per kilometer squared area of the county)\nlog_pri_length_10000 | Count of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\nimp_a10000 | Impervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\ncounty_pop | Population of the county of the monitor",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#log-transforming-right-skewed-data",
    "href": "content/lectures/16-cs02-eda-slides.html#log-transforming-right-skewed-data",
    "title": "16-cs02-eda",
    "section": "Log-transforming right-skewed data",
    "text": "Log-transforming right-skewed data\n\npm %&gt;%\n  mutate(log_popdens_county=log(popdens_county),\n         log_pop_county = log(county_pop)) %&gt;%\n  select(log_nei_2008_pm25_sum_10000, log_popdens_county, \n       log_pri_length_10000, imp_a10000, log_pop_county) %&gt;%\n  GGally::ggpairs()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#your-turn",
    "href": "content/lectures/16-cs02-eda-slides.html#your-turn",
    "title": "16-cs02-eda",
    "section": "Your Turn",
    "text": "Your Turn\n💪 Try to learn at least three things about the data that we haven’t yet discussed now on your own.\n\nOutcome variable\n\nsummary(pm$value)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.024   9.268  11.153  10.808  12.369  23.161 \n\nggplot(pm, aes(value)) + geom_histogram()\n\n\n\n\n\n\n\n\nWhat do we know about the monitors that have very high values?\n\npm |&gt; \n  filter(value &gt; 20) |&gt;\n  select(state) \n\n# A tibble: 6 × 1\n  state     \n  &lt;chr&gt;     \n1 California\n2 California\n3 California\n4 California\n5 California\n6 California\n\n\n\n\n\n\nhttps://cogs137.github.io/website/\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "16-cs02-eda"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html",
    "href": "content/lectures/03-tidyr.html",
    "title": "03-tidyr",
    "section": "",
    "text": "Join DS3 at their Fall General Body Meeting to learn more about the events they’re offering this quarter, open board positions for the year, and free food! It will be happening on Wednesday (10/11) from 6-8pm, at PC Ballroom West\n\n\n\n\n\nQ: Is it possible to integrate to github using other systems than datahub? Datahub has already been spotty for me in this course and is notorious for slumping at critical pts in the quarter.\nA: Yup. The same steps can be carried out by downloading RStudio onto your computer and connecting it with GitHub.\n\n\nQ: Should we write in the console or in the rmd file first when writing code?\nA: Great question! I’d suggest starting in the Rmd file and editing there. That way you don’t have to copy+paste once you get it right. It’s already there.\n\n\nQ: How do you take notes for coding classes? I know there are lecture notes available, but how would you recommend taking notes for this class?\nA: I would recommend opening a blank Rmd each day for class and saving it with the lecture number. I’d keep notes and things I tried in that file. But, I wouldn’t copy+paste everything, since the other lecture notes are available.\n\n\n\n\nDue Dates:\n\nLab 02 due Friday\nHW01 now available; due Monday (10/16; 11:59 PM)\nLecture Participation survey open until Thursday\n\n. . .\nNotes:\n\nLab01 scores and feedback posted\nDatahub: Launch RStudio (possible solution?)\nStaff office hours updated (see Canvas or website)\n\n\n\n\n\nI have been struggling to grasp the material in the course. It feels like we are diving into the content in the labs, but I don’t even feel like I truly understand what I’m doing. It often seems like I’m just copying and pasting code from the website without a clear understanding of the bigger picture. I’m particularly stuck because I feel like I don’t have a solid grasp of the fundamental concepts of coding in R; it feels so new. I understand that the pace of the course may be challenging, but I think a bit more stronger focus on the foundational aspects of coding in R would greatly benefit students like me who are struggling with the content. I’m looking forward to the course and I hope I can grasp the content as we go through the next week. I’m concerned about learning the material and also how that may affect my grade.\n\n. . .\nLet’s see how y’all feel in a week. The first week can be a lot in this course. Often, students feel a lot more comfortable come week 3.\n\n\n\n\n89% know Python; 15% know R; most (but not all!) have programmed before\n64% feel confident about effective data science communication\nReasons for taking course: learn R, add to resume, analyze data, improve data science skills\n\n. . .\nMy favorite boring facts:\n\nI was actually born on my birthday\ni don’t like to eat eggs but my roommate loves them\nI like to have a midday nap.\nI can raise my eyebrows really well\nI eat peanut butter straight from the jar\ni have a jack russell terrier.. named jack (we weren’t feeling creative)\n\n\n\n\nR4DS:\n\nChapter 12: Tidy Data\nChapter 13: Relational Data\n\n\n\n\nThe opinionated tidyverse is named as such b/c it assumes/necessitates your data be “tidy”.\n. . .\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. —- Hadley Wickham\n\n. . .\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nSource: https://r4ds.had.co.nz/tidy-data.html\n\n\n\n❓ Given the rules discussed, is the cat_lovers dataset tidy?\n\ncat_lovers &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")\n\nRows: 60 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, number_of_cats, handedness\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncat_lovers |&gt; datatable()\n\n\n\n\n\n. . .\n❓ Given the rules discussed, is the bike dataset tidy?\n\nbike &lt;- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 5716 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (44): AmbulanceR, BikeAge_Gr, Bike_Alc_D, Bike_Dir, Bike_Injur, Bike_Po...\ndbl   (8): FID, OBJECTID, Bike_Age, Crash_Hour, Crash_Ty_1, Crash_Year, Drvr...\ndttm  (1): Crash_Time\ndate  (1): Crash_Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike |&gt; datatable()\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\n\n\n\n❓ Which is a dataset? Which is a summary table?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are four representations of the same data/information provided in the tidyr packages: table1, table2, table3, and the combination of table4a and table4b. Given what we’ve discussed, which is the best (tidiest) way to represent these data?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n. . .\nSolution: pivoting!\n\n\n\n\npivot_longerpivot_widerlong vs. wide\n\n\nFor when some of the column names are not names of variables, but values of a variable…\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n❓ Why are there backticks around the years? (Note: we have not discussed this yet)\n\n\nFor when an observation is scattered across multiple rows…\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable2 |&gt;\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n❓ Why aren’t there quotes around column names here…but there were in pivot_longer? (Note: we have not discussed this yet.)\n\n\n\nwide data contains values that do not repeat in the first column.\nlong format contains values that do repeat in the first column.\n\nBoth are good/helpful! We’ll return to this idea and discuss more during dataviz next week.\nBriefly:\n\nwide data: analysis\nlong data: plotting\n\n\n\n\n\n\n\n\nseparateunite\n\n\nFor when multiple pieces of information are stored in a single column…\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\ntable3 |&gt; \n  separate(rate, into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n…but…but…cases and population should be numeric…\n\ntable3 |&gt; \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nUnite is the opposite…it combines data stored across multiple columns.\nThe general syntax is:\n\ndf |&gt;\n  unite(new_col, first_col, second_col)\n\n\n\n\n\n\n\nIf we look at table4a, it’s missing the population information. That’s stored in a separate table…table4b\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n…which is also in the “wide” format\n. . .\n…so we pivot both tables longer\n\ntidy4a &lt;- table4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntidy4b &lt;- table4b |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\ntidy4b\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n. . .\n…but how do we get them into a single tidy dataset?\n. . .\nA join!\n\nleft_join(tidy4a, tidy4b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\nSource: R4DS\n\n\n\n\nlibrary(nycflights13)\n\n\nairlines : links airline to two letter code\nairports : ID’ed by FAA code\nplanes : ID’ed by tailnum\nairport : weather each hour; id’ed by two letter airport code\n\n. . .\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html)\n\n\n. . .\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time).\n\n\n\n\nmutating joins - add new variables to a data frame from matching observations in another\n. . .\nFor simplicity, we’ll work with only a handful of columns…\n\nflights |&gt; \n  select(year:day, hour, tailnum, carrier) |&gt; \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 7\n    year month   day  hour tailnum carrier name                    \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                   \n 1  2013     1     1     5 N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 N3ALAA  AA      American Airlines Inc.  \n# ℹ 336,766 more rows\n\n\nThere is now a new column name…coming from the airlines data frame.\n. . .\nleft_join:\n\nkeeps all rows in first df (here: flights)\nadds all matching information from second df (here: airlines); adds NAs for any observations not in airlines\n\n. . .\nOther joins:\nright_join: keeps all observations in second df full_join: keeps all observations in either df\n. . .\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html\n\n\n. . .\ninner_join:\n\ntakes only rows in both dfs\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#ad-data-science-student-society",
    "href": "content/lectures/03-tidyr.html#ad-data-science-student-society",
    "title": "03-tidyr",
    "section": "",
    "text": "Join DS3 at their Fall General Body Meeting to learn more about the events they’re offering this quarter, open board positions for the year, and free food! It will be happening on Wednesday (10/11) from 6-8pm, at PC Ballroom West",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#qa",
    "href": "content/lectures/03-tidyr.html#qa",
    "title": "03-tidyr",
    "section": "",
    "text": "Q: Is it possible to integrate to github using other systems than datahub? Datahub has already been spotty for me in this course and is notorious for slumping at critical pts in the quarter.\nA: Yup. The same steps can be carried out by downloading RStudio onto your computer and connecting it with GitHub.\n\n\nQ: Should we write in the console or in the rmd file first when writing code?\nA: Great question! I’d suggest starting in the Rmd file and editing there. That way you don’t have to copy+paste once you get it right. It’s already there.\n\n\nQ: How do you take notes for coding classes? I know there are lecture notes available, but how would you recommend taking notes for this class?\nA: I would recommend opening a blank Rmd each day for class and saving it with the lecture number. I’d keep notes and things I tried in that file. But, I wouldn’t copy+paste everything, since the other lecture notes are available.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#course-announcements",
    "href": "content/lectures/03-tidyr.html#course-announcements",
    "title": "03-tidyr",
    "section": "",
    "text": "Due Dates:\n\nLab 02 due Friday\nHW01 now available; due Monday (10/16; 11:59 PM)\nLecture Participation survey open until Thursday\n\n. . .\nNotes:\n\nLab01 scores and feedback posted\nDatahub: Launch RStudio (possible solution?)\nStaff office hours updated (see Canvas or website)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#student-comment",
    "href": "content/lectures/03-tidyr.html#student-comment",
    "title": "03-tidyr",
    "section": "",
    "text": "I have been struggling to grasp the material in the course. It feels like we are diving into the content in the labs, but I don’t even feel like I truly understand what I’m doing. It often seems like I’m just copying and pasting code from the website without a clear understanding of the bigger picture. I’m particularly stuck because I feel like I don’t have a solid grasp of the fundamental concepts of coding in R; it feels so new. I understand that the pace of the course may be challenging, but I think a bit more stronger focus on the foundational aspects of coding in R would greatly benefit students like me who are struggling with the content. I’m looking forward to the course and I hope I can grasp the content as we go through the next week. I’m concerned about learning the material and also how that may affect my grade.\n\n. . .\nLet’s see how y’all feel in a week. The first week can be a lot in this course. Often, students feel a lot more comfortable come week 3.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#student-survey",
    "href": "content/lectures/03-tidyr.html#student-survey",
    "title": "03-tidyr",
    "section": "",
    "text": "89% know Python; 15% know R; most (but not all!) have programmed before\n64% feel confident about effective data science communication\nReasons for taking course: learn R, add to resume, analyze data, improve data science skills\n\n. . .\nMy favorite boring facts:\n\nI was actually born on my birthday\ni don’t like to eat eggs but my roommate loves them\nI like to have a midday nap.\nI can raise my eyebrows really well\nI eat peanut butter straight from the jar\ni have a jack russell terrier.. named jack (we weren’t feeling creative)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#suggested-reading",
    "href": "content/lectures/03-tidyr.html#suggested-reading",
    "title": "03-tidyr",
    "section": "",
    "text": "R4DS:\n\nChapter 12: Tidy Data\nChapter 13: Relational Data",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#tidy-data",
    "href": "content/lectures/03-tidyr.html#tidy-data",
    "title": "03-tidyr",
    "section": "",
    "text": "The opinionated tidyverse is named as such b/c it assumes/necessitates your data be “tidy”.\n. . .\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. —- Hadley Wickham\n\n. . .\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nSource: https://r4ds.had.co.nz/tidy-data.html",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#tidy-or-not",
    "href": "content/lectures/03-tidyr.html#tidy-or-not",
    "title": "03-tidyr",
    "section": "",
    "text": "❓ Given the rules discussed, is the cat_lovers dataset tidy?\n\ncat_lovers &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")\n\nRows: 60 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, number_of_cats, handedness\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncat_lovers |&gt; datatable()\n\n\n\n\n\n. . .\n❓ Given the rules discussed, is the bike dataset tidy?\n\nbike &lt;- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 5716 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (44): AmbulanceR, BikeAge_Gr, Bike_Alc_D, Bike_Dir, Bike_Injur, Bike_Po...\ndbl   (8): FID, OBJECTID, Bike_Age, Crash_Hour, Crash_Ty_1, Crash_Year, Drvr...\ndttm  (1): Crash_Time\ndate  (1): Crash_Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike |&gt; datatable()\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#summary-tables",
    "href": "content/lectures/03-tidyr.html#summary-tables",
    "title": "03-tidyr",
    "section": "",
    "text": "❓ Which is a dataset? Which is a summary table?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#your-turn",
    "href": "content/lectures/03-tidyr.html#your-turn",
    "title": "03-tidyr",
    "section": "",
    "text": "There are four representations of the same data/information provided in the tidyr packages: table1, table2, table3, and the combination of table4a and table4b. Given what we’ve discussed, which is the best (tidiest) way to represent these data?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#common-issues",
    "href": "content/lectures/03-tidyr.html#common-issues",
    "title": "03-tidyr",
    "section": "",
    "text": "One variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n. . .\nSolution: pivoting!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#pivoting",
    "href": "content/lectures/03-tidyr.html#pivoting",
    "title": "03-tidyr",
    "section": "",
    "text": "pivot_longerpivot_widerlong vs. wide\n\n\nFor when some of the column names are not names of variables, but values of a variable…\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n❓ Why are there backticks around the years? (Note: we have not discussed this yet)\n\n\nFor when an observation is scattered across multiple rows…\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable2 |&gt;\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n❓ Why aren’t there quotes around column names here…but there were in pivot_longer? (Note: we have not discussed this yet.)\n\n\n\nwide data contains values that do not repeat in the first column.\nlong format contains values that do repeat in the first column.\n\nBoth are good/helpful! We’ll return to this idea and discuss more during dataviz next week.\nBriefly:\n\nwide data: analysis\nlong data: plotting",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#separating-uniting",
    "href": "content/lectures/03-tidyr.html#separating-uniting",
    "title": "03-tidyr",
    "section": "",
    "text": "separateunite\n\n\nFor when multiple pieces of information are stored in a single column…\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\ntable3 |&gt; \n  separate(rate, into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n…but…but…cases and population should be numeric…\n\ntable3 |&gt; \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nUnite is the opposite…it combines data stored across multiple columns.\nThe general syntax is:\n\ndf |&gt;\n  unite(new_col, first_col, second_col)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#joins",
    "href": "content/lectures/03-tidyr.html#joins",
    "title": "03-tidyr",
    "section": "",
    "text": "If we look at table4a, it’s missing the population information. That’s stored in a separate table…table4b\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n…which is also in the “wide” format\n. . .\n…so we pivot both tables longer\n\ntidy4a &lt;- table4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntidy4b &lt;- table4b |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\ntidy4b\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n. . .\n…but how do we get them into a single tidy dataset?\n. . .\nA join!\n\nleft_join(tidy4a, tidy4b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\nSource: R4DS",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#the-data-nycflights13",
    "href": "content/lectures/03-tidyr.html#the-data-nycflights13",
    "title": "03-tidyr",
    "section": "",
    "text": "library(nycflights13)\n\n\nairlines : links airline to two letter code\nairports : ID’ed by FAA code\nplanes : ID’ed by tailnum\nairport : weather each hour; id’ed by two letter airport code\n\n. . .\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html)\n\n\n. . .\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr.html#mutating-joins",
    "href": "content/lectures/03-tidyr.html#mutating-joins",
    "title": "03-tidyr",
    "section": "",
    "text": "mutating joins - add new variables to a data frame from matching observations in another\n. . .\nFor simplicity, we’ll work with only a handful of columns…\n\nflights |&gt; \n  select(year:day, hour, tailnum, carrier) |&gt; \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 7\n    year month   day  hour tailnum carrier name                    \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                   \n 1  2013     1     1     5 N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 N3ALAA  AA      American Airlines Inc.  \n# ℹ 336,766 more rows\n\n\nThere is now a new column name…coming from the airlines data frame.\n. . .\nleft_join:\n\nkeeps all rows in first df (here: flights)\nadds all matching information from second df (here: airlines); adds NAs for any observations not in airlines\n\n. . .\nOther joins:\nright_join: keeps all observations in second df full_join: keeps all observations in either df\n. . .\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html\n\n\n. . .\ninner_join:\n\ntakes only rows in both dfs\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#ad-data-science-student-society",
    "href": "content/lectures/03-tidyr-slides.html#ad-data-science-student-society",
    "title": "03-tidyr",
    "section": "[ad] Data Science Student Society",
    "text": "[ad] Data Science Student Society\nJoin DS3 at their Fall General Body Meeting to learn more about the events they’re offering this quarter, open board positions for the year, and free food! It will be happening on Wednesday (10/11) from 6-8pm, at PC Ballroom West",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#qa",
    "href": "content/lectures/03-tidyr-slides.html#qa",
    "title": "03-tidyr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Is it possible to integrate to github using other systems than datahub? Datahub has already been spotty for me in this course and is notorious for slumping at critical pts in the quarter.\nA: Yup. The same steps can be carried out by downloading RStudio onto your computer and connecting it with GitHub.\n\n\nQ: Should we write in the console or in the rmd file first when writing code?\nA: Great question! I’d suggest starting in the Rmd file and editing there. That way you don’t have to copy+paste once you get it right. It’s already there.\n\n\nQ: How do you take notes for coding classes? I know there are lecture notes available, but how would you recommend taking notes for this class?\nA: I would recommend opening a blank Rmd each day for class and saving it with the lecture number. I’d keep notes and things I tried in that file. But, I wouldn’t copy+paste everything, since the other lecture notes are available.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#course-announcements",
    "href": "content/lectures/03-tidyr-slides.html#course-announcements",
    "title": "03-tidyr",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 02 due Friday\nHW01 now available; due Monday (10/16; 11:59 PM)\nLecture Participation survey open until Thursday\n\n\nNotes:\n\nLab01 scores and feedback posted\nDatahub: Launch RStudio (possible solution?)\nStaff office hours updated (see Canvas or website)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#student-comment",
    "href": "content/lectures/03-tidyr-slides.html#student-comment",
    "title": "03-tidyr",
    "section": "Student Comment",
    "text": "Student Comment\n\nI have been struggling to grasp the material in the course. It feels like we are diving into the content in the labs, but I don’t even feel like I truly understand what I’m doing. It often seems like I’m just copying and pasting code from the website without a clear understanding of the bigger picture. I’m particularly stuck because I feel like I don’t have a solid grasp of the fundamental concepts of coding in R; it feels so new. I understand that the pace of the course may be challenging, but I think a bit more stronger focus on the foundational aspects of coding in R would greatly benefit students like me who are struggling with the content. I’m looking forward to the course and I hope I can grasp the content as we go through the next week. I’m concerned about learning the material and also how that may affect my grade.\n\n\nLet’s see how y’all feel in a week. The first week can be a lot in this course. Often, students feel a lot more comfortable come week 3.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#student-survey",
    "href": "content/lectures/03-tidyr-slides.html#student-survey",
    "title": "03-tidyr",
    "section": "Student Survey",
    "text": "Student Survey\n\n89% know Python; 15% know R; most (but not all!) have programmed before\n64% feel confident about effective data science communication\nReasons for taking course: learn R, add to resume, analyze data, improve data science skills\n\n\nMy favorite boring facts:\n\nI was actually born on my birthday\ni don’t like to eat eggs but my roommate loves them\nI like to have a midday nap.\nI can raise my eyebrows really well\nI eat peanut butter straight from the jar\ni have a jack russell terrier.. named jack (we weren’t feeling creative)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#suggested-reading",
    "href": "content/lectures/03-tidyr-slides.html#suggested-reading",
    "title": "03-tidyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 12: Tidy Data\nChapter 13: Relational Data",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#tidy-data",
    "href": "content/lectures/03-tidyr-slides.html#tidy-data",
    "title": "03-tidyr",
    "section": "Tidy Data",
    "text": "Tidy Data\nThe opinionated tidyverse is named as such b/c it assumes/necessitates your data be “tidy”.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. —- Hadley Wickham\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\n\nSource: https://r4ds.had.co.nz/tidy-data.html",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#tidy-or-not",
    "href": "content/lectures/03-tidyr-slides.html#tidy-or-not",
    "title": "03-tidyr",
    "section": "Tidy or not?",
    "text": "Tidy or not?\n❓ Given the rules discussed, is the cat_lovers dataset tidy?\n\ncat_lovers &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")\n\nRows: 60 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, number_of_cats, handedness\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncat_lovers |&gt; datatable()\n\n\n\n\n\n\n❓ Given the rules discussed, is the bike dataset tidy?\n\nbike &lt;- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 5716 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (44): AmbulanceR, BikeAge_Gr, Bike_Alc_D, Bike_Dir, Bike_Injur, Bike_Po...\ndbl   (8): FID, OBJECTID, Bike_Age, Crash_Hour, Crash_Ty_1, Crash_Year, Drvr...\ndttm  (1): Crash_Time\ndate  (1): Crash_Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike |&gt; datatable()\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#summary-tables",
    "href": "content/lectures/03-tidyr-slides.html#summary-tables",
    "title": "03-tidyr",
    "section": "Summary tables",
    "text": "Summary tables\n❓ Which is a dataset? Which is a summary table?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#your-turn",
    "href": "content/lectures/03-tidyr-slides.html#your-turn",
    "title": "03-tidyr",
    "section": "Your Turn",
    "text": "Your Turn\nThere are four representations of the same data/information provided in the tidyr packages: table1, table2, table3, and the combination of table4a and table4b. Given what we’ve discussed, which is the best (tidiest) way to represent these data?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#common-issues",
    "href": "content/lectures/03-tidyr-slides.html#common-issues",
    "title": "03-tidyr",
    "section": "Common issues",
    "text": "Common issues\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\nSolution: pivoting!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#pivoting",
    "href": "content/lectures/03-tidyr-slides.html#pivoting",
    "title": "03-tidyr",
    "section": "Pivoting",
    "text": "Pivoting\n\npivot_longerpivot_widerlong vs. wide\n\n\nFor when some of the column names are not names of variables, but values of a variable…\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n❓ Why are there backticks around the years? (Note: we have not discussed this yet)\n\n\nFor when an observation is scattered across multiple rows…\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable2 |&gt;\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n❓ Why aren’t there quotes around column names here…but there were in pivot_longer? (Note: we have not discussed this yet.)\n\n\n\nwide data contains values that do not repeat in the first column.\nlong format contains values that do repeat in the first column.\n\nBoth are good/helpful! We’ll return to this idea and discuss more during dataviz next week.\nBriefly:\n\nwide data: analysis\nlong data: plotting",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#separating-uniting",
    "href": "content/lectures/03-tidyr-slides.html#separating-uniting",
    "title": "03-tidyr",
    "section": "Separating & Uniting",
    "text": "Separating & Uniting\n\nseparateunite\n\n\nFor when multiple pieces of information are stored in a single column…\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\ntable3 |&gt; \n  separate(rate, into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n…but…but…cases and population should be numeric…\n\ntable3 |&gt; \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nUnite is the opposite…it combines data stored across multiple columns.\nThe general syntax is:\n\ndf |&gt;\n  unite(new_col, first_col, second_col)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#joins",
    "href": "content/lectures/03-tidyr-slides.html#joins",
    "title": "03-tidyr",
    "section": "Joins",
    "text": "Joins\nIf we look at table4a, it’s missing the population information. That’s stored in a separate table…table4b\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n…which is also in the “wide” format\n\n…so we pivot both tables longer\n\ntidy4a &lt;- table4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntidy4b &lt;- table4b |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\ntidy4b\n\n# A tibble: 6 × 3\n  country     year  population\n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n\n\n…but how do we get them into a single tidy dataset?\n\n\nA join!\n\nleft_join(tidy4a, tidy4b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\n\nSource: R4DS",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#the-data-nycflights13",
    "href": "content/lectures/03-tidyr-slides.html#the-data-nycflights13",
    "title": "03-tidyr",
    "section": "The Data: nycflights13",
    "text": "The Data: nycflights13\n\nlibrary(nycflights13)\n\n\nairlines : links airline to two letter code\nairports : ID’ed by FAA code\nplanes : ID’ed by tailnum\nairport : weather each hour; id’ed by two letter airport code\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html)\n\n\n\n\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#mutating-joins",
    "href": "content/lectures/03-tidyr-slides.html#mutating-joins",
    "title": "03-tidyr",
    "section": "Mutating Joins",
    "text": "Mutating Joins\nmutating joins - add new variables to a data frame from matching observations in another\n\nFor simplicity, we’ll work with only a handful of columns…\n\nflights |&gt; \n  select(year:day, hour, tailnum, carrier) |&gt; \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 7\n    year month   day  hour tailnum carrier name                    \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                   \n 1  2013     1     1     5 N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 N3ALAA  AA      American Airlines Inc.  \n# ℹ 336,766 more rows\n\n\nThere is now a new column name…coming from the airlines data frame.\n\n\nleft_join:\n\nkeeps all rows in first df (here: flights)\nadds all matching information from second df (here: airlines); adds NAs for any observations not in airlines\n\n\n\nOther joins:\nright_join: keeps all observations in second df full_join: keeps all observations in either df\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html\n\n\n\n\ninner_join:\n\ntakes only rows in both dfs\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "03-tidyr"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html",
    "href": "content/lectures/15-cs02-data.html",
    "title": "15-cs02-data",
    "section": "",
    "text": "Background\nQuestion\nData Intro\nWrangle",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#agenda",
    "href": "content/lectures/15-cs02-data.html#agenda",
    "title": "15-cs02-data",
    "section": "",
    "text": "Background\nQuestion\nData Intro\nWrangle",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#opencasestudies",
    "href": "content/lectures/15-cs02-data.html#opencasestudies",
    "title": "15-cs02-data",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\nWright, Carrie and Meng, Qier and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com//opencasestudies/ocs-bp-air-pollution. Predicting Annual Air Pollution (Version v1.0.0).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#air-pollutants",
    "href": "content/lectures/15-cs02-data.html#air-pollutants",
    "title": "15-cs02-data",
    "section": "Air Pollutants",
    "text": "Air Pollutants\nSome sources are natural while others are anthropogenic (human-derived):\n\n\n\n[source]\n. . .\nMajor types of air pollutants\n\nGaseous - Carbon Monoxide (CO), Ozone (O3), Nitrogen Oxides(NO, NO2), Sulfur Dioxide (SO2)\nParticulate - small liquids and solids suspended in the air (includes lead- can include certain types of dust)\nDust - small solids (larger than particulates) that can be suspended in the air for some time but eventually settle\nBiological - pollen, bacteria, viruses, mold spores\n\nSee here for more detail on the types of pollutants in the air.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#particulate-pollution",
    "href": "content/lectures/15-cs02-data.html#particulate-pollution",
    "title": "15-cs02-data",
    "section": "Particulate Pollution",
    "text": "Particulate Pollution\nAir pollution particulates are generally described by their size:\n\nLarge Coarse Particulate Matter - has diameter of &gt;10 micrometers (10 µm)\nCoarse Particulate Matter (called PM10-2.5) - has diameter of between 2.5 µm and 10 µm\nFine Particulate Matter (called PM2.5) - has diameter of &lt; 2.5 µm\n\nPM10 includes any particulate matter &lt;10 µm (both coarse and fine particulate matter)\n. . .\nIn relation to a piece of human hair:\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#common-pollutants-and-their-size",
    "href": "content/lectures/15-cs02-data.html#common-pollutants-and-their-size",
    "title": "15-cs02-data",
    "section": "Common Pollutants and their size",
    "text": "Common Pollutants and their size\n\n\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#penetration-into-the-human-body",
    "href": "content/lectures/15-cs02-data.html#penetration-into-the-human-body",
    "title": "15-cs02-data",
    "section": "Penetration into the human body",
    "text": "Penetration into the human body\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#negative-health-impacts",
    "href": "content/lectures/15-cs02-data.html#negative-health-impacts",
    "title": "15-cs02-data",
    "section": "Negative Health Impacts",
    "text": "Negative Health Impacts\nExposure to air pollution is:\n\nassociated with higher rates of mortality in older adults\nknown to be a risk factor for many diseases and conditions including (but not limited to):\n\n\n\nAsthma - fine particle exposure (PM2.5) was found to be associated with higher rates of asthma in children\nInflammation in type 1 diabetes - fine particle exposure (PM2.5) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with Type 1 diabetes\nLung function and emphysema - higher concentrations of ozone (O3), nitrogen oxides (NOx), black carbon, and fine particle exposure PM2.5 , at study baseline were significantly associated with greater increases in percent emphysema per 10 years\nLow birthweight - fine particle exposure(PM2.5) was associated with lower birth weight in full-term live births\nViral Infection - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (PM2.5)\n\n\nSee this review article for more information about sources of air pollution and the influence of air pollution on health.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#sparse-monitoring-ph-issue",
    "href": "content/lectures/15-cs02-data.html#sparse-monitoring-ph-issue",
    "title": "15-cs02-data",
    "section": "Sparse monitoring PH issue",
    "text": "Sparse monitoring PH issue\n\n\nHistorically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country.\nHowever, these monitors are relatively sparse in certain regions of the country and are not necessarily located near pollution sources.\ndramatic differences in pollution rates can be seen even within the same city. (In fact, the term micro-environments describes environments within cities or counties which may vary greatly from one block to another.)\n\n\n. . .\n\n\n\n[source]\n. . .\nLack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#machine-learning-offers-a-solution",
    "href": "content/lectures/15-cs02-data.html#machine-learning-offers-a-solution",
    "title": "15-cs02-data",
    "section": "Machine Learning offers a solution",
    "text": "Machine Learning offers a solution\nAn article published in the Environmental Health journal dealt with this issue by using data, including population density and road density, among other features, to model or predict air pollution levels at a more localized scale using machine learning (ML) methods.\n\n\n\n[source]\n. . .\nThe authors of this article state that:\n\n“Exposure to atmospheric particulate matter (PM) remains an important public health concern, although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or county-specific ambient concentrations.”\n\n. . .\nThe article above demonstrates that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems.\n. . .\nSo…we’re going to do the same",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#the-state-of-global-air",
    "href": "content/lectures/15-cs02-data.html#the-state-of-global-air",
    "title": "15-cs02-data",
    "section": "The State of Global Air",
    "text": "The State of Global Air\nThe State of Global Air is a report released every year to communicate the impact of air pollution on public health.\n. . .\nThe State of Global Air 2019 report (which uses data from 2017) stated that:\n\nAir pollution is the fifth leading risk factor for mortality worldwide. It is responsible for more deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity. Each year, more people die from air pollution–related disease than from road traffic injuries or malaria.\n\n\n\n\n[source]\n. . .\n\nIn 2017, air pollution is estimated to have contributed to close to 5 million deaths globally — nearly 1 in every 10 deaths.\n\n\n[source]\n. . .\nThe State of Global Air 2018 report (using data from 2016) separated different types of air pollution & found that particulate pollution was particularly associated with mortality.\n\n[source]\n. . .\nThe 2019 report shows that the highest levels of fine particulate pollution occur in Africa and Asia and that:\n\nMore than 90% of people worldwide live in areas exceeding the World Health Organization (WHO) Guideline for healthy air. More than half live in areas that do not even meet WHO’s least-stringent air quality target.\n\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#overall-improvement",
    "href": "content/lectures/15-cs02-data.html#overall-improvement",
    "title": "15-cs02-data",
    "section": "Overall Improvement",
    "text": "Overall Improvement\nLooking at the US specifically, air pollution levels are generally improving, with declining national air pollutant concentration averages as shown from the 2019 Our Nation’s Air report from the US Environmental Protection Agency (EPA):\n\n\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#an-issue-nonetheless",
    "href": "content/lectures/15-cs02-data.html#an-issue-nonetheless",
    "title": "15-cs02-data",
    "section": "An Issue Nonetheless",
    "text": "An Issue Nonetheless\n\nair pollution continues to contribute to health risk for Americans, in particular in regions with higher than national average rates of pollution that, at times, exceed the WHO’s recommended level.\nimportant to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.\n\n. . .\nYou can see that current air quality conditions at this website, and you will notice variation across different cities.\nFor example, here are the conditions in San Francisco yesterday:\n\n[source]\n. . .\n\nreports particulate values using what is called the Air Quality Index (AQI).\nThis calculator indicates that 138 AQI is equivalent to 50.5 ug/m3 and is considered unhealthy for sensitive individuals.\nThus, some areas exceed the WHO annual exposure guideline (10 ug/m3), and this may adversely affect the health of people living in these locations.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#adverse-health-effects",
    "href": "content/lectures/15-cs02-data.html#adverse-health-effects",
    "title": "15-cs02-data",
    "section": "Adverse health effects",
    "text": "Adverse health effects\n\nAdverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines.\nit appears that the composition of the particulate matter and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. (For example, see this article for more details.)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#monitor-data",
    "href": "content/lectures/15-cs02-data.html#monitor-data",
    "title": "15-cs02-data",
    "section": "Monitor Data",
    "text": "Monitor Data\n\n\nMonitor data in this case study come from a system of monitors in which roughly 90% are located within cities.\nThere is an equity issue in terms of capturing the air pollution levels of more rural areas.\nTo get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be useful to estimate air pollution levels in areas with little to no monitoring.\nSpecifically, these methods can be used to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:\n\n\n. . .\n\n\n\n[source]\nThis is what we aim to achieve in this case study.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#limitations",
    "href": "content/lectures/15-cs02-data.html#limitations",
    "title": "15-cs02-data",
    "section": "Limitations",
    "text": "Limitations\n\n\nThe data do not include information about the composition of particulate matter. Different types of particulates may be more benign or deleterious for health outcomes.\nOutdoor pollution levels are not necessarily an indication of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. Researchers are now developing personal monitoring systems to track air pollution levels on the personal level.\nOur analysis will use annual mean estimates of pollution levels, but these can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data; however, we are interested in long term exposures, as these appear to be the most influential for health outcomes.\nThese data are US-focused.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#supervised-ml",
    "href": "content/lectures/15-cs02-data.html#supervised-ml",
    "title": "15-cs02-data",
    "section": "Supervised ML",
    "text": "Supervised ML\nHere, we’ll need:\n\nA continuous outcome variable that we want to predict\nA set of feature(s) (or predictor variables) that we use to predict the outcome variable\n\n. . .\nTo build (or train) our model, we use both the outcome and features.\n. . .\nThe goal is to identify informative features that can explain a large amount of variation in our outcome variable.\n. . .\nUsing this model, we can then predict the outcome from new observations with the same features where have not observed the outcome.\n(More details here)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#outcome",
    "href": "content/lectures/15-cs02-data.html#outcome",
    "title": "15-cs02-data",
    "section": "Outcome",
    "text": "Outcome\nThe monitor data that we will be using comes from gravimetric monitors (see picture below) operated by the US Environmental Protection Agency (EPA).\n\n[image courtesy of Kirsten Koehler]\n. . .\nThese monitors use a filtration system to specifically capture fine particulate matter.\n\n[source]\n. . .\nThe weight of this particulate matter is manually measured daily or weekly.\nFor the EPA standard operating procedure for PM gravimetric analysis in 2008, we refer the reader to here.\n. . .\nIn our data set, the value column indicates the PM2.5 monitor average for 2008 in mass of fine particles/volume of air for 876 gravimetric monitors.\n. . .\nThe units are micrograms of fine particulate matter (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m3).\n. . .\nRecall the WHO exposure guideline is &lt; 10 ug/m3 on average annually for PM2.5.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-import",
    "href": "content/lectures/15-cs02-data.html#data-import",
    "title": "15-cs02-data",
    "section": "Data Import",
    "text": "Data Import\nAll of our data was previously collected by a researcher at the Johns Hopkins School of Public Health who studies air pollution and climate change. (Roger now works at UT Austin)\n. . .\nWe have one CSV file that contains both our single outcome variable and all of our features (or predictor variables). You can download this file using the OCSdata package:\n\n# install.packages(\"OCSdata\")\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\n\n. . .\nhere::here() helps manage file paths; will always locate files relative to your project root\n\n# install.packages(\"here\")\npm &lt;- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#pm-2.5-data",
    "href": "content/lectures/15-cs02-data.html#pm-2.5-data",
    "title": "15-cs02-data",
    "section": "PM 2.5 Data",
    "text": "PM 2.5 Data\n\n876 monitors\n40 columns\n\nvalue | outcome variable\n\n\n\npm |&gt;\n  glimpse()\n\nRows: 876\nColumns: 50\n$ id                          &lt;dbl&gt; 1003.001, 1027.000, 1033.100, 1049.100, 10…\n$ value                       &lt;dbl&gt; 9.597647, 10.800000, 11.212174, 11.659091,…\n$ fips                        &lt;dbl&gt; 1003, 1027, 1033, 1049, 1055, 1069, 1073, …\n$ lat                         &lt;dbl&gt; 30.49800, 33.28126, 34.75878, 34.28763, 33…\n$ lon                         &lt;dbl&gt; -87.88141, -85.80218, -87.65056, -85.96830…\n$ state                       &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\"…\n$ county                      &lt;chr&gt; \"Baldwin\", \"Clay\", \"Colbert\", \"DeKalb\", \"E…\n$ city                        &lt;chr&gt; \"Fairhope\", \"Ashland\", \"Muscle Shoals\", \"C…\n$ CMAQ                        &lt;dbl&gt; 8.098836, 9.766208, 9.402679, 8.534772, 9.…\n$ zcta                        &lt;dbl&gt; 36532, 36251, 35660, 35962, 35901, 36303, …\n$ zcta_area                   &lt;dbl&gt; 190980522, 374132430, 16716984, 203836235,…\n$ zcta_pop                    &lt;dbl&gt; 27829, 5103, 9042, 8300, 20045, 30217, 901…\n$ imp_a500                    &lt;dbl&gt; 0.01730104, 1.96972318, 19.17301038, 5.782…\n$ imp_a1000                   &lt;dbl&gt; 1.4096021, 0.8531574, 11.1448962, 3.867647…\n$ imp_a5000                   &lt;dbl&gt; 3.3360118, 0.9851479, 15.1786154, 1.231141…\n$ imp_a10000                  &lt;dbl&gt; 1.9879187, 0.5208189, 9.7253870, 1.0316469…\n$ imp_a15000                  &lt;dbl&gt; 1.4386207, 0.3359198, 5.2472094, 0.9730444…\n$ county_area                 &lt;dbl&gt; 4117521611, 1564252280, 1534877333, 201266…\n$ county_pop                  &lt;dbl&gt; 182265, 13932, 54428, 71109, 104430, 10154…\n$ log_dist_to_prisec          &lt;dbl&gt; 4.648181, 7.219907, 5.760131, 3.721489, 5.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 8.517193, 8.517193, 8.517193, 9.…\n$ log_pri_length_10000        &lt;dbl&gt; 9.210340, 9.210340, 9.274303, 10.409411, 1…\n$ log_pri_length_15000        &lt;dbl&gt; 9.630228, 9.615805, 9.658899, 11.173626, 1…\n$ log_pri_length_25000        &lt;dbl&gt; 11.32735, 10.12663, 10.15769, 11.90959, 12…\n$ log_prisec_length_500       &lt;dbl&gt; 7.295356, 6.214608, 8.611945, 7.310155, 8.…\n$ log_prisec_length_1000      &lt;dbl&gt; 8.195119, 7.600902, 9.735569, 8.585843, 9.…\n$ log_prisec_length_5000      &lt;dbl&gt; 10.815042, 10.170878, 11.770407, 10.214200…\n$ log_prisec_length_10000     &lt;dbl&gt; 11.88680, 11.40554, 12.84066, 11.50894, 12…\n$ log_prisec_length_15000     &lt;dbl&gt; 12.205723, 12.042963, 13.282656, 12.353663…\n$ log_prisec_length_25000     &lt;dbl&gt; 13.41395, 12.79980, 13.79973, 13.55979, 13…\n$ log_nei_2008_pm25_sum_10000 &lt;dbl&gt; 0.318035438, 3.218632928, 6.573127301, 0.0…\n$ log_nei_2008_pm25_sum_15000 &lt;dbl&gt; 1.967358961, 3.218632928, 6.581917457, 3.2…\n$ log_nei_2008_pm25_sum_25000 &lt;dbl&gt; 5.067308, 3.218633, 6.875900, 4.887665, 4.…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 1.35588511, 3.31111648, 6.69187313, 0.0000…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 2.26783411, 3.31111648, 6.70127741, 3.3500…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 5.628728, 3.311116, 7.148858, 5.171920, 4.…\n$ popdens_county              &lt;dbl&gt; 44.265706, 8.906492, 35.460814, 35.330814,…\n$ popdens_zcta                &lt;dbl&gt; 145.716431, 13.639555, 540.887040, 40.7189…\n$ nohs                        &lt;dbl&gt; 3.3, 11.6, 7.3, 14.3, 4.3, 5.8, 7.1, 2.7, …\n$ somehs                      &lt;dbl&gt; 4.9, 19.1, 15.8, 16.7, 13.3, 11.6, 17.1, 6…\n$ hs                          &lt;dbl&gt; 25.1, 33.9, 30.6, 35.0, 27.8, 29.8, 37.2, …\n$ somecollege                 &lt;dbl&gt; 19.7, 18.8, 20.9, 14.9, 29.2, 21.4, 23.5, …\n$ associate                   &lt;dbl&gt; 8.2, 8.0, 7.6, 5.5, 10.1, 7.9, 7.3, 8.0, 4…\n$ bachelor                    &lt;dbl&gt; 25.3, 5.5, 12.7, 7.9, 10.0, 13.7, 5.9, 17.…\n$ grad                        &lt;dbl&gt; 13.5, 3.1, 5.1, 5.8, 5.4, 9.8, 2.0, 8.7, 2…\n$ pov                         &lt;dbl&gt; 6.1, 19.5, 19.0, 13.8, 8.8, 15.6, 25.5, 7.…\n$ hs_orless                   &lt;dbl&gt; 33.3, 64.6, 53.7, 66.0, 45.4, 47.2, 61.4, …\n$ urc2013                     &lt;dbl&gt; 4, 6, 4, 6, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ urc2006                     &lt;dbl&gt; 5, 6, 4, 5, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ aod                         &lt;dbl&gt; 37.36364, 34.81818, 36.00000, 33.08333, 43…\n\n\n. . .\nThere are 48 features with values for each of the 876 monitors (observations).\nThe data comes from the US Environmental Protection Agency (EPA), the National Aeronautics and Space Administration (NASA), the US Census, and the National Center for Health Statistics (NCHS).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#features",
    "href": "content/lectures/15-cs02-data.html#features",
    "title": "15-cs02-data",
    "section": "Features",
    "text": "Features\n\n\n\nVariable\nDetails\n\n\n\n\nid\nMonitor number  – the county number is indicated before the decimal  – the monitor number is indicated after the decimal  Example: 1073.0023 is Jefferson county (1073) and .0023 one of 8 monitors\n\n\nfips\nFederal information processing standard number for the county where the monitor is located  – 5 digit id code for counties (zero is often the first value and sometimes is not shown)  – the first 2 numbers indicate the state  – the last three numbers indicate the county  Example: Alabama’s state code is 01 because it is first alphabetically  (note: Alaska and Hawaii are not included because they are not part of the contiguous US)\n\n\nLat\nLatitude of the monitor in degrees\n\n\nLon\nLongitude of the monitor in degrees\n\n\nstate\nState where the monitor is located\n\n\ncounty\nCounty where the monitor is located\n\n\ncity\nCity where the monitor is located\n\n\nCMAQ\nEstimated values of air pollution from a computational model called Community Multiscale Air Quality (CMAQ)  – A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution  – Does not use any of the PM2.5 gravimetric monitoring data. (There is a version that does use the gravimetric monitoring data, but not this one!)  – Data from the EPA\n\n\nzcta\nZip Code Tabulation Area where the monitor is located  – Postal Zip codes are converted into “generalized areal representations” that are non-overlapping  – Data from the 2010 Census\n\n\nzcta_area\nLand area of the zip code area in meters squared  – Data from the 2010 Census\n\n\nzcta_pop\nPopulation in the zip code area  – Data from the 2010 Census\n\n\nimp_a500\nImpervious surface measure  – Within a circle with a radius of 500 meters around the monitor  – Impervious surface are roads, concrete, parking lots, buildings  – This is a measure of development\n\n\nimp_a1000\nImpervious surface measure  – Within a circle with a radius of 1000 meters around the monitor\n\n\nimp_a5000\nImpervious surface measure  – Within a circle with a radius of 5000 meters around the monitor\n\n\nimp_a10000\nImpervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\n\nimp_a15000\nImpervious surface measure  – Within a circle with a radius of 15000 meters around the monitor\n\n\ncounty_area\nLand area of the county of the monitor in meters squared\n\n\ncounty_pop\nPopulation of the county of the monitor\n\n\nLog_dist_to_prisec\nLog (Natural log) distance to a primary or secondary road from the monitor  – Highway or major road\n\n\nlog_pri_length_5000\nCount of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_10000\nCount of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_15000\nCount of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_25000\nCount of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_prisec_length_500\nCount of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_1000\nCount of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_5000\nCount of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_10000\nCount of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_15000\nCount of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_25000\nCount of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_nei_2008_pm25_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\npopdens_county\nPopulation density (number of people per kilometer squared area of the county)\n\n\npopdens_zcta\nPopulation density (number of people per kilometer squared area of zcta)\n\n\nnohs\nPercentage of people in zcta area where the monitor is that do not have a high school degree  – Data from the Census\n\n\nsomehs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was some high school education  – Data from the Census\n\n\nhs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing a high school degree  – Data from the Census\n\n\nsomecollege\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing some college education  – Data from the Census\n\n\nassociate\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing an associate degree  – Data from the Census\n\n\nbachelor\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a bachelor’s degree  – Data from the Census\n\n\ngrad\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a graduate degree  – Data from the Census\n\n\npov\nPercentage of people in zcta area where the monitor is that lived in poverty in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines  – Data from the Census\n\n\nhs_orless\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a high school degree or less (sum of nohs, somehs, and hs)\n\n\nurc2013\n2013 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\nurc2006\n2006 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\naod\nAerosol Optical Depth measurement from a NASA satellite  – based on the diffraction of a laser  – used as a proxy of particulate pollution  – unit-less - higher value indicates more pollution  – Data from NASA\n\n\n\n. . .\nMany of these features have to do with the circular area around the monitor called the “buffer”. These are illustrated in the following figure:\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#skimr",
    "href": "content/lectures/15-cs02-data.html#skimr",
    "title": "15-cs02-data",
    "section": "skimr",
    "text": "skimr\nskimr | A helpful way to get an overall sense of a dataset\n\n# install.packages(\"skimr\")\nskimr::skim(pm)\n\n\nData summary\n\n\nName\npm\n\n\nNumber of rows\n876\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n47\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n49\n0\n\n\ncounty\n0\n1\n3\n20\n0\n471\n0\n\n\ncity\n0\n1\n4\n48\n0\n607\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n26987.96\n1.578761e+04\n1003.00\n13089.15\n26132.00\n39118.00\n5.603910e+04\n▇▇▆▇▆\n\n\nvalue\n0\n1\n10.81\n2.580000e+00\n3.02\n9.27\n11.15\n12.37\n2.316000e+01\n▂▆▇▁▁\n\n\nfips\n0\n1\n26987.89\n1.578763e+04\n1003.00\n13089.00\n26132.00\n39118.00\n5.603900e+04\n▇▇▆▇▆\n\n\nlat\n0\n1\n38.48\n4.620000e+00\n25.47\n35.03\n39.30\n41.66\n4.840000e+01\n▁▃▅▇▂\n\n\nlon\n0\n1\n-91.74\n1.496000e+01\n-124.18\n-99.16\n-87.47\n-80.69\n-6.804000e+01\n▃▂▃▇▃\n\n\nCMAQ\n0\n1\n8.41\n2.970000e+00\n1.63\n6.53\n8.62\n10.24\n2.313000e+01\n▃▇▃▁▁\n\n\nzcta\n0\n1\n50890.29\n2.778447e+04\n1022.00\n28788.25\n48172.00\n74371.00\n9.920200e+04\n▅▇▇▅▇\n\n\nzcta_area\n0\n1\n183173481.91\n5.425989e+08\n15459.00\n14204601.75\n37653560.50\n160041508.25\n8.164821e+09\n▇▁▁▁▁\n\n\nzcta_pop\n0\n1\n24227.58\n1.777216e+04\n0.00\n9797.00\n22014.00\n35004.75\n9.539700e+04\n▇▇▃▁▁\n\n\nimp_a500\n0\n1\n24.72\n1.934000e+01\n0.00\n3.70\n25.12\n40.22\n6.961000e+01\n▇▅▆▃▂\n\n\nimp_a1000\n0\n1\n24.26\n1.802000e+01\n0.00\n5.32\n24.53\n38.59\n6.750000e+01\n▇▅▆▃▁\n\n\nimp_a5000\n0\n1\n19.93\n1.472000e+01\n0.05\n6.79\n19.07\n30.11\n7.460000e+01\n▇▆▃▁▁\n\n\nimp_a10000\n0\n1\n15.82\n1.381000e+01\n0.09\n4.54\n12.36\n24.17\n7.209000e+01\n▇▃▂▁▁\n\n\nimp_a15000\n0\n1\n13.43\n1.312000e+01\n0.11\n3.24\n9.67\n20.55\n7.110000e+01\n▇▃▁▁▁\n\n\ncounty_area\n0\n1\n3768701992.12\n6.212830e+09\n33703512.00\n1116536297.50\n1690826566.50\n2878192209.00\n5.194723e+10\n▇▁▁▁▁\n\n\ncounty_pop\n0\n1\n687298.44\n1.293489e+06\n783.00\n100948.00\n280730.50\n743159.00\n9.818605e+06\n▇▁▁▁▁\n\n\nlog_dist_to_prisec\n0\n1\n6.19\n1.410000e+00\n-1.46\n5.43\n6.36\n7.15\n1.045000e+01\n▁▁▃▇▁\n\n\nlog_pri_length_5000\n0\n1\n9.82\n1.080000e+00\n8.52\n8.52\n10.05\n10.73\n1.205000e+01\n▇▂▆▅▂\n\n\nlog_pri_length_10000\n0\n1\n10.92\n1.130000e+00\n9.21\n9.80\n11.17\n11.83\n1.302000e+01\n▇▂▇▇▃\n\n\nlog_pri_length_15000\n0\n1\n11.50\n1.150000e+00\n9.62\n10.87\n11.72\n12.40\n1.359000e+01\n▆▂▇▇▃\n\n\nlog_pri_length_25000\n0\n1\n12.24\n1.100000e+00\n10.13\n11.69\n12.46\n13.05\n1.436000e+01\n▅▃▇▇▃\n\n\nlog_prisec_length_500\n0\n1\n6.99\n9.500000e-01\n6.21\n6.21\n6.21\n7.82\n9.400000e+00\n▇▁▂▂▁\n\n\nlog_prisec_length_1000\n0\n1\n8.56\n7.900000e-01\n7.60\n7.60\n8.66\n9.20\n1.047000e+01\n▇▅▆▃▁\n\n\nlog_prisec_length_5000\n0\n1\n11.28\n7.800000e-01\n8.52\n10.91\n11.42\n11.83\n1.278000e+01\n▁▁▃▇▃\n\n\nlog_prisec_length_10000\n0\n1\n12.41\n7.300000e-01\n9.21\n11.99\n12.53\n12.94\n1.385000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_15000\n0\n1\n13.03\n7.200000e-01\n9.62\n12.59\n13.13\n13.57\n1.441000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_25000\n0\n1\n13.82\n7.000000e-01\n10.13\n13.38\n13.92\n14.35\n1.523000e+01\n▁▁▃▇▆\n\n\nlog_nei_2008_pm25_sum_10000\n0\n1\n3.97\n2.350000e+00\n0.00\n2.15\n4.29\n5.69\n9.120000e+00\n▆▅▇▆▂\n\n\nlog_nei_2008_pm25_sum_15000\n0\n1\n4.72\n2.250000e+00\n0.00\n3.47\n5.00\n6.35\n9.420000e+00\n▃▃▇▇▂\n\n\nlog_nei_2008_pm25_sum_25000\n0\n1\n5.67\n2.110000e+00\n0.00\n4.66\n5.91\n7.28\n9.650000e+00\n▂▂▇▇▃\n\n\nlog_nei_2008_pm10_sum_10000\n0\n1\n4.35\n2.320000e+00\n0.00\n2.69\n4.62\n6.07\n9.340000e+00\n▅▅▇▇▂\n\n\nlog_nei_2008_pm10_sum_15000\n0\n1\n5.10\n2.180000e+00\n0.00\n3.87\n5.39\n6.72\n9.710000e+00\n▂▃▇▇▂\n\n\nlog_nei_2008_pm10_sum_25000\n0\n1\n6.07\n2.010000e+00\n0.00\n5.10\n6.37\n7.52\n9.880000e+00\n▁▂▆▇▃\n\n\npopdens_county\n0\n1\n551.76\n1.711510e+03\n0.26\n40.77\n156.67\n510.81\n2.682191e+04\n▇▁▁▁▁\n\n\npopdens_zcta\n0\n1\n1279.66\n2.757490e+03\n0.00\n101.15\n610.35\n1382.52\n3.041884e+04\n▇▁▁▁▁\n\n\nnohs\n0\n1\n6.99\n7.210000e+00\n0.00\n2.70\n5.10\n8.80\n1.000000e+02\n▇▁▁▁▁\n\n\nsomehs\n0\n1\n10.17\n6.200000e+00\n0.00\n5.90\n9.40\n13.90\n7.220000e+01\n▇▂▁▁▁\n\n\nhs\n0\n1\n30.32\n1.140000e+01\n0.00\n23.80\n30.75\n36.10\n1.000000e+02\n▂▇▂▁▁\n\n\nsomecollege\n0\n1\n21.58\n8.600000e+00\n0.00\n17.50\n21.30\n24.70\n1.000000e+02\n▆▇▁▁▁\n\n\nassociate\n0\n1\n7.13\n4.010000e+00\n0.00\n4.90\n7.10\n8.80\n7.140000e+01\n▇▁▁▁▁\n\n\nbachelor\n0\n1\n14.90\n9.710000e+00\n0.00\n8.80\n12.95\n19.22\n1.000000e+02\n▇▂▁▁▁\n\n\ngrad\n0\n1\n8.91\n8.650000e+00\n0.00\n3.90\n6.70\n11.00\n1.000000e+02\n▇▁▁▁▁\n\n\npov\n0\n1\n14.95\n1.133000e+01\n0.00\n6.50\n12.10\n21.22\n6.590000e+01\n▇▅▂▁▁\n\n\nhs_orless\n0\n1\n47.48\n1.675000e+01\n0.00\n37.92\n48.65\n59.10\n1.000000e+02\n▁▃▇▃▁\n\n\nurc2013\n0\n1\n2.92\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\nurc2006\n0\n1\n2.97\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\naod\n0\n1\n43.70\n1.956000e+01\n5.00\n31.66\n40.17\n49.67\n1.430000e+02\n▃▇▁▁▁\n\n\n\n\n\n. . .\n❓ Given the dataset we’re working with, what wrangling should we consider doing here?”\n❓ What’s something you’ve learned about the data from the skimr output?\n\nConsider variable type - need more factors?\nUnderstand why ID is not uniformally distributed; figure out which are overrepresented; decide what to do\nlog or other transformations necessary? decide during EDA\n\nReminder: to read the data in and run skimr if you haven’t already:\n\n# install.packages(\"OCSdata\")\n# install.packages(\"here\")\n# install.packages(\"skimr\")\n\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\npm &lt;- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))\nskimr::skim(pm)\n\n. . .\nThings to note:\n\n\ndata are summarized by variable type\nempty/n_missing gives you a sense of how much data are missing for each variable\nn_unique for state indicates that we have data for 49 states\nmany different distributions for continuous data, but many show bimodal distribution\nlarge range of possible values for many variables (i.e. population)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#agenda",
    "href": "content/lectures/15-cs02-data-slides.html#agenda",
    "title": "15-cs02-data",
    "section": "Agenda",
    "text": "Agenda\n\nBackground\nQuestion\nData Intro\nWrangle",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#opencasestudies",
    "href": "content/lectures/15-cs02-data-slides.html#opencasestudies",
    "title": "15-cs02-data",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\nWright, Carrie and Meng, Qier and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com//opencasestudies/ocs-bp-air-pollution. Predicting Annual Air Pollution (Version v1.0.0).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#air-pollutants",
    "href": "content/lectures/15-cs02-data-slides.html#air-pollutants",
    "title": "15-cs02-data",
    "section": "Air Pollutants",
    "text": "Air Pollutants\nSome sources are natural while others are anthropogenic (human-derived):\n\n\n\n[source]\n\nMajor types of air pollutants\n\nGaseous - Carbon Monoxide (CO), Ozone (O3), Nitrogen Oxides(NO, NO2), Sulfur Dioxide (SO2)\nParticulate - small liquids and solids suspended in the air (includes lead- can include certain types of dust)\nDust - small solids (larger than particulates) that can be suspended in the air for some time but eventually settle\nBiological - pollen, bacteria, viruses, mold spores\n\nSee here for more detail on the types of pollutants in the air.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#particulate-pollution",
    "href": "content/lectures/15-cs02-data-slides.html#particulate-pollution",
    "title": "15-cs02-data",
    "section": "Particulate Pollution",
    "text": "Particulate Pollution\nAir pollution particulates are generally described by their size:\n\nLarge Coarse Particulate Matter - has diameter of &gt;10 micrometers (10 µm)\nCoarse Particulate Matter (called PM10-2.5) - has diameter of between 2.5 µm and 10 µm\nFine Particulate Matter (called PM2.5) - has diameter of &lt; 2.5 µm\n\nPM10 includes any particulate matter &lt;10 µm (both coarse and fine particulate matter)\n\nIn relation to a piece of human hair:\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#common-pollutants-and-their-size",
    "href": "content/lectures/15-cs02-data-slides.html#common-pollutants-and-their-size",
    "title": "15-cs02-data",
    "section": "Common Pollutants and their size",
    "text": "Common Pollutants and their size\n\n\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#penetration-into-the-human-body",
    "href": "content/lectures/15-cs02-data-slides.html#penetration-into-the-human-body",
    "title": "15-cs02-data",
    "section": "Penetration into the human body",
    "text": "Penetration into the human body\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#negative-health-impacts",
    "href": "content/lectures/15-cs02-data-slides.html#negative-health-impacts",
    "title": "15-cs02-data",
    "section": "Negative Health Impacts",
    "text": "Negative Health Impacts\nExposure to air pollution is:\n\nassociated with higher rates of mortality in older adults\nknown to be a risk factor for many diseases and conditions including (but not limited to):\n\n\n\nAsthma - fine particle exposure (PM2.5) was found to be associated with higher rates of asthma in children\nInflammation in type 1 diabetes - fine particle exposure (PM2.5) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with Type 1 diabetes\nLung function and emphysema - higher concentrations of ozone (O3), nitrogen oxides (NOx), black carbon, and fine particle exposure PM2.5 , at study baseline were significantly associated with greater increases in percent emphysema per 10 years\nLow birthweight - fine particle exposure(PM2.5) was associated with lower birth weight in full-term live births\nViral Infection - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (PM2.5)\n\n\nSee this review article for more information about sources of air pollution and the influence of air pollution on health.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#sparse-monitoring-ph-issue",
    "href": "content/lectures/15-cs02-data-slides.html#sparse-monitoring-ph-issue",
    "title": "15-cs02-data",
    "section": "Sparse monitoring PH issue",
    "text": "Sparse monitoring PH issue\n\n\nHistorically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country.\nHowever, these monitors are relatively sparse in certain regions of the country and are not necessarily located near pollution sources.\ndramatic differences in pollution rates can be seen even within the same city. (In fact, the term micro-environments describes environments within cities or counties which may vary greatly from one block to another.)\n\n\n\n\n\n\n[source]\n\n\nLack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#machine-learning-offers-a-solution",
    "href": "content/lectures/15-cs02-data-slides.html#machine-learning-offers-a-solution",
    "title": "15-cs02-data",
    "section": "Machine Learning offers a solution",
    "text": "Machine Learning offers a solution\nAn article published in the Environmental Health journal dealt with this issue by using data, including population density and road density, among other features, to model or predict air pollution levels at a more localized scale using machine learning (ML) methods.\n\n\n\n[source]\n\nThe authors of this article state that:\n\n“Exposure to atmospheric particulate matter (PM) remains an important public health concern, although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or county-specific ambient concentrations.”\n\n\n\nThe article above demonstrates that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems.\n\n\nSo…we’re going to do the same",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#the-state-of-global-air",
    "href": "content/lectures/15-cs02-data-slides.html#the-state-of-global-air",
    "title": "15-cs02-data",
    "section": "The State of Global Air",
    "text": "The State of Global Air\nThe State of Global Air is a report released every year to communicate the impact of air pollution on public health.\n\nThe State of Global Air 2019 report (which uses data from 2017) stated that:\n\nAir pollution is the fifth leading risk factor for mortality worldwide. It is responsible for more deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity. Each year, more people die from air pollution–related disease than from road traffic injuries or malaria.\n\n\n\n\n[source]\n\n\n\nIn 2017, air pollution is estimated to have contributed to close to 5 million deaths globally — nearly 1 in every 10 deaths.\n\n\n[source]\n\n\nThe State of Global Air 2018 report (using data from 2016) separated different types of air pollution & found that particulate pollution was particularly associated with mortality.\n\n[source]\n\n\nThe 2019 report shows that the highest levels of fine particulate pollution occur in Africa and Asia and that:\n\nMore than 90% of people worldwide live in areas exceeding the World Health Organization (WHO) Guideline for healthy air. More than half live in areas that do not even meet WHO’s least-stringent air quality target.\n\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#overall-improvement",
    "href": "content/lectures/15-cs02-data-slides.html#overall-improvement",
    "title": "15-cs02-data",
    "section": "Overall Improvement",
    "text": "Overall Improvement\nLooking at the US specifically, air pollution levels are generally improving, with declining national air pollutant concentration averages as shown from the 2019 Our Nation’s Air report from the US Environmental Protection Agency (EPA):\n\n\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#an-issue-nonetheless",
    "href": "content/lectures/15-cs02-data-slides.html#an-issue-nonetheless",
    "title": "15-cs02-data",
    "section": "An Issue Nonetheless",
    "text": "An Issue Nonetheless\n\nair pollution continues to contribute to health risk for Americans, in particular in regions with higher than national average rates of pollution that, at times, exceed the WHO’s recommended level.\nimportant to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.\n\n\nYou can see that current air quality conditions at this website, and you will notice variation across different cities.\nFor example, here are the conditions in San Francisco yesterday:\n\n[source]\n\n\n\nreports particulate values using what is called the Air Quality Index (AQI).\nThis calculator indicates that 138 AQI is equivalent to 50.5 ug/m3 and is considered unhealthy for sensitive individuals.\nThus, some areas exceed the WHO annual exposure guideline (10 ug/m3), and this may adversely affect the health of people living in these locations.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#adverse-health-effects",
    "href": "content/lectures/15-cs02-data-slides.html#adverse-health-effects",
    "title": "15-cs02-data",
    "section": "Adverse health effects",
    "text": "Adverse health effects\n\nAdverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines.\nit appears that the composition of the particulate matter and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. (For example, see this article for more details.)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#monitor-data",
    "href": "content/lectures/15-cs02-data-slides.html#monitor-data",
    "title": "15-cs02-data",
    "section": "Monitor Data",
    "text": "Monitor Data\n\n\nMonitor data in this case study come from a system of monitors in which roughly 90% are located within cities.\nThere is an equity issue in terms of capturing the air pollution levels of more rural areas.\nTo get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be useful to estimate air pollution levels in areas with little to no monitoring.\nSpecifically, these methods can be used to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:\n\n\n\n\n\n\n[source]\nThis is what we aim to achieve in this case study.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#limitations",
    "href": "content/lectures/15-cs02-data-slides.html#limitations",
    "title": "15-cs02-data",
    "section": "Limitations",
    "text": "Limitations\n\n\nThe data do not include information about the composition of particulate matter. Different types of particulates may be more benign or deleterious for health outcomes.\nOutdoor pollution levels are not necessarily an indication of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. Researchers are now developing personal monitoring systems to track air pollution levels on the personal level.\nOur analysis will use annual mean estimates of pollution levels, but these can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data; however, we are interested in long term exposures, as these appear to be the most influential for health outcomes.\nThese data are US-focused.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#supervised-ml",
    "href": "content/lectures/15-cs02-data-slides.html#supervised-ml",
    "title": "15-cs02-data",
    "section": "Supervised ML",
    "text": "Supervised ML\nHere, we’ll need:\n\nA continuous outcome variable that we want to predict\nA set of feature(s) (or predictor variables) that we use to predict the outcome variable\n\n\nTo build (or train) our model, we use both the outcome and features.\n\n\nThe goal is to identify informative features that can explain a large amount of variation in our outcome variable.\n\n\nUsing this model, we can then predict the outcome from new observations with the same features where have not observed the outcome.\n(More details here)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#outcome",
    "href": "content/lectures/15-cs02-data-slides.html#outcome",
    "title": "15-cs02-data",
    "section": "Outcome",
    "text": "Outcome\nThe monitor data that we will be using comes from gravimetric monitors (see picture below) operated by the US Environmental Protection Agency (EPA).\n\n[image courtesy of Kirsten Koehler]\n\nThese monitors use a filtration system to specifically capture fine particulate matter.\n\n[source]\n\n\nThe weight of this particulate matter is manually measured daily or weekly.\nFor the EPA standard operating procedure for PM gravimetric analysis in 2008, we refer the reader to here.\n\n\nIn our data set, the value column indicates the PM2.5 monitor average for 2008 in mass of fine particles/volume of air for 876 gravimetric monitors.\n\n\nThe units are micrograms of fine particulate matter (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m3).\n\n\nRecall the WHO exposure guideline is &lt; 10 ug/m3 on average annually for PM2.5.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-import",
    "href": "content/lectures/15-cs02-data-slides.html#data-import",
    "title": "15-cs02-data",
    "section": "Data Import",
    "text": "Data Import\nAll of our data was previously collected by a researcher at the Johns Hopkins School of Public Health who studies air pollution and climate change. (Roger now works at UT Austin)\n\nWe have one CSV file that contains both our single outcome variable and all of our features (or predictor variables). You can download this file using the OCSdata package:\n\n# install.packages(\"OCSdata\")\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\n\n\n\nhere::here() helps manage file paths; will always locate files relative to your project root\n\n# install.packages(\"here\")\npm &lt;- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#pm-2.5-data",
    "href": "content/lectures/15-cs02-data-slides.html#pm-2.5-data",
    "title": "15-cs02-data",
    "section": "PM 2.5 Data",
    "text": "PM 2.5 Data\n\n876 monitors\n40 columns\n\nvalue | outcome variable\n\n\n\npm |&gt;\n  glimpse()\n\nRows: 876\nColumns: 50\n$ id                          &lt;dbl&gt; 1003.001, 1027.000, 1033.100, 1049.100, 10…\n$ value                       &lt;dbl&gt; 9.597647, 10.800000, 11.212174, 11.659091,…\n$ fips                        &lt;dbl&gt; 1003, 1027, 1033, 1049, 1055, 1069, 1073, …\n$ lat                         &lt;dbl&gt; 30.49800, 33.28126, 34.75878, 34.28763, 33…\n$ lon                         &lt;dbl&gt; -87.88141, -85.80218, -87.65056, -85.96830…\n$ state                       &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\"…\n$ county                      &lt;chr&gt; \"Baldwin\", \"Clay\", \"Colbert\", \"DeKalb\", \"E…\n$ city                        &lt;chr&gt; \"Fairhope\", \"Ashland\", \"Muscle Shoals\", \"C…\n$ CMAQ                        &lt;dbl&gt; 8.098836, 9.766208, 9.402679, 8.534772, 9.…\n$ zcta                        &lt;dbl&gt; 36532, 36251, 35660, 35962, 35901, 36303, …\n$ zcta_area                   &lt;dbl&gt; 190980522, 374132430, 16716984, 203836235,…\n$ zcta_pop                    &lt;dbl&gt; 27829, 5103, 9042, 8300, 20045, 30217, 901…\n$ imp_a500                    &lt;dbl&gt; 0.01730104, 1.96972318, 19.17301038, 5.782…\n$ imp_a1000                   &lt;dbl&gt; 1.4096021, 0.8531574, 11.1448962, 3.867647…\n$ imp_a5000                   &lt;dbl&gt; 3.3360118, 0.9851479, 15.1786154, 1.231141…\n$ imp_a10000                  &lt;dbl&gt; 1.9879187, 0.5208189, 9.7253870, 1.0316469…\n$ imp_a15000                  &lt;dbl&gt; 1.4386207, 0.3359198, 5.2472094, 0.9730444…\n$ county_area                 &lt;dbl&gt; 4117521611, 1564252280, 1534877333, 201266…\n$ county_pop                  &lt;dbl&gt; 182265, 13932, 54428, 71109, 104430, 10154…\n$ log_dist_to_prisec          &lt;dbl&gt; 4.648181, 7.219907, 5.760131, 3.721489, 5.…\n$ log_pri_length_5000         &lt;dbl&gt; 8.517193, 8.517193, 8.517193, 8.517193, 9.…\n$ log_pri_length_10000        &lt;dbl&gt; 9.210340, 9.210340, 9.274303, 10.409411, 1…\n$ log_pri_length_15000        &lt;dbl&gt; 9.630228, 9.615805, 9.658899, 11.173626, 1…\n$ log_pri_length_25000        &lt;dbl&gt; 11.32735, 10.12663, 10.15769, 11.90959, 12…\n$ log_prisec_length_500       &lt;dbl&gt; 7.295356, 6.214608, 8.611945, 7.310155, 8.…\n$ log_prisec_length_1000      &lt;dbl&gt; 8.195119, 7.600902, 9.735569, 8.585843, 9.…\n$ log_prisec_length_5000      &lt;dbl&gt; 10.815042, 10.170878, 11.770407, 10.214200…\n$ log_prisec_length_10000     &lt;dbl&gt; 11.88680, 11.40554, 12.84066, 11.50894, 12…\n$ log_prisec_length_15000     &lt;dbl&gt; 12.205723, 12.042963, 13.282656, 12.353663…\n$ log_prisec_length_25000     &lt;dbl&gt; 13.41395, 12.79980, 13.79973, 13.55979, 13…\n$ log_nei_2008_pm25_sum_10000 &lt;dbl&gt; 0.318035438, 3.218632928, 6.573127301, 0.0…\n$ log_nei_2008_pm25_sum_15000 &lt;dbl&gt; 1.967358961, 3.218632928, 6.581917457, 3.2…\n$ log_nei_2008_pm25_sum_25000 &lt;dbl&gt; 5.067308, 3.218633, 6.875900, 4.887665, 4.…\n$ log_nei_2008_pm10_sum_10000 &lt;dbl&gt; 1.35588511, 3.31111648, 6.69187313, 0.0000…\n$ log_nei_2008_pm10_sum_15000 &lt;dbl&gt; 2.26783411, 3.31111648, 6.70127741, 3.3500…\n$ log_nei_2008_pm10_sum_25000 &lt;dbl&gt; 5.628728, 3.311116, 7.148858, 5.171920, 4.…\n$ popdens_county              &lt;dbl&gt; 44.265706, 8.906492, 35.460814, 35.330814,…\n$ popdens_zcta                &lt;dbl&gt; 145.716431, 13.639555, 540.887040, 40.7189…\n$ nohs                        &lt;dbl&gt; 3.3, 11.6, 7.3, 14.3, 4.3, 5.8, 7.1, 2.7, …\n$ somehs                      &lt;dbl&gt; 4.9, 19.1, 15.8, 16.7, 13.3, 11.6, 17.1, 6…\n$ hs                          &lt;dbl&gt; 25.1, 33.9, 30.6, 35.0, 27.8, 29.8, 37.2, …\n$ somecollege                 &lt;dbl&gt; 19.7, 18.8, 20.9, 14.9, 29.2, 21.4, 23.5, …\n$ associate                   &lt;dbl&gt; 8.2, 8.0, 7.6, 5.5, 10.1, 7.9, 7.3, 8.0, 4…\n$ bachelor                    &lt;dbl&gt; 25.3, 5.5, 12.7, 7.9, 10.0, 13.7, 5.9, 17.…\n$ grad                        &lt;dbl&gt; 13.5, 3.1, 5.1, 5.8, 5.4, 9.8, 2.0, 8.7, 2…\n$ pov                         &lt;dbl&gt; 6.1, 19.5, 19.0, 13.8, 8.8, 15.6, 25.5, 7.…\n$ hs_orless                   &lt;dbl&gt; 33.3, 64.6, 53.7, 66.0, 45.4, 47.2, 61.4, …\n$ urc2013                     &lt;dbl&gt; 4, 6, 4, 6, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ urc2006                     &lt;dbl&gt; 5, 6, 4, 5, 4, 4, 1, 1, 1, 1, 1, 1, 1, 2, …\n$ aod                         &lt;dbl&gt; 37.36364, 34.81818, 36.00000, 33.08333, 43…\n\n\n\nThere are 48 features with values for each of the 876 monitors (observations).\nThe data comes from the US Environmental Protection Agency (EPA), the National Aeronautics and Space Administration (NASA), the US Census, and the National Center for Health Statistics (NCHS).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#features",
    "href": "content/lectures/15-cs02-data-slides.html#features",
    "title": "15-cs02-data",
    "section": "Features",
    "text": "Features\n\n\n\nVariable\nDetails\n\n\n\n\nid\nMonitor number  – the county number is indicated before the decimal  – the monitor number is indicated after the decimal  Example: 1073.0023 is Jefferson county (1073) and .0023 one of 8 monitors\n\n\nfips\nFederal information processing standard number for the county where the monitor is located  – 5 digit id code for counties (zero is often the first value and sometimes is not shown)  – the first 2 numbers indicate the state  – the last three numbers indicate the county  Example: Alabama’s state code is 01 because it is first alphabetically  (note: Alaska and Hawaii are not included because they are not part of the contiguous US)\n\n\nLat\nLatitude of the monitor in degrees\n\n\nLon\nLongitude of the monitor in degrees\n\n\nstate\nState where the monitor is located\n\n\ncounty\nCounty where the monitor is located\n\n\ncity\nCity where the monitor is located\n\n\nCMAQ\nEstimated values of air pollution from a computational model called Community Multiscale Air Quality (CMAQ)  – A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution  – Does not use any of the PM2.5 gravimetric monitoring data. (There is a version that does use the gravimetric monitoring data, but not this one!)  – Data from the EPA\n\n\nzcta\nZip Code Tabulation Area where the monitor is located  – Postal Zip codes are converted into “generalized areal representations” that are non-overlapping  – Data from the 2010 Census\n\n\nzcta_area\nLand area of the zip code area in meters squared  – Data from the 2010 Census\n\n\nzcta_pop\nPopulation in the zip code area  – Data from the 2010 Census\n\n\nimp_a500\nImpervious surface measure  – Within a circle with a radius of 500 meters around the monitor  – Impervious surface are roads, concrete, parking lots, buildings  – This is a measure of development\n\n\nimp_a1000\nImpervious surface measure  – Within a circle with a radius of 1000 meters around the monitor\n\n\nimp_a5000\nImpervious surface measure  – Within a circle with a radius of 5000 meters around the monitor\n\n\nimp_a10000\nImpervious surface measure  – Within a circle with a radius of 10000 meters around the monitor\n\n\nimp_a15000\nImpervious surface measure  – Within a circle with a radius of 15000 meters around the monitor\n\n\ncounty_area\nLand area of the county of the monitor in meters squared\n\n\ncounty_pop\nPopulation of the county of the monitor\n\n\nLog_dist_to_prisec\nLog (Natural log) distance to a primary or secondary road from the monitor  – Highway or major road\n\n\nlog_pri_length_5000\nCount of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_10000\nCount of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_15000\nCount of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_pri_length_25000\nCount of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highways only\n\n\nlog_prisec_length_500\nCount of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_1000\nCount of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_5000\nCount of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_10000\nCount of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_15000\nCount of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_prisec_length_25000\nCount of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  – Highway and secondary roads\n\n\nlog_nei_2008_pm25_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm25_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_10000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_15000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)\n\n\nlog_nei_2008_pm10_sum_25000\nTons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)\n\n\npopdens_county\nPopulation density (number of people per kilometer squared area of the county)\n\n\npopdens_zcta\nPopulation density (number of people per kilometer squared area of zcta)\n\n\nnohs\nPercentage of people in zcta area where the monitor is that do not have a high school degree  – Data from the Census\n\n\nsomehs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was some high school education  – Data from the Census\n\n\nhs\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing a high school degree  – Data from the Census\n\n\nsomecollege\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing some college education  – Data from the Census\n\n\nassociate\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was completing an associate degree  – Data from the Census\n\n\nbachelor\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a bachelor’s degree  – Data from the Census\n\n\ngrad\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a graduate degree  – Data from the Census\n\n\npov\nPercentage of people in zcta area where the monitor is that lived in poverty in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines  – Data from the Census\n\n\nhs_orless\nPercentage of people in zcta area where the monitor whose highest formal educational attainment was a high school degree or less (sum of nohs, somehs, and hs)\n\n\nurc2013\n2013 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\nurc2006\n2006 Urban-rural classification of the county where the monitor is located  – 6 category variable - 1 is totally urban 6 is completely rural  – Data from the National Center for Health Statistics\n\n\naod\nAerosol Optical Depth measurement from a NASA satellite  – based on the diffraction of a laser  – used as a proxy of particulate pollution  – unit-less - higher value indicates more pollution  – Data from NASA\n\n\n\n\nMany of these features have to do with the circular area around the monitor called the “buffer”. These are illustrated in the following figure:\n\n[source]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#skimr",
    "href": "content/lectures/15-cs02-data-slides.html#skimr",
    "title": "15-cs02-data",
    "section": "skimr",
    "text": "skimr\nskimr | A helpful way to get an overall sense of a dataset\n\n# install.packages(\"skimr\")\nskimr::skim(pm)\n\n\nData summary\n\n\nName\npm\n\n\nNumber of rows\n876\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n47\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1\n4\n20\n0\n49\n0\n\n\ncounty\n0\n1\n3\n20\n0\n471\n0\n\n\ncity\n0\n1\n4\n48\n0\n607\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n26987.96\n1.578761e+04\n1003.00\n13089.15\n26132.00\n39118.00\n5.603910e+04\n▇▇▆▇▆\n\n\nvalue\n0\n1\n10.81\n2.580000e+00\n3.02\n9.27\n11.15\n12.37\n2.316000e+01\n▂▆▇▁▁\n\n\nfips\n0\n1\n26987.89\n1.578763e+04\n1003.00\n13089.00\n26132.00\n39118.00\n5.603900e+04\n▇▇▆▇▆\n\n\nlat\n0\n1\n38.48\n4.620000e+00\n25.47\n35.03\n39.30\n41.66\n4.840000e+01\n▁▃▅▇▂\n\n\nlon\n0\n1\n-91.74\n1.496000e+01\n-124.18\n-99.16\n-87.47\n-80.69\n-6.804000e+01\n▃▂▃▇▃\n\n\nCMAQ\n0\n1\n8.41\n2.970000e+00\n1.63\n6.53\n8.62\n10.24\n2.313000e+01\n▃▇▃▁▁\n\n\nzcta\n0\n1\n50890.29\n2.778447e+04\n1022.00\n28788.25\n48172.00\n74371.00\n9.920200e+04\n▅▇▇▅▇\n\n\nzcta_area\n0\n1\n183173481.91\n5.425989e+08\n15459.00\n14204601.75\n37653560.50\n160041508.25\n8.164821e+09\n▇▁▁▁▁\n\n\nzcta_pop\n0\n1\n24227.58\n1.777216e+04\n0.00\n9797.00\n22014.00\n35004.75\n9.539700e+04\n▇▇▃▁▁\n\n\nimp_a500\n0\n1\n24.72\n1.934000e+01\n0.00\n3.70\n25.12\n40.22\n6.961000e+01\n▇▅▆▃▂\n\n\nimp_a1000\n0\n1\n24.26\n1.802000e+01\n0.00\n5.32\n24.53\n38.59\n6.750000e+01\n▇▅▆▃▁\n\n\nimp_a5000\n0\n1\n19.93\n1.472000e+01\n0.05\n6.79\n19.07\n30.11\n7.460000e+01\n▇▆▃▁▁\n\n\nimp_a10000\n0\n1\n15.82\n1.381000e+01\n0.09\n4.54\n12.36\n24.17\n7.209000e+01\n▇▃▂▁▁\n\n\nimp_a15000\n0\n1\n13.43\n1.312000e+01\n0.11\n3.24\n9.67\n20.55\n7.110000e+01\n▇▃▁▁▁\n\n\ncounty_area\n0\n1\n3768701992.12\n6.212830e+09\n33703512.00\n1116536297.50\n1690826566.50\n2878192209.00\n5.194723e+10\n▇▁▁▁▁\n\n\ncounty_pop\n0\n1\n687298.44\n1.293489e+06\n783.00\n100948.00\n280730.50\n743159.00\n9.818605e+06\n▇▁▁▁▁\n\n\nlog_dist_to_prisec\n0\n1\n6.19\n1.410000e+00\n-1.46\n5.43\n6.36\n7.15\n1.045000e+01\n▁▁▃▇▁\n\n\nlog_pri_length_5000\n0\n1\n9.82\n1.080000e+00\n8.52\n8.52\n10.05\n10.73\n1.205000e+01\n▇▂▆▅▂\n\n\nlog_pri_length_10000\n0\n1\n10.92\n1.130000e+00\n9.21\n9.80\n11.17\n11.83\n1.302000e+01\n▇▂▇▇▃\n\n\nlog_pri_length_15000\n0\n1\n11.50\n1.150000e+00\n9.62\n10.87\n11.72\n12.40\n1.359000e+01\n▆▂▇▇▃\n\n\nlog_pri_length_25000\n0\n1\n12.24\n1.100000e+00\n10.13\n11.69\n12.46\n13.05\n1.436000e+01\n▅▃▇▇▃\n\n\nlog_prisec_length_500\n0\n1\n6.99\n9.500000e-01\n6.21\n6.21\n6.21\n7.82\n9.400000e+00\n▇▁▂▂▁\n\n\nlog_prisec_length_1000\n0\n1\n8.56\n7.900000e-01\n7.60\n7.60\n8.66\n9.20\n1.047000e+01\n▇▅▆▃▁\n\n\nlog_prisec_length_5000\n0\n1\n11.28\n7.800000e-01\n8.52\n10.91\n11.42\n11.83\n1.278000e+01\n▁▁▃▇▃\n\n\nlog_prisec_length_10000\n0\n1\n12.41\n7.300000e-01\n9.21\n11.99\n12.53\n12.94\n1.385000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_15000\n0\n1\n13.03\n7.200000e-01\n9.62\n12.59\n13.13\n13.57\n1.441000e+01\n▁▁▃▇▅\n\n\nlog_prisec_length_25000\n0\n1\n13.82\n7.000000e-01\n10.13\n13.38\n13.92\n14.35\n1.523000e+01\n▁▁▃▇▆\n\n\nlog_nei_2008_pm25_sum_10000\n0\n1\n3.97\n2.350000e+00\n0.00\n2.15\n4.29\n5.69\n9.120000e+00\n▆▅▇▆▂\n\n\nlog_nei_2008_pm25_sum_15000\n0\n1\n4.72\n2.250000e+00\n0.00\n3.47\n5.00\n6.35\n9.420000e+00\n▃▃▇▇▂\n\n\nlog_nei_2008_pm25_sum_25000\n0\n1\n5.67\n2.110000e+00\n0.00\n4.66\n5.91\n7.28\n9.650000e+00\n▂▂▇▇▃\n\n\nlog_nei_2008_pm10_sum_10000\n0\n1\n4.35\n2.320000e+00\n0.00\n2.69\n4.62\n6.07\n9.340000e+00\n▅▅▇▇▂\n\n\nlog_nei_2008_pm10_sum_15000\n0\n1\n5.10\n2.180000e+00\n0.00\n3.87\n5.39\n6.72\n9.710000e+00\n▂▃▇▇▂\n\n\nlog_nei_2008_pm10_sum_25000\n0\n1\n6.07\n2.010000e+00\n0.00\n5.10\n6.37\n7.52\n9.880000e+00\n▁▂▆▇▃\n\n\npopdens_county\n0\n1\n551.76\n1.711510e+03\n0.26\n40.77\n156.67\n510.81\n2.682191e+04\n▇▁▁▁▁\n\n\npopdens_zcta\n0\n1\n1279.66\n2.757490e+03\n0.00\n101.15\n610.35\n1382.52\n3.041884e+04\n▇▁▁▁▁\n\n\nnohs\n0\n1\n6.99\n7.210000e+00\n0.00\n2.70\n5.10\n8.80\n1.000000e+02\n▇▁▁▁▁\n\n\nsomehs\n0\n1\n10.17\n6.200000e+00\n0.00\n5.90\n9.40\n13.90\n7.220000e+01\n▇▂▁▁▁\n\n\nhs\n0\n1\n30.32\n1.140000e+01\n0.00\n23.80\n30.75\n36.10\n1.000000e+02\n▂▇▂▁▁\n\n\nsomecollege\n0\n1\n21.58\n8.600000e+00\n0.00\n17.50\n21.30\n24.70\n1.000000e+02\n▆▇▁▁▁\n\n\nassociate\n0\n1\n7.13\n4.010000e+00\n0.00\n4.90\n7.10\n8.80\n7.140000e+01\n▇▁▁▁▁\n\n\nbachelor\n0\n1\n14.90\n9.710000e+00\n0.00\n8.80\n12.95\n19.22\n1.000000e+02\n▇▂▁▁▁\n\n\ngrad\n0\n1\n8.91\n8.650000e+00\n0.00\n3.90\n6.70\n11.00\n1.000000e+02\n▇▁▁▁▁\n\n\npov\n0\n1\n14.95\n1.133000e+01\n0.00\n6.50\n12.10\n21.22\n6.590000e+01\n▇▅▂▁▁\n\n\nhs_orless\n0\n1\n47.48\n1.675000e+01\n0.00\n37.92\n48.65\n59.10\n1.000000e+02\n▁▃▇▃▁\n\n\nurc2013\n0\n1\n2.92\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\nurc2006\n0\n1\n2.97\n1.520000e+00\n1.00\n2.00\n3.00\n4.00\n6.000000e+00\n▇▅▃▂▁\n\n\naod\n0\n1\n43.70\n1.956000e+01\n5.00\n31.66\n40.17\n49.67\n1.430000e+02\n▃▇▁▁▁\n\n\n\n\n\n\n❓ Given the dataset we’re working with, what wrangling should we consider doing here?”\n❓ What’s something you’ve learned about the data from the skimr output?\n\nConsider variable type - need more factors?\nUnderstand why ID is not uniformally distributed; figure out which are overrepresented; decide what to do\nlog or other transformations necessary? decide during EDA\n\nReminder: to read the data in and run skimr if you haven’t already:\n\n# install.packages(\"OCSdata\")\n# install.packages(\"here\")\n# install.packages(\"skimr\")\n\nOCSdata::raw_data(\"ocs-bp-air-pollution\", outpath = getwd())\npm &lt;- readr::read_csv(here::here(\"OCS_data\", \"data\",\"raw\", \"pm25_data.csv\"))\nskimr::skim(pm)\n\n\n\nThings to note:\n\n\ndata are summarized by variable type\nempty/n_missing gives you a sense of how much data are missing for each variable\nn_unique for state indicates that we have data for 49 states\nmany different distributions for continuous data, but many show bimodal distribution\nlarge range of possible values for many variables (i.e. population)\n\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "15-cs02-data"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23-ans.html",
    "href": "content/exams/midterm-fa23-ans.html",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "",
    "text": "You will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these packages have been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrichmondway &lt;- read_csv(\"data/richmondway.csv\")",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm (Ans)"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23-ans.html#packages",
    "href": "content/exams/midterm-fa23-ans.html#packages",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "",
    "text": "You will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these packages have been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrichmondway &lt;- read_csv(\"data/richmondway.csv\")",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm (Ans)"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23-ans.html#the-data",
    "href": "content/exams/midterm-fa23-ans.html#the-data",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Richmondway R Pacakge and have been provided by the TidyTuesday team.\nThe data are stored in data/richmondway.csv You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, this dataset includes data from three seasons of the TV show Ted Lasso. Each observation is a single episode of the show. The variables, generally, relate to the number of times Roy Kent (a foul-mouthed character on the show) and the entire cast say the F-word (often referred to as dropping the “F bomb”).",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm (Ans)"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23-ans.html#questions",
    "href": "content/exams/midterm-fa23-ans.html#questions",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nF-bomb summary:\n\nCalculate how many times total Roy Kent said the F-word within each season.\nComment on in which season Roy Kent said the F-word the most overall.\n\n\nrichmondway |&gt;\n  group_by(Season) |&gt;\n  summarize(count = sum(F_count_RK))\n\n# A tibble: 3 × 2\n  Season count\n   &lt;dbl&gt; &lt;dbl&gt;\n1      1    56\n2      2   106\n3      3   138\n\n\n\nSeason 1: 56; Season 2: 106; Season 3: 138\nSeason 3\n\n\n\nQuestion 2 (0.5 points)\nDetermine how many episodes had more F bombs by Roy Kent than every other character on the show combined (excluding Roy Kent)?\n\n# utilizing F_perc column\nrichmondway |&gt; filter(F_perc &gt; 50)\n\n# A tibble: 9 × 16\n  Character Episode_order Season Episode Season_Episode F_count_RK F_count_total\n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Roy Kent              3      1       3 S1_e3                   7            13\n2 Roy Kent              9      1       9 S1_e9                  14            22\n3 Roy Kent             11      2       1 S2_e1                  11            16\n4 Roy Kent             15      2       5 S2_e5                  23            32\n5 Roy Kent             16      2       6 S2_e6                  12            18\n6 Roy Kent             22      2      12 S2_e12                 23            44\n7 Roy Kent             24      3       2 S3_e2                  16            31\n8 Roy Kent             28      3       6 S3_e6                  13            21\n9 Roy Kent             32      3      10 S3_e10                 10            18\n# ℹ 9 more variables: cum_rk_season &lt;dbl&gt;, cum_total_season &lt;dbl&gt;,\n#   cum_rk_overall &lt;dbl&gt;, cum_total_overall &lt;dbl&gt;, F_score &lt;dbl&gt;, F_perc &lt;dbl&gt;,\n#   Dating_flag &lt;chr&gt;, Coaching_flag &lt;chr&gt;, Imdb_rating &lt;dbl&gt;\n\n# OR \n\n# calculating diff if you didn't see/underestand F_perc column\nrichmondway |&gt; \n  mutate(not_roy = F_count_total - F_count_RK) |&gt;\n  filter(F_count_RK &gt; not_roy)\n\n# A tibble: 9 × 17\n  Character Episode_order Season Episode Season_Episode F_count_RK F_count_total\n  &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Roy Kent              3      1       3 S1_e3                   7            13\n2 Roy Kent              9      1       9 S1_e9                  14            22\n3 Roy Kent             11      2       1 S2_e1                  11            16\n4 Roy Kent             15      2       5 S2_e5                  23            32\n5 Roy Kent             16      2       6 S2_e6                  12            18\n6 Roy Kent             22      2      12 S2_e12                 23            44\n7 Roy Kent             24      3       2 S3_e2                  16            31\n8 Roy Kent             28      3       6 S3_e6                  13            21\n9 Roy Kent             32      3      10 S3_e10                 10            18\n# ℹ 10 more variables: cum_rk_season &lt;dbl&gt;, cum_total_season &lt;dbl&gt;,\n#   cum_rk_overall &lt;dbl&gt;, cum_total_overall &lt;dbl&gt;, F_score &lt;dbl&gt;, F_perc &lt;dbl&gt;,\n#   Dating_flag &lt;chr&gt;, Coaching_flag &lt;chr&gt;, Imdb_rating &lt;dbl&gt;, not_roy &lt;dbl&gt;\n\n\nThere were nine episodes where Roy Kent had more F bombs than all of the other characters combined.\n\n\nQuestion 3 (1.5 points)\nGenerate an exploratory* visualization that displays the typical range of Roy Kent F-bombs in an episode, broken down by season and explain three things you’ve learned about the data from this plot.\n\n\n*Note: exploratory here means that it does NOT have to be polished. Do NOT worry about title, axis labels, etc. We just care about understanding the data here. (If you do customize, you will NOT be penalized. It’s just not required for this question.)\n\nggplot(richmondway, aes(x=factor(Season), y=F_count_RK)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nHere we see that the median value increases from season 1 to season 3, with a typical season 1 episode having approximately 6 F bombs from Roy Kent. (However, there was an outlier episode with almost 15 in Season 1). By season 3, the median value was more than 10. The episode with the most Roy Kent F bombs was season 2, with more than 25!\nRubric:\n\nhandles season as a factor\ncorrect variables\nboxplot (or other viz that displays range)\nthree things observed are accurate\n\n\n\nQuestion 4 (1 point)\nGenerate an exploratory* visualization that displays the relationship between Imdb_rating and Roy Kent F-bombs. Describe the relationship you see in this plot.\n\n\n*Note: exploratory here means that it does NOT have to be polished. Do NOT worry about title, axis labels, etc. We just care about understanding the data here. (If you do customize, you will NOT be penalized. It’s just not required for this question.)\n\nggplot(richmondway, aes(x=Imdb_rating, y=F_count_RK)) + \n  geom_point()\n\n\n\n\n\n\n\n\nRubric:\n\nscatterplot most typical\ncorrect variables plotted\ninterpretation correct (little to no relationship)\n\n\n\nQuestion 5 (1 point)\nBackground: Keeley is a character on Ted Lasso who is dating Roy Kent for some but not all of the episodes.\nGenerate a visualization that enables you to answer the questions below: - Does the median number of Roy Kent F bombs differ when Roy is dating Keeley (vs. when he is not)? - In the episode when Roy Kent dropped the most F bombs, was Roy dating Keeley?\n\nggplot(richmondway, aes(x=Dating_flag, y=F_count_RK)) + \n  geom_boxplot()  +\n  geom_jitter(width=0.25)\n\n\n\n\n\n\n\n\n\nboxplot most typical\nanswers correct (fewer when dating Keeley; most when he is dating Keeley)\n\n\n\nQuestion 6 (1.5 points)\nWhat is the effect of dating Keeley on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results.\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(F_count_RK ~ factor(Dating_flag) , data = richmondway) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term                   estimate std.error statistic     p.value\n  &lt;chr&gt;                     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)              8.84        1.31    6.74   0.000000128\n2 factor(Dating_flag)Yes  -0.0421      1.97   -0.0213 0.983      \n\n\nRubric:\n\noutcome is F bombs\ndating handled as a factor\ninterpretation is correct. (A typical episode when they’re not dating has 8.84 F bombs. And, on average there are slightly fewer F bombs when Keeley and Roy are dating (-0.04); however, this effect is quite small (in magnitude and significance) ). Must interpret intercept and effect/slope.\n\n\n\nQuestion 7 (1.5 points)\nBackground: In Season 1, Roy Kent is a player. After retiring, he eventually becomes a coach. So, Roy is a coach in some but not all of the episodes.\nWhat is the effect of whether or not Roy Kent is coaching on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results. Then, comment on whether coaching or dating Keeley is a better predictor of Roy Kent F bombs and explain how you came to that conclusion.\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(F_count_RK ~ factor(Coaching_flag) , data = richmondway) |&gt;\n  tidy()\n\n# A tibble: 2 × 5\n  term                     estimate std.error statistic  p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                  5.79      1.36      4.26 0.000167\n2 factor(Coaching_flag)Yes     5.16      1.77      2.92 0.00639 \n\n\nRubric:\n\nmodel correct\ninterpretation is correct. (When not coaching, 5.79 F bombs; when coaching 5.16 more (on average)). Must interpret intercept and effect/slope.\ncomment states that coaching is better (likely uses R^2 and/or p-value; effect size not appropriate for model comparison)\n\n\n\nQuestion 8 (2.5 points)\nGenerate a polished visualization that allows viewers to compare proportion/percentage of F-bombs broken down by season for Roy Kent vs those by everyone other than Roy Kent. Be sure to consider effective visualization principles discussed in class in this plot.\n\ndf &lt;- richmondway |&gt;\n  mutate(`All Other Characters` = F_count_total - F_count_RK,\n         `Roy Kent` = F_count_RK) |&gt;\n  select(Season, `Roy Kent`, `All Other Characters` ) |&gt;\n  pivot_longer(cols=-Season, names_to=\"character\")\n\nggplot(df, aes(x=value, y=str_wrap(character, width = 10), fill=factor(Season), group=character)) + \n  geom_col(position=\"fill\") +\n  scale_fill_manual(values = c(\"1\" = \"#deebf7\",\n                                \"2\" = \"#9ecae1\",\n                                \"3\" = \"#3182bd\")) +\n  labs(title=\"Third Season of Ted Lasso comprises almost half of the total F bombs\", \n       subtitle=\"Similar proportion of F bombs across seasons for Roy Kent vs. all other characters\",\n       x = \"Proportion of F Bombs\",\n       y = NULL,\n       fill = \"Season\") +\n  guides(color = \"none\") +\n  theme_classic(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\nRubric:\n\ndetermined/calculated number of not-roy\nproportions displayed\ntitle describes take-home\naxes labeled correctly\ngood design principles (colors, labels, etc.)\n\n\n\nQuestion 9 (3 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, describe at least one change that you would make to improve the design of the plot.\nNote: the hex values for the colors used in this plot are: “#deebf7” (lightest), “#9ecae1”, and “#3182bd” (darkest)\n\nCode to reproduce plot:\n\nggplot(richmondway, aes(x=Episode, y=F_count_total, group=Season, color=as.factor(Season))) + \n  geom_line(linewidth=2) +\n  geom_point(size=3) +\n  scale_x_continuous(n.breaks=12) +\n  scale_color_manual(values = c(\"1\" = \"#deebf7\",\n                                \"2\" = \"#9ecae1\",\n                                \"3\" = \"#3182bd\")) + \n  theme_classic() +\n  labs(title = \"Ted Lasso Episodes always have at least a handful of F bombs, but typically there are at least 20\",\n       subtitle = \"Number of F Bombs Dropped by all characters per episode across 3 seasons of Ted Lasso\",\n       y=\"F Count\",\n       color=\"Season\") +\n  theme(plot.title.position = \"plot\")\n\n\n\nQuestion 10 (1 point)\nDescribe at least 1) two things you like about how the plot in Question 9 communicates the data and 2) two things you would do differently to make this a more effective visualization for communication.\nLots of possible answers here. Most common cons: title too wordy!, line color, text size, would add grid lines, woudl visualize differently",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm (Ans)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html",
    "href": "content/exams/practice-exam-wi23-ans.html",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-01.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Feb 13th to complete this exam and turn it in via your personal Github repo - late work will not be accepted. Technical difficulties are not an excuse for late work - do not wait until the last minute to knit / commit / push.\nThere will be no Campuswire posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please DM or email Prof Ellis as soon as possible.\nEach question requires a (brief) narrative as well as a (brief) description of your approach. You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#rules",
    "href": "content/exams/practice-exam-wi23-ans.html#rules",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-01.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Feb 13th to complete this exam and turn it in via your personal Github repo - late work will not be accepted. Technical difficulties are not an excuse for late work - do not wait until the last minute to knit / commit / push.\nThere will be no Campuswire posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please DM or email Prof Ellis as soon as possible.\nEach question requires a (brief) narrative as well as a (brief) description of your approach. You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#academic-integrity-statement",
    "href": "content/exams/practice-exam-wi23-ans.html#academic-integrity-statement",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nI, ____________, hereby state that I have not communicated with or gained information in any way from my classmates or anyone during this exam, and that all work is my own.\nA note on sharing / reusing code: I am well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#grading-and-feedback",
    "href": "content/exams/practice-exam-wi23-ans.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThis exam is worth 14% of your grade. You will be graded on the correctness of your code, correctness of your answers, the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization but the template gets you started on a well-organized exam. We should be able to easily navigate your midterm to find what we’re looking for.) Organization + Clarity in written communication - 1pt",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#logistics",
    "href": "content/exams/practice-exam-wi23-ans.html#logistics",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called midterm-01.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#packages",
    "href": "content/exams/practice-exam-wi23-ans.html#packages",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these package has been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#the-data",
    "href": "content/exams/practice-exam-wi23-ans.html#the-data",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Axios and Harris Poll and have been provided by the TidyTuesday team.\nThe data are stored in two different files in the data/ folder: poll.csv and reputation.csv. You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, these two files include data about the 100 “most visible” brands in America. Specifically, reputation.csv includes information from the 2022 poll about these 100 stores across different reputation categories. poll.csv includes information about the same 100 stores but includes information about their rankings across multiple years.\n\npoll &lt;- read_csv(\"data/poll.csv\")\nreputation &lt;- read_csv(\"data/reputation.csv\")",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23-ans.html#questions",
    "href": "content/exams/practice-exam-wi23-ans.html#questions",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nHow many different industries (industry variable) are represented in these data?\n\n# either df can be used here\npoll |&gt; \n  distinct(industry) # |&gt;\n\n# A tibble: 19 × 1\n   industry          \n   &lt;chr&gt;             \n 1 Retail            \n 2 Food & Beverage   \n 3 Groceries         \n 4 Tech              \n 5 Ecommerce         \n 6 Automotive        \n 7 Healthcare        \n 8 Other             \n 9 Logistics         \n10 Financial Services\n11 Industrial        \n12 Consumer Goods    \n13 Pharma            \n14 Telecom           \n15 Insurance         \n16 Media             \n17 Energy            \n18 Airline           \n19 Food Delivery     \n\n  # count() # can optionally include count()\n\n# OR \nn_distinct(poll$industry)\n\n[1] 19\n\n\nThere are 19 different industries represented.\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here.\n\n\n\n\nQuestion 2 (0.5 points)\n\nWhich company had the lowest overall ranking in 2022?\nAnd for which category (from the name variable) did this organization score lowest?\n\n\nreputation |&gt;\n  filter(rank == max(rank)) |&gt; # could also hard code rank == 100 as you know 100 companies included in dataset\n  arrange(score)\n\n# A tibble: 7 × 5\n  company                industry name        score  rank\n  &lt;chr&gt;                  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 The Trump Organization Other    ETHICS       51.2   100\n2 The Trump Organization Other    TRUST        52.9   100\n3 The Trump Organization Other    CULTURE      53.0   100\n4 The Trump Organization Other    CITIZENSHIP  53.6   100\n5 The Trump Organization Other    GROWTH       55.1   100\n6 The Trump Organization Other    P&S          55.7   100\n7 The Trump Organization Other    VISION       59.4   100\n\n# ChatGPT used a really circuitous approach using baseR indexing/filtering that would not work for this dataset\n\n\nThe Trump Organization\nEthics\n\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here. Most common error was miscalculating the score within category, due to misunderstanding the structure of the data.\n\n\n\n\nQuestion 3 (1 point)\nWhich company in the reputation.csv dataset has the “best” average (mean) rank across all seven categories?\n\nreputation |&gt; \n  group_by(company) |&gt; \n  summarize(avg_rank = mean(rank)) |&gt; \n  arrange(avg_rank)\n\n# A tibble: 100 × 2\n   company                  avg_rank\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Trader Joe's                 4.86\n 2 The Hershey Company          5   \n 3 Patagonia                    6.71\n 4 HEB Grocery                  7   \n 5 Samsung                      8.86\n 6 Wegmans                     10.4 \n 7 Amazon.com                  12.1 \n 8 Toyota Motor Corporation    12.4 \n 9 Honda Motor Company         13.1 \n10 Microsoft                   13.1 \n# ℹ 90 more rows\n\n\nTrader Joe’s has the highest average ranking.\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here. Most common error was misreading the question.\n\n\n\n\nQuestion 4 (1 point)\nWhich company had the biggest increase in rank from 2021 to 2022?\n\npoll |&gt; \n  filter(year==2021) |&gt; \n  arrange(desc(change))\n\n# A tibble: 100 × 8\n   company               industry `2022_rank` `2022_rq` change  year  rank    rq\n   &lt;chr&gt;                 &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 The Home Depot        Retail            16      78.9     29  2021    45  75.4\n 2 Google                Tech              31      77.8     29  2021    60  73.3\n 3 IBM                   Tech              11      79.5     28  2021    39  76.3\n 4 Samsung               Tech               6      80.5     25  2021    31  77.5\n 5 Sony                  Tech              10      79.6     24  2021    34  77.3\n 6 Starbucks Corporation Food & …          43      76.6     22  2021    65  72.3\n 7 Microsoft             Tech              15      79       21  2021    36  76.8\n 8 Adidas                Retail            29      77.9     20  2021    49  75.1\n 9 General Motors        Automot…          51      75.4     17  2021    68  72  \n10 Yum! Brands           Food & …          53      75.3     17  2021    70  71.5\n# ℹ 90 more rows\n\n# if \"change\" column not noticed \npoll |&gt; \n  filter(year==2021) |&gt; \n  mutate(rank_diff = rank - `2022_rank`) |&gt; \n  arrange(desc(rank_diff))\n\n# A tibble: 100 × 9\n   company     industry `2022_rank` `2022_rq` change  year  rank    rq rank_diff\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 The Home D… Retail            16      78.9     29  2021    45  75.4        29\n 2 Google      Tech              31      77.8     29  2021    60  73.3        29\n 3 IBM         Tech              11      79.5     28  2021    39  76.3        28\n 4 Samsung     Tech               6      80.5     25  2021    31  77.5        25\n 5 Sony        Tech              10      79.6     24  2021    34  77.3        24\n 6 Starbucks … Food & …          43      76.6     22  2021    65  72.3        22\n 7 Microsoft   Tech              15      79       21  2021    36  76.8        21\n 8 Adidas      Retail            29      77.9     20  2021    49  75.1        20\n 9 General Mo… Automot…          51      75.4     17  2021    68  72          17\n10 Yum! Brands Food & …          53      75.3     17  2021    70  71.5        17\n# ℹ 90 more rows\n\n# Chat GPT assumes you need a join/doesn't understand the structure of the data\n\nThe Home Depot and Google saw the biggest jump, each increasing by 29 places.\n\n\n\n\n\n\nGrader Note\n\n\n\nBecause of the wording of the question, suggesting that there was only one company, credit was granted if students said either Home Depot or Google (or both).\n\n\n\n\nQuestion 5 (1.5 points)\nFor the industry with only a single “most visible” company in the dataset, has their RQ score been increasing or decreasing overall since 2017?\n\n# find the industry programmatically (or do a group by and figure out that it's insurance)\nindus &lt;- poll |&gt; \n  group_by(industry) |&gt; \n  count() |&gt; \n  arrange(n) |&gt; \n  ungroup() |&gt; \n  slice(1) |&gt; \n  pull(industry)\n\n# or\n\nindus &lt;- poll |&gt;\n  distinct(company, industry) |&gt;\n  group_by(industry) |&gt;\n  summarise(count = n()) |&gt;\n  filter(count == \"1\") |&gt;\n  pull(industry)\n\n# look at this output\npoll |&gt; \n  filter(industry == indus)\n\n# A tibble: 5 × 8\n  company                industry `2022_rank` `2022_rq` change  year  rank    rq\n  &lt;chr&gt;                  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Progressive Corporati… Insuran…          57      74.4     NA  2017    NA  72.7\n2 Progressive Corporati… Insuran…          57      74.4     NA  2018    NA  73.2\n3 Progressive Corporati… Insuran…          57      74.4     NA  2019    NA  71.5\n4 Progressive Corporati… Insuran…          57      74.4     NA  2020    NA  74  \n5 Progressive Corporati… Insuran…          57      74.4     NA  2021    NA  NA  \n\n# could also plot to determine but would take more work b/c data are across multiple columns - not tidy!\n\nWhile initially there was some variability, Progressive’s RQ has been increasing overall (74.4 in 2022, lower than that years prior).\n\n\n\n\n\n\nGrader Note\n\n\n\nAvoid hard-coding whenever possible. Porgrammatically determining the industry takes more work here, but avoids possible typos. Credit was granted if hard-coded. Most common error here was likely misreading the question.\n\n\n\n\nQuestion 6 (2 points)\nHow many companies from each industry category are represented in the 2022 ‘100 Most Visible’ companies in America data? Generate a visualization to display the answer to this question. Be sure to follow best visualization practices discussed in class.\n\n# note - should only display 100 companies total\npoll |&gt; distinct(company, .keep_all=TRUE) |&gt;\nggplot(aes(y=fct_rev(fct_infreq(industry)))) +\n  geom_bar() + \n  labs(title = \"Retail Companies Most Common Among 100 'Most Visible' in 2022\",\n       subtitle = \"Industries for the 100 Companies Included in the Axios and Harris Poll\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title.position = \"plot\",\n        axis.title = element_blank())\n\n\n\n\n\n\n\n# ChatGPT does data manipulation first and then plots from there; does include some \"best practices\" for data viz...but their operations are not correct\n\n\n\n\n\n\n\nGrader Note\n\n\n\nPlot should only display 100 companies. Barplot great here. Remember that categorical variables should be ordered meaningfully. Credit was lost if they were not, unless you directly labeled the values. Titles and axis labels are required. Because feedback for hw02 had not yet been returned, points were not deducted if your title was not as informative as it could have been. Most common errors: 1) not considering how to best display the plot so that labels were most readable (for most, this involved putting them on the y-axis); 2) not ordering axis with categorical data; 3) including more than 100 companies.\n\n\n\n\nQuestion 7 (2 points)\nOf industries that have at least 5 companies in the dataset, which industry has the highest median 2022 rank? Generate a visualization that allows you to answer this question. Be sure to follow best practices.\n\nindustries &lt;- poll |&gt; \n  distinct(company, .keep_all=TRUE) |&gt;\n  group_by(industry) |&gt;\n  count() |&gt; \n  filter(n&gt;5) |&gt;\n  pull(industry)\n\npoll |&gt; \n  filter(industry %in% industries) |&gt;\n  ggplot(aes(x=fct_reorder(industry, `2022_rank`), y=`2022_rank`)) +\n  geom_boxplot() +\n  labs(title = \"Automotive Industry Has the Highest Median Rank in 2022\",\n       subtitle = \"...among industries with at least 5 companies on the 'most visible' list\",\n       x = \"Industries\") +\n  theme_classic(base_size = 12) +\n  theme(plot.title.position = \"plot\",\n        axis.title.y = element_blank())\n\n\n\n\n\n\n\n# a bit more convoluted answer on Chat GPT\n\n\n\n\n\n\n\nGrader Note\n\n\n\nFull credit was granted whether a boxplot or a barplot was used; however, a boxplot displays more information overall. Remember that categorical variables should be ordered meaningfully. Credit was lost if they were not, unless you directly labeled the values. Titles and axis labels are required. Because feedback for hw02 had not yet been returned, points were not deducted if your title was not as informative as it could have been. Most common errors: 1) not filtering w/ correct logic and 2) not ordering axis with categorical information\n\n\n\n\nQuestion 8 (2 points)\nYour boss is curious about how much rankings change from one year to the next. To answer this question, they ask you to determine how well 2021 rankings explain the following year’s 2022 rankings. Generate a linear model to answer this question. Be sure to include your interpretation of the model (in other words your answer to the question “how well do 2021 rankings explain 2022’s rankings?”)\n\ndf &lt;- poll %&gt;% filter(year == 2021)\n\nmod &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(`2022_rank` ~ rank, data = df)\n\nmod |&gt; tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    6.63     3.37        1.97 5.27e- 2\n2 rank           0.864    0.0573     15.1  6.08e-25\n\n\n\nmod |&gt;\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.742         0.739  14.8      227. 6.08e-25     1  -332.  671.  678.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Chat GPT gets the estimate incorrect (0.8 instead of 0.86) and Rsquared states 60% rather than 74%\n\nWe can see here that the 2021 data explain 74% of the variance in the 2022 data, making it a pretty good model for explaining 2022 rankings. Further, by looking at the coefficient, a company who was ranked in both 2021 and 2022 could expect their rank to increase, on average, by 0.86 in 2022.\n\n\n\n\n\n\nGrader Note\n\n\n\nModel had to be interpreted accurately for full credit and answer the question posed. Most common errors were: 1) flipping the predictor and outcome; 2) not answering the question posed\n\n\n\n\nQuestion 9 (2.5 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be.\n\n\nggplot(df, aes(x=rank, y=`2022_rank`)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", col=\"orange\") + \n  theme_classic(base_size = 14) + \n  labs(title = \"2021 Rank Explains 74% of the variance in 2022 rank\",\n       subtitle = \"On average, companies saw their rank increase almost one spot higher in 2022\",\n       x = \"2021 rank\",\n       y = \"2022 rank\") +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrader Note\n\n\n\nOn the answer key I have fixed the % variance explained; this original 78% was not meant to trick. It was a mistake by prof. As such, I have graded this question leniently. Most common point deductions here were for: 1) not matching theme; 2) not changing line color; 3) not left-aligning title. No points were lost for not increasing text size as that’s hard to visually see.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html",
    "href": "content/exams/practice-exam-fa21.html",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal.\n\n\n\n\n\n\nNote\n\n\n\nThis is the midterm from when the course was offered in fa21. Linear regression was not covered prior to the midterm the last time this course was offered. There will be a question or two on linear regression and the interpretation of linear models on this year’s midterm.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#rules",
    "href": "content/exams/practice-exam-fa21.html#rules",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal.\n\n\n\n\n\n\nNote\n\n\n\nThis is the midterm from when the course was offered in fa21. Linear regression was not covered prior to the midterm the last time this course was offered. There will be a question or two on linear regression and the interpretation of linear models on this year’s midterm.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#logistics",
    "href": "content/exams/practice-exam-fa21.html#logistics",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam-fa21.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#packages",
    "href": "content/exams/practice-exam-fa21.html#packages",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse package for this (practice) midterm. (For the real deal, you’ll need tidyverse and tidymodels.) If working on datahub, this package has been installed, but you will need to load it. No other packages are required, but if for some reason you want to load in another package, you are permitted to do so.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#the-data",
    "href": "content/exams/practice-exam-fa21.html#the-data",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "The data",
    "text": "The data\nThe dataset you’ll be working with on this practice midterm is all about beach volleyball. The full dataset is explained in detail here and includes match-level data from 76,756 volleyball matches. You should click on that link to see what information is stored in each column in this dataset and what information is included in each column.\nBriefly, what you’ll use for this midterm is a subset of the full dataset, including only the 11,699 observations (rows) from 2018 and 2019 but all of the original columns. Each row summarizes the results from a single, distinct match played in a volleyball tournament.\nTo briefly describe beach volleyball, it is a sport played 2 on 2, so each match involves only 4 players. These data include matches from two different volleyball circuits, the international FIVB and the US-centric AVP. You will not need to know much at all about this sport to complete this midterm, and anything you need to know will be explained.\nThe data are stored in data/vb_matches.csv. You’ll need to read the dataset in prior to answering any questions on the midterm.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21.html#questions",
    "href": "content/exams/practice-exam-fa21.html#questions",
    "title": "COGS 137 - Practice Midterm (Fa21)",
    "section": "Questions",
    "text": "Questions\nQuestion 1 (0.75 points) - How many FIVB and AVP matches are included in this dataset?\nQuestion 2 (0.75 points) - Find the match with the longest duration.\na.  Where was this tournament played (City & Country)?\nb.  How long did the match last?\nc.  Who were the two winners? &lt;/br&gt;\nQuestion 3 (1.5 points) - Across all tournaments included in this dataset, which teams have won the most tournaments? Your response should include both the winning players, their gender, and the number of tournaments they’ve won in descending order. Who has the most wins? How many men’s and how many women’s teams are in the top 10? Note: “winning a tournament” is indicated by winning either a “Gold Medal” (FIVB) or “Finals” (AVP) match, specified in the bracket column.\nQuestion 4 (1.5 points) - Of only the AVP tournaments included in this dataset, how many different cities hosted tournaments in 2018 and 2019? And, which cities (if any) hosted a tournament in both 2018 and 2019? Note that tournaments are named for the city hosting the tournament.\nQuestion 5 (2.5 points) - Prof Ellis plays a lot of women’s beach volleyball and is only 5’5” (65 inches). Despite not having the sheer talent or raw athletic ability to make it as a professional volleyball player, she wonders if she ever had a chance at her height. (Reminder: there are 4 players in each match whose height should be considered.) To help her out, answer each of the following:\na.  Who was the shortest women's player to compete in a tournament in 2018/2019?\nb.  How tall are they?\nc.  Did they *win* a tournament in 2018 or 2019? &lt;/br&gt;\nQuestion 6 (3 points) - Which country has hosted the most FIVB tournaments? Did this differ by year? Generate a visualization that shows how many FIVB tournaments each country hosted. Allow viewer to visualize this by year. And, be sure each tournament is only counted once (regardless of how many games were played).\nQuestion 7 (3 points) - Recreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be. (Hint: The visualization uses the variable avg_team_height, which is not included in the provided data frame. You will have to create avg_team_height yourself, by determining the average (mean) team height for each winning team.)\n\n\n\n\n\n\n\nNote\n\n\n\nQ7 had a typo when this course was offered previously leading to students spending wayyyyy longer than intended on this exam. That typo has been fixed for this practice midterm.\n\n\nQuestion 8 (1 pts) - If you were in charge of designing the plot you just recreated in the plot above, what changes would you make to improve its effectiveness as a visualization? (You do not have to write any code for this question, just explain the different design/viz choices you would make.)",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Fa21)"
    ]
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#agenda",
    "href": "content/labslides/01-lab-deck.html#agenda",
    "title": "Lab 01: Tooling",
    "section": "Agenda",
    "text": "Agenda\n\nLab structure: Lab structure overview.\nLab 01 intro and demos: Introduce the lab, and work through the first section as a class.\nOn your own: Work on the rest of the lab “on your own”, but feel free to check in with classmates as much as you like.",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab01"
    ]
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#lab-structure-1",
    "href": "content/labslides/01-lab-deck.html#lab-structure-1",
    "title": "Lab 01: Tooling",
    "section": "Lab structure",
    "text": "Lab structure\n\n5-10 minute introduction (a bit longer today)\nUse the remaining time to work through the lab exercises and fill out your lab report\n\nSubmit: on your own\nWorking: always allowed to work together\n\nLab instructions posted on the course website on the left panel under “Labs”\n\nLet’s go find today’s lab!",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab01"
    ]
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#tips",
    "href": "content/labslides/01-lab-deck.html#tips",
    "title": "Lab 01: Tooling",
    "section": "Tips",
    "text": "Tips\n\nYou do not have to finish the lab in class; you have until midnight to submit. But, you might choose to get through portions that you think will be challenging (which initially might be the coding component) in class when staff can help you on the spot, and leave the narrative writing until later.\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality lab.\nWhen working with others, do not split up lab among classmates, work on it together in its entirety.\nSometimes you may not finish the entire lab…and that’s ok! When this happens or you’re unsure about what you turn in, be sure to go back and check your thoughts/work against the posted answer key.",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab01"
    ]
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#goals",
    "href": "content/labslides/01-lab-deck.html#goals",
    "title": "Lab 01: Tooling",
    "section": "Goals",
    "text": "Goals\n\nIntroduce you to Git and GitHub: collaboration and version control system that we will be using throughout the course\n\nGit is a version control system – like “Track Changes” features from Microsoft Word/Google Docs on steroids\nGitHub is the home for your git-based projects on the internet\nConnect your RStudio on datahub to your GitHub account\n\nIntroduce you to R and RStudio:\n\nR is the name of the programming language itself\nRStudio is a convenient interface",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab01"
    ]
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#getting-started-github-datahub",
    "href": "content/labslides/01-lab-deck.html#getting-started-github-datahub",
    "title": "Lab 01: Tooling",
    "section": "Getting started: GitHub & datahub",
    "text": "Getting started: GitHub & datahub\nFirst, put away computers, and watch me do it:\n\nDemo of the process\nSteps are spelled out in the “GitHub Housekeeping” portion of the lab",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab01"
    ]
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#getting-started-assignment-retrieval",
    "href": "content/labslides/01-lab-deck.html#getting-started-assignment-retrieval",
    "title": "Lab 01: Tooling",
    "section": "Getting started: Assignment Retrieval",
    "text": "Getting started: Assignment Retrieval\nFirst, put away computers, and watch me do it:\n\nClick on the assignment link on Canvas for today’s lab to create your GitHub repository (which we’ll refer to as “repo” going forward) for the lab. This repo contains a template you can build on to complete your lab.\nOn GitHub, accept the assignment. Click on the link to navigate to the repo.\nOn the repo, click the green “&lt;&gt; Code” button, ensure that “SSH” is selected, and then copy the URL.\nGo to datahub. and open RStudio. Go to File &gt; New Project… and select to create a New Project from Version Control. On the following menu, select Git.\nCopy and paste the URL of your assignment repo into the “Repository URL” dialog box.\nHit Create Project.\n\nNow it’s your turn! Place a green sticky on your laptop when you’re done with this part (you can continue if you like). Place a pink sticky if you have questions.",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab01"
    ]
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#agenda",
    "href": "content/labslides/05-lab-deck.html#agenda",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Agenda",
    "text": "Agenda\n\nDocumentation\nLab 05: Modelling course evaluations\nGetting started with lab\n\nReminder to complete mid-course survey",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab05"
    ]
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#documentation",
    "href": "content/labslides/05-lab-deck.html#documentation",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Documentation",
    "text": "Documentation\nDemo on how to utilize/read/understand R Documentation",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab05"
    ]
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#reminder-data-come-from",
    "href": "content/labslides/05-lab-deck.html#reminder-data-come-from",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Reminder: Data come from…",
    "text": "Reminder: Data come from…\n“Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity”\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005. http://www.sciencedirect.com/science/article/pii/S0272775704001165",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab05"
    ]
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#some-notes-on-this-lab",
    "href": "content/labslides/05-lab-deck.html#some-notes-on-this-lab",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Some notes on this lab",
    "text": "Some notes on this lab\n\nThis is an extension of lab04. It may be worth briefly looking over that lab and/or the answer key to get that fresh in your mind prior to beginning this lab.\nThere are three parts. Ideally, you’d get through Exercise 12. We’ll be looking to see you’ve fit and interpreted models with multiple predictors.\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab05"
    ]
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#agenda",
    "href": "content/labslides/03-lab-deck.html#agenda",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Agenda",
    "text": "Agenda\n\nTips:\n\nBriefly review a question regarding sorting.\nReview numeric vs categorical variable types.\n\nLab introduction:\n\nReview FiveThirtyEight article on college majors.",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab03"
    ]
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#lab-intro",
    "href": "content/labslides/03-lab-deck.html#lab-intro",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Lab Intro",
    "text": "Lab Intro\n\nLab instructions posted on the course website.\nThe Economic Guide To Picking A College Major by Ben Casselman",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab03"
    ]
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#reminders",
    "href": "content/labslides/03-lab-deck.html#reminders",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Reminders",
    "text": "Reminders\n\nStart with library(tidyverse) (includes tidyr, readr, dplyr, etc.)\nClone using ‘SSH’ link from GitHub\nKnit to .html & push both .Rmd and .html to GitHub",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab03"
    ]
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#tips",
    "href": "content/labslides/03-lab-deck.html#tips",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Tips",
    "text": "Tips\n\nBe ready to troubleshoot your document, since it will likely fail to knit on multiple occasions throughout the process. Read the error message carefully and take note of which line is preventing a successful knit.\nMake sure to keep track of your various chunks and to keep text and code in the right place.\nRemember that your R Markdown file is not aware of your project’s global environment and can only make use of variables, functions, etc. that you have loaded or defined in the document.\nRemind yourself how the pipe operator (|&gt;) works.\nIf you’re unsure how a function works or what its arguments are, type ? in front of it and hit enter (?read_csv for instance). The “Help” tab will open and provide a summary of the function as well as some examples.\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab03"
    ]
  },
  {
    "objectID": "content/cs/cs02.html",
    "href": "content/cs/cs02.html",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "",
    "text": "Important\n\n\n\nCS02 is not required fa23 quarter. Students have the option to complete CS02 in lieu of the typical final project. This will be completed in your final project groups and will require use of some outside source of data.\nThis is your second case study report, so you get to incorporate the general feedback from cs01 and carry out another complete data science project! This report will include your analysis from top (the background and question) to bottom (your analysis, interpretation, and conclusions.)\nWe’ll be grading to see that you have: 1) all necessary code for each section of the project; 2) explanatory text that guides the reader from start to finish; 3) polished visualizations that allow the reader to both understand the data you’re working with an your conclusions.\nThis will be submitted and graded as a group. One submission per group.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS02"
    ]
  },
  {
    "objectID": "content/cs/cs02.html#getting-started",
    "href": "content/cs/cs02.html#getting-started",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nThis will be completed in cs02 group repository that has been created for you and your group mates.\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit and commit changes (for example, once per each new part)1\nPush all your changes back to your GitHub repo\nThis case study will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading.\n\nImports\nYou are allowed to import whichever packages you like for this case study report.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS02"
    ]
  },
  {
    "objectID": "content/cs/cs02.html#case-study-report",
    "href": "content/cs/cs02.html#case-study-report",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "Case Study Report",
    "text": "Case Study Report\nYour case study can be organized however you see best fit, but we’ll be looking for the following general sections:\n\nTitle\nAuthors\nBackground/Introduction\nQuestion(s)\nData\n\nData Explanation\nData Import\nData Wrangling\n\nAnalysis\n\nExploratory Data Analysis\nData Analysis\n\nResults\nDiscussion of results\nConclusion\n\nNow, you may want to combine some of these sections (i.e. include your results and discussion among your analysis code). That’s totally allowed, but we’ll be looking to see that your report includes sufficient information to understand what you did, why you did it, and what your results are.\n\nRequired Questions\nAll groups will analyze the data and answer the following question in their report:\nCan we predict US annual average air pollution concentrations at the granularity of zip code regional levels using predictors such as data about population density, urbanization, road density, as well as, satellite pollution data and chemical modeling data?\n\n\nExtending the Analysis\nIn addition to getting the code presented in class working, adding explanatory text to your report, and generating polished visualizations, you and your group must “extend the analysis” presented in class in a meaningful way. Now “meaningful” is not a very-easily-measured term. A meaningful extension could be carrying out analysis to answer an additional sub-question beyond what was presented in class, or including a really extensive exploratory data analysis, including data from additional years, and/or or generating a really superb set of visualizations to convey your groups’ results, or finding a related dataset and incorporating it into your case study. To determine whether your extension is “meaningful,” you and your group should be able to answer “yes” to the question “Does our extension add something important to this report beyond what was presented in class?”\nThis extension should be included/weaved into your report, meaning it should only be “separated out” as its own section if it makes most sense for the story you’re telling.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS02"
    ]
  },
  {
    "objectID": "content/cs/cs02.html#general-communication",
    "href": "content/cs/cs02.html#general-communication",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "General Communication",
    "text": "General Communication\nEach group will need to convey the most important finding(s) to a general audience through some form of communication.\nThis is very open-ended in its format. It could be a short video, an infographic, an effective email, a graphic, instagram slides, a short presentation, etc. It will be submitted by one group member on Canvas. (All group members will receive credit.)\nThe specific audience you want to target can be specified (i.e. undergraduate students, policy makers, local government officials, etc.); however, the assumption is that these are NOT data scientists.\nYour communication SHOULD include your take-home message…and that may be all it includes! Basically, we want you to distill down your case study to its most important message and then convey that to the general public in an effective manner.\nIt should NOT contain specifics of your analysis or anywhere near all the information included in your report.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS02"
    ]
  },
  {
    "objectID": "content/cs/cs02.html#group-feedback",
    "href": "content/cs/cs02.html#group-feedback",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the case study to provide feedback about working with your group mates. This is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary. This form is “due” 24h after the case study, to give you time to reflect/complete your feedback after completing the case study itself.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS02"
    ]
  },
  {
    "objectID": "content/cs/cs02.html#footnotes",
    "href": "content/cs/cs02.html#footnotes",
    "title": "CS02: Predicting Annual Air Pollution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAvoid waiting until the end to knit for the first time. It will be better/easier/less of a headache if you knit periodically and know it’s all working as intended.↩︎",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS02"
    ]
  },
  {
    "objectID": "content/cs/cs-example.html",
    "href": "content/cs/cs-example.html",
    "title": "CS: Example",
    "section": "",
    "text": "library(OCSdata)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(pdftools)\nlibrary(tesseract)\nlibrary(magick)\nlibrary(knitr)\nlibrary(Kendall)\nlibrary(broom)\n\n\n\n\n\n\n\nProf Note\n\n\n\nI would recommend packages be imported after the introduction, but this is very specific and not something anyone would lose credit for.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS: Example"
    ]
  },
  {
    "objectID": "content/cs/cs-example.html#introduction",
    "href": "content/cs/cs-example.html#introduction",
    "title": "CS: Example",
    "section": "Introduction",
    "text": "Introduction\nIn this case study, we seek to analyze the youth disconnection among minority groups within the United States. The disconnected youth are the young people who are between the ages of 16 and 24 who are neither working nor in school. Following the aftermath of the Great Recession in 2008, we witnessed a peak youth disconnection rate of 14.7 percent in 2010 which has since been trending downwards to as low as a youth disconnection rate of 11.5 percent in 2017. While analyzing racial and ethnic groups is imperative for viewing any evident disparities, we also intend on analyzing how gender may play a role in affecting the rates of disconnection. Seeing as how we can already see existing disparities between genders within the workforce, we also want to see to what extent, if at all, gender plays a role in youth disconnection among groups.\nThis specific period of young adulthood is one of the most critical in developing the necessary skills and capabilities required of them to be successful in adulthood and their professional careers as youth disconnection during this period stunts development and limits their potential. Measure of America, a nonpartisan project of the nonprofit Social Science Research, claims that “people who experience a period of disconnection as young adults go on to earn less and are less likely to be employed, own a home, or report good health by the time they reach their thirties”. Understanding how these trends among groups arise may be critical for finding solutions to preventing youth disconnection, and analyzing what groups may need more resources in order to prevent further prevent youth disconnection.\n\n\n\n\n\n\nProf Note\n\n\n\nThis introduction is missing citations! Statements that are not your own original thoughts must be cited. See Effective Communication lecture for possible ways to cite. We don’t care what format you use. We do care that you cite others’ work. Also, definitely feel free to find information on the topic beyond what was presented in class.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS: Example"
    ]
  },
  {
    "objectID": "content/cs/cs-example.html#questions",
    "href": "content/cs/cs-example.html#questions",
    "title": "CS: Example",
    "section": "Questions",
    "text": "Questions\n\nHow have youth disconnection rates in American youth changed since 2008?\nIn particular, how has this changed for different gender and ethnic groups? Are any groups particularly disconnected?\nDoes removing 2008 (pre-Great Recession time period) data points change our answer?\n\n\n\n\n\n\n\nProf Note\n\n\n\nThe third question here was their extension. Note that this is an ok extension all things considered. This was earlier in the quarter, a more straightforward dataset, there wasn’t as much to do as there is in our dataset. Explaining the why this is their focus in introduction would have been great.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS: Example"
    ]
  },
  {
    "objectID": "content/cs/cs-example.html#the-data",
    "href": "content/cs/cs-example.html#the-data",
    "title": "CS: Example",
    "section": "The Data",
    "text": "The Data\nThe data set that we will be using for our case study is provided by two reports from the Measure of America project related to youth disconnect. The data provided will be in the form of images.\n\nData Import\nIn order to begin our analysis, we imported our raw data from the OCSdata library on youth disconnection.\n\n# Data is imported so I get overwrite error when not commented - Adrian\n# Download raw data files\n# load_raw_data(\"ocs-bp-youth-disconnection\", outpath = '.')\n\n\n\n\n\n\n\nProf Note\n\n\n\nReminder that you can control chunk behavior for each chunk within the curly braces. This also applies for the warnings that are displayed later on. Best to supress those in final HTML.\n\n\nSince the raw data is in images, we need to convert to a usable R data set. We started with the major racial ethnic groups.\n\n## Import data for major racial/ethnic groups\nmajor_racial_ethnic_groups &lt;- magick::image_read(\"data/Major_ethnic_groups_screenshot.png\")\nmajor_groups &lt;- magick::image_ocr(major_racial_ethnic_groups)\n\nFor Asian subgroups, we had to import the raw data for 2017 and 2018.\n\n## Import data for Asian subgroups\n\n# 2017 data\nasian_sub_2017 &lt;- image_read(\"data/asian_subgroups_2017.png\")\nasian_sub_2017_A &lt;- image_read(\"data/asian_sub_2017_A.png\")\nasian_sub_2017_B &lt;- image_read(\"data/asian_sub_2017_B.png\")\nasian_sub_2017_C &lt;- image_read(\"data/asian_sub_2017_C.png\")\nasian_sub_2017 &lt;- image_ocr(asian_sub_2017)\nasian_sub_2017_A &lt;- image_ocr(asian_sub_2017_A)\nasian_sub_2017_B &lt;- image_ocr(asian_sub_2017_B)\nasian_sub_2017_C &lt;- image_ocr(asian_sub_2017_C)\n\n# 2018 data\nasian_sub_2018_A &lt;- image_read(\"data/asian_sub_2018_A.png\")\nasian_sub_2018_A &lt;- image_ocr(asian_sub_2018_A)\nasian_sub_2018_B &lt;- image_read(\"data/asian_sub_2018_B.png\")\nasian_sub_2018_B &lt;- image_ocr(asian_sub_2018_B)\n\nFor Latinx subgroups, we also imported 2017 and 2018 data.\n\n## Import data for Latinx subgroups\n\n# 2017 data\nlatinx_imageA &lt;- image_read(\"data/latinx_sub_2017_A.png\")\nlatinx_imageB &lt;- image_read(\"data/latinx_sub_2017_B.png\")\nlatinx_imageC &lt;- image_read(\"data/latinx_sub_2017_C.png\")\nlatinx_sub_2017_A &lt;- image_ocr(latinx_imageA)\nlatinx_sub_2017_B &lt;- image_ocr(latinx_imageB)\nlatinx_sub_2017_C &lt;- image_ocr(latinx_imageC)\n\n# 2018 data\nlatinx_sub_2018 &lt;- image_read(\"data/latinx_subgroups_2018.png\")\nlatinx_sub_2018 &lt;- image_ocr(latinx_sub_2018)\n\nOnce the data import is complete, we saved our data.\n\n# Save data\nsave(\n  major_groups,\n  asian_sub_2017,\n  asian_sub_2017_A,\n  asian_sub_2017_B,\n  asian_sub_2017_C,\n  latinx_sub_2017_A,\n  latinx_sub_2017_B,\n  latinx_sub_2017_C,\n  asian_sub_2018_A,\n  asian_sub_2018_B,\n  latinx_sub_2018,\n  file = \"data/imported_data.rda\")\n\n\n\nData Wrangling\nNow that we have our data properly imported, we need to wrangle our data in R so we can begin our analysis.\n\n\n\n\n\n\nProf Note\n\n\n\nThere is text to guide the viewer, clear code, and code comments as needed. I like this.\n\n\n\nMajor Group Data\nTo begin our data wrangling, we took the imported data that is in a single string and separated the data by new line characters and transformed it into a table.\n\n# Separates string by new line and transforms into a table\nmajor_groups &lt;- major_groups |&gt;\n  stringr::str_split(pattern = \"\\n\") |&gt;\n  unlist() |&gt; \n  tibble::as_tibble()\n\nNow that our data has rows, we created columns Group and Year and assigned the corresponding values according to the data as well as normalizing the capitalization throughout the data set.\n\n# Separate into columns Group and Year\nmajor_groups &lt;- \n  major_groups |&gt;\n  tidyr::separate(col = value, \n                  into = c(\"Group\", \"Years\"), # Set column names\n                  sep = \"(?&lt;=[[:alpha:]])\\\\s(?=[0-9])\")  # Separate after letter string; beginning numerics\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [19].\n\n# Make capitalization format the same\nmajor_groups &lt;- major_groups |&gt; \n  mutate(Group = stringr::str_to_title(Group))\n\nWe have our data in two columns but need to separate our Years column into each individual year. We thus separated the column by every space character, assigned appropriate years for its corresponding values, and made the data values numeric.\n\n# Separate `Years` into columns by individual years\nmajor_groups &lt;- major_groups |&gt; \n  tidyr::separate(col = Years, \n                  into = c(\"2008\", \"2010\", \"2012\", \"2014\", \"2016\", \"2017\"),  # Set column names\n                  sep = \" \") # Separate by spaces\n\n# Remove empty rows\nmajor_groups &lt;- major_groups |&gt; \n  tidyr::drop_na()\n \n# Make data numeric\nmajor_groups &lt;- major_groups |&gt;\n  mutate(\n    across(.cols = -Group,\n           ~ str_remove(string = ., pattern = \"\\\\.\")),  # Remove decimal points from string\n    across(.cols = -Group, as.numeric),  # Convert to numeric\n    across(.cols = -Group, ~ . * 0.1)   # Add decimal point back\n  )\n\nWe have a lot of information in our Group column, so we separated that information and created two new columns, Race_Ethnicity and Gender, to make it easier to analyze our data.\n\n# Create Race_Ethnicity column\nmajor_groups  &lt;- major_groups |&gt;\n  # First, create entries for `All_races`\n  mutate(Race_Ethnicity = dplyr::recode(Group, \"United States\" = \"All_races\",\n                                        \"Female\" = \"All_races\",\n                                        \"Male\" = \"All_races\"),\n  # Second, create entries for all other races by importing data from group column and removing gender  \n         Race_Ethnicity = str_remove(string = Race_Ethnicity,\n                                     pattern = \"Female|Male\"))\n# Create Gender column\nmajor_groups  &lt;- major_groups |&gt;\n  # Extract gender from `Group` column\n  mutate(Gender = str_extract(string = Group, \n                              pattern = \"Female|Male\")) |&gt;\n  # Assign `All' if gender was not in `Group`\n  mutate(Gender = replace_na(Gender, replace = \"All\"))\n\nNow that we have all of our data, we moved the columns containing years into rows and added a new column Percent to log the value contained in the, now former, year column.\n\n# Puts years in rows instead of columns with percent values\nmajor_groups_long &lt;- major_groups |&gt;\n  tidyr::pivot_longer(cols = contains(\"20\"), # any column containing 20\n                      names_to = \"Year\", # Assigns column for year\n                      values_to = \"Percent\", # Assigns column for percent\n                      names_prefix = \"Perc_\") |&gt;\n  dplyr::mutate(Year = as.numeric(Year))\n\n\n\nSubgroup Data\n\nData Wrangling Functions\nWe wrangled the major group data but now have to wrangle the subgroup data, which is a little more complicated. We first created a make_rows function to take the imported raw data and convert it into a table with rows separated by new line characters.\n\n# Create function to separate string into rows\nmake_rows &lt;- function(text){\n  text |&gt;\n  str_split(\"\\n\") |&gt;\n  unlist() |&gt;\n  as_tibble()\n}\n\nWe also created two functions clean_table and clean_table_2018 to clean our subgroup data just like the major group data – separating Group and Years columns, converting data to numeric, and creating Race_Ethnicity and Gender columns.\n\n# Create function to clean 2017 subgroup data \nclean_table &lt;- function(table){\n  table |&gt;\n    separate(col = value,\n             into = c(\"Group\", \"Percentage\"), # Create column names\n             sep =  \"(?&lt;=[[:alpha:]])\\\\s(?=[0-9])\") |&gt; # Split into columns after string of letters\n    drop_na() |&gt; # Remove NA rows\n    # Make percentage data numeric\n    mutate(Group = str_to_title(Group)) |&gt;\n    mutate(Percentage = str_remove(string = Percentage,\n                                   pattern = \"\\\\.\")) |&gt; # Remove decimal points from string\n    separate(Percentage, c(\"Percent\"), sep = \" \") |&gt; # Separate by space\n    mutate(Percent = as.numeric(Percent)) |&gt; # Convert to numeric\n    mutate(Percent = Percent * 0.1) |&gt; # Add back decimal point\n    # Create Race_Ethnicity column\n    mutate(Race_Ethnicity = recode(Group, \n                                   # Create entries for All_races\n                                   \"United States\" = \"All_races\",\n                                   \"Female\" = \"All_races\",\n                                   \"Male\" = \"All_races\")) |&gt;\n    # Create entries for all other races by importing data from group column and removing gender  \n    mutate(Race_Ethnicity = str_remove(string = Race_Ethnicity, \n                                       pattern = \" Female| Male\")) |&gt; \n    # Create Gender column\n    mutate(Gender = str_extract(string = Group,\n                                pattern =\"Female|Male\")) |&gt; # Extract gender from `Group` column\n    mutate(Gender = replace_na(Gender, replace = \"All\"))  # Assign `All' if gender was not in `Group`\n}\n\n# Create function to clean 2018 subgroup data \nclean_table_2018 &lt;- function(table){\n  table |&gt;\n    separate(col = value, \n             into = c(\"Group\", \"Percent\"), # Create column names\n             sep =  \"(?&lt;=[[:alpha:]])\\\\s:\\\\s|\\\\s(?=[0-9])\") |&gt;  # Split into columns after colon\n    mutate(Group = str_remove(string = Group, \n                            pattern = \":\")) |&gt; # Remove colon\n    drop_na() |&gt; # Remove NA rows\n    # Make percentage data numeric\n    mutate(Group = str_to_title(string = Group)) |&gt; \n    mutate(Percent = str_remove(string = Percent, \n                               pattern = \"\\\\.\")) |&gt; # Remove decimal points from string\n    mutate(Percent = as.numeric(Percent)) |&gt; # Convert to numeric\n    mutate(Percent = Percent * 0.1) |&gt; # Add back decimal point\n    # Create Race_Ethnicity column\n    mutate(Race_Ethnicity = str_replace(string = Group,\n                                        pattern = \"Men|Women\", \n                                        replacement = \"missing\")) |&gt; # Replace gender with missing\n    mutate(Race_Ethnicity = na_if(Race_Ethnicity, \"missing\")) |&gt; # Make missing values NA\n    fill(Race_Ethnicity, .direction = \"down\") |&gt; # Fill Race/Ethnicity in missing fields\n    # Create Gender column\n    mutate(Gender = str_extract(string = Group, \n                                pattern = \"Men|Women\")) |&gt; # Extract gender from `Group` column\n    mutate(Gender = replace_na(Gender, replace = \"All\")) # Assign `All' if gender was not in `Group`\n}\n\n\n\nAsian Subgroup Data\nWe ran our make_rows function on our raw data and combined the initially wrangled 2017 data.\n\n## Asian subgroup data\n# 2017 data\n# Apply make_rows function to subgroup data\nasian_sub_2017 &lt;- make_rows(asian_sub_2017)\nasian_sub_2017_A &lt;- make_rows(asian_sub_2017_A)\nasian_sub_2017_B &lt;- make_rows(asian_sub_2017_B)\nasian_sub_2017_C &lt;- make_rows(asian_sub_2017_C)\n\n# Combine data\nasian_sub_2017 &lt;- bind_rows(asian_sub_2017_A, \n                            asian_sub_2017_B,\n                            asian_sub_2017_C)\n\nSimilarly, we applied make_rows to 2018 data.\n\n# 2018 data\n# Combine data\nasian_sub_2018 &lt;- str_c(asian_sub_2018_A, asian_sub_2018_B)\n\n# Apply make_rows function\nasian_sub_2018 &lt;- make_rows(asian_sub_2018)\n\nWe next cleaned both the 2017 and 2018 data using our clean_table and clean_table_2018 functions.\n\n# Apply clean table function to both years data\nasian_sub_2017 &lt;- clean_table(asian_sub_2017)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [17, 22,\n28].\n\nasian_sub_2018 &lt;- clean_table_2018(asian_sub_2018)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 6 rows [4, 8, 15, 19, 21,\n23].\n\n\nThe imported data was missing part of the data, so we manually added it back in as well as created a Year column.\n\n# Add missing data\nasian_sub_2018 &lt;- asian_sub_2018 |&gt;\n  add_row(Group = \"Asian\", Percent = 6.2,\n          Race_Ethnicity = \"Asian\", Gender = \"All\") |&gt;\n  add_row(Group = \"Asian\", Percent = 6.4,\n          Race_Ethnicity = \"Asian\", Gender = \"Men\") |&gt;\n  add_row(Group = \"Asian\", Percent = 6.1,\n          Race_Ethnicity = \"Asian\", Gender = \"Women\")\n\n# Add years to data\nasian_sub_2017 &lt;- asian_sub_2017 |&gt;\n  mutate(Year = 2017)\nasian_sub_2018 &lt;- asian_sub_2018 |&gt;\n  mutate(Year = 2018)\n\nSince the naming convention for males and females different across the 2017 and 2018 data, we standardized the Gender column to only contain Male and Female entries and then combined the data into one data set, and once again moved the years into rows and added a Percent column, just like our major group data.\n\n# Make Male/Female the values for gender across both datasets\nasian_sub_2018 &lt;- asian_sub_2018 |&gt; \n  mutate(across(.cols = c(Gender, Group),\n               ~ str_replace(string = ., \n                             pattern = \"Men\", \n                             replacement = \"Male\")),\n         across(.cols = c(Gender, Group),\n               ~ str_replace(string = ., \n                             pattern = \"Women\", \n                             replacement = \"Female\")))\n\n# Combine 2017 and 2018 data\nasian_subgroups &lt;- bind_rows(asian_sub_2017, asian_sub_2018)\n\n# Add missing categories\nasian_subgroups &lt;- asian_subgroups |&gt; \n  select(-Group) |&gt;\n  pivot_wider(names_from = Year, \n              values_from = Percent) |&gt;\n  pivot_longer(cols = -c(Race_Ethnicity, Gender),\n               names_to = \"Year\",\n               values_to= \"Percent\")\n\n\n\nLatinx Subgroup Data\nTo wrangle our Latinx subgroup data, we followed the same steps as our Asian subgroup data – applying make_rows, clean_table, and clean_table_2018 functions, standardizing the Gender naming convention, combining 2017 and 2018 data, and moving the years into rows while adding a Percent column. For the Latinx data, we also appropriately labeled the Puerto Rican, Dominican, Cuban groups and cleaned the names for Latino/Latina group.\n\n## Latinx subgroup data\n# 2017 data\n# Combine data\nlatinx_sub_2017 &lt;- stringr::str_c(latinx_sub_2017_A,\n                                  latinx_sub_2017_B, \n                                  latinx_sub_2017_C)\n\n# Fix typo\nlatinx_sub_2017 &lt;- latinx_sub_2017 |&gt;\n  str_replace(pattern = \"DR, Cuban Female 15.7\\nPR\", # Identify typo\n              replacement = \"DR, Cuban Male 15.7\\nPR\") # Replace gender\n\n# Apply functions to Latinx data\nlatinx_sub_2017 &lt;- make_rows(latinx_sub_2017)\nlatinx_sub_2017 &lt;- clean_table(table = latinx_sub_2017)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [19].\n\n# 2018 data\n# Clean the data string\nlatinx_sub_2018 &lt;- str_replace_all(string = latinx_sub_2018, \n                                  pattern = \"\\\\s:\\n{2}|\\n{2}\", #remove two newline characters\n                                  replacement = \" \")\n\n# Apply make_rows function\nlatinx_sub_2018 &lt;- make_rows(latinx_sub_2018)\n\n# Apply clean table function\nlatinx_sub_2018 &lt;- clean_table_2018(latinx_sub_2018)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [12].\n\n# Create function to fix naming issues\nfix_latinx_naming &lt;- function(table){\n  table |&gt;\n    # Appropriately label Puerto Rican, Dominican, Cuban group\n  mutate(Group = str_replace(string = Group,\n                             pattern = \"Pr, Dr, Cuban\",\n                             replacement = \"Puerto Rican, Dominican, Cuban\"),\n          Race_Ethnicity = str_replace(string = Race_Ethnicity,\n                                       pattern = \"Pr, Dr, Cuban\",\n                                       replacement = \"Puerto Rican, Dominican, Cuban\"))\n}\n\n# Apply function to both data sets\nlatinx_sub_2017 &lt;- fix_latinx_naming(latinx_sub_2017)\nlatinx_sub_2018 &lt;- fix_latinx_naming(latinx_sub_2018)\n\n# Add missing data\nlatinx_sub_2018 &lt;- latinx_sub_2018 |&gt;\n  add_row(Group = \"Latinx\", Percent = 12.8,\n          Race_Ethnicity = \"Latinx\", Gender = \"All\") |&gt;\n  add_row(Group = \"Latinx\", Percent = 12.3,\n          Race_Ethnicity = \"Latinx\", Gender = \"Men\") |&gt;\n  add_row(Group = \"Latinx\", Percent = 13.3,\n          Race_Ethnicity = \"Latinx\", Gender = \"Women\")\n\n# Make Male/Female the values for gender across both datasets\nlatinx_sub_2018 &lt;- latinx_sub_2018 |&gt;\n  mutate(across(.cols = c(Gender, Group),\n                ~ str_replace(string = ., pattern = \"Men\", replacement = \"Male\")),\n         across(.cols = c(Gender, Group),\n                ~ str_replace(string = ., pattern = \"Women\", replacement = \"Female\")))\n\n# Add years to data\nlatinx_sub_2017 &lt;- latinx_sub_2017 |&gt;\n  mutate(Year = 2017)\nlatinx_sub_2018 &lt;- latinx_sub_2018 |&gt;\n  mutate(Year = 2018)\n\n# Combine 2017 and 2018 data\nlatinx_subgroups &lt;- bind_rows(latinx_sub_2017, latinx_sub_2018)\n\n# Add missing categories\nlatinx_subgroups &lt;- latinx_subgroups |&gt;\n  select(-Group) |&gt;\n  pivot_wider(names_from = Year,\n              values_from = Percent) |&gt;\n  pivot_longer(cols = -c(Race_Ethnicity, Gender),\n               names_to =\"Year\" ,\n               values_to=\"Percent\")\n\n# Clean up Latinx group names\nlatinx_subgroups &lt;- latinx_subgroups |&gt;\n  # Convert Latino/Latina to Latinx\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latina\", \"Latinx\")) |&gt;\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latino\", \"Latinx\")) |&gt;\n  drop_na()\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\nProf Note\n\n\n\nNote that plots are clear, have titles, it’s clear what’s plotted, and there’s a corresponding interpretation guiding the reader. This is all good! These could be improved by having more informative titles that make clear to the viewer what the take-home message is and by handling years as factors.\n\n\nTo get a glance of our overall data set, we plotted overall youth disconnection over time in the United States.\n\nmajor_groups_long |&gt;\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |&gt;\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n  labs(\n    title = \"Youth Disconnection over Time\",\n    x = \"Year\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nWe are able to visualize that from 2008 to 2010 there was an increase in youth disconnect; however, after 2010, there seems to be a consistent downtrend in youth disconnect.\nTo account for differences in gender, we plotted overall youth disconnection over time by gender.\n\nmajor_groups_long |&gt;\n  filter(Gender != \"All\", Race_Ethnicity == \"All_races\") |&gt;\n  ggplot(aes(x = Year, y = Percent, color = Gender)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n    scale_color_viridis_d() +\n  labs(\n    title = \"Youth Disconnection over Time by Gender\"\n  )\n\n\n\n\n\n\n\n\nInitially in 2008, females faced more disconnect than males. However, throughout the next years, males consistently faced greater disconnect that females. Both male and female disconnect trended downwards 2010 and after.\nWe then separated major groups by race/ethnicity and visualized youth disconnect over time.\n\nmajor_groups_long |&gt;\n  filter(Gender == \"All\", Group != \"United States\") |&gt;\n  ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\"\n  )\n\n\n\n\n\n\n\n\nWe can see that Native Americans faced the most youth disconnect at about 25% and Asian faced the least with about 7.5% throughout all years in our data set. Overall, there is a small downtrend throughout the pass years in all races/ethnicities 2010 and after.\nWe visualized male/female youth disconnection by race/ethnicity across the observed years.\n\nmajor_groups_long |&gt;\n  filter(Gender != \"All\") |&gt; # Filter for only Male and Female\n  # Combine Latino and Latina into Latinx\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latina \", \"Latinx\")) |&gt;\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latino \", \"Latinx\")) |&gt;\n  #renaming all races column\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"All_races\", \"All Races\")) |&gt;\n  # Plot scatter line plot of Percent vs Year by Gender and Ethnicity\n  ggplot(aes(x = Year, y = Percent, color = Gender)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n  facet_wrap(~Race_Ethnicity, nrow = 2) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Youth Disconnection over Time by Race/Ethnicity and Gender\"\n  )\n\n\n\n\n\n\n\n\nGender does not seem to affect the percentage much except in Black, Latinx, and Native American groups where there is a marginal difference. However, the main indicator in determining youth disconnection seems to be stronger aligned to Race/Ethnicity as that is where we see the largest disparity, while gender has smaller differences within a group.\nTaking a deeper dive into the Latinx group, we visualized youth disconnection of the different subgroups within the Latinx group.\n\nlatinx_subgroups |&gt;\n  filter(Gender == \"All\", Race_Ethnicity != \"Latinx\") |&gt; # Filter by overall not specific gender\n  # Plot Percent vs Year by Ethnicity\n  ggplot(aes(x = as.numeric(Year), y = Percent, color = Race_Ethnicity)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection of Latinx Subgroups over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\",\n    x = \"Year\"\n  )\n\n\n\n\n\n\n\n\nMost groups have a relatively similar youth disconnection percentage, however, the South American group clearly has the lowest rate of youth disconnection at around 8% while Puerto Rico, Cuba, Dominican Republic have the highest with around 14.5% across 2017 and 2018.\nTaking a deeper dive into the Asian group, we visualized youth disconnection of the different subgroups within the Asian group.\n\nasian_subgroups |&gt;\n  filter(Gender == \"All\", Race_Ethnicity != \"All_races\") |&gt; # Filter by overall not specific gender\n  # Plot Percent vs Year by Ethnicity\n  ggplot(aes(x = as.numeric(Year), y = Percent, color = Race_Ethnicity)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection of Asian Subgroups over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\",\n    x = \"Year\"\n  )\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nMost groups are in the 4-7% range for youth disconnection percentage; however, China has the lowest at about 4% and Hmong and Cambodia have the highest at about 13% across 2017 and 2018.\n\n\nData Analysis\nWe graphed youth disconnection over time with a line of best-fit layered on top of it.\n\nmajor_groups_long |&gt;\n  # Scatterplot with linear model for overall youth disconnection over time\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |&gt;\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\") + \n    labs(\n    title = \"Youth Disconnection over Time\",\n    subtitle = \"Includes linear model and standard error\",\n    x = \"Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe line of best-fit exhibits a slight negative relationship, showing there is some correlation with youth disconnection and time; however, there is a decent amount of standard error throughout, which may be due to the 2008 data point defying the trend of the other data points.\nTo test our hypothesis that youth disconnection does not have a decreasing trend over time, we did a Mann-Kendall test for monotonicity.\n\n# M-K test\nmajor_groups_long |&gt;\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |&gt;\n  pull(Percent) |&gt;\n  MannKendall() |&gt;\n  tidy()\n\n# A tibble: 1 × 5\n  statistic p.value kendall_score denominator var_kendall_score\n      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1    -0.600   0.133            -9        15.0              28.3\n\n\nWe see there the p-value is 13.3% percent, which is not statistically significant at the traditional 5% level. This means our data is likely not monotonic and we would fail to reject our null hypothesis that youth disconnection is decreasing over time.\nKnowing that the Great Recession occurred in 2008 and could provide an anomaly in our data, we ran a linear model on the data for 2010 and after.\n\n# Scatterplot with linear model for overall youth disconnection over time after 2010\nmajor_groups_long |&gt;\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\", Year != 2008) |&gt;\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Youth Disconnection over Time (2010 and after)\",\n    subtitle = \"Includes linear model and standard error\",\n    x = \"Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis graph seems to show a stronger linear relationship between youth disconnection over time as there is a steeper slope and less standard error than our first linear model visualization.\nAgain, to test our hypothesis that youth disconnection does not have a decreasing trend over time for 2010 and after, we did another Mann-Kendall test.\n\n# M-K test for data after 2008\nmajor_groups_long |&gt;\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\", Year != 2008) |&gt;\n  pull(Percent) |&gt;\n  MannKendall() |&gt;\n  tidy()\n\n# A tibble: 1 × 5\n  statistic p.value kendall_score denominator var_kendall_score\n      &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n1        -1  0.0275           -10          10              16.7\n\n\nThis time, the p-value is 2.75% and significant at the 5% level. Thus, this data set is likely monotonic and we would reject the null hypothesis that there is no decreasing trend between youth disconnection and time.\n\n\n\n\n\n\nProf Note\n\n\n\nModel is not fully quantified nor explained in the context of the data. Be sure that if you’re stating numbers, you’re contextualizing them. Be sure if you’re stating things that are true, that you’re quantifying them when possible.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS: Example"
    ]
  },
  {
    "objectID": "content/cs/cs-example.html#resultsdiscussion",
    "href": "content/cs/cs-example.html#resultsdiscussion",
    "title": "CS: Example",
    "section": "Results/Discussion",
    "text": "Results/Discussion\nThrough our exploratory data analysis, we observed that there does seem to be a decreasing trend of youth disconnection in the United States over the time frame of our data with the 2008 data being a slight anomaly. The decreasing trend with the exception of the initial 2010 spike remained generally consistent regardless of separation by gender, race/ethnicity, and both as seen through out first four visualizations. Although the trend is fairly consistent, there are additional takeaways from our visualizations.\nRace/ethnicity is a major determinant in the magnitude of youth disconnection rates while gender is a much weaker determinant. Through our third visualization we see that the Native American group has a youth disconnection percentage of about 25%, which is over three time the lowest group, Asian, at about 7.5%. The Black group, although not as high as Native American, has a youth disconnection rate higher than the overall average across the years at about 20% versus 13%. Additionally, every group’s rate is 2-5% different from the nearest group with no two groups being similar to each other. This leads us to believe that although a decreasing trend is apparent across all groups, groups still have differences in how disconnected their youth are. Gender, on the other hand, does not show the same disparity. Our second visualization shows that males may be more disconnected, but when broken down into race/ethnicity in our fourth visualization we see the effect of gender depends greatly on the race/ethnic group. Females are more disconnected in Latinx and Asian groups, while the youth disconnection rate is fairly similar across gender overall and in the Asian and White group. The Black group has the most interesting data where males are fairly more disconnected than females but even in other groups such as Latinx and Native American where there is a difference in female and male disconnection, it is not a major difference.\nIn taking a deeper dive into the Latinx and Asian subgroups in visualizations five and six, it can be seen that there are not big differences in youth disconnection between subgroups except in two cases. The South American subgroup has a substantially lower rate at about 8% versus the 12-15% range that the other Latinx subgroups are in. Oppositely, Hmong and Cambodia groups have a much higher youth disconnection rate at about 13% versus its Asian peers in the 4-7% range.\nTo quantify our observations in our exploratory data analysis, we created a linear model for our major group data and performed a Mann-Kendall test. The linear model shows a decreasing trend in youth disconnection rate over time but it does not seem to be a very strong relationship and has relatively high standard error. When performing the Mann-Kendall test, the p-value was 13.3%, which is not statistically significant at the 5% level, implying that there is not a monotonic trend in the data. We decided to create a second linear model and Mann-Kendall test without the 2008 data since youth disconnection likely increased in 2010 due to the Great Recession that the United States faced. When doing this, a much stronger decreasing trend can be seen in the linear model and the Mann-Kendall test has a p-value of 2.75%, which is statistically significant at the 5%, implying that there is a monotonic trend in the data. In this case, a decreasing trend over time.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS: Example"
    ]
  },
  {
    "objectID": "content/cs/cs-example.html#conclusion",
    "href": "content/cs/cs-example.html#conclusion",
    "title": "CS: Example",
    "section": "Conclusion",
    "text": "Conclusion\nWhen taking account our entire data set, we cannot mathematically state that youth disconnection rates have changed over time, although visually and when excluding 2008 data, we can see that youth disconnection has been trending downwards starting in 2010 and after. This trend remains consistent across gender and race/ethnicity; however, there are still differences between groups in how disconnected are youth. Native American and Black are the most disconnected groups with Native American clearly have the highest youth disconnection rate. Although gender is not a major factor in determining youth disconnection in most cases, Black males are notably more disconnected than Black females. All other Latinx groups are more disconnected than the South American group and Hmong and Combodian groups are notably more disconnected than the other Asian groups. In conclusion, youth disconnection rates are experiencing a downward trend in 2010 and after and its magnitude is most heavily influenced by race/ethnicity in most cases.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS: Example"
    ]
  },
  {
    "objectID": "content/hw/hw-03.html",
    "href": "content/hw/hw-03.html",
    "title": "HW 03 - Bike rentals in DC",
    "section": "",
    "text": "Bike sharing systems take traditional bike rentals but automate the entire process (membership, rental, return, etc.). Through these systems, users are able to easily rent a bike from a particular position and return it at another position. There are hundreds of bike-sharing programs around the world comprising hundreds of thousands of bicycles. Today, there exists great interest in these (and other “alternative” transit) systems due to their important role in traffic, environmental, and health issues.\nApart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.\nSource: UCI Machine Learning Repository - Bike Sharing Dataset",
    "crumbs": [
      "Home",
      "Homework",
      "HW03"
    ]
  },
  {
    "objectID": "content/hw/hw-03.html#getting-started",
    "href": "content/hw/hw-03.html#getting-started",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nStart with an assignment link that creates the GitHub repo with starter documents (link on Canvas).\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically commit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\nThis assignment will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading. (This means even if you made mistakes before that submission on GitHub, you won’t be penalized for them, so long as the final state of your work is correct).",
    "crumbs": [
      "Home",
      "Homework",
      "HW03"
    ]
  },
  {
    "objectID": "content/hw/hw-03.html#data",
    "href": "content/hw/hw-03.html#data",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Data",
    "text": "Data\nThe data include daily bike rental counts (by members and casual users) of Capital Bikeshare in Washington, DC in 2011 and 2012 as well as weather information on these days.\nThe original data sources are http://capitalbikeshare.com/system-data and http://www.freemeteo.com.\nThe codebook is below:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\ninstant\nrecord index\n\n\ndteday\ndate\n\n\nseason\nseason (1:winter, 2:spring, 3:summer, 4:fall)\n\n\nyr\nyear (0: 2011, 1:2012)\n\n\nmnth\nmonth (1 to 12)\n\n\nholiday\nweather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n\n\nweekday\nday of the week\n\n\nworkingday\nif day is neither weekend nor holiday is 1, otherwise is 0.\n\n\nweathersit\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\n\n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\n\n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n\n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n\ntemp\nNormalized temperature in Celsius. The values are divided by 41 (max)\n\n\natemp\nNormalized feeling temperature in Celsius. The values are divided by 50 (max)\n\n\nhum\nNormalized humidity. The values are divided by 100 (max)\n\n\nwindspeed\nNormalized wind speed. The values are divided by 67 (max)\n\n\ncasual\nCount of casual users\n\n\nregistered\nCount of registered users\n\n\ncnt\nCount of total rental bikes including both casual and registered",
    "crumbs": [
      "Home",
      "Homework",
      "HW03"
    ]
  },
  {
    "objectID": "content/hw/hw-03.html#setup",
    "href": "content/hw/hw-03.html#setup",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Setup",
    "text": "Setup\nYou are free to utilize any packages in this homework. We think you’ll likely use tidyverse, tidymodels (…and maybe olsrr)",
    "crumbs": [
      "Home",
      "Homework",
      "HW03"
    ]
  },
  {
    "objectID": "content/hw/hw-03.html#questions",
    "href": "content/hw/hw-03.html#questions",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Questions",
    "text": "Questions\n\nData wrangling\n\nQuestion 1\nRecode the season variable to be a factor with meaningful level names as outlined in the codebook, with spring as the baseline level.\n\n\nQuestion 2\nRecode the binary variables holiday and workingday to be factors with levels no (0) and yes (1), with no as the baseline level.\n\n\nQuestion 3\nRecode the yr variable to be a factor with levels 2011 and 2012, with 2011 as the baseline level.\n\n\nQuestion 4\nRecode the weathersit variable as 1 - clear, 2 - mist, 3 - light precipitation, and 4 - heavy precipitation, with clear as the baseline.\n\n\nQuestion 5\nCalculate raw temperature, feeling temperature, humidity, and windspeed as their values given in the dataset multiplied by the maximum raw values stated in the codebook for each variable. Instead of writing over the existing variables, create new ones with concise but informative names.\n\n\nQuestion 6\nCheck that the sum of casual and registered adds up to cnt for each record.\n\n\n\nExploratory data analysis\n\nQuestion 7\nRecreate the following visualization, and interpret it in context of the data. Hint: You will need to use one of the variables you created above.\n\n\n\nQuestion 8\nCreate a visualization displaying the relationship between bike rentals and season. Interpret the plot in context of the data.\n\n\n\nModelling\n\nQuestion 9\nFit a linear model predicting total daily bike rentals from daily temperature. Write the linear model, interpret the slope and the intercept in context of the data, and determine and interpret the \\(R^2\\).\n\n\nQuestion 10\nFit another linear model predicting total daily bike rentals from daily feeling temperature. Write the linear model, interpret the slope and the intercept in context of the data, and determine and interpret the \\(R^2\\). Is temperature or feeling temperature a better predictor of bike rentals?\n\n\nQuestion 11\nFit a full model predicting total daily bike rentals from season, year, whether the day is holiday or not, whether the day is a workingday or not, the weather category, temperature, feeling temperature, humidity, and windspeed, as well as the interaction between at least one numerical and one categorical variable.\n\n\nQuestion 12\nPerform backward selection using adjusted \\(R^2\\) as the decision criterion to find the “best” model. Provide the model output for the final model.\n\n\nQuestion 13\nInterpret slope coefficients associated with two of the variables in your final model in context of the data. Note: If one of these is categorical with multiple levels, make sure you interpret all of the slope coefficients associated with the levels of the variable.\n\n\nQuestion 14\nBased on the final model you found in the previous question, discuss what makes for a good day to bike in DC (as measured by rental bikes being more in demand).",
    "crumbs": [
      "Home",
      "Homework",
      "HW03"
    ]
  },
  {
    "objectID": "content/hw/hw-03.html#submission",
    "href": "content/hw/hw-03.html#submission",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Submission",
    "text": "Submission\nBe sure to knit your file to HTML, look at the output HTML file to make sure everything looks as you expected, and then commit and push your final changes to GitHub. We will be grading from the HTML file. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo.",
    "crumbs": [
      "Home",
      "Homework",
      "HW03"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html",
    "href": "content/labs/01-lab-intro-r.html",
    "title": "Lab 01 - Tooling",
    "section": "",
    "text": "The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course to learn and practice programming and analyze data.\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\nAn additional goal is to introduce you to git and GitHub, which is the collaboration and version control system that we will be using throughout the course.\n\n\ngit is a version control system (like “Track Changes” features from Microsoft Word on steroids) and GitHub is the home for your Git-based projects on the internet (like DropBox but much, much better).\nAs the labs progress, you are encouraged to explore beyond what the labs say directly; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nYou are encouraged to ask one another questions and work together, but each individual must turn in their own lab each week.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#github-housekeeping",
    "href": "content/labs/01-lab-intro-r.html#github-housekeeping",
    "title": "Lab 01 - Tooling",
    "section": "GitHub Housekeeping",
    "text": "GitHub Housekeeping\n\n\n\n\n\n\nNote\n\n\n\nYour email address is the address tied to your GitHub account and your name should be first and last name.\n\n\nBefore we can get started we need to take care of some required housekeeping (if you didn’t complete it during class). Specifically, we need to configure your git so that RStudio can communicate with GitHub…and so you do not have to type in your username and password every time you want to communicate with GitHub from RStudio. These steps will be demo-ed during lab and you’ll have time to walk through the steps!\n\n\n\n\n\n\nNote\n\n\n\nIf you aren’t able to attend lab and get stuck here, there is a podcast recording from lecture on 9/29 from the COGS 137 Fa21 iteration where you can see these steps demo-ed.\n\n\n\nStep 1: Email and Username\nThe first step requires two pieces of information: your email address and your name.\nTo do so, follow these steps:\n\nGo to the Terminal pane\nType the following two lines of code, replacing the information in the quotation marks with your info:\n\n\ngit config --global user.email \"your email\"\ngit config --global user.name \"your name\"\n\nFor example, for me these are:\n\ngit config --global user.email \"sellis@ucsd.edu\"\ngit config --global user.name \"Shannon Ellis\"\n\nTo confirm that the changes have been implemented, run the following:\n\ngit config --global user.email\ngit config --global user.name\n\n\n\nStep 2: Generate ssh key\nIn the terminal, you’ll want to generate an ssh key by typing:\n\nssh-keygen\n\nAfter hitting enter/return to execute the above, you’ll press return/enter three times to bypass specifying a location and passphrase.\n\n\nStep 3: Copy your ssh key\nFrom the terminal type:\n\ncat ~/.ssh/id_rsa.pub\n\nYou’ll want to highlight and copy the full result of this command. It will start with ssh-rsa and end with something including dsmlp toward the end.\n\n\nStep 4: Let GitHub know your key\n\nIn your browser, navigate to https://github.com/settings/keys\nClick “New SSH Key”\nSet title to DSMLP\nPaste what you copied in step 3 into the “Key” box\nClick “Add SSH Key”\n\n\n\nStep 5: Finalize\nReturn to the terminal in RStudio and run the following command:\n\nssh git@github.com\n\nYou may see a message like `The authenticity of host … can’t be established`. You can type yes and hit return/enter after doing so if you do.\nYou’ll then see a message like You've successfully authenticated, but GitHub does not provide shell access. At this point, you’re all set!\nThis will be the only time you have to do this. From here on out, you’ll be able to “communicate” with GitHub from RStudio without typing your username/password.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#cloning-the-lab",
    "href": "content/labs/01-lab-intro-r.html#cloning-the-lab",
    "title": "Lab 01 - Tooling",
    "section": "Cloning the lab",
    "text": "Cloning the lab\nEach of your assignments will begin with the following steps. You saw these once in class and, they’re outlined in detail here again. Going forward each lab will start with a “Getting started” section but details will be a bit more sparse than this. You can always refer back to this lab for a detailed list of the steps involved for getting started with an assignment.\nClick on the assignment link for this week’s lab on the Canvas homepage. You will have to Accept before proceeding. Refresh the page and follow the link to the repo created for you. This repo contains a template you can build on to complete your lab.\n\n\n\n\n\n\n\n\n\n\n\nOn this page on GitHub, click the URL provided for you. This will bring you to your copy of the repo on GitHub. Click on the green &lt;&gt; Code button to clone the repo. Be sure that SSH is selected and copy this URL.\n\n\n\n\n\n\nImportant\n\n\n\nBe sure that any time you are copying a link from GitHub under the &lt;&gt;Code button, you select and use the ‘SSH’ URL as this is what will allow you to not have to type your username and password.\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo to datahub and open RStudio. Go to File &gt; New Project… and select to create a New Project from Version Control. On the following menu, select Git.\nCopy and paste the URL of your assignment repo into the “Repository URL” dialog box:\n\n\n\n\n\n\n\n\n\n\n\nHit Create Project. Open up lab-01.Rmd and continue through this lab. Your work/answers for this lab will be submitted in that document.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#packages",
    "href": "content/labs/01-lab-intro-r.html#packages",
    "title": "Lab 01 - Tooling",
    "section": "Packages",
    "text": "Packages\nIn this lab we will work with two packages: datasauRus which contains the dataset, and tidyverse which is a collection of packages for doing data analysis in a “tidy” way.\ntidyverse has already been installed for you, so it only needs to be loaded using library. (see below)\nHowever, datasauRus has not yet been installed. Run the following in your console to install this package:\n\ninstall.packages(\"datasauRus\")\n\n\n\nNote that often packages will already be installed for you on datahub in this course, but we want you to know how to install them if you need to install one at any point.\nNote that package installation happens a single time. But, any time you want to use a package (after it’s been installed), it has to be loaded, as we do here:\n\nlibrary(tidyverse) \nlibrary(datasauRus)\n\nYou should be able to Knit your document and see the results.\nNote that the packages are also loaded with the same commands in your R Markdown document.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#yaml",
    "href": "content/labs/01-lab-intro-r.html#yaml",
    "title": "Lab 01 - Tooling",
    "section": "YAML:",
    "text": "YAML:\n\n\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “Yet Another Markup Language”. It is a human-friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\nOpen the R Markdown (Rmd) file in your project, change the author name to your name and knit the document. This will generate an HTML document.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#committing-changes",
    "href": "content/labs/01-lab-intro-r.html#committing-changes",
    "title": "Lab 01 - Tooling",
    "section": "Committing changes:",
    "text": "Committing changes:\nThen Go to the Git pane in your RStudio on Datahub.\nIf you have made changes to your Rmd file, you should see it listed here. Click on it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. Be sure to also select your HTML document. Once you’re happy with these changes, write “Update author name” in the Commit message box and hit Commit.\n\n\n\n\n\n\n\n\n\n\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the quarter progresses, we will let you make these decisions.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#pushing-changes",
    "href": "content/labs/01-lab-intro-r.html#pushing-changes",
    "title": "Lab 01 - Tooling",
    "section": "Pushing changes:",
    "text": "Pushing changes:\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only).\nIn order to push your changes to GitHub, click on Push. Go check your repo on GitHub - you’ll see your updated documents there!\n\n\n\n\n\n\nThought exercise\n\n\n\nFor which of the above steps (changing project name, making updates to the document, committing, and pushing changes) do you need to have an internet connection? Discuss with your classmates.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#data",
    "href": "content/labs/01-lab-intro-r.html#data",
    "title": "Lab 01 - Tooling",
    "section": "Data",
    "text": "Data\nThe data frame we will be working with today is called datasaurus_dozen2 and it’s in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualization is important and how summary statistics alone can be misleading. The different datasets are marked by the dataset variable.\nTo find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark before the name of an object will always bring up its help file. This command must be ran in the Console.\n\nExercise 1\nBased on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report. When you’re done, commit your changes with the commit message “Added answer for Ex 1”, and push.\nLet’s take a look at what these datasets are. To do so we can make a frequency table of the dataset variable:\n\ndatasaurus_dozen |&gt;\n  count(dataset)\n\n# A tibble: 13 × 2\n   dataset        n\n   &lt;chr&gt;      &lt;int&gt;\n 1 away         142\n 2 bullseye     142\n 3 circle       142\n 4 dino         142\n 5 dots         142\n 6 h_lines      142\n 7 high_lines   142\n 8 slant_down   142\n 9 slant_up     142\n10 star         142\n11 v_lines      142\n12 wide_lines   142\n13 x_shape      142\n\n\nThe original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice.3 In the paper, the authors simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#data-visualization-and-summary",
    "href": "content/labs/01-lab-intro-r.html#data-visualization-and-summary",
    "title": "Lab 01 - Tooling",
    "section": "Data visualization and summary",
    "text": "Data visualization and summary\n\nExercise 2\nPlot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset.\nBelow is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Rmd document and successfully knit it and view the results.\nStart with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data.\n\ndino_data &lt;- datasaurus_dozen |&gt;\n  filter(dataset == \"dino\")\n\nThere is a lot going on here, so let’s slow down and unpack it a bit.\nFirst, the pipe operator: |&gt;, takes what comes before it and sends it as the first argument to what comes after it. So here, we’re saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\".\nSecond, the assignment operator: &lt;-, assigns the name dino_data to the filtered data frame.\nNext, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data you’re visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case we want these to be points,m hence geom_point.\n\nggplot(data = dino_data, mapping = aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\nIf this seems like a lot, it is. And you will learn about the philosophy of building data visualizations in layer in detail next week. For now, follow along with the code that is provided.\nFor the second part of this exercises, we need to calculate a summary statistic: the correlation coefficient. Correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear – it’s dinosaurial!\nBut, for illustrative purposes, let’s calculate correlation coefficient between x and y.\n\n\nStart with dino_data and calculate a summary statistic that we will call r as the correlation between x and y.\n\ndino_data |&gt;\n  summarize(r = cor(x, y))\n\n# A tibble: 1 × 1\n        r\n    &lt;dbl&gt;\n1 -0.0645\n\n\nThis is a good place to pause, commit changes with the commit message “Added answer for Ex 2”, and push.\n\n\nExercise 3\nPlot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nThis is another good place to pause, commit changes with the commit message “Added answer for Ex 3”, and push.\n\n\nExercise 4\nPlot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nYou should pause again, commit changes with the commit message “Added answer for Ex 4”, and push.\n\n\nFacet by the dataset variable, placing the plots in a 3 column grid, and don’t add a legend.\n\n\nExercise 5\nFinally, let’s plot all datasets at once. In order to do this we will make use of facetting.\n\nggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset)) +\n  geom_point() +\n  facet_wrap(~ dataset, ncol = 3) +\n  theme(legend.position = \"none\")\n\nAnd we can use the group_by function to generate all the summary correlation coefficients.\n\ndatasaurus_dozen |&gt;\n  group_by(dataset) |&gt;\n  summarize(r = cor(x, y))\n\nYou’re done with the data analysis exercises, but we’d like you to do two more things:\n\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#bonus-exercises",
    "href": "content/labs/01-lab-intro-r.html#bonus-exercises",
    "title": "Lab 01 - Tooling",
    "section": "Bonus Exercises",
    "text": "Bonus Exercises\nComplete these as time permits to further your experience with, comfort in, and understanding of R Markdown documents.\n\nResize your figures\nClick on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the pop up dialogue box go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until you’re happy with the figure sizes. Note that these values get saved in the YAML.\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\nYou can also use different figure sizes for different figures. To do so click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well.\n\n\n\n\n\n\n\n\n\n\n\nChange the look of your report\nOnce again click on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the General tab of the pop up dialogue box try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until you’re happy with the look.\n\n\n\n\n\n\nNote\n\n\n\nNot sure how to use emojis on your computer? Maybe a classmate can help? Or you can ask your TA as well!",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#footnotes",
    "href": "content/labs/01-lab-intro-r.html#footnotes",
    "title": "Lab 01 - Tooling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHappy git with R by Jenny Bryan.↩︎\nIf it’s confusing that the data frame is called datasaurus_dozen when it contains 13 datasets, you’re not alone! Have you heard of a baker’s dozen?↩︎\nMatejka, Justin, and George Fitzmaurice. “Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing.” Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2017.↩︎",
    "crumbs": [
      "Home",
      "Labs",
      "Lab01: Tooling and R"
    ]
  },
  {
    "objectID": "content/labs/06-lab-cs01.html",
    "href": "content/labs/06-lab-cs01.html",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "",
    "text": "This week in class we’ve been discussing data related to Biomarkers of Recent Cannabis Use in Blood, Oral Fluid and Breath. You’ve seen the data in class, but you may not have had a chance to really work with the data yourself yet. This lab will allow you to get more comfortable with the data ahead of the associated case study report coming due. While the case study report will be submitted in groups, this will be submitted individually.The idea is that while you are still encouraged to work together during lab, you and your groupmates may come up with separate ideas. This will allow you to have more ideas when you get together and start working on the case study together.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab06: CS01"
    ]
  },
  {
    "objectID": "content/labs/06-lab-cs01.html#introduction",
    "href": "content/labs/06-lab-cs01.html#introduction",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "",
    "text": "This week in class we’ve been discussing data related to Biomarkers of Recent Cannabis Use in Blood, Oral Fluid and Breath. You’ve seen the data in class, but you may not have had a chance to really work with the data yourself yet. This lab will allow you to get more comfortable with the data ahead of the associated case study report coming due. While the case study report will be submitted in groups, this will be submitted individually.The idea is that while you are still encouraged to work together during lab, you and your groupmates may come up with separate ideas. This will allow you to have more ideas when you get together and start working on the case study together.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab06: CS01"
    ]
  },
  {
    "objectID": "content/labs/06-lab-cs01.html#part-1-exploratory-data-analysis-eda",
    "href": "content/labs/06-lab-cs01.html#part-1-exploratory-data-analysis-eda",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "Part 1: Exploratory Data Analysis (EDA)",
    "text": "Part 1: Exploratory Data Analysis (EDA)\nCreate at least two (2) visualizations that help you learn more about these data beyond what was presented in class. (This is intentionally vague. We want you to look at the data and figure out what would be most helpful to visualize from the provided data. These could be different variables than what we looked at in class. Data could be faceted. Something totally different!) These do not have to be fully polished visualizations, but it should be clear from the visualization and accompanying text what’s to be learned from the visualization.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab06: CS01"
    ]
  },
  {
    "objectID": "content/labs/06-lab-cs01.html#part-2-possible-extensions",
    "href": "content/labs/06-lab-cs01.html#part-2-possible-extensions",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "Part 2: Possible extensions?",
    "text": "Part 2: Possible extensions?\nThink about the data you have access to, the EDA/analysis presented in class, and the questions we said we’re going to address. What possible extensions to this analysis would you be interested in carrying out? This is a space for brainstorming. Include any possible thoughts you have here, even if they aren’t “good” or you aren’t sure if they are “possible.” This can be used as a jumping off point for when you start discussing analysis extensions with your group.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab06: CS01"
    ]
  },
  {
    "objectID": "content/labs/02-lab-wrangling.html",
    "href": "content/labs/02-lab-wrangling.html",
    "title": "Lab 02 - Tooling",
    "section": "",
    "text": "Introduction\nIn the first lab, you got acquainted with RMarkdown documents, knitting, code chunks, and interacting with GitHub; however, most of the required code was provided for you. In this and subsequent labs, there will be less code provided, and it will be up to you to write the requisite code.\nThe goal of this lab is to get you comfortable working with tidy datasets and using dplyr to do so.\n\n\nGetting started\nTo get started, accept the lab02 assignment (link on Canvas), clone the repo (using SSH) into RStudio on datahub. And, then you’re ready to go!\n\n\nPackages\nThe only package required for completion of this lab is tidyverse, as dplyr (which you’ll be using a lot in this lab) is one of the packages in the tidyverse. Be sure to import the tidyverse prior to completing the lab.\n\n\nData\nFor this lab, we’ll be using the storms dataset from the dplyr package, which includes data about a subset of storms from 1975. The description from this dataset states “This data is a subset of the NOAA Atlantic hurricane database best track data, https://www.nhc.noaa.gov/data/#hurdat. The data includes the positions and attributes of 198 tropical storms, measured every six hours during the lifetime of a storm.”\nRemember that you can use ?storms to look up the documentation for the dataset. Be sure to read and understand what information is stored in each variable before proceeding. The instructions below are not very guided and will require that you have read and understood the information in the dataset first.\n\n\nExercises\nFor each of the following, write code using dplyr functions to determine the answers to each of the questions. Your responses should include the code, its output, and a few words that answer the question.\nNote that the final two questions are optional. Definitely give them a try, but it’s OK if you don’t have time to figure them out!\n\nExercise 1\nHow many unique hurricanes are included in this dataset?\n\n\nExercise 2\nWhich tropical storm affected the largest area experiencing tropical storm strength winds? And, what was the maximum sustained wind speed for that storm?\n\n\nExercise 3\nAmong all storms in this dataset, in which month are storms most common? Does this depend on the status of the storm? (In other words, are hurricanes more common in certain months than tropical depressions? or tropical storms?)\n\n\nExercise 4\nYour boss asks for the name, year, and status of all category 5 storms that have happened in the 2000s. Carry out the operations that would deliver what they’re looking for.\n\n\nExercise 5\nFilter these data to only include storms that occurred during your lifetime (your code and results may differ from your classmates!). Among storms that have occurred during your lifetime, what’s the mean and median air pressure across all measurements taken?\n\n\nExercise 6\n(optional challenge) Which decade (of the storms included in the dataset) had the largest number of unique reported storms?\n\n\nExercise 7\n(optional challenge) - Among the subset of storms occurring in your lifetime, which storm lasted the longest? Include your code and explain your answer.\nYay, you’re done! Knit your file, commit all remaining changes to your .Rmd and .html files, use the commit message “Done with Lab 2! r emo::ji(\"muscle\")”, and push. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab02: Wrangling"
    ]
  },
  {
    "objectID": "content/labs/03-lab-viz.html",
    "href": "content/labs/03-lab-viz.html",
    "title": "Lab 03 - Data Visualization",
    "section": "",
    "text": "A note on expectations: For each exercise, include any relevant output (tables, summary statistics, plots) in your answer along with text to guide the reader. Place any relevant R code in a code chunk, any relevant text outside of code chunks, and hit Knit HTML.\nSome define statistics as the field that focuses on turning information into knowledge. The first step in that process is to summarize and describe raw information - the data. In this lab we explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While outside the scope of this lab, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab03: Data Viz"
    ]
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads |&gt;\n  arrange(unemployment_rate)\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\nNote that your output here likely has a whole bunch of decimal places in the unemployment variable? You likely don’t want all those values to be displayed.\nThere are two ways we can address this problem. One would be to round the unemployment_rate variable in the dataset or we can change the number of digits displayed, without touching the input data.\nBelow are instructions for how you would do both of these:\n\nRound unemployment_rate: We create a new variable with the mutate function. In this case, we’re overwriting the existing unemployment_rate variable, by rounding it to 4 decimal places.For example, the call to mutate would be: mutate(unemployment_rate = round(unemployment_rate, digits = 4))\nChange displayed number of digits without touching data: We can add an option to our R Markdown document to change the displayed number of digits in the output. To do so, add a new chunk, and set:\n\n\noptions(digits = 2)\n\nNote that the digits in options is scientific digits, and in round they are decimal places. If you’re thinking “Wouldn’t it be nice if they were consistent?”, you’re right…\nYou don’t need to do both of these; that would be redundant. The next exercise asks you to choose one.\n\nExercise 1\nWhich of these options, changing the input data or altering the number of digits displayed without touching the input data, is the better option? Explain your reasoning. Then, implement the option you chose.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab03: Data Viz"
    ]
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\n\nThe desc function specifies that we want unemployment_rate in descending order.\n\ncollege_recent_grads |&gt;\n  arrange(desc(unemployment_rate)) |&gt;\n  select(rank, major, unemployment_rate)\n\n\nExercise 2\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab03: Data Viz"
    ]
  },
  {
    "objectID": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "Lab 03 - Data Visualization",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\n\n\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nExercise 3\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\n\nExercise 4\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\nWe can also calculate summary statistics for this distribution using the summarise function. Note here that you can calculate multiple summary statistics within a single summarise call:\n\ncollege_recent_grads |&gt;\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n\n\nExercise 5\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\n\n\nExercise 6\nNow, plot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\n\nExercise 7\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Also note that we are looking for the highest statistic, so make sure if you arrange to do so in the correct direction.\n\n\nExercise 8\nWhich major category is the least popular in this sample?",
    "crumbs": [
      "Home",
      "Labs",
      "Lab03: Data Viz"
    ]
  },
  {
    "objectID": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "href": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "title": "Lab 03 - Data Visualization",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories &lt;- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads &lt;- college_recent_grads |&gt;\n  mutate(major_type = case_when(major_category %in% stem_categories ~ \"stem\",\n                                TRUE ~ \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx &lt; y\nless than\n\n\nx &gt; y\ngreater than\n\n\nx &lt;= y\nless than or equal to\n\n\nx &gt;= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads |&gt;\n  filter(\n    major_type == \"stem\",\n    median &lt; 36000\n  )\n\n\nExercise 9\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab03: Data Viz"
    ]
  },
  {
    "objectID": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "Lab 03 - Data Visualization",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nExercise 10\nCreate a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab03: Data Viz"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "content/labs/04-lab-modelling.html",
    "href": "content/labs/04-lab-modelling.html",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "",
    "text": "Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings.\n\n\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. link.\nFor this assignment you will analyze the data from this study in order to learn what goes into a positive professor evaluation.\nThe data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors.\n\n\n\n\n\n\nImportant\n\n\n\nThis lab is maybe longer than what you’ll be able to complete in an hour. We will be looking to see that you minimally ran and interpreted at least a single model (completed Part 3). If you’re able to finish the whole thing, awesome! If not, that’s OK.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab04: Modelling"
    ]
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#introduction",
    "href": "content/labs/04-lab-modelling.html#introduction",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "",
    "text": "Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings.\n\n\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. link.\nFor this assignment you will analyze the data from this study in order to learn what goes into a positive professor evaluation.\nThe data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors.\n\n\n\n\n\n\nImportant\n\n\n\nThis lab is maybe longer than what you’ll be able to complete in an hour. We will be looking to see that you minimally ran and interpreted at least a single model (completed Part 3). If you’re able to finish the whole thing, awesome! If not, that’s OK.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab04: Modelling"
    ]
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#codebook",
    "href": "content/labs/04-lab-modelling.html#codebook",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Codebook",
    "text": "Codebook\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nscore\nAverage professor evaluation score: (1) very unsatisfactory - (5) excellent\n\n\nrank\nRank of professor: teaching, tenure track, tenure\n\n\nethnicity\nEthnicity of professor: not minority, minority\n\n\ngender\nGender of professor: female, male\n\n\nlanguage\nLanguage of school where professor received education: english or non-english\n\n\nage\nAge of professor\n\n\ncls_perc_eval\nPercent of students in class who completed evaluation\n\n\ncls_did_eval\nNumber of students in class who completed evaluation\n\n\ncls_students\nTotal number of students in class\n\n\ncls_level\nClass level: lower, upper\n\n\ncls_profs\nNumber of professors teaching sections in course in sample: single, multiple\n\n\ncls_credits\nNumber of credits of class: one credit (lab, PE, etc.), multi credit\n\n\nbty_f1lower\nBeauty rating of professor from lower level female: (1) lowest - (10) highest\n\n\nbty_f1upper\nBeauty rating of professor from upper level female: (1) lowest - (10) highest\n\n\nbty_f2upper\nBeauty rating of professor from upper level female: (1) lowest - (10) highest\n\n\nbty_m1lower\nBeauty rating of professor from lower level male: (1) lowest - (10) highest\n\n\nbty_m1upper\nBeauty rating of professor from upper level male: (1) lowest - (10) highest\n\n\nbty_m2upper\nBeauty rating of professor from upper level male: (1) lowest - (10) highest",
    "crumbs": [
      "Home",
      "Labs",
      "Lab04: Modelling"
    ]
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-1-data-manipulation",
    "href": "content/labs/04-lab-modelling.html#part-1-data-manipulation",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 1: Data Manipulation",
    "text": "Part 1: Data Manipulation\n\n\nThe rowwise function is useful for applying mathematical operations to each row.\n\nExercise 1\nCreate a new variable called bty_avg that is the average attractiveness score of the six students for each professor (bty_f1lower through bty_m2upper). Add this new variable to the evals data frame. Do this in one pipe, using the rowwise function. Since rowwise is new to you, incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.\n\n___ &lt;- evals |&gt;\n  rowwise() |&gt;\n  ___(bty_avg = mean( c( ___ ) )) |&gt;\n  ungroup()\n\nNote that we end the pipeline with ungroup() to remove the effect of the rowwise function from earlier in the pipeline. The rowwise function works a lot like group_by, except it groups the data frame one row at a time so that any operations applied to the data frame is done once per each row. This is helpful for finding the mean beauty score for each row. However in the remainder of the analysis we don’t want to, say, calculate summary statistics for each row, or fit a model for each row. Hence we need to undo the effect of rowwise, which we can do with ungroup.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab04: Modelling"
    ]
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-2-exploratory-data-analysis",
    "href": "content/labs/04-lab-modelling.html#part-2-exploratory-data-analysis",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 2: Exploratory Data Analysis",
    "text": "Part 2: Exploratory Data Analysis\n\nExercise 2\nVisualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response.\n\n\nExercise 3\nVisualize and describe the relationship between score and the new variable you created, bty_avg.\n\n\nHint: See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html.\n\n\nExercise 4\nReplot the scatterplot from Exercise 3, but this time use\ngeom_jitter()? What does “jitter” mean? What was misleading about the initial scatterplot?",
    "crumbs": [
      "Home",
      "Labs",
      "Lab04: Modelling"
    ]
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-3-linear-regression-with-a-numerical-predictor",
    "href": "content/labs/04-lab-modelling.html#part-3-linear-regression-with-a-numerical-predictor",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 3: Linear regression with a numerical predictor",
    "text": "Part 3: Linear regression with a numerical predictor\n\n\nLinear model is in the form \\(\\hat{y} = b_0 + b_1 x\\).\n\nExercise 5\nLet’s see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model.\n\n\nExercise 6\nReplot your visualization from Exercise 3, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line.\n\n\nExercise 7\nInterpret the slope of the linear model in context of the data.\n\n\nExercise 8\nInterpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.\n\n\nExercise 9\nDetermine the \\(R^2\\) of the model and interpret it in context of the data.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab04: Modelling"
    ]
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-4-linear-regression-with-a-categorical-predictor",
    "href": "content/labs/04-lab-modelling.html#part-4-linear-regression-with-a-categorical-predictor",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 4: Linear regression with a categorical predictor",
    "text": "Part 4: Linear regression with a categorical predictor\n\nExercise 10\nFit a new linear model called m_gen to predict average professor evaluation score based on gender of the professor. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data.\n\n\nExercise 11\nWhat is the equation of the line corresponding to male professors? What is it for female professors?\n\n\nExercise 12\nFit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data.\n\n\nSee the course slides on using the forcats package for changing the order of levels.\n\n\nExercise 3\nCreate a new variable called rank_relevel where \"tenure track\" is the baseline level.\n\n\nExercise 14\nFit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in Exercise 13. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model.\n\n\nExercise 15\nCreate another new variable called tenure_eligible that labels \"teaching\" faculty as \"no\" and labels \"tenure track\" and \"tenured\" faculty as \"yes\".\n\n\nExercise 16\nFit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab04: Modelling"
    ]
  },
  {
    "objectID": "content/labs/05-lab-mlr.html",
    "href": "content/labs/05-lab-mlr.html",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "",
    "text": "In this lab we revisit the professor evaluations data we modeled in an earlier lab. In the modelling lab we modeled evaluation scores using a single predictor at a time. However, this time we use multiple predictors to model evaluation scores.\nIf you don’t remember the data, review the modelling lab’s introduction before continuing to the exercises.\n\n\n\n\n\n\nImportant\n\n\n\nThis lab is likely longer than what you’ll be able to complete in an hour. We will be looking to see that you minimally ran and interpreted a model using multiple linear regression (completed Part 2). If you’re able to finish the whole thing, awesome! If not, that’s OK.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab05: MLR"
    ]
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#introduction",
    "href": "content/labs/05-lab-mlr.html#introduction",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "",
    "text": "In this lab we revisit the professor evaluations data we modeled in an earlier lab. In the modelling lab we modeled evaluation scores using a single predictor at a time. However, this time we use multiple predictors to model evaluation scores.\nIf you don’t remember the data, review the modelling lab’s introduction before continuing to the exercises.\n\n\n\n\n\n\nImportant\n\n\n\nThis lab is likely longer than what you’ll be able to complete in an hour. We will be looking to see that you minimally ran and interpreted a model using multiple linear regression (completed Part 2). If you’re able to finish the whole thing, awesome! If not, that’s OK.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab05: MLR"
    ]
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-1-simple-linear-regression",
    "href": "content/labs/05-lab-mlr.html#part-1-simple-linear-regression",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 1: Simple linear regression",
    "text": "Part 1: Simple linear regression\n\nExercise 2\n[Review from linear regression lab] Fit a linear model (one you have fit before): m_bty, predicting average professor evaluation score based on average beauty rating (bty_avg) only. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\).",
    "crumbs": [
      "Home",
      "Labs",
      "Lab05: MLR"
    ]
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-2-multiple-linear-regression",
    "href": "content/labs/05-lab-mlr.html#part-2-multiple-linear-regression",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 2: Multiple linear regression",
    "text": "Part 2: Multiple linear regression\n\nExercise 3\nFit a linear model: m_bty_gen, predicting average professor evaluation score based on average beauty rating (bty_avg) and gender. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\).\n\n\nExercise 4\nInterpret the slopes and intercept of m_bty_gen in context of the data.\n\n\nExercise 5\nWhat percent of the variability in score is explained by the model m_bty_gen.\n\n\nExercise 6\nWhat is the equation of the line corresponding to just male professors?\n\n\nExercise 7\nFor two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?\n\n\nExercise 8\nHow do the adjusted \\(R^2\\) values of m_bty_gen and m_bty compare? What does this tell us about how useful gender is in explaining the variability in evaluation scores when we already have information on the beauty score of the professor.\n\n\nExercise 9\nCompare the slopes of bty_avg under the two models (m_bty and m_bty_gen). Has the addition of gender to the model changed the parameter estimate (slope) for bty_avg?\n\n\nExercise 10\nCreate a new model called m_bty_rank with gender removed and rank added in. Write the equation of the linear model and interpret the slopes and intercept in context of the data.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab05: MLR"
    ]
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-3-the-search-for-the-best-model",
    "href": "content/labs/05-lab-mlr.html#part-3-the-search-for-the-best-model",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 3: The search for the best model",
    "text": "Part 3: The search for the best model\nGoing forward, only consider the following variables as potential predictors: rank, ethnicity, gender, language, age, cls_perc_eval, cls_did_eval, cls_students, cls_level, cls_profs, cls_credits, bty_avg.\n\nExercise 11\nWhich variable, on its own, would you expect to be the worst predictor of evaluation scores? Why? Hint: Think about which variable would you expect to not have any association with the professor’s score.\n\n\nExercise 12\nCheck your suspicions from the previous exercise. Include the model output for that variable in your response.\n\n\nExercise 13\nSuppose you wanted to fit a full model with the variables listed above. If you are already going to include cls_perc_eval and cls_students, which variable should you not include as an additional predictor? Why?\n\n\nExercise 14\nFit a full model with all predictors listed above (except for the one you decided to exclude) in the previous question.\n\n\nExercise 15\nUsing backward-selection (meaning fit all predictors and remove those that are not needed in the model) with adjusted R-squared as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.\n\n\nExercise 16\nInterpret the slopes of one numerical and one categorical predictor based on your final model.\n\n\nExercise 17\nBased on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score.\n\n\nExercise 18\nWould you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not?",
    "crumbs": [
      "Home",
      "Labs",
      "Lab05: MLR"
    ]
  },
  {
    "objectID": "content/labs/07-lab-cs02.html",
    "href": "content/labs/07-lab-cs02.html",
    "title": "Lab 07 - Exploring Case Study 02",
    "section": "",
    "text": "This week in class we’ve been discussing data related to predicting annual air polution. You’ve seen the data in class and maybe have started to work with it, but now is your chance to really fully get to understand the dataset. While the case study report will be submitted in groups, this will be submitted individually.The idea is that while you are still encouraged to work together during lab, you and your groupmates may come up with separate ideas. This will allow you to have more ideas when you get together and start working on the case study together.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab07: CS02"
    ]
  },
  {
    "objectID": "content/labs/07-lab-cs02.html#introduction",
    "href": "content/labs/07-lab-cs02.html#introduction",
    "title": "Lab 07 - Exploring Case Study 02",
    "section": "",
    "text": "This week in class we’ve been discussing data related to predicting annual air polution. You’ve seen the data in class and maybe have started to work with it, but now is your chance to really fully get to understand the dataset. While the case study report will be submitted in groups, this will be submitted individually.The idea is that while you are still encouraged to work together during lab, you and your groupmates may come up with separate ideas. This will allow you to have more ideas when you get together and start working on the case study together.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab07: CS02"
    ]
  },
  {
    "objectID": "content/labs/07-lab-cs02.html#part-1-exploratory-data-analysis-eda",
    "href": "content/labs/07-lab-cs02.html#part-1-exploratory-data-analysis-eda",
    "title": "Lab 07 - Exploring Case Study 02",
    "section": "Part 1: Exploratory Data Analysis (EDA)",
    "text": "Part 1: Exploratory Data Analysis (EDA)\nCreate at least two (2) visualizations or tables that help you learn more about these data beyond what was presented in class. (This is intentionally vague. We want you to look at the data and figure out what would be most helpful to visualize from the provided data. These could be different variables than what we looked at in class. Data could be faceted. Something totally different!) These do not have to be fully polished visualizations, but it should be clear from the visualization and accompanying text what’s to be learned from the visualization.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab07: CS02"
    ]
  },
  {
    "objectID": "content/labs/07-lab-cs02.html#part-2-possible-extensions",
    "href": "content/labs/07-lab-cs02.html#part-2-possible-extensions",
    "title": "Lab 07 - Exploring Case Study 02",
    "section": "Part 2: Possible extensions?",
    "text": "Part 2: Possible extensions?\nThink about the data you have access to, the EDA/analysis presented in class, and the questions we said we’re going to address. What possible extensions to this analysis would you be interested in carrying out? This is a space for brainstorming. Include any possible thoughts you have here, even if they aren’t “good” or you aren’t sure if they are “possible.” This can be used as a jumping off point for when you start discussing analysis extensions with your group.",
    "crumbs": [
      "Home",
      "Labs",
      "Lab07: CS02"
    ]
  },
  {
    "objectID": "content/hw/hw-01.html",
    "href": "content/hw/hw-01.html",
    "title": "HW 01 - R Basics",
    "section": "",
    "text": "This assignment is meant to get you comfortable with 1) the format of homework assignments in this course and 2) writing R code in RStudio. The specific questions on this assignment will be simpler and should take you less time than those on future assignments. This assignment focuses on variables, operators, datasets, and dplyr basics.\n\nGetting started\nHere are the steps for getting started:\n\nStart by navigating to the hw01 GitHub URL (found on Canvas)\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit your file and take a look at the document generated\nCommit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\n\nYou can of course push multiple to GitHub times throughout the assignment. Your final push at the deadline will be used for grading. (This means even if you made mistakes before that on GitHub, you wouldn’t be penalized for them, so long as the final state of your work is correct).\n\n\n\n\n\n\nImportant\n\n\n\nYou’ll always want to knit your RMarkdown document to HTML and review that HTML document to ensure it includes all the information you want and looks as you intended, as we grade from the knit HTML. Both your .Rmd and .html files should be on GitHub.",
    "crumbs": [
      "Home",
      "Homework",
      "HW01"
    ]
  },
  {
    "objectID": "content/hw/hw-02.html",
    "href": "content/hw/hw-02.html",
    "title": "HW 02 - Data Visualization",
    "section": "",
    "text": "This assignment is meant to get you more comfortable with generating and customizing visualizations in R using ggplot2. The first section will guide you toward the visualizations you’re expected to generate, while the final two sections will be more open-ended. There are multiple distinct visualizations that could be totally “correct” for each question. You may make a different decision than your classmate and could both be correct.\nAlso, note that the final two parts of this assignment will take you way longer than you think they will. Definitely do not wait until the last minute to start this assignment.",
    "crumbs": [
      "Home",
      "Homework",
      "HW02"
    ]
  },
  {
    "objectID": "content/hw/hw-02.html#getting-started",
    "href": "content/hw/hw-02.html#getting-started",
    "title": "HW 02 - Data Visualization",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nStart with an assignment link that creates a repo on GitHub with starter documents (link on Canvas).\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically commit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\n\nYour final GitHub push prior to the deadline will be used for grading. (This means even if you made mistakes before that submission on GitHub, you won’t be penalized for them, so long as the final state of your work is correct).\n\nImports\nThe following packages must be imported prior to completing this homework: tidyverse and palmerpenguins (Note: If you did not install palmerpenguins during lecture, you’ll have to run install.packages(\"palmerpenguins\") prior to importing it.)\n\n\nGround Rules\nFor this assignment, all visualizations must:\n\nbe completed using ggplot2\nhave an informative title and labeled axes\nfollow good visualization practices (discussed in class)",
    "crumbs": [
      "Home",
      "Homework",
      "HW02"
    ]
  },
  {
    "objectID": "content/hw/hw-02.html#part-i-ggplot2",
    "href": "content/hw/hw-02.html#part-i-ggplot2",
    "title": "HW 02 - Data Visualization",
    "section": "Part I: ggplot2",
    "text": "Part I: ggplot2\nThis first section will continue to use the penguins dataset from the palmerpenguins package that was used during the ggplot2 lecture.\n\nQuestion 1\nGenerate a visualization that will allow readers to determine whether male or female penguins are larger (by mass).\n\n\nQuestion 2\nGenerate a plot that clearly visualizes how many penguins there are from each species on each island. Each island should be a different panel, and each chart should visualize the species count.\n\n\nQuestion 3\nGenerate a plot that will allow the viewer to determine whether flipper length has differed over the study year. Be sure to color the points on this plot by species. And, remember that year is often best handled as a factor.",
    "crumbs": [
      "Home",
      "Homework",
      "HW02"
    ]
  },
  {
    "objectID": "content/hw/hw-02.html#part-ii-imitation-is-the-highest-form-of-flattery",
    "href": "content/hw/hw-02.html#part-ii-imitation-is-the-highest-form-of-flattery",
    "title": "HW 02 - Data Visualization",
    "section": "Part II: Imitation is the highest form of flattery",
    "text": "Part II: Imitation is the highest form of flattery\nIn class we learned a handful of ways to customize visualizations. Now, it’s your turn to apply what you learned by recreating someone else’s visualization.\n\nQuestion 4\nFor this question, find a visualization somewhere on the Internet and recreate the visualization as close as you can using ggplot2. To make this easier on yourself, you’ll likely want to find a visualization where the data are readily available. (To get started, FiveThirtyEight makes a lot of the data from their articles available and has many charts in their articles. You are not required to recreate a visualization from FiveThirtyEight; however, if you’re not sure where to start, you have this option.) Your answer should include an image of the original visualization, a reference to the original image (this could simply be a URL), and your code + recreation.\nNotes:\n\nTo insert an image in an RMarkdown document, you can use the syntax ![alt text](path/to/image.png).\nThe R/ggplot2 code to create your visualization cannot already exist on the internet. (For example, choosing to recreate a plot from the R Graph Gallery would not be an option b/c all the code is already there and you wouldn’t learn as much.)\n\n\n\nQuestion 5\nBriefly explain what you learned about ggplot2 in the process of re-creating this visualization.\n\n\nQuestion 6\nExplain how your visualization differs from the original (It’s OK if your’s is not a perfect recreation!)",
    "crumbs": [
      "Home",
      "Homework",
      "HW02"
    ]
  },
  {
    "objectID": "content/hw/hw-02.html#part-iii-take-a-sad-plot-and-make-it-better",
    "href": "content/hw/hw-02.html#part-iii-take-a-sad-plot-and-make-it-better",
    "title": "HW 02 - Data Visualization",
    "section": "Part III: Take a sad plot and make it better",
    "text": "Part III: Take a sad plot and make it better\n\nQuestion 7\nThis question was inspired by Alison Hill’s talk. The idea here is that there is a lot of data all around us and a whole bunch of visualizations. Some of them are really excellent, and some could be improved. Choose a visualization you’ve created in the past OR a visualization you’ve found out in the world that could benefit from a redesign and/or significant visual improvement. (This could be the same visualization you recreated above, but for most it will likely be a totally different visualization.) Your answer should include an image of the original visualization, a reference to the original image (this could simply be a URL), and your code + improved version.\nNote: If you’re unsure where to look for visualizations that would benefit from improvement, check out Flowing Data’s Ugly Charts or Reddit’s Data is ugly. You may need to recreate/approximate the dataset (meaning store the values from the visualization in a tibble) needed to generate the visualization prior to improving the design.\n\n\nQuestion 8\nBriefly explain what you learned about ggplot2 in the process of re-creating this visualization.\n\n\nQuestion 9\nExplain why you made the design and visualization choices you did for your improved version.",
    "crumbs": [
      "Home",
      "Homework",
      "HW02"
    ]
  },
  {
    "objectID": "content/hw/hw-02.html#submission",
    "href": "content/hw/hw-02.html#submission",
    "title": "HW 02 - Data Visualization",
    "section": "Submission",
    "text": "Submission\nBe sure to knit your file to HTML, look at the output HTML file to make sure everything looks as you expected, and then commit and push your final changes to GitHub. We will be grading from the HTML file. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo.",
    "crumbs": [
      "Home",
      "Homework",
      "HW02"
    ]
  },
  {
    "objectID": "content/cs/cs01.html",
    "href": "content/cs/cs01.html",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "",
    "text": "This is where you get to put together all you’ve learned so far this quarter into a full data science report! This report will include your analysis from top (the background and question) to bottom (your analysis, interpretation, and conclusions.)\nWe’ll be grading to see that you have: 1) all necessary code for each section of the project. 2) explanatory text that guides the reader from start to finish. 3) polished visualizations that allow the reader to both understand the data you’re working with an your conclusions.\nThis will be submitted and graded as a group. One submission per group.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS01"
    ]
  },
  {
    "objectID": "content/cs/cs01.html#getting-started",
    "href": "content/cs/cs01.html#getting-started",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nThis will be completed in cs01 group repository that has been created for you and your group mates.\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit and commit changes (for example, once per each new part)1\nPush all your changes back to your GitHub repo\nThis case study will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading.\n\nImports\nYou are allowed to import whichever packages you like for this case study report.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS01"
    ]
  },
  {
    "objectID": "content/cs/cs01.html#case-study-report",
    "href": "content/cs/cs01.html#case-study-report",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "Case Study Report",
    "text": "Case Study Report\nYour case study can be organized however you see best fit, but we’ll be looking for the following general sections:\n\nTitle\nAuthors\nBackground/Introduction\nQuestion(s)\nData\n\nData Explanation\nData Import\nData Wrangling\n\nAnalysis\n\nExploratory Data Analysis\nData Analysis\n\nResults\nDiscussion of results\nConclusion\n\nNow, you may want to combine some of these sections (i.e. include your results and discussion among your analysis code). That’s totally allowed, but we’ll be looking to see that your report includes sufficient information to understand what you did, why you did it, and what your results are.\n\nExtending the Analysis\nIn addition to getting the code presented in class working, adding explanatory text to your report, and generating polished visualizations, you and your group must “extend the analysis” presented in class in a meaningful way. Now “meaningful” is not a very-easily-measured term. A meaningful extension could be carrying out analysis to answer an additional sub-question beyond what was presented in class, or including a really extensive exploratory data analysis, or generating a really superb set of visualizations to convey your groups’ results, or finding a related dataset and incorporating it into your case study. To determine whether your extension is “meaningful,” you and your group should be able to answer “yes” to the question “Does our extension add something important to this report beyond what was presented in class?”\nThis extension should be included/weaved into your report, meaning it should only be “separated out” as its own section if it makes most sense for the story you’re telling.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS01"
    ]
  },
  {
    "objectID": "content/cs/cs01.html#general-communication",
    "href": "content/cs/cs01.html#general-communication",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "General Communication",
    "text": "General Communication\nEach group will need to convey the most important finding(s) to a general audience through some form of communication.\nThis is very open-ended in its format. It could be a short video, an infographic, an effective email, a graphic, instagram slides, a short presentation, etc. It will be submitted by one group member on Canvas. (All group members will receive credit.)\nThe specific audience you want to target can be specified (i.e. police officers, undergraduate students, parents, etc.); however, the assumption is that these are NOT data scientists.\nYour communication SHOULD include your take-home message…and that may be all it includes! Basically, we want you to distill down your case study to its most important message and then convey that to the general public in an effective manner.\nIt should NOT contain specifics of your analysis or anywhere near all the information included in your report.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS01"
    ]
  },
  {
    "objectID": "content/cs/cs01.html#group-feedback",
    "href": "content/cs/cs01.html#group-feedback",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the case study to provide feedback about working with your group mates. This is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary.",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS01"
    ]
  },
  {
    "objectID": "content/cs/cs01.html#footnotes",
    "href": "content/cs/cs01.html#footnotes",
    "title": "CS01: Biomarkers of Recent Use",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAvoid waiting until the end to knit for the first time. It will be better/easier/less of a headache if you knit periodically and know it’s all working as intended.↩︎",
    "crumbs": [
      "Home",
      "Case Studies",
      "CS01"
    ]
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#agenda",
    "href": "content/labslides/04-lab-deck.html#agenda",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Agenda",
    "text": "Agenda\n\nLab 04: Modelling course evaluations\nGetting started with lab",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab04"
    ]
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#lab-06-modelling-course-evaluations",
    "href": "content/labslides/04-lab-deck.html#lab-06-modelling-course-evaluations",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Lab 06: Modelling course evaluations",
    "text": "Lab 06: Modelling course evaluations\n\nMany college courses give students the opportunity to evaluate the course and the instructor anonymously\nThe use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, e.g. the physical appearance of the instructor",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab04"
    ]
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#youve-all-seen-something-like-this",
    "href": "content/labslides/04-lab-deck.html#youve-all-seen-something-like-this",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "You’ve all seen something like this…",
    "text": "You’ve all seen something like this…",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab04"
    ]
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#and-then-theres-also-this",
    "href": "content/labslides/04-lab-deck.html#and-then-theres-also-this",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "and then there’s also this",
    "text": "and then there’s also this",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab04"
    ]
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#data-comes-from",
    "href": "content/labslides/04-lab-deck.html#data-comes-from",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Data comes from…",
    "text": "Data comes from…\n“Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity”\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005. http://www.sciencedirect.com/science/article/pii/S0272775704001165",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab04"
    ]
  },
  {
    "objectID": "content/labslides/04-lab-deck.html#some-new-challenges-in-this-lab",
    "href": "content/labslides/04-lab-deck.html#some-new-challenges-in-this-lab",
    "title": "Lab 04: Modelling course evals (Pt. 1)",
    "section": "Some new challenges in this lab",
    "text": "Some new challenges in this lab\n\nThere isn’t much code on the lab instructions, you might need to refer to course slides to put the pieces together, however most of the time\n\nyou’ll be visualizing with ggplot,\nfitting a model with lm,\nand viewing some model statistics with glance\n\nInterpretation in the context of the data matters\n\nNumbers need context: saying (for example) “the \\(\\beta_1\\) is 5” isn’t enough; what does that mean? In particular, what does that mean for these data and for this question\n\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab04"
    ]
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#reminders",
    "href": "content/labslides/02-lab-deck.html#reminders",
    "title": "Lab 02: Wrangling",
    "section": "Reminders",
    "text": "Reminders\n\nStart with library(tidyverse) (includes tidyr, readr, dplyr, etc.)\nClone using ‘SSH’ link from GitHub\nKnit to .html & push both .Rmd and .html to GitHub",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab02"
    ]
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#starting-a-new-project",
    "href": "content/labslides/02-lab-deck.html#starting-a-new-project",
    "title": "Lab 02: Wrangling",
    "section": "Starting a new project",
    "text": "Starting a new project\n\nGo to Canvas to find the link for today’s lab: lab03-wi23.\nOn GitHub, click on the green Clone or download button, select use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nGo to RStudio on datahub. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option.\nCopy and paste the URL of your assignment repo into the dialog box and hit OK.\nOpen the .Rmd file with your template in it. Be sure to update the author to your name.",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab02"
    ]
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#agenda",
    "href": "content/labslides/02-lab-deck.html#agenda",
    "title": "Lab 02: Wrangling",
    "section": "Agenda",
    "text": "Agenda\n\nLab 02 intro and demos: Introduce the lab, and work through the first question as a class.\nOn your own: Work on the rest of the lab “on your own”, but feel free to check in with classmates as much as you like.",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab02"
    ]
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#dplyr-review",
    "href": "content/labslides/02-lab-deck.html#dplyr-review",
    "title": "Lab 02: Wrangling",
    "section": "dplyr: Review",
    "text": "dplyr: Review\ndplyr provides a “Grammar of Data Manipulation” and is based on the concepts of functions as verbs that manipulate data frames.\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab02"
    ]
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#the-data",
    "href": "content/labslides/02-lab-deck.html#the-data",
    "title": "Lab 02: Wrangling",
    "section": "The Data",
    "text": "The Data\n\nstorms |&gt;\n  slice(1:20)\n\n# A tibble: 20 × 13\n   name   year month   day  hour   lat  long status      category  wind pressure\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n 1 Amy    1975     6    27     0  27.5 -79   tropical d…       NA    25     1013\n 2 Amy    1975     6    27     6  28.5 -79   tropical d…       NA    25     1013\n 3 Amy    1975     6    27    12  29.5 -79   tropical d…       NA    25     1013\n 4 Amy    1975     6    27    18  30.5 -79   tropical d…       NA    25     1013\n 5 Amy    1975     6    28     0  31.5 -78.8 tropical d…       NA    25     1012\n 6 Amy    1975     6    28     6  32.4 -78.7 tropical d…       NA    25     1012\n 7 Amy    1975     6    28    12  33.3 -78   tropical d…       NA    25     1011\n 8 Amy    1975     6    28    18  34   -77   tropical d…       NA    30     1006\n 9 Amy    1975     6    29     0  34.4 -75.8 tropical s…       NA    35     1004\n10 Amy    1975     6    29     6  34   -74.8 tropical s…       NA    40     1002\n11 Amy    1975     6    29    12  33.8 -73.8 tropical s…       NA    45     1000\n12 Amy    1975     6    29    18  33.8 -72.8 tropical s…       NA    50      998\n13 Amy    1975     6    30     0  34.3 -71.6 tropical s…       NA    50      998\n14 Amy    1975     6    30     6  35.6 -70.8 tropical s…       NA    55      998\n15 Amy    1975     6    30    12  35.9 -70.5 tropical s…       NA    60      987\n16 Amy    1975     6    30    18  36.2 -70.2 tropical s…       NA    60      987\n17 Amy    1975     7     1     0  36.2 -69.8 tropical s…       NA    60      984\n18 Amy    1975     7     1     6  36.2 -69.4 tropical s…       NA    60      984\n19 Amy    1975     7     1    12  36.2 -68.3 tropical s…       NA    60      984\n20 Amy    1975     7     1    18  36.7 -67.2 tropical s…       NA    60      984\n# ℹ 2 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n#   hurricane_force_diameter &lt;int&gt;",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab02"
    ]
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#the-data-documentation",
    "href": "content/labslides/02-lab-deck.html#the-data-documentation",
    "title": "Lab 02: Wrangling",
    "section": "The Data: Documentation",
    "text": "The Data: Documentation\nFrom the console…\n\n?storms\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lab Slides",
      "Lab02"
    ]
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#agenda",
    "href": "content/labslides/07-lab-deck.html#agenda",
    "title": "Lab 07: Logistic Regression",
    "section": "Agenda",
    "text": "Agenda\n\nLab 07: Modelling resumes\nGetting started with lab"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#data-come-from",
    "href": "content/labslides/07-lab-deck.html#data-come-from",
    "title": "Lab 07: Logistic Regression",
    "section": "Data come from…",
    "text": "Data come from…\n“Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.”\nBertrand, Marianne, and Sendhil Mullainathan. 2003. https://doi.org/10.3386/w9873."
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#study",
    "href": "content/labslides/07-lab-deck.html#study",
    "title": "Lab 07: Logistic Regression",
    "section": "Study",
    "text": "Study\nGoal: understand the influence of race and gender on job application callback rates\n\nSpecs:\n\nTwo cities: Boston and Chicago\nTime: several months in 2001 and 2002\n\n. . .\nPlan:\n\nResearchers generated resumes, randomizing years of experience and education details\nThen: randomly assigned a name to the resume that would communicate the applicant’s gender and race\n\n\nthey tested these names and removed those that did not suggest gender and race consistently\ni.e. Lakisha was a name that their survey indicated would be interpreted as a black woman, while Greg was a name that would generally be interpreted to be associated with a white male.”"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#backwards-elimination-logistic-regression",
    "href": "content/labslides/07-lab-deck.html#backwards-elimination-logistic-regression",
    "title": "Lab 07: Logistic Regression",
    "section": "Backwards elimination (Logistic Regression)",
    "text": "Backwards elimination (Logistic Regression)\n\nStart with full model (including all candidate explanatory variables and all candidate interactions)\nRemove one variable at a time, and select the model with the lowest AIC\nContinue until AIC does not decrease\n\n\n\nYou do NOT have to include every model in your lab…just the final one you settle on."
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#a-note-on-this-lab",
    "href": "content/labslides/07-lab-deck.html#a-note-on-this-lab",
    "title": "Lab 07: Logistic Regression",
    "section": "A note on this lab",
    "text": "A note on this lab\n\nThere are three parts. We’ll be grading to see that you’ve done some EDA and have fit and interpreted at least two models (single and multiple predictors model)\nIt’s OK if you don’t get to backwards elimination\n\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html",
    "href": "content/exams/practice-exam-fa21-ans.html",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#rules",
    "href": "content/exams/practice-exam-fa21-ans.html#rules",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#academic-integrity-statement",
    "href": "content/exams/practice-exam-fa21-ans.html#academic-integrity-statement",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nI, ____________, hereby state that I have not communicated with or gained information in any way from my classmates or anyone during this exam, and that all work is my own.\nA note on sharing / reusing code: We are well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#getting-help",
    "href": "content/exams/practice-exam-fa21-ans.html#getting-help",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Getting help",
    "text": "Getting help\nBecause we cannot be available to all students across the entire length of the (real) exam, there will be no questions of instructional staff about the exam. If you find wording confusing or are unsure, note that in your answer and explain how you interpreted it. This will be taken into consideration during grading. If you are having technical difficulties or think there is an error on the exam, DM or email Prof Ellis immediately and she’ll work with you as soon as possible.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#grading-and-feedback",
    "href": "content/exams/practice-exam-fa21-ans.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThe (real) exam is worth 16% of your grade. You will be graded on the correctness of your code, correctness of your answers (often there are multiple “correct” answers, by design), the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization, but we should be able to easily navigate your midterm to find what we’re looking for.)",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#logistics",
    "href": "content/exams/practice-exam-fa21-ans.html#logistics",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#packages",
    "href": "content/exams/practice-exam-fa21-ans.html#packages",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse package for this (practice) midterm. (For the real deal, you’ll need tidyverse and tidymodels.) If working on datahub, this package has been installed, but you will need to load it. No other packages are required, but if for some reason you want to load in another package, you are permitted to do so.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#the-data",
    "href": "content/exams/practice-exam-fa21-ans.html#the-data",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "The data",
    "text": "The data\nThe dataset you’ll be working with on this practice midterm is all about beach volleyball. The full dataset is explained in detail here and includes match-level data from 76,756 volleyball matches. You should click on that link to see what information is stored in each column in this dataset and what information is included in each column.\nBriefly, what you’ll use for this midterm is a subset of the full dataset, including only the 11,699 observations (rows) from 2018 and 2019 but all of the original columns. Each row summarizes the results from a single, distinct match played in a volleyball tournament.\nTo briefly describe beach volleyball, it is a sport played 2 on 2, so each match involves only 4 players. These data include matches from two different volleyball circuits, the international FIVB and the US-centric AVP. You will not need to know much at all about this sport to complete this midterm, and anything you need to know will be explained.\nThe data are stored in data/vb_matches.csv. You’ll need to read the dataset in prior to answering any questions on the midterm.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;- read_csv('data/vb_matches.csv')\n\nRows: 11699 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (17): circuit, tournament, country, gender, w_player1, w_p1_country, w_...\ndbl  (42): year, match_num, w_p1_age, w_p1_hgt, w_p2_age, w_p2_hgt, l_p1_age...\ndate  (5): date, w_p1_birthdate, w_p2_birthdate, l_p1_birthdate, l_p2_birthdate\ntime  (1): duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-fa21-ans.html#questions",
    "href": "content/exams/practice-exam-fa21-ans.html#questions",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Questions",
    "text": "Questions\nQuestion 1 (0.75 points) - How many FIVB and AVP matches are included in this dataset?\n\ndf |&gt; \n  group_by(circuit) |&gt;\n  count()\n\n# A tibble: 2 × 2\n# Groups:   circuit [2]\n  circuit     n\n  &lt;chr&gt;   &lt;int&gt;\n1 AVP      2656\n2 FIVB     9043\n\n\n\n2656 AVP\n9043 FIVB\n\nQuestion 2 (0.75 points) - Find the match with the longest duration. a. Where was this tournament played (City & Country)? b. How long did the match last? c. Who were the two winners?\n\ndf |&gt; \n  slice(which.max(duration))\n\n# A tibble: 1 × 65\n  circuit tournament country        year date       gender match_num w_player1  \n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      \n1 AVP     Austin     United States  2018 2018-05-17 M             37 Nathan Yang\n# ℹ 57 more variables: w_p1_birthdate &lt;date&gt;, w_p1_age &lt;dbl&gt;, w_p1_hgt &lt;dbl&gt;,\n#   w_p1_country &lt;chr&gt;, w_player2 &lt;chr&gt;, w_p2_birthdate &lt;date&gt;, w_p2_age &lt;dbl&gt;,\n#   w_p2_hgt &lt;dbl&gt;, w_p2_country &lt;chr&gt;, w_rank &lt;chr&gt;, l_player1 &lt;chr&gt;,\n#   l_p1_birthdate &lt;date&gt;, l_p1_age &lt;dbl&gt;, l_p1_hgt &lt;dbl&gt;, l_p1_country &lt;chr&gt;,\n#   l_player2 &lt;chr&gt;, l_p2_birthdate &lt;date&gt;, l_p2_age &lt;dbl&gt;, l_p2_hgt &lt;dbl&gt;,\n#   l_p2_country &lt;chr&gt;, l_rank &lt;chr&gt;, score &lt;chr&gt;, duration &lt;time&gt;,\n#   bracket &lt;chr&gt;, round &lt;chr&gt;, w_p1_tot_attacks &lt;dbl&gt;, w_p1_tot_kills &lt;dbl&gt;, …\n\n\n\nAustin, USA\n1h 42 min\nNathan Yang & Steven Irvin\n\n\nQuestion 3 (1.5 points) - Across all tournaments included in this dataset, which teams have won the most tournaments? Your response should include both the winning players, their gender, and the number of tournaments they’ve won in descending order. Who has the most wins? How many men’s and how many women’s teams are in the top 10?\nNote: “winning a tournament” is indicated by winning either a “Gold Medal” (FIVB) or “Finals” (AVP) match, specified in the bracket column.\n\ndf |&gt; \n  filter(bracket %in% c(\"Gold Medal\", \"Finals\")) |&gt; \n  group_by(w_player1, w_player2, gender) |&gt;\n  count() |&gt; \n  arrange(desc(n)) |&gt;\n  head(10)\n\n# A tibble: 10 × 4\n# Groups:   w_player1, w_player2, gender [10]\n   w_player1              w_player2                 gender     n\n   &lt;chr&gt;                  &lt;chr&gt;                     &lt;chr&gt;  &lt;int&gt;\n 1 Alix Klineman          \"April Ross\"              W         11\n 2 Anders Mol             \"Christian Sorum\"         M         10\n 3 Melissa Humana-Paredes \"Sarah Pavan\"             W          6\n 4 Nick Lucena            \"Phil Dalhausser\"         M          6\n 5 Jake Gibb              \"Taylor Crabb\"            M          5\n 6 Jingzhe Wang           \"Shuhui Wen\"              W          5\n 7 Agatha Bednarczuk      \"Eduarda \\\"Duda\\\" Lisboa\" W          4\n 8 Aleksandrs Samoilovs   \"Janis Smedins\"           M          4\n 9 Betsi Flint            \"Emily Day\"               W          4\n10 Alexander Brouwer      \"Robert Meeuwsen\"         M          3\n\n\n\nAlix Klineman and April Ross have won the most tournaments\nThere are 5 men’s teams and 5 women’s teams\n\nQuestion 4 (1.5 points) - Of only the AVP tournaments included in this dataset, how many different cities hosted tournaments in 2018 and 2019? And, which cities (if any) hosted a tournament in both 2018 and 2019?\nNote that tournaments are named for the city hosting the tournament.\n\n# distinct locations\ndf |&gt; \n  filter(circuit == 'AVP') |&gt;\n  distinct(tournament)\n\n# A tibble: 9 × 1\n  tournament      \n  &lt;chr&gt;           \n1 Austin          \n2 New York        \n3 Seattle         \n4 San Francisco   \n5 Hermosa Beach   \n6 Manhattan Beach \n7 Chicago         \n8 Waikiki         \n9 Huntington Beach\n\n\n\n9 distinct locations\n\n\nlocations &lt;- df |&gt; \n  filter(circuit == 'AVP') |&gt;\n  group_by(year, date) |&gt; \n    distinct(tournament) \n\ndf |&gt;\n  filter(circuit == \"AVP\") |&gt;\n  group_by(year, date) |&gt;\n  distinct(tournament) |&gt;\n  group_by(tournament) |&gt;\n  count() |&gt;\n  filter(n &gt; 1)\n\n# A tibble: 6 × 2\n# Groups:   tournament [6]\n  tournament          n\n  &lt;chr&gt;           &lt;int&gt;\n1 Austin              2\n2 Chicago             2\n3 Hermosa Beach       2\n4 Manhattan Beach     2\n5 New York            2\n6 Seattle             2\n\n\n\n6 locations were duplicates between 2018 and 2019, including Chicago, Manhattan Beach, Hermosa Beach, Seattle, New York, and Austin\n\nQuestion 5 (2.5 points) - Prof Ellis plays a lot of women’s beach volleyball and is only 5’5” (65 inches). Despite not having the sheer talent or raw athletic ability to make it as a professional volleyball player, she wonders if she ever had a chance at her height. To help her out, answer each of the following: a. Who was the shortest women’s player to compete in a tournament in 2018/2019? b. How tall are they? c. Did they win a tournament in 2018 or 2019?\nReminder: there are 4 players in each match whose height should be considered.\n\n# find shortest in each column\ndf |&gt;\n  filter(gender == \"W\") |&gt;\n  summarize(min_p1 = min(w_p1_hgt, na.rm=TRUE), \n            min_p2  = min(w_p2_hgt, na.rm=TRUE), \n            min_l_p1 = min(l_p1_hgt, na.rm=TRUE),\n            min_l_p2 = min(l_p2_hgt, na.rm=TRUE))\n\n# A tibble: 1 × 4\n  min_p1 min_p2 min_l_p1 min_l_p2\n   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     63     64       62       61\n\n# determine the shortest player\ndf |&gt; filter(l_p2_hgt == 61)\n\n# A tibble: 2 × 65\n  circuit tournament    country  year date       gender match_num w_player1     \n  &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         \n1 FIVB    Visakhapatnam India    2019 2019-02-28 W              7 Melissa Fuchs…\n2 FIVB    Visakhapatnam India    2019 2019-02-28 W             16 Ekaterina Fil…\n# ℹ 57 more variables: w_p1_birthdate &lt;date&gt;, w_p1_age &lt;dbl&gt;, w_p1_hgt &lt;dbl&gt;,\n#   w_p1_country &lt;chr&gt;, w_player2 &lt;chr&gt;, w_p2_birthdate &lt;date&gt;, w_p2_age &lt;dbl&gt;,\n#   w_p2_hgt &lt;dbl&gt;, w_p2_country &lt;chr&gt;, w_rank &lt;chr&gt;, l_player1 &lt;chr&gt;,\n#   l_p1_birthdate &lt;date&gt;, l_p1_age &lt;dbl&gt;, l_p1_hgt &lt;dbl&gt;, l_p1_country &lt;chr&gt;,\n#   l_player2 &lt;chr&gt;, l_p2_birthdate &lt;date&gt;, l_p2_age &lt;dbl&gt;, l_p2_hgt &lt;dbl&gt;,\n#   l_p2_country &lt;chr&gt;, l_rank &lt;chr&gt;, score &lt;chr&gt;, duration &lt;time&gt;,\n#   bracket &lt;chr&gt;, round &lt;chr&gt;, w_p1_tot_attacks &lt;dbl&gt;, w_p1_tot_kills &lt;dbl&gt;, …\n\n\nPerumal Yogeshwari was the shortest to compete (at 61 inches). By deduction, since this is the l_p2 column, we know that she did not win any tournaments.\nQuestion 6 (3 pts) - Which country has hosted the most FIVB tournaments? Did this differ by year? Generate a visualization that shows how many FIVB tournaments each country hosted. Allow viewer to visualize this by year. And, be sure each tournament is only counted once (regardless of how many games were played).\n\ndf |&gt;\n  filter(circuit == 'FIVB') |&gt;\n  distinct(tournament, year, .keep_all = TRUE) |&gt;\n  ggplot(aes(y=fct_infreq(country))) + \n  geom_bar() + \n  facet_wrap(~year) +\n  labs(y=NULL,\n       x=\"Count\",\n       title = \"Number of FIVB tournaments hosted by each country\") + \n  theme_bw() +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\nQuestion 7 (3 pts) - Recreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be.\nHint: The visualization uses the variable avg_team_height, which is not included in the provided data frame. You will have to create avg_team_height yourself, be determining the average (mean) team height for each winning team.\n\n\ndf |&gt;\n  filter(circuit == 'AVP') |&gt;\n  mutate(avg_team_height = (w_p1_hgt + w_p2_hgt)/2) |&gt;\n  ggplot(aes(x = fct_infreq(w_p1_country), y = avg_team_height, fill=w_p1_country)) +\n  geom_boxplot() + \n  facet_wrap(~gender, \n             scales=\"free_y\", \n             nrow=2) +\n  labs(y = 'Average Team Height (in)',\n       x = 'Country',\n       title = 'Average team heights for AVP match winners in 2018 and 2019') +\n  theme(plot.title.position = \"plot\") +\n  guides(fill=\"none\") \n\nWarning: Removed 1288 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nVisualizes which countries have the tallest winning teams on average. For men, we see that players from the Netherlands are quite tall. For Women, it’s the US and Canada.\n\nQuestion 8 (1 pts) - If you were in charge of designing the plot you just recreated in the plot above, what changes would you make to improve its effectiveness as a visualization? (You do not have to write any code for this question, just explain the different design/viz choices you would make.)\n\n\nswap axes\naccurate/more informative labelling labeling\nmeaningful ordering of countries (geography?)\nimproved color choices (flag colors?)\nincrease text size\nremove background\netc.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Ans-Fa21)"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html",
    "href": "content/exams/midterm-fa23.html",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-fa23.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Nov 6th to complete this exam and turn it in via your personal Github repo.\nThere will be no Piazza posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please message or email Prof Ellis as soon as possible.\nEach question requires R code to determine the answer and text explaining your answer (except Q10, which just requires a text response). You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html#rules",
    "href": "content/exams/midterm-fa23.html#rules",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-fa23.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Nov 6th to complete this exam and turn it in via your personal Github repo.\nThere will be no Piazza posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please message or email Prof Ellis as soon as possible.\nEach question requires R code to determine the answer and text explaining your answer (except Q10, which just requires a text response). You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html#academic-integrity",
    "href": "content/exams/midterm-fa23.html#academic-integrity",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBe sure to complete the AI statement in the exam you submit itself.\nA note on sharing / reusing code: I am well aware that a huge volume of code is available on the web to solve any number of problems and that LLMs have the ability to provide you code to prompts you give it. For this exam you are allowed to make use of any online resources but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden.",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html#grading-and-feedback",
    "href": "content/exams/midterm-fa23.html#grading-and-feedback",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThis exam is worth 15% of your grade. You will be graded on the correctness of your code, correctness of your answers, the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization but the template gets you started on a well-organized exam. We should be able to easily navigate your midterm to find what we’re looking for.) Organization + Clarity in written communication - 1pt",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html#logistics",
    "href": "content/exams/midterm-fa23.html#logistics",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called midterm-fa23.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in.",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html#packages",
    "href": "content/exams/midterm-fa23.html#packages",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these packages have been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html#the-data",
    "href": "content/exams/midterm-fa23.html#the-data",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Richmondway R Pacakge and have been provided by the TidyTuesday team.\nThe data are stored in data/richmondway.csv You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, this dataset includes data from three seasons of the TV show Ted Lasso. Each observation is a single episode of the show. The variables, generally, relate to the number of times Roy Kent (a foul-mouthed character on the show) and the entire cast say the F-word (often referred to as dropping the “F bomb”).",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/midterm-fa23.html#questions",
    "href": "content/exams/midterm-fa23.html#questions",
    "title": "COGS 137 - Midterm (Fa23)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nF-bomb summary:\n\nCalculate how many times total Roy Kent said the F-word within each season.\nComment on in which season Roy Kent said the F-word the most overall.\n\n\n\nQuestion 2 (0.5 points)\nDetermine how many episodes had more F bombs by Roy Kent than every other character on the show combined (excluding Roy Kent)?\n\n\nQuestion 3 (1.5 points)\nGenerate an exploratory* visualization that displays the typical range of Roy Kent F-bombs in an episode, broken down by season and explain three things you’ve learned about the data from this plot.\n\n\n*Note: exploratory here means that it does NOT have to be polished. Do NOT worry about title, axis labels, etc. We just care about understanding the data here. (If you do customize, you will NOT be penalized. It’s just not required for this question.)\n\n\nQuestion 4 (1 point)\nGenerate an exploratory* visualization that displays the relationship between Imdb_rating and Roy Kent F-bombs. Describe the relationship you see in this plot.\n\n\nQuestion 5 (1 point)\nBackground: Keeley is a character on Ted Lasso who is dating Roy Kent for some but not all of the episodes.\nGenerate a visualization that enables you to answer the questions below: - Does the median number of Roy Kent F bombs differ when Roy is dating Keeley (vs. when he is not)? - In the episode when Roy Kent dropped the most F bombs, was Roy dating Keeley?\n\n\nQuestion 6 (1.5 points)\nWhat is the effect of dating Keeley on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results.\n\n\nQuestion 7 (1.5 points)\nBackground: In Season 1, Roy Kent is a player. After retiring, he eventually becomes a coach. So, Roy is a coach in some but not all of the episodes.\nWhat is the effect of whether or not Roy Kent is coaching on the number of Roy Kent F bombs? Generate a linear model that answers this question. Interpret the results. Then, comment on whether coaching or dating Keeley is a better predictor of Roy Kent F bombs and explain how you came to that conclusion.\n\n\nQuestion 8 (2.5 points)\nGenerate a polished* visualization that allows viewers to compare proportion/percentage of F-bombs broken down by season for Roy Kent vs those by everyone other than Roy Kent.\n\n\n*Note: polished here means we want you to take the time to make a finished visualization that adheres to the design principles discussed in class. There is more than one correct answer here, but we want you to pay attention to details and ensure this visualization effectively communicates what we’re asking.\n\n\nQuestion 9 (3 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, describe at least one change that you would make to improve the design of the plot.\n\n\nNote: the hex values for the colors used in this plot are: “#deebf7” (lightest), “#9ecae1”, and “#3182bd” (darkest)\n\n\n\nQuestion 10 (1 point)\nDescribe at least 1) two things you like about how the plot in Question 9 communicates the data and 2) two things you would do differently to make this a more effective visualization for communication.",
    "crumbs": [
      "Home",
      "Exam",
      "Midterm"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html",
    "href": "content/exams/practice-exam-wi23.html",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the exam. There will also be an academic integrity statement for you to complete. Replace the `____________` with your name on the real deal.\n\n\n\n\n\n\nNote\n\n\n\nThis is the midterm from when the course was offered in wi23.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#rules",
    "href": "content/exams/practice-exam-wi23.html#rules",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the exam. There will also be an academic integrity statement for you to complete. Replace the `____________` with your name on the real deal.\n\n\n\n\n\n\nNote\n\n\n\nThis is the midterm from when the course was offered in wi23.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#logistics",
    "href": "content/exams/practice-exam-wi23.html#logistics",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam-wi23.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#packages",
    "href": "content/exams/practice-exam-wi23.html#packages",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these packages have been installed, but you will need to load them. You are allowed, but not required, to use additional packages.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#the-data",
    "href": "content/exams/practice-exam-wi23.html#the-data",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Axios and Harris Poll and have been provided by the TidyTuesday team.\nThe data are stored in two different files in the data/ folder: poll.csv and reputation.csv. You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, these two files include data about the 100 “most visible” brands in America. Specifically, reputation.csv includes information from the 2022 poll about these 100 stores across different reputation categories. poll.csv includes information about the same 100 stores but includes information about their rankings across multiple years.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Wi23)"
    ]
  },
  {
    "objectID": "content/exams/practice-exam-wi23.html#questions",
    "href": "content/exams/practice-exam-wi23.html#questions",
    "title": "COGS 137 - Practice Midterm (Wi23)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nHow many different industries (industry variable) are represented in these data?\n\n\nQuestion 2 (0.5 points)\n\nWhich company had the lowest overall ranking in 2022?\nAnd for which category (from the name variable) did this organization score lowest?\n\n\n\nQuestion 3 (1 point)\nWhich company in the reputation.csv dataset has the “best” average (mean) rank across all seven categories?\n\n\nQuestion 4 (1 point)\nWhich company had the biggest increase in rank from 2021 to 2022?\n\n\nQuestion 5 (1.5 points)\nFor the industry with only a single “most visible” company in the dataset, has their RQ score been increasing or decreasing overall since 2017?\n\n\nQuestion 6 (2 points)\nHow many companies from each industry category are represented in the 2022 ‘100 Most Visible’ companies in America data? Generate a visualization to display the answer to this question. Be sure to follow best visualization practices discussed in class.\n\n\nQuestion 7 (2 points)\nOf industries that have at least 5 companies in the dataset, which industry has the highest median 2022 rank? Generate a visualization that allows you to answer this question. Be sure to follow best practices.\n\n\nQuestion 8 (2 points)\nYour boss is curious about how much rankings change from one year to the next. To answer this question, they ask you to determine how well 2021 rankings explain the following year’s 2022 rankings. Generate a linear model to answer this question. Be sure to include your interpretation of the model (in other words your answer to the question “how well do 2021 rankings explain 2022’s rankings?”)\n\n\nQuestion 9 (2.5 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be.",
    "crumbs": [
      "Home",
      "Exam",
      "Practice Midterm (Wi23)"
    ]
  },
  {
    "objectID": "content/final/final.html",
    "href": "content/final/final.html",
    "title": "Final Project",
    "section": "",
    "text": "For the final project, you and your group mates (groups of 3-4 people) get to choose one of the following two options: 1) Technical Presentation or 2) Data Analysis.\nEach group will be provided with a private repo that all members as well as course instructional staff will have access to. Final projects will be “submitted” by pushing the project requirements to the group repo by the deadline.\nPresentations will be submitted on Canvas.\nWritten, visual, and presented content will be graded on their technical merits as well as the effectiveness of their communication.",
    "crumbs": [
      "Home",
      "Final",
      "Final Project"
    ]
  },
  {
    "objectID": "content/final/final.html#introduction",
    "href": "content/final/final.html#introduction",
    "title": "Final Project",
    "section": "",
    "text": "For the final project, you and your group mates (groups of 3-4 people) get to choose one of the following two options: 1) Technical Presentation or 2) Data Analysis.\nEach group will be provided with a private repo that all members as well as course instructional staff will have access to. Final projects will be “submitted” by pushing the project requirements to the group repo by the deadline.\nPresentations will be submitted on Canvas.\nWritten, visual, and presented content will be graded on their technical merits as well as the effectiveness of their communication.",
    "crumbs": [
      "Home",
      "Final",
      "Final Project"
    ]
  },
  {
    "objectID": "content/final/final.html#option-1-technical-presentation",
    "href": "content/final/final.html#option-1-technical-presentation",
    "title": "Final Project",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\nGroups who choose the technical presentation route will make slides for a presentation that effectively communicates/teaches an advanced statistical topic1 and/or an R package2.\n\nDeliverable: R Markdown + Slides\nSlides will be required for the presentation and they must be generated from either an RMarkdown document or a quarto document. Chapters 4, 7, and 8 of the R Markdown: The Definitive Guide discussion options for generating slides/presentations from R Markdown documents. (Presentations from quarto have similar documentation here.) Students should commit both the .Rmd (or .qmd) document and the rendered slides to their GitHub repo.\nThis presentation must teach the details of the R package, the statistical topic, or both at a level appropriate for students in this course. (i.e. You can assume your audience knows how to program in R, know about the tidyverse, know linear regression, etc.) And, you must demonstrate how to use the package and/or carry out the statistical analysis in R.\n\n\nDeliverable: Presentation\nStudents must also present their slides in a presentation that is 10-15min long. This presentation will be pre-recorded and submitted on Canvas. For this option, all students must participate in the presentation.\n\n\nDeliverable: General Communication\nThis will be a communication targeted to the people who you think should know about this package/statistical analysis. Here, you can assume your audience knows about R/data analysis in general, but you want to distill your presentation down to the most important aspect someone would want/need to know if they were going to use what you’ve chosen to present on.",
    "crumbs": [
      "Home",
      "Final",
      "Final Project"
    ]
  },
  {
    "objectID": "content/final/final.html#option-2-data-analysis",
    "href": "content/final/final.html#option-2-data-analysis",
    "title": "Final Project",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\nGroups who choose the data analysis route will carry out a full data science project. This will include question formation, finding the data, doing background research, wrangling the data, doing EDA, analyzing the data, and answering your question of interest.\nYou can think of this as a mini case report in the fact that the process is the same, but we would not expect the data wrangling to be quite as extensive as what was done in the case studies. That said, we want to see demonstration of the skills you’ve learned in the class, so we will be looking for some data wrangling in your case study. If you have a single dataset that requires no wrangling, consider if additional datasets could be incorporated to answer your question(s) of interest more deeply.\nYou are strongly encouraged to think of your topic/question before looking for datasets. More interesting case studies start with the topic/question. Boring case studies look for the dataset first.\n\nDeliverable: Report (.Rmd + HTML)\nYour analysis will be submitted as an .Rmd document and rendered to HTML (both of which should be pushed to GitHub).\nThis will likely not be quite as long as a case study in this course, but will likely have the same sections.\n\n\nDeliverable: Presentation\nStudents must present their case study in a presentation that is 3-5min long. What you use to visually support this presentation (slides, or something else) is up to you but should follow the effective communication aspects discussed in class. This presentation will pre-recorded and submitted on Canvas. For this option, at least one group member must present the project (in other words, not everyone has to “speak” but everyone in the group is responsible for the contents).\n\n\nDeliverable: General Communication\nThis will be a communication targeted to the general public (non-technical, non-data scientists) conveying the most important finding(s) from your project.",
    "crumbs": [
      "Home",
      "Final",
      "Final Project"
    ]
  },
  {
    "objectID": "content/final/final.html#option-3-cs02-additional-data",
    "href": "content/final/final.html#option-3-cs02-additional-data",
    "title": "Final Project",
    "section": "Option 3: CS02 + Additional Data",
    "text": "Option 3: CS02 + Additional Data\nStudents can choose to carry out CS02 for their final project; however, students will have to find an additional dataset on a related topic (pollution, climate change, etc.) and incorporate that into the ir final report. See CS02 documentation for details on report and general communication deliverables.\n\nDeliverable: Presentation\nStudents must also present their project in a presentation that is 3-5min long. This presentation will be pre-recorded and submitted on Canvas. For this option, at least one group member must present the project (in other words, not everyone has to “speak” but everyone in the group is responsible for the contents).\nNOTE: Prior to 12/11 The above paragraph was incorrect. As a result, if a CS02 + additional data group submits a longer presentation 10-15min presentation, there will NOT be a deduction given the error I made.",
    "crumbs": [
      "Home",
      "Final",
      "Final Project"
    ]
  },
  {
    "objectID": "content/final/final.html#group-feedback",
    "href": "content/final/final.html#group-feedback",
    "title": "Final Project",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the final project to provide feedback about working with your group mates. As with the case studies, this is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary.",
    "crumbs": [
      "Home",
      "Final",
      "Final Project"
    ]
  },
  {
    "objectID": "content/final/final.html#footnotes",
    "href": "content/final/final.html#footnotes",
    "title": "Final Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAdvanced statistical topics are any topics beyond what would be covered in an intro stats course or any topics covered in depth in this course.↩︎\nR Packages that can be chosen in this course are any R package that is not covered in detail in this course. (i.e. tidyverse packages, tidymodels and broom would not be options). If a package was used but only briefly mentioned in class, you can choose that. If you’re unsure, ask!↩︎",
    "crumbs": [
      "Home",
      "Final",
      "Final Project"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#qa",
    "href": "content/lectures/19-wrap-up-slides.html#qa",
    "title": "19-wrap-up",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Are we required to try models other than linear models/random forest if we are doing cs02 as our final project?\nA: No. In fact, I think you could just do the random forest model (and not discuss the linear regression models.) Using additional models would be a good extension. Of course, you’d also need to consider some outside dataset as well.\n\n\nQ: How much workload are you expecting for the final project compared to CS01? Our group spent like at least 14 hours on CS01 when your expectation was to spend 4-6 hours if I remember correctly. Should I expect to spend a similar amount of time for the final project or more?\nA: Historically groups have spend less time on the final relative to the case studies b/c they choose more straightforward datasets/questions.\n\n\nQ: I am still curious about presentation styles, and what is the most effective manner. In my time taking courses in the COGS and DSC departments at UCSD, I have noticed the usage of emojis a lot in programming assignments (esp. Jupyter notebook) but recently got some mixed opinions of emoji use in my Jupyter notebook by someone looking over a personal project. I am curious to know what the conventions are, and to delve deeper into presenting things catered to a specific audience. Additionally, I was wondering if there are any resources on how to make a data science portfolio/website for graduate school and internship/job applications.\nA: We’ll discuss a bit about the second question soon. As for the first, my response is that it depends on your audience and the setting. If it’s a very serios/stuffy conference, maybe be more formal (fewer emojis)….but in data science, typically presentations are more casual/fun (relative to other fields), so I’d say do what you’re comfortable with.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#course-announcements",
    "href": "content/lectures/19-wrap-up-slides.html#course-announcements",
    "title": "19-wrap-up",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nPlease fill out your SET course evaluations (due Sat 12/9 at 8AM)\nFinal Project due Tues 12/12 at 11:59 PM\n\n.Rmd (report/slides)\nPresentation (recording; submit on Canvas)\nGeneral Communication\ngroup work survey (due Wednesday)\n\nPost-course survey “due” next Wednesday (for EC)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#final-project-details",
    "href": "content/lectures/19-wrap-up-slides.html#final-project-details",
    "title": "19-wrap-up",
    "section": "Final Project Details",
    "text": "Final Project Details\n\n\nData Analysis option\n\nif wrangling not needed…don’t make wrangling up\nwant you to demonstrate your skills across the final project\n\nPresentation: at the level of a COGS 137 student\n\npre-recorded\nthe time limit matters\nprobably best to reference the effective communication notes\n\nGeneral communication: to a non-technical audience\n\nfor a technical presentation, likely best to think of it as an “ad” for the package/statistical approach",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#open-qa",
    "href": "content/lectures/19-wrap-up-slides.html#open-qa",
    "title": "19-wrap-up",
    "section": "Open Q&A",
    "text": "Open Q&A\nWhat questions do you have about data science, stats, R, jobs/internships, life, analysis, communication, my life/opinions, etc.?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#where-is-r-used",
    "href": "content/lectures/19-wrap-up-slides.html#where-is-r-used",
    "title": "19-wrap-up",
    "section": "Where is R used?",
    "text": "Where is R used?\n\n\nR is used by data scientists\nparticularly popular in certain fields: (bio)statistics, biology, economics, psychology, finance, healthcare, business analytics, government/public policy, data journalism, education, etc.\nIt is less popular than Python\nReally great for: data wrangling, visualization, and modelling",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#next-steps-in-r",
    "href": "content/lectures/19-wrap-up-slides.html#next-steps-in-r",
    "title": "19-wrap-up",
    "section": "Next Steps in R",
    "text": "Next Steps in R\n\nInteractive Visualization\nPackage Development\nBooks, Slides, and Personal websites\nShiny Apps\n\n\nNote: Any packages described today ARE allowed to be used for the final project, if you’re going the technical presentation route.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#packages",
    "href": "content/lectures/19-wrap-up-slides.html#packages",
    "title": "19-wrap-up",
    "section": "Packages",
    "text": "Packages\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(gganimate)\nlibrary(gapminder) # the dataset being used",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#the-data-gapminder",
    "href": "content/lectures/19-wrap-up-slides.html#the-data-gapminder",
    "title": "19-wrap-up",
    "section": "The Data: Gapminder",
    "text": "The Data: Gapminder\nThe gapminder visualization was made famous by Hans Rosling. The dataset used here includes life expectancy, population, and GDP across 142 countries and 5 continents from 1952-2007.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#plotly",
    "href": "content/lectures/19-wrap-up-slides.html#plotly",
    "title": "19-wrap-up",
    "section": "plotly",
    "text": "plotly\n\nwrapper around ggplot plots: ggplotly()\nwhen it works, it works\nless control over specifics\n\n\nCodePlot\n\n\n\np &lt;- gapminder |&gt;\n  filter(year==1977) |&gt;\n  ggplot(aes(gdpPercap, lifeExp, size = pop, color=continent)) +\n  geom_point() +\n  theme_bw()\n\np &lt;- ggplotly(p)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#plotly-1",
    "href": "content/lectures/19-wrap-up-slides.html#plotly-1",
    "title": "19-wrap-up",
    "section": "plotly",
    "text": "plotly\n\nCodePlot\n\n\n\ngg &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, color = continent, frame = year)) +\n  geom_point() +\n  theme_bw()\n\ngg &lt;- ggplotly(gg) |&gt; \n  highlight(\"plotly_hover\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#gganimate",
    "href": "content/lectures/19-wrap-up-slides.html#gganimate",
    "title": "19-wrap-up",
    "section": "gganimate",
    "text": "gganimate\nExtends grammar of graphics for use in animation:\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\nSource: https://gganimate.com/\n\n\nmore control\nslower to render\ngenerates GIFs\n\n\n\nFor example…\n\ngg &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, color = continent, frame = year)) +\n  geom_point() +\n  theme_bw() +\n  #gganimate specific bits\n  labs(title = 'Year: {frame_time}', x = 'GDP per capita', y = 'life expectancy') +\n  transition_time(year) +\n  ease_aes('linear')",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#r2d3",
    "href": "content/lectures/19-wrap-up-slides.html#r2d3",
    "title": "19-wrap-up",
    "section": "r2d3",
    "text": "r2d3\n\nD3 is a javascript library for producing viz for HTML\nable to use custom D3 Visualizations within R\ncreate D3.js scripts\ncall them from RMarkdown/Shiny/etc.\nExample here",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#why-develop-an-r-package",
    "href": "content/lectures/19-wrap-up-slides.html#why-develop-an-r-package",
    "title": "19-wrap-up",
    "section": "Why develop an R package?",
    "text": "Why develop an R package?\n\nreproducibility\ninclude data + code\norganize a project\ntools needed: devtools and usethis\n\n\n\n\nBook: R Packages, by Jenny Bryan and Hadley Wickham\nBlogpost: Writing an R Package from Scratch, by Hilary Parker (Disclaimer: this is from 2014 and does not implement usethis)\nBlogpost: Your First Package in 1 hour, by Shannon Pileggi for R-Ladies Philly in 2020",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#other-package-suggestsions",
    "href": "content/lectures/19-wrap-up-slides.html#other-package-suggestsions",
    "title": "19-wrap-up",
    "section": "Other Package Suggestsions",
    "text": "Other Package Suggestsions\n\nDataviz: handful of packages that extend the functionality of ggplot2; Good Tables: gt, formattable, and reactable\nModelling: we only really used the tidymodels and broom packages, so the other packages in tidymodels are options\nThere are lots of packages for doing machine learning. caret is a precursor to tidymodels and good for this\nWebscraping: rvest\nSports: nwslR, baseballr, NFL\nA whole more curated here",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#books-bookdown",
    "href": "content/lectures/19-wrap-up-slides.html#books-bookdown",
    "title": "19-wrap-up",
    "section": "Books: bookdown",
    "text": "Books: bookdown\nAn R package by Yihui Xie to write online books, with the philosophy that it “should be technically easy to write a book, visually pleasant to view the book, fun to interact with the book, convenient to navigate through the book, straightforward for readers to contribute or leave feedback to the book author(s), and more importantly, authors should not always be distracted by typesetting details”\n\nbookdown: Authoring Books and Technical Documents with R Markdown, by Yihui Xie\nbookdown gallery\nExample: What they forgot to teach you about R, by Jenny Bryan and Jim Hester",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#slides-xaringan",
    "href": "content/lectures/19-wrap-up-slides.html#slides-xaringan",
    "title": "19-wrap-up",
    "section": "Slides: xaringan",
    "text": "Slides: xaringan\nAn RMarkdown extension (based on JS library remark.js) to generate slides from .Rmd documents.\n\nBook Chapter: xaringan Presentations\nSlide Show: Meet Xaringan, by Alison Hill",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#websites-blogdown",
    "href": "content/lectures/19-wrap-up-slides.html#websites-blogdown",
    "title": "19-wrap-up",
    "section": "Websites: blogdown",
    "text": "Websites: blogdown\nEnables personal website creation using R Markdown and Hugo (or Jekyll)\n\nBook: blogdown: Creating websites with R Markdown, by Yihui Xie, Amber Thomas, and Alison Presmanes Hill\nBlogpost Up & running with blogdown in 2021, by Alison Presmanes Hill\nSome examples: Alison Hill, Yihui, Prof",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#quarto",
    "href": "content/lectures/19-wrap-up-slides.html#quarto",
    "title": "19-wrap-up",
    "section": "Quarto",
    "text": "Quarto\n\nan open-source scientific and technical publishing system built on Pandoc\n\n\noutputs: HTML, PDF, MS Word, ePub, etc.\nlanguage-agnostic\nallows for multiple programming languages in a single document\nWebsite: Quarto\n\n\n\nClass notes, slides, and website for COGS 137 were all built utilizing quarto. Generating slides for technical presentation using quarto is an option for the final project. (.qmd rather than .Rmd)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#shiny",
    "href": "content/lectures/19-wrap-up-slides.html#shiny",
    "title": "19-wrap-up",
    "section": "Shiny",
    "text": "Shiny\nShiny is an R package that allows you to build interactive web apps directly from R (initially developed by Winston Chang)\n\nWebsite: Shiny\nExamples: Freedom of the Press Index, COVID-19 Tracker, and recount\nHow-To: How To Build a Shiny app",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#quarto-dashboards",
    "href": "content/lectures/19-wrap-up-slides.html#quarto-dashboards",
    "title": "19-wrap-up",
    "section": "Quarto Dashboards",
    "text": "Quarto Dashboards\nIn the upcoming release of quarto, dashboards will be even simpler to generate…(currently available in pre-release)…here",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#whats-a-ds-portfolio",
    "href": "content/lectures/19-wrap-up-slides.html#whats-a-ds-portfolio",
    "title": "19-wrap-up",
    "section": "What’s a DS portfolio?",
    "text": "What’s a DS portfolio?\nA public showcase of your work!\n\nKaggle is a great place to get practice, but not necessarily for personal projects for your portfolio\n\n\n…b/c literally millions of other people have already worked with the data/done the project.\n\n\nYou want your portfolio to 1) demonstrate your skills and 2) set you apart",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#ds-portfolio-examples",
    "href": "content/lectures/19-wrap-up-slides.html#ds-portfolio-examples",
    "title": "19-wrap-up",
    "section": "DS Portfolio Examples",
    "text": "DS Portfolio Examples\n\nEnrique Sanchez\nMyra Haider\nTyler Richards\nVivian Peng\nMac Strelioff",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#your-turn-get-started-on-one-of-these",
    "href": "content/lectures/19-wrap-up-slides.html#your-turn-get-started-on-one-of-these",
    "title": "19-wrap-up",
    "section": "Your Turn: Get started on one of these…",
    "text": "Your Turn: Get started on one of these…\nAlways wanted a personal website? Get Started with blogdown! Have a data-centric app you want to share with the world? Shiny it up! Have slides that need to be created for a final project? Give xaringan a go! Have a visualization that needs animation? Make it move!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#cogs-137-where-weve-been",
    "href": "content/lectures/19-wrap-up-slides.html#cogs-137-where-weve-been",
    "title": "19-wrap-up",
    "section": "COGS 137: Where We’ve Been",
    "text": "COGS 137: Where We’ve Been\n\nR, RMarkdown & RStudio\nData Wrangling w/ the tidyverse\nDataviz w/ ggplot2\nCS01: Biomarkers of Recent THC Use (Inference)\nCS02: Predicting Air Pollution (ML)\nNext Steps in R: Shiny, bookdown, blogdown, plotly/gganimate",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#cogs-137-a-semi-new-course",
    "href": "content/lectures/19-wrap-up-slides.html#cogs-137-a-semi-new-course",
    "title": "19-wrap-up",
    "section": "COGS 137: A Semi-New Course",
    "text": "COGS 137: A Semi-New Course\nLots of thanks!\n\n\ncourse staff! (Kunal & Shenova - feedback, grading, labs, office hours, etc.)\nall of you\nMine Çetinkaya-Rundel, Open Case Studies Team, Posit (RStudio, quarto & tidyverse teams)\nSean Kross & Prof Drew Walker",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up-slides.html#section",
    "href": "content/lectures/19-wrap-up-slides.html#section",
    "title": "19-wrap-up",
    "section": "",
    "text": "Good Luck on Finals, Get Sleep, Be Safe, Drink Water, Take Care of Yourselves, & Have a Wonderful Winter Break!\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html",
    "href": "content/lectures/19-wrap-up.html",
    "title": "19-wrap-up",
    "section": "",
    "text": "Q: Are we required to try models other than linear models/random forest if we are doing cs02 as our final project?\nA: No. In fact, I think you could just do the random forest model (and not discuss the linear regression models.) Using additional models would be a good extension. Of course, you’d also need to consider some outside dataset as well.\n\n\nQ: How much workload are you expecting for the final project compared to CS01? Our group spent like at least 14 hours on CS01 when your expectation was to spend 4-6 hours if I remember correctly. Should I expect to spend a similar amount of time for the final project or more?\nA: Historically groups have spend less time on the final relative to the case studies b/c they choose more straightforward datasets/questions.\n\n\nQ: I am still curious about presentation styles, and what is the most effective manner. In my time taking courses in the COGS and DSC departments at UCSD, I have noticed the usage of emojis a lot in programming assignments (esp. Jupyter notebook) but recently got some mixed opinions of emoji use in my Jupyter notebook by someone looking over a personal project. I am curious to know what the conventions are, and to delve deeper into presenting things catered to a specific audience. Additionally, I was wondering if there are any resources on how to make a data science portfolio/website for graduate school and internship/job applications.\nA: We’ll discuss a bit about the second question soon. As for the first, my response is that it depends on your audience and the setting. If it’s a very serios/stuffy conference, maybe be more formal (fewer emojis)….but in data science, typically presentations are more casual/fun (relative to other fields), so I’d say do what you’re comfortable with.\n\n\n\n\n\nPlease fill out your SET course evaluations (due Sat 12/9 at 8AM)\nFinal Project due Tues 12/12 at 11:59 PM\n\n.Rmd (report/slides)\nPresentation (recording; submit on Canvas)\nGeneral Communication\ngroup work survey (due Wednesday)\n\nPost-course survey “due” next Wednesday (for EC)\n\n\n\n\n\n\nData Analysis option\n\nif wrangling not needed…don’t make wrangling up\nwant you to demonstrate your skills across the final project\n\nPresentation: at the level of a COGS 137 student\n\npre-recorded\nthe time limit matters\nprobably best to reference the effective communication notes\n\nGeneral communication: to a non-technical audience\n\nfor a technical presentation, likely best to think of it as an “ad” for the package/statistical approach",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#qa",
    "href": "content/lectures/19-wrap-up.html#qa",
    "title": "19-wrap-up",
    "section": "",
    "text": "Q: Are we required to try models other than linear models/random forest if we are doing cs02 as our final project?\nA: No. In fact, I think you could just do the random forest model (and not discuss the linear regression models.) Using additional models would be a good extension. Of course, you’d also need to consider some outside dataset as well.\n\n\nQ: How much workload are you expecting for the final project compared to CS01? Our group spent like at least 14 hours on CS01 when your expectation was to spend 4-6 hours if I remember correctly. Should I expect to spend a similar amount of time for the final project or more?\nA: Historically groups have spend less time on the final relative to the case studies b/c they choose more straightforward datasets/questions.\n\n\nQ: I am still curious about presentation styles, and what is the most effective manner. In my time taking courses in the COGS and DSC departments at UCSD, I have noticed the usage of emojis a lot in programming assignments (esp. Jupyter notebook) but recently got some mixed opinions of emoji use in my Jupyter notebook by someone looking over a personal project. I am curious to know what the conventions are, and to delve deeper into presenting things catered to a specific audience. Additionally, I was wondering if there are any resources on how to make a data science portfolio/website for graduate school and internship/job applications.\nA: We’ll discuss a bit about the second question soon. As for the first, my response is that it depends on your audience and the setting. If it’s a very serios/stuffy conference, maybe be more formal (fewer emojis)….but in data science, typically presentations are more casual/fun (relative to other fields), so I’d say do what you’re comfortable with.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#course-announcements",
    "href": "content/lectures/19-wrap-up.html#course-announcements",
    "title": "19-wrap-up",
    "section": "",
    "text": "Please fill out your SET course evaluations (due Sat 12/9 at 8AM)\nFinal Project due Tues 12/12 at 11:59 PM\n\n.Rmd (report/slides)\nPresentation (recording; submit on Canvas)\nGeneral Communication\ngroup work survey (due Wednesday)\n\nPost-course survey “due” next Wednesday (for EC)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#final-project-details",
    "href": "content/lectures/19-wrap-up.html#final-project-details",
    "title": "19-wrap-up",
    "section": "",
    "text": "Data Analysis option\n\nif wrangling not needed…don’t make wrangling up\nwant you to demonstrate your skills across the final project\n\nPresentation: at the level of a COGS 137 student\n\npre-recorded\nthe time limit matters\nprobably best to reference the effective communication notes\n\nGeneral communication: to a non-technical audience\n\nfor a technical presentation, likely best to think of it as an “ad” for the package/statistical approach",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#open-qa",
    "href": "content/lectures/19-wrap-up.html#open-qa",
    "title": "19-wrap-up",
    "section": "Open Q&A",
    "text": "Open Q&A\nWhat questions do you have about data science, stats, R, jobs/internships, life, analysis, communication, my life/opinions, etc.?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#where-is-r-used",
    "href": "content/lectures/19-wrap-up.html#where-is-r-used",
    "title": "19-wrap-up",
    "section": "Where is R used?",
    "text": "Where is R used?\n\n\nR is used by data scientists\nparticularly popular in certain fields: (bio)statistics, biology, economics, psychology, finance, healthcare, business analytics, government/public policy, data journalism, education, etc.\nIt is less popular than Python\nReally great for: data wrangling, visualization, and modelling",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#next-steps-in-r",
    "href": "content/lectures/19-wrap-up.html#next-steps-in-r",
    "title": "19-wrap-up",
    "section": "Next Steps in R",
    "text": "Next Steps in R\n\nInteractive Visualization\nPackage Development\nBooks, Slides, and Personal websites\nShiny Apps\n\n. . .\nNote: Any packages described today ARE allowed to be used for the final project, if you’re going the technical presentation route.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#packages",
    "href": "content/lectures/19-wrap-up.html#packages",
    "title": "19-wrap-up",
    "section": "Packages",
    "text": "Packages\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(gganimate)\nlibrary(gapminder) # the dataset being used",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#the-data-gapminder",
    "href": "content/lectures/19-wrap-up.html#the-data-gapminder",
    "title": "19-wrap-up",
    "section": "The Data: Gapminder",
    "text": "The Data: Gapminder\nThe gapminder visualization was made famous by Hans Rosling. The dataset used here includes life expectancy, population, and GDP across 142 countries and 5 continents from 1952-2007.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#plotly",
    "href": "content/lectures/19-wrap-up.html#plotly",
    "title": "19-wrap-up",
    "section": "plotly",
    "text": "plotly\n\nwrapper around ggplot plots: ggplotly()\nwhen it works, it works\nless control over specifics\n\n\nCodePlot\n\n\n\np &lt;- gapminder |&gt;\n  filter(year==1977) |&gt;\n  ggplot(aes(gdpPercap, lifeExp, size = pop, color=continent)) +\n  geom_point() +\n  theme_bw()\n\np &lt;- ggplotly(p)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#plotly-1",
    "href": "content/lectures/19-wrap-up.html#plotly-1",
    "title": "19-wrap-up",
    "section": "plotly",
    "text": "plotly\n\nCodePlot\n\n\n\ngg &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, color = continent, frame = year)) +\n  geom_point() +\n  theme_bw()\n\ngg &lt;- ggplotly(gg) |&gt; \n  highlight(\"plotly_hover\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#gganimate",
    "href": "content/lectures/19-wrap-up.html#gganimate",
    "title": "19-wrap-up",
    "section": "gganimate",
    "text": "gganimate\nExtends grammar of graphics for use in animation:\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\nSource: https://gganimate.com/\n. . .\n\nmore control\nslower to render\ngenerates GIFs\n\n. . .\nFor example…\n\ngg &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, color = continent, frame = year)) +\n  geom_point() +\n  theme_bw() +\n  #gganimate specific bits\n  labs(title = 'Year: {frame_time}', x = 'GDP per capita', y = 'life expectancy') +\n  transition_time(year) +\n  ease_aes('linear')",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#r2d3",
    "href": "content/lectures/19-wrap-up.html#r2d3",
    "title": "19-wrap-up",
    "section": "r2d3",
    "text": "r2d3\n\nD3 is a javascript library for producing viz for HTML\nable to use custom D3 Visualizations within R\ncreate D3.js scripts\ncall them from RMarkdown/Shiny/etc.\nExample here",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#why-develop-an-r-package",
    "href": "content/lectures/19-wrap-up.html#why-develop-an-r-package",
    "title": "19-wrap-up",
    "section": "Why develop an R package?",
    "text": "Why develop an R package?\n\nreproducibility\ninclude data + code\norganize a project\ntools needed: devtools and usethis\n\n\n\n\nBook: R Packages, by Jenny Bryan and Hadley Wickham\nBlogpost: Writing an R Package from Scratch, by Hilary Parker (Disclaimer: this is from 2014 and does not implement usethis)\nBlogpost: Your First Package in 1 hour, by Shannon Pileggi for R-Ladies Philly in 2020",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#other-package-suggestsions",
    "href": "content/lectures/19-wrap-up.html#other-package-suggestsions",
    "title": "19-wrap-up",
    "section": "Other Package Suggestsions",
    "text": "Other Package Suggestsions\n\nDataviz: handful of packages that extend the functionality of ggplot2; Good Tables: gt, formattable, and reactable\nModelling: we only really used the tidymodels and broom packages, so the other packages in tidymodels are options\nThere are lots of packages for doing machine learning. caret is a precursor to tidymodels and good for this\nWebscraping: rvest\nSports: nwslR, baseballr, NFL\nA whole more curated here",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#books-bookdown",
    "href": "content/lectures/19-wrap-up.html#books-bookdown",
    "title": "19-wrap-up",
    "section": "Books: bookdown",
    "text": "Books: bookdown\nAn R package by Yihui Xie to write online books, with the philosophy that it “should be technically easy to write a book, visually pleasant to view the book, fun to interact with the book, convenient to navigate through the book, straightforward for readers to contribute or leave feedback to the book author(s), and more importantly, authors should not always be distracted by typesetting details”\n\nbookdown: Authoring Books and Technical Documents with R Markdown, by Yihui Xie\nbookdown gallery\nExample: What they forgot to teach you about R, by Jenny Bryan and Jim Hester",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#slides-xaringan",
    "href": "content/lectures/19-wrap-up.html#slides-xaringan",
    "title": "19-wrap-up",
    "section": "Slides: xaringan",
    "text": "Slides: xaringan\nAn RMarkdown extension (based on JS library remark.js) to generate slides from .Rmd documents.\n\nBook Chapter: xaringan Presentations\nSlide Show: Meet Xaringan, by Alison Hill",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#websites-blogdown",
    "href": "content/lectures/19-wrap-up.html#websites-blogdown",
    "title": "19-wrap-up",
    "section": "Websites: blogdown",
    "text": "Websites: blogdown\nEnables personal website creation using R Markdown and Hugo (or Jekyll)\n\nBook: blogdown: Creating websites with R Markdown, by Yihui Xie, Amber Thomas, and Alison Presmanes Hill\nBlogpost Up & running with blogdown in 2021, by Alison Presmanes Hill\nSome examples: Alison Hill, Yihui, Prof",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#quarto",
    "href": "content/lectures/19-wrap-up.html#quarto",
    "title": "19-wrap-up",
    "section": "Quarto",
    "text": "Quarto\n\nan open-source scientific and technical publishing system built on Pandoc\n\n\noutputs: HTML, PDF, MS Word, ePub, etc.\nlanguage-agnostic\nallows for multiple programming languages in a single document\nWebsite: Quarto\n\n\n\nClass notes, slides, and website for COGS 137 were all built utilizing quarto. Generating slides for technical presentation using quarto is an option for the final project. (.qmd rather than .Rmd)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#shiny",
    "href": "content/lectures/19-wrap-up.html#shiny",
    "title": "19-wrap-up",
    "section": "Shiny",
    "text": "Shiny\nShiny is an R package that allows you to build interactive web apps directly from R (initially developed by Winston Chang)\n\nWebsite: Shiny\nExamples: Freedom of the Press Index, COVID-19 Tracker, and recount\nHow-To: How To Build a Shiny app",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#quarto-dashboards",
    "href": "content/lectures/19-wrap-up.html#quarto-dashboards",
    "title": "19-wrap-up",
    "section": "Quarto Dashboards",
    "text": "Quarto Dashboards\nIn the upcoming release of quarto, dashboards will be even simpler to generate…(currently available in pre-release)…here",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#whats-a-ds-portfolio",
    "href": "content/lectures/19-wrap-up.html#whats-a-ds-portfolio",
    "title": "19-wrap-up",
    "section": "What’s a DS portfolio?",
    "text": "What’s a DS portfolio?\nA public showcase of your work!\n. . .\nKaggle is a great place to get practice, but not necessarily for personal projects for your portfolio\n. . .\n…b/c literally millions of other people have already worked with the data/done the project.\n. . .\nYou want your portfolio to 1) demonstrate your skills and 2) set you apart",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#ds-portfolio-examples",
    "href": "content/lectures/19-wrap-up.html#ds-portfolio-examples",
    "title": "19-wrap-up",
    "section": "DS Portfolio Examples",
    "text": "DS Portfolio Examples\n\nEnrique Sanchez\nMyra Haider\nTyler Richards\nVivian Peng\nMac Strelioff",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#your-turn-get-started-on-one-of-these",
    "href": "content/lectures/19-wrap-up.html#your-turn-get-started-on-one-of-these",
    "title": "19-wrap-up",
    "section": "Your Turn: Get started on one of these…",
    "text": "Your Turn: Get started on one of these…\nAlways wanted a personal website? Get Started with blogdown! Have a data-centric app you want to share with the world? Shiny it up! Have slides that need to be created for a final project? Give xaringan a go! Have a visualization that needs animation? Make it move!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#cogs-137-where-weve-been",
    "href": "content/lectures/19-wrap-up.html#cogs-137-where-weve-been",
    "title": "19-wrap-up",
    "section": "COGS 137: Where We’ve Been",
    "text": "COGS 137: Where We’ve Been\n\nR, RMarkdown & RStudio\nData Wrangling w/ the tidyverse\nDataviz w/ ggplot2\nCS01: Biomarkers of Recent THC Use (Inference)\nCS02: Predicting Air Pollution (ML)\nNext Steps in R: Shiny, bookdown, blogdown, plotly/gganimate",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#cogs-137-a-semi-new-course",
    "href": "content/lectures/19-wrap-up.html#cogs-137-a-semi-new-course",
    "title": "19-wrap-up",
    "section": "COGS 137: A Semi-New Course",
    "text": "COGS 137: A Semi-New Course\nLots of thanks!\n\n\ncourse staff! (Kunal & Shenova - feedback, grading, labs, office hours, etc.)\nall of you\nMine Çetinkaya-Rundel, Open Case Studies Team, Posit (RStudio, quarto & tidyverse teams)\nSean Kross & Prof Drew Walker",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/19-wrap-up.html#section",
    "href": "content/lectures/19-wrap-up.html#section",
    "title": "19-wrap-up",
    "section": "",
    "text": "Good Luck on Finals, Get Sleep, Be Safe, Drink Water, Take Care of Yourselves, & Have a Wonderful Winter Break!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "19-wrap-up"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#qa",
    "href": "content/lectures/09-mlr-slides.html#qa",
    "title": "09-mlr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: What are some ways we can make our visuals accessible? For example, how can we account for color blind people when incorporating graphs and figures in our presentation?\nA: Love this question! One way to do this is to use tools built for just this purpose! For example, for color blindness, ColorBrewer2 allows you to toggle for color palletes that are “colorblind safe.” Similarly, for written content online, ensuring that you’re including alt-text for all images, to enable visual understanding by those with vision impariments/differences. For those with learning differences (i.e. dyslexia, ADHD), I’m sure there’s research out there with respect to viz, and I should make myself more familiar, but ensuring that you’re not using very similar acronyms/initialisms for different context or using consistent icons (Rather than letters) across visualizations, etc. can really help individuals with learning differences and overall make your viz more consistent/understandable. This is just the tip of the iceberg. Additional suggestions here.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#course-announcements",
    "href": "content/lectures/09-mlr-slides.html#course-announcements",
    "title": "09-mlr",
    "section": "Course Announcements",
    "text": "Course Announcements\n\n🎃 Happy Halloween!\n\nNo OH today after class\nMake up: tomorrow 10-11AM (CSB 243)\n\n🔬 There is NO lab to turn in this week\n\nLab will be used for midterm review\nCome to lab with questions!\n\n🏫 Midterm is due next Monday at 11:59PM\n\nwill be released 5PM Friday\ncompleted individually\nopen notes/open Internet\nNOT timed\ntypically takes students ~2.5h to complete (big range: 2-15h)\n\nLab05 & HW03 will be released next Monday",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#suggested-reading",
    "href": "content/lectures/09-mlr-slides.html#suggested-reading",
    "title": "09-mlr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nIntroduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#agenda",
    "href": "content/lectures/09-mlr-slides.html#agenda",
    "title": "09-mlr",
    "section": "Agenda",
    "text": "Agenda\n\nMultiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#packages-data",
    "href": "content/lectures/09-mlr-slides.html#packages-data",
    "title": "09-mlr",
    "section": "Packages & Data",
    "text": "Packages & Data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nData: Paris Paintings\n\npp &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |&gt; \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#multiple-predictors",
    "href": "content/lectures/09-mlr-slides.html#multiple-predictors",
    "title": "09-mlr",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nResponse variable: log_price\nExplanatory variables: Width and height\n\n\n\nlin_mod &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\npp_fit &lt;- lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in, data = pp)\ntidy(pp_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#linear-model-with-multiple-predictors",
    "href": "content/lectures/09-mlr-slides.html#linear-model-with-multiple-predictors",
    "title": "09-mlr",
    "section": "Linear model with multiple predictors",
    "text": "Linear model with multiple predictors\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4\n\n\n\n\\[\\widehat{log\\_price} = 4.77 + 0.0269 \\times width - 0.0133 \\times height\\]\n\n❓ How do we interpret this model?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#price-surface-area-and-living-artist",
    "href": "content/lectures/09-mlr-slides.html#price-surface-area-and-living-artist",
    "title": "09-mlr",
    "section": "Price, surface area, and living artist",
    "text": "Price, surface area, and living artist\n\nExplore the relationship between price of paintings and surface area, conditioned on whether or not the artist is still living\nFirst visualize and explore, then model\n\n\n\nBut first, prep the data:\n\n\npp &lt;- pp |&gt;\n  mutate(artistliving = case_when(artistliving == 0 ~ \"Deceased\", \n                                  TRUE ~ \"Living\"))\n\npp |&gt;\n  count(artistliving)\n\n# A tibble: 2 × 2\n  artistliving     n\n  &lt;chr&gt;        &lt;int&gt;\n1 Deceased      2937\n2 Living         456",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#typical-surface-area",
    "href": "content/lectures/09-mlr-slides.html#typical-surface-area",
    "title": "09-mlr",
    "section": "Typical surface area",
    "text": "Typical surface area\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nTypical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.\n\n\n\nggplot(data = pp, aes(x = Surface, fill = artistliving)) +\n  geom_histogram(binwidth = 500) + \n  facet_grid(artistliving ~ .) +\n  scale_fill_manual(values = c(\"#E48957\", \"#071381\")) +\n  guides(fill = \"none\") +\n  labs(x = \"Surface area\", y = NULL) +\n  geom_vline(xintercept = 1000) +\n  geom_vline(xintercept = 5000, linetype = \"dashed\", color = \"gray\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#narrowing-the-scope",
    "href": "content/lectures/09-mlr-slides.html#narrowing-the-scope",
    "title": "09-mlr",
    "section": "Narrowing the scope",
    "text": "Narrowing the scope\n\nPlotCode\n\n\nFor simplicity let’s focus on the paintings with Surface &lt; 5000:\n\n\n\n\n\n\n\n\n\n\n\n\npp_Surf_lt_5000 &lt;- pp |&gt;\n  filter(Surface &lt; 5000)\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Artist\", shape = \"Artist\") +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\"))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#facet-to-get-a-better-look",
    "href": "content/lectures/09-mlr-slides.html#facet-to-get-a-better-look",
    "title": "09-mlr",
    "section": "Facet to get a better look",
    "text": "Facet to get a better look\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~artistliving) +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\")) +\n  labs(color = \"Artist\", shape = \"Artist\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#two-ways-to-model",
    "href": "content/lectures/09-mlr-slides.html#two-ways-to-model",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living.\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interacting-explanatory-variables",
    "href": "content/lectures/09-mlr-slides.html#interacting-explanatory-variables",
    "title": "09-mlr",
    "section": "Interacting explanatory variables",
    "text": "Interacting explanatory variables\n\nIncluding an interaction effect in the model allows for different slopes, i.e. nonparallel lines.\nThis implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\nThis can be accomplished by adding an interaction variable: the product of two explanatory variables.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#two-ways-to-model-1",
    "href": "content/lectures/09-mlr-slides.html#two-ways-to-model-1",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\n\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❓ Which does your intuition/knowledge of the data suggest is more appropriate?\nPut a green sticky if you think main; pink if you think interaction.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#fit-model-with-main-effects",
    "href": "content/lectures/09-mlr-slides.html#fit-model-with-main-effects",
    "title": "09-mlr",
    "section": "Fit model with main effects",
    "text": "Fit model with main effects\n\nResponse variable: log_price\nExplanatory variables: Surface area and artistliving\n\n\npp_main_fit &lt;- lin_mod |&gt;\n  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)\ntidy(pp_main_fit)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        4.88     0.0424       115.   0       \n2 Surface            0.000265 0.0000415      6.39 1.85e-10\n3 artistlivingLiving 0.137    0.0970         1.41 1.57e- 1\n\n\n\n\\[\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times artistliving\\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#solving-the-model",
    "href": "content/lectures/09-mlr-slides.html#solving-the-model",
    "title": "09-mlr",
    "section": "Solving the model",
    "text": "Solving the model\n\nNon-living artist: Plug in 0 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 0\\)\n\\(= 4.88 + 0.000265 \\times surface\\)\n\n\nLiving artist: Plug in 1 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 1\\)\n\\(= 5.017 + 0.000265 \\times surface\\)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#visualizing-main-effects",
    "href": "content/lectures/09-mlr-slides.html#visualizing-main-effects",
    "title": "09-mlr",
    "section": "Visualizing main effects",
    "text": "Visualizing main effects\n\n\n\nSame slope: Rate of change in price as the surface area increases does not vary between paintings by living and non-living artists.\nDifferent intercept: Paintings by living artists are consistently more expensive than paintings by non-living artists.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interpreting-main-effects",
    "href": "content/lectures/09-mlr-slides.html#interpreting-main-effects",
    "title": "09-mlr",
    "section": "Interpreting main effects",
    "text": "Interpreting main effects\n\ntidy(pp_main_fit) |&gt; \n  mutate(exp_estimate = exp(estimate)) |&gt;\n  select(term, estimate, exp_estimate)\n\n# A tibble: 3 × 3\n  term               estimate exp_estimate\n  &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)        4.88           132.  \n2 Surface            0.000265         1.00\n3 artistlivingLiving 0.137            1.15\n\n\n\n\nAll else held constant, for each additional square inch in painting’s surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.\nAll else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.\nPaintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#main-vs.-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#main-vs.-interaction-effects",
    "title": "09-mlr",
    "section": "Main vs. interaction effects",
    "text": "Main vs. interaction effects\n\nThe way we specified our main effects model only lets artistliving affect the intercept.\nModel implicitly assumes that paintings with living and deceased artists have the same slope and only allows for different intercepts.\n\n\n❓ What seems more appropriate in this case?\n\nSame slope and same intercept for both colors\nSame slope and different intercept for both colors\nDifferent slope and different intercept for both colors",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interaction-surface-artistliving",
    "href": "content/lectures/09-mlr-slides.html#interaction-surface-artistliving",
    "title": "09-mlr",
    "section": "Interaction: Surface * artistliving",
    "text": "Interaction: Surface * artistliving",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#fit-model-with-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#fit-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Fit model with interaction effects",
    "text": "Fit model with interaction effects\n\nResponse variable: log_price\nExplanatory variables: Surface area, artistliving, and their interaction\n\n\npp_int_fit &lt;- lin_mod |&gt;\n  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)\ntidy(pp_int_fit)\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#linear-model-with-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#linear-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Linear model with interaction effects",
    "text": "Linear model with interaction effects\n\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139  \n\n\n\\[\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface - 0.126 \\times artistliving\\] \\[+ ~ 0.00048 \\times surface * artistliving\\]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#interpretation-of-interaction-effects",
    "href": "content/lectures/09-mlr-slides.html#interpretation-of-interaction-effects",
    "title": "09-mlr",
    "section": "Interpretation of interaction effects",
    "text": "Interpretation of interaction effects\n\n\nRate of change in price as the surface area of the painting increases does vary between paintings by living and non-living artists (different slopes)\nSome paintings by living artists are more expensive than paintings by non-living artists, and some are not (different intercept).\n\n\n\n\n\n\n\nNon-living artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 0 + 0.00048 \\times surface \\times 0\\) \\(= 4.91 + 0.00021 \\times surface\\)\nLiving artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 1 + 0.00048 \\times surface \\times 1\\) \\(= 4.91 + 0.00021 \\times surface\\) \\(- 0.126 + 0.00048 \\times surface\\) \\(= 4.784 + 0.00069 \\times surface\\)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#r-squared",
    "href": "content/lectures/09-mlr-slides.html#r-squared",
    "title": "09-mlr",
    "section": "R-squared",
    "text": "R-squared\n\n\\(R^2\\) is the percentage of variability in the response variable explained by the regression model.\n\n\nglance(pp_main_fit)$r.squared\n\n[1] 0.01320884\n\nglance(pp_int_fit)$r.squared\n\n[1] 0.0176922\n\n\n\n\nClearly the model with interactions has a higher \\(R^2\\).\n\n\n\n\nHowever using \\(R^2\\) for model selection in models with multiple explanatory variables is not a good idea as \\(R^2\\) increases when any variable is added to the model.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#adjusted-r-squared",
    "href": "content/lectures/09-mlr-slides.html#adjusted-r-squared",
    "title": "09-mlr",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\nIt appears that adding the interaction actually increased adjusted \\(R^2\\), so we should indeed use the model with the interactions.\n\nglance(pp_main_fit)$adj.r.squared\n\n[1] 0.01258977\n\nglance(pp_int_fit)$adj.r.squared\n\n[1] 0.01676753",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#third-order-interactions",
    "href": "content/lectures/09-mlr-slides.html#third-order-interactions",
    "title": "09-mlr",
    "section": "Third order interactions",
    "text": "Third order interactions\n\nCan you? Yes\nShould you? Probably not if you want to interpret these interactions in context of the data.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#in-pursuit-of-occams-razor",
    "href": "content/lectures/09-mlr-slides.html#in-pursuit-of-occams-razor",
    "title": "09-mlr",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#backward-selection",
    "href": "content/lectures/09-mlr-slides.html#backward-selection",
    "title": "09-mlr",
    "section": "Backward selection",
    "text": "Backward selection\nFor this demo, we’ll ignore interaction effects…and just model main effects to start:\n\npp_full &lt;-  lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp) \n\nglance(pp_full)$adj.r.squared\n\n[1] 0.02570141\n\n\n\n\\(R^2\\) (full): 0.0257014\n\n\nRemove artistliving\n\npp_noartist &lt;- lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in + Surface, data=pp) \n\nglance(pp_noartist)$adj.r.squared\n\n[1] 0.02579859\n\n\n. . .\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\n…Improved variance explained\n. . .\nRemove Surface\n\npp_noartist_nosurface &lt;- lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in, data=pp) \n\nglance(pp_noartist_nosurface)$adj.r.squared\n\n[1] 0.02231559\n\n\n. . .\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\\(R^2\\) (no artistliving or Surface): 0.0223\n\n. . .\n…no longer gaining improvement, so we stick with: log_price ~ Width_in + Height_in + Surface",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#other-approach",
    "href": "content/lectures/09-mlr-slides.html#other-approach",
    "title": "09-mlr",
    "section": "Other approach:",
    "text": "Other approach:\n\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n\nStep 1: Fit model (w/o tidymodels)\n\n# fit the model (not using tidymodels)\nmod &lt;- lm(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp_Surf_lt_5000)\n\n\n\nStep 2: Determine which variables to remove\n\nols_step_backward_p(mod)\n\n\n                             Elimination Summary                               \n------------------------------------------------------------------------------\n        Variable                      Adj.                                        \nStep      Removed       R-Square    R-Square     C(p)        AIC         RMSE     \n------------------------------------------------------------------------------\n   1    artistliving      0.0261      0.0251    3.8495    12603.7727    1.8315    \n------------------------------------------------------------------------------\n\n\n…specifies that artistliving should be removed\n\n\nStep 2 (alternate): Compare all possible models…\n\nols_step_all_possible(mod) |&gt;\n  arrange(desc(adjr))\n\n   Index N                              Predictors     R-Square Adj. R-Square\n1     11 3              Width_in Height_in Surface 0.0260749939  0.0251349118\n2     15 4 Width_in Height_in Surface artistliving 0.0263412027  0.0250876993\n3      5 2                      Width_in Height_in 0.0256902566  0.0250634893\n4     12 3         Width_in Height_in artistliving 0.0259732581  0.0250330779\n5      6 2                        Width_in Surface 0.0249136264  0.0242863596\n6     13 3           Width_in Surface artistliving 0.0251787948  0.0242378477\n7      7 2                   Width_in artistliving 0.0212864021  0.0206568018\n8      1 1                                Width_in 0.0209415833  0.0206267736\n9      8 2                    Surface artistliving 0.0132088377  0.0125897717\n10     2 1                                 Surface 0.0125899681  0.0122803381\n11    14 3          Height_in Surface artistliving 0.0130836930  0.0121310711\n12     9 2                       Height_in Surface 0.0126782901  0.0120431523\n13    10 2                  Height_in artistliving 0.0062698155  0.0056305552\n14     3 1                               Height_in 0.0058797727  0.0055601199\n15     4 1                            artistliving 0.0005531617  0.0002397573\n   Mallow's Cp\n1     3.849487\n2     5.000000\n3     3.077206\n4     4.174132\n5     5.555476\n6     6.709309\n7    17.130153\n8    16.230489\n9    51.971190\n10   52.001268\n11   45.305459\n12   44.599123\n13   65.048926\n14   64.293574\n15   91.485605",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr-slides.html#recap",
    "href": "content/lectures/09-mlr-slides.html#recap",
    "title": "09-mlr",
    "section": "Recap",
    "text": "Recap\n\nCan you model and interpret linear models with multiple predictors?\nCan you explain the difference in a model with main effects vs. interaction effects?\nCan you compare different models and determine how to proceed?\nCan you carry out and explain backward selection?\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html",
    "href": "content/lectures/09-mlr.html",
    "title": "09-mlr",
    "section": "",
    "text": "Q: What are some ways we can make our visuals accessible? For example, how can we account for color blind people when incorporating graphs and figures in our presentation?\nA: Love this question! One way to do this is to use tools built for just this purpose! For example, for color blindness, ColorBrewer2 allows you to toggle for color palletes that are “colorblind safe.” Similarly, for written content online, ensuring that you’re including alt-text for all images, to enable visual understanding by those with vision impariments/differences. For those with learning differences (i.e. dyslexia, ADHD), I’m sure there’s research out there with respect to viz, and I should make myself more familiar, but ensuring that you’re not using very similar acronyms/initialisms for different context or using consistent icons (Rather than letters) across visualizations, etc. can really help individuals with learning differences and overall make your viz more consistent/understandable. This is just the tip of the iceberg. Additional suggestions here.\n\n\n\n\n\n🎃 Happy Halloween!\n\nNo OH today after class\nMake up: tomorrow 10-11AM (CSB 243)\n\n🔬 There is NO lab to turn in this week\n\nLab will be used for midterm review\nCome to lab with questions!\n\n🏫 Midterm is due next Monday at 11:59PM\n\nwill be released 5PM Friday\ncompleted individually\nopen notes/open Internet\nNOT timed\ntypically takes students ~2.5h to complete (big range: 2-15h)\n\nLab05 & HW03 will be released next Monday\n\n\n\n\nIntroduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors\n\n\n\n\nMultiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n. . .\nData: Paris Paintings\n\npp &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |&gt; \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#qa",
    "href": "content/lectures/09-mlr.html#qa",
    "title": "09-mlr",
    "section": "",
    "text": "Q: What are some ways we can make our visuals accessible? For example, how can we account for color blind people when incorporating graphs and figures in our presentation?\nA: Love this question! One way to do this is to use tools built for just this purpose! For example, for color blindness, ColorBrewer2 allows you to toggle for color palletes that are “colorblind safe.” Similarly, for written content online, ensuring that you’re including alt-text for all images, to enable visual understanding by those with vision impariments/differences. For those with learning differences (i.e. dyslexia, ADHD), I’m sure there’s research out there with respect to viz, and I should make myself more familiar, but ensuring that you’re not using very similar acronyms/initialisms for different context or using consistent icons (Rather than letters) across visualizations, etc. can really help individuals with learning differences and overall make your viz more consistent/understandable. This is just the tip of the iceberg. Additional suggestions here.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#course-announcements",
    "href": "content/lectures/09-mlr.html#course-announcements",
    "title": "09-mlr",
    "section": "",
    "text": "🎃 Happy Halloween!\n\nNo OH today after class\nMake up: tomorrow 10-11AM (CSB 243)\n\n🔬 There is NO lab to turn in this week\n\nLab will be used for midterm review\nCome to lab with questions!\n\n🏫 Midterm is due next Monday at 11:59PM\n\nwill be released 5PM Friday\ncompleted individually\nopen notes/open Internet\nNOT timed\ntypically takes students ~2.5h to complete (big range: 2-15h)\n\nLab05 & HW03 will be released next Monday",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#suggested-reading",
    "href": "content/lectures/09-mlr.html#suggested-reading",
    "title": "09-mlr",
    "section": "",
    "text": "Introduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#agenda",
    "href": "content/lectures/09-mlr.html#agenda",
    "title": "09-mlr",
    "section": "",
    "text": "Multiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#packages-data",
    "href": "content/lectures/09-mlr.html#packages-data",
    "title": "09-mlr",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\n. . .\nData: Paris Paintings\n\npp &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |&gt; \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#multiple-predictors",
    "href": "content/lectures/09-mlr.html#multiple-predictors",
    "title": "09-mlr",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nResponse variable: log_price\nExplanatory variables: Width and height\n\n. . .\n\nlin_mod &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\npp_fit &lt;- lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in, data = pp)\ntidy(pp_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#linear-model-with-multiple-predictors",
    "href": "content/lectures/09-mlr.html#linear-model-with-multiple-predictors",
    "title": "09-mlr",
    "section": "Linear model with multiple predictors",
    "text": "Linear model with multiple predictors\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4\n\n\n\n\\[\\widehat{log\\_price} = 4.77 + 0.0269 \\times width - 0.0133 \\times height\\]\n. . .\n❓ How do we interpret this model?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#price-surface-area-and-living-artist",
    "href": "content/lectures/09-mlr.html#price-surface-area-and-living-artist",
    "title": "09-mlr",
    "section": "Price, surface area, and living artist",
    "text": "Price, surface area, and living artist\n\nExplore the relationship between price of paintings and surface area, conditioned on whether or not the artist is still living\nFirst visualize and explore, then model\n\n. . .\n\nBut first, prep the data:\n\n\npp &lt;- pp |&gt;\n  mutate(artistliving = case_when(artistliving == 0 ~ \"Deceased\", \n                                  TRUE ~ \"Living\"))\n\npp |&gt;\n  count(artistliving)\n\n# A tibble: 2 × 2\n  artistliving     n\n  &lt;chr&gt;        &lt;int&gt;\n1 Deceased      2937\n2 Living         456",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#typical-surface-area",
    "href": "content/lectures/09-mlr.html#typical-surface-area",
    "title": "09-mlr",
    "section": "Typical surface area",
    "text": "Typical surface area\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\nTypical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.\n\n\n\nggplot(data = pp, aes(x = Surface, fill = artistliving)) +\n  geom_histogram(binwidth = 500) + \n  facet_grid(artistliving ~ .) +\n  scale_fill_manual(values = c(\"#E48957\", \"#071381\")) +\n  guides(fill = \"none\") +\n  labs(x = \"Surface area\", y = NULL) +\n  geom_vline(xintercept = 1000) +\n  geom_vline(xintercept = 5000, linetype = \"dashed\", color = \"gray\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#narrowing-the-scope",
    "href": "content/lectures/09-mlr.html#narrowing-the-scope",
    "title": "09-mlr",
    "section": "Narrowing the scope",
    "text": "Narrowing the scope\n\nPlotCode\n\n\nFor simplicity let’s focus on the paintings with Surface &lt; 5000:\n\n\n\n\n\n\n\n\n\n\n\n\npp_Surf_lt_5000 &lt;- pp |&gt;\n  filter(Surface &lt; 5000)\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Artist\", shape = \"Artist\") +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\"))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#facet-to-get-a-better-look",
    "href": "content/lectures/09-mlr.html#facet-to-get-a-better-look",
    "title": "09-mlr",
    "section": "Facet to get a better look",
    "text": "Facet to get a better look\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~artistliving) +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\")) +\n  labs(color = \"Artist\", shape = \"Artist\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#two-ways-to-model",
    "href": "content/lectures/09-mlr.html#two-ways-to-model",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living.\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#interacting-explanatory-variables",
    "href": "content/lectures/09-mlr.html#interacting-explanatory-variables",
    "title": "09-mlr",
    "section": "Interacting explanatory variables",
    "text": "Interacting explanatory variables\n\nIncluding an interaction effect in the model allows for different slopes, i.e. nonparallel lines.\nThis implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\nThis can be accomplished by adding an interaction variable: the product of two explanatory variables.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#two-ways-to-model-1",
    "href": "content/lectures/09-mlr.html#two-ways-to-model-1",
    "title": "09-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\n\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living\n\n\n\n\n\n\n\n\n\n\n\n\n\n. . .\n❓ Which does your intuition/knowledge of the data suggest is more appropriate?\nPut a green sticky if you think main; pink if you think interaction.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#fit-model-with-main-effects",
    "href": "content/lectures/09-mlr.html#fit-model-with-main-effects",
    "title": "09-mlr",
    "section": "Fit model with main effects",
    "text": "Fit model with main effects\n\nResponse variable: log_price\nExplanatory variables: Surface area and artistliving\n\n\npp_main_fit &lt;- lin_mod |&gt;\n  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)\ntidy(pp_main_fit)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        4.88     0.0424       115.   0       \n2 Surface            0.000265 0.0000415      6.39 1.85e-10\n3 artistlivingLiving 0.137    0.0970         1.41 1.57e- 1\n\n\n. . .\n\\[\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times artistliving\\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#solving-the-model",
    "href": "content/lectures/09-mlr.html#solving-the-model",
    "title": "09-mlr",
    "section": "Solving the model",
    "text": "Solving the model\n\nNon-living artist: Plug in 0 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 0\\)\n\\(= 4.88 + 0.000265 \\times surface\\)\n. . .\n\nLiving artist: Plug in 1 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 1\\)\n\\(= 5.017 + 0.000265 \\times surface\\)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#visualizing-main-effects",
    "href": "content/lectures/09-mlr.html#visualizing-main-effects",
    "title": "09-mlr",
    "section": "Visualizing main effects",
    "text": "Visualizing main effects\n\n\n\nSame slope: Rate of change in price as the surface area increases does not vary between paintings by living and non-living artists.\nDifferent intercept: Paintings by living artists are consistently more expensive than paintings by non-living artists.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#interpreting-main-effects",
    "href": "content/lectures/09-mlr.html#interpreting-main-effects",
    "title": "09-mlr",
    "section": "Interpreting main effects",
    "text": "Interpreting main effects\n\ntidy(pp_main_fit) |&gt; \n  mutate(exp_estimate = exp(estimate)) |&gt;\n  select(term, estimate, exp_estimate)\n\n# A tibble: 3 × 3\n  term               estimate exp_estimate\n  &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)        4.88           132.  \n2 Surface            0.000265         1.00\n3 artistlivingLiving 0.137            1.15\n\n\n\n\nAll else held constant, for each additional square inch in painting’s surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.\nAll else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.\nPaintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#main-vs.-interaction-effects",
    "href": "content/lectures/09-mlr.html#main-vs.-interaction-effects",
    "title": "09-mlr",
    "section": "Main vs. interaction effects",
    "text": "Main vs. interaction effects\n\nThe way we specified our main effects model only lets artistliving affect the intercept.\nModel implicitly assumes that paintings with living and deceased artists have the same slope and only allows for different intercepts.\n\n. . .\n❓ What seems more appropriate in this case?\n\nSame slope and same intercept for both colors\nSame slope and different intercept for both colors\nDifferent slope and different intercept for both colors",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#interaction-surface-artistliving",
    "href": "content/lectures/09-mlr.html#interaction-surface-artistliving",
    "title": "09-mlr",
    "section": "Interaction: Surface * artistliving",
    "text": "Interaction: Surface * artistliving",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#fit-model-with-interaction-effects",
    "href": "content/lectures/09-mlr.html#fit-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Fit model with interaction effects",
    "text": "Fit model with interaction effects\n\nResponse variable: log_price\nExplanatory variables: Surface area, artistliving, and their interaction\n\n\npp_int_fit &lt;- lin_mod |&gt;\n  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)\ntidy(pp_int_fit)\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#linear-model-with-interaction-effects",
    "href": "content/lectures/09-mlr.html#linear-model-with-interaction-effects",
    "title": "09-mlr",
    "section": "Linear model with interaction effects",
    "text": "Linear model with interaction effects\n\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  &lt;chr&gt;                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139  \n\n\n\\[\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface - 0.126 \\times artistliving\\] \\[+ ~ 0.00048 \\times surface * artistliving\\]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#interpretation-of-interaction-effects",
    "href": "content/lectures/09-mlr.html#interpretation-of-interaction-effects",
    "title": "09-mlr",
    "section": "Interpretation of interaction effects",
    "text": "Interpretation of interaction effects\n\n\nRate of change in price as the surface area of the painting increases does vary between paintings by living and non-living artists (different slopes)\nSome paintings by living artists are more expensive than paintings by non-living artists, and some are not (different intercept).\n\n\n. . .\n\n\n\n\nNon-living artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 0 + 0.00048 \\times surface \\times 0\\) \\(= 4.91 + 0.00021 \\times surface\\)\nLiving artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 1 + 0.00048 \\times surface \\times 1\\) \\(= 4.91 + 0.00021 \\times surface\\) \\(- 0.126 + 0.00048 \\times surface\\) \\(= 4.784 + 0.00069 \\times surface\\)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#r-squared",
    "href": "content/lectures/09-mlr.html#r-squared",
    "title": "09-mlr",
    "section": "R-squared",
    "text": "R-squared\n\n\\(R^2\\) is the percentage of variability in the response variable explained by the regression model.\n\n\nglance(pp_main_fit)$r.squared\n\n[1] 0.01320884\n\nglance(pp_int_fit)$r.squared\n\n[1] 0.0176922\n\n\n. . .\n\nClearly the model with interactions has a higher \\(R^2\\).\n\n. . .\n\nHowever using \\(R^2\\) for model selection in models with multiple explanatory variables is not a good idea as \\(R^2\\) increases when any variable is added to the model.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#adjusted-r-squared",
    "href": "content/lectures/09-mlr.html#adjusted-r-squared",
    "title": "09-mlr",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\nIt appears that adding the interaction actually increased adjusted \\(R^2\\), so we should indeed use the model with the interactions.\n\nglance(pp_main_fit)$adj.r.squared\n\n[1] 0.01258977\n\nglance(pp_int_fit)$adj.r.squared\n\n[1] 0.01676753",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#third-order-interactions",
    "href": "content/lectures/09-mlr.html#third-order-interactions",
    "title": "09-mlr",
    "section": "Third order interactions",
    "text": "Third order interactions\n\nCan you? Yes\nShould you? Probably not if you want to interpret these interactions in context of the data.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#in-pursuit-of-occams-razor",
    "href": "content/lectures/09-mlr.html#in-pursuit-of-occams-razor",
    "title": "09-mlr",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#backward-selection",
    "href": "content/lectures/09-mlr.html#backward-selection",
    "title": "09-mlr",
    "section": "Backward selection",
    "text": "Backward selection\nFor this demo, we’ll ignore interaction effects…and just model main effects to start:\n\npp_full &lt;-  lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp) \n\nglance(pp_full)$adj.r.squared\n\n[1] 0.02570141\n\n\n\n\\(R^2\\) (full): 0.0257014\n\n. . .\n\nRemove artistliving\n\npp_noartist &lt;- lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in + Surface, data=pp) \n\nglance(pp_noartist)$adj.r.squared\n\n[1] 0.02579859\n\n\n. . .\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\n…Improved variance explained\n. . .\n\n\nRemove Surface\n\npp_noartist_nosurface &lt;- lin_mod |&gt;\n  fit(log_price ~ Width_in + Height_in, data=pp) \n\nglance(pp_noartist_nosurface)$adj.r.squared\n\n[1] 0.02231559\n\n\n. . .\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\\(R^2\\) (no artistliving or Surface): 0.0223\n\n. . .\n…no longer gaining improvement, so we stick with: log_price ~ Width_in + Height_in + Surface",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#other-approach",
    "href": "content/lectures/09-mlr.html#other-approach",
    "title": "09-mlr",
    "section": "Other approach:",
    "text": "Other approach:\n\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n. . .\nStep 1: Fit model (w/o tidymodels)\n\n# fit the model (not using tidymodels)\nmod &lt;- lm(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp_Surf_lt_5000)\n\n. . .\nStep 2: Determine which variables to remove\n\nols_step_backward_p(mod)\n\n\n                             Elimination Summary                               \n------------------------------------------------------------------------------\n        Variable                      Adj.                                        \nStep      Removed       R-Square    R-Square     C(p)        AIC         RMSE     \n------------------------------------------------------------------------------\n   1    artistliving      0.0261      0.0251    3.8495    12603.7727    1.8315    \n------------------------------------------------------------------------------\n\n\n…specifies that artistliving should be removed\n. . .\nStep 2 (alternate): Compare all possible models…\n\nols_step_all_possible(mod) |&gt;\n  arrange(desc(adjr))\n\n   Index N                              Predictors     R-Square Adj. R-Square\n1     11 3              Width_in Height_in Surface 0.0260749939  0.0251349118\n2     15 4 Width_in Height_in Surface artistliving 0.0263412027  0.0250876993\n3      5 2                      Width_in Height_in 0.0256902566  0.0250634893\n4     12 3         Width_in Height_in artistliving 0.0259732581  0.0250330779\n5      6 2                        Width_in Surface 0.0249136264  0.0242863596\n6     13 3           Width_in Surface artistliving 0.0251787948  0.0242378477\n7      7 2                   Width_in artistliving 0.0212864021  0.0206568018\n8      1 1                                Width_in 0.0209415833  0.0206267736\n9      8 2                    Surface artistliving 0.0132088377  0.0125897717\n10     2 1                                 Surface 0.0125899681  0.0122803381\n11    14 3          Height_in Surface artistliving 0.0130836930  0.0121310711\n12     9 2                       Height_in Surface 0.0126782901  0.0120431523\n13    10 2                  Height_in artistliving 0.0062698155  0.0056305552\n14     3 1                               Height_in 0.0058797727  0.0055601199\n15     4 1                            artistliving 0.0005531617  0.0002397573\n   Mallow's Cp\n1     3.849487\n2     5.000000\n3     3.077206\n4     4.174132\n5     5.555476\n6     6.709309\n7    17.130153\n8    16.230489\n9    51.971190\n10   52.001268\n11   45.305459\n12   44.599123\n13   65.048926\n14   64.293574\n15   91.485605",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/09-mlr.html#recap",
    "href": "content/lectures/09-mlr.html#recap",
    "title": "09-mlr",
    "section": "Recap",
    "text": "Recap\n\nCan you model and interpret linear models with multiple predictors?\nCan you explain the difference in a model with main effects vs. interaction effects?\nCan you compare different models and determine how to proceed?\nCan you carry out and explain backward selection?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "09-mlr"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#qa",
    "href": "content/lectures/14-tidymodels-slides.html#qa",
    "title": "14-tidymodels",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I had a question about the presentations for the final projects; since it is due during finals week, is it a live presentation in class or do we submit a video? If it is a live presentation, do we present during our designated final day/time on webreg?\nA: Video submission!\n\n\nQ: I also wanted to mention that the mid/pre-course extra credit surveys doesn’t reflect a change in grade on canvas. (For ex. if i put a 0 or 100 for E.C my grade stays the same).\nA: Correct - I add these in at the end. Canvas can do many things, but it doesn’t handle EC well (from what I can tell).\n\n\nQ: I’m overwhelmed/confused by “the code :’) it’s quite a bit to take in”\nA: Yes! It’s a lot! This is why we have group mates on the case study. I encourage everyone to sit with the code after class and then work through it together as you complete the case study!\n\n\nQ: For oral fluid you mentioned looking more into why there’s that big dip in specificity and that we should look more into that on Friday with eda but would that be slightly guided because I have no idea where to start with that.\nA: I would make some plots that specifically look at the data/numbers there to figure out what could be leading to that drop at that particular time window.\n\n\nQ: Why are specificity graphs so high?  A: Good question - this is generally b/c people who didn’t smoke have values very close to zero across compounds…so they will rarely be above the cutoff, making this very effective at identifying individuals who did not smoke\n\n\nQ: What is the dplyr::select notation, like is it a way to use select from dplyr without librarying first?\nA: Yes!\n\n\nQ: Also separate topic, but do we have information on impairment so we can account for that with recent use?  A: Great question - impairment is very hard to define here. We (the researchers) have data on self-reported high and what the police officers determined, but y’all don’t have that data. So, we’re using knowledge from other studies (see 11-cs01-data notes) to understand what we know on impoairment but only focusing on detecting recent use here.\n\n\nQ: I am unable to locate where to sign up for groups for the final project\nA: This form was just released (sorry for delay). link to survey\n\n\nQ: I think I need more time to digest how the code works together to produce the visuals that we saw.\nA: I agree. I think I could balance and give more time in class…but I will say this is an exercise I want groups to work through together!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#course-announcements",
    "href": "content/lectures/14-tidymodels-slides.html#course-announcements",
    "title": "14-tidymodels",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nNo class This Th; No Lab this Fri (Happy Thanksgiving!)\nCS01 due Monday 11/27\n\ngroup work survey due Tues 11/28\n\n\n\nNotes:\n\nBe sure you watch the video from last Thursday on Canvas\nAny questions about CS01?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#agenda",
    "href": "content/lectures/14-tidymodels-slides.html#agenda",
    "title": "14-tidymodels",
    "section": "Agenda",
    "text": "Agenda\n\nmachine learning intro\n(re)introduce tidymodels\nworked example: ML in tidymodels",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#suggested-resources",
    "href": "content/lectures/14-tidymodels-slides.html#suggested-resources",
    "title": "14-tidymodels",
    "section": "Suggested Resources",
    "text": "Suggested Resources\n\nThe package itself has some worked examples: https://www.tidymodels.org/start/models/\nThere’s a whole book (written by the developer of tidymodels) that covers the tidymodels package: https://www.tmwr.org/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-philosophy",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-philosophy",
    "title": "14-tidymodels",
    "section": "tidymodels: philosophy",
    "text": "tidymodels: philosophy\n\n“Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: pre-processing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret. The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do.” - Max Kuhn (tidymodels developer)\n\n\nBenefits:\n\nStandardized workflow/format/notation across different types of machine learning algorithms\nCan easily modify pre-processing, algorithm choice, and hyper-parameter tuning making optimization easy",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-ecosystem",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-ecosystem",
    "title": "14-tidymodels",
    "section": "tidymodels: ecosystem",
    "text": "tidymodels: ecosystem\nThe main packages (and their roles):",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#machine-learning-intro",
    "href": "content/lectures/14-tidymodels-slides.html#machine-learning-intro",
    "title": "14-tidymodels",
    "section": "Machine Learning: intro",
    "text": "Machine Learning: intro\nIn intro stats, you should have learned the central dogma of statistics: we sample from a population\n\n\nThe data from the sample are used to make an inference about the population:\n\n\n\nFor prediction, we have a similar sampling problem:\n\n\n\nBut now we are trying to build a rule that can be used to predict a single observation’s value of some characteristic using characteristics of the other observations.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#ml-the-goal",
    "href": "content/lectures/14-tidymodels-slides.html#ml-the-goal",
    "title": "14-tidymodels",
    "section": "ML: the goal",
    "text": "ML: the goal\nThe goal is to:\nbuild a machine learning algorithm\n\nthat uses features as input\n\n\nand predicts an outcome variable\n\n\nin the situation where we do not know the outcome variable.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#classic-ml",
    "href": "content/lectures/14-tidymodels-slides.html#classic-ml",
    "title": "14-tidymodels",
    "section": "Classic ML",
    "text": "Classic ML\nTypically, you use data where you have both the input and output data to train a machine learning algorithm.\n\nWhat you need:\n\n\nA data set to train from.\nAn algorithm or set of algorithms you can use to try values of \\(f\\).\nA distance metric \\(d\\) for measuring how close \\(Y\\) is to \\(\\hat{Y}\\).\nA definition of what a “good” distance is.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-for-ml",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-for-ml",
    "title": "14-tidymodels",
    "section": "tidymodels for ML",
    "text": "tidymodels for ML\nHow these packages fit together for carrying out machine learning:",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#tidymodels-steps",
    "href": "content/lectures/14-tidymodels-slides.html#tidymodels-steps",
    "title": "14-tidymodels",
    "section": "tidymodels: steps",
    "text": "tidymodels: steps",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels-slides.html#recap",
    "href": "content/lectures/14-tidymodels-slides.html#recap",
    "title": "14-tidymodels",
    "section": "Recap",
    "text": "Recap\n\nCan you describe the basics of machine learning?\nCan you describe the goals of and general steps in tidymodels?\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html",
    "href": "content/lectures/14-tidymodels.html",
    "title": "14-tidymodels",
    "section": "",
    "text": "Q: I had a question about the presentations for the final projects; since it is due during finals week, is it a live presentation in class or do we submit a video? If it is a live presentation, do we present during our designated final day/time on webreg?\nA: Video submission!\n\n\nQ: I also wanted to mention that the mid/pre-course extra credit surveys doesn’t reflect a change in grade on canvas. (For ex. if i put a 0 or 100 for E.C my grade stays the same).\nA: Correct - I add these in at the end. Canvas can do many things, but it doesn’t handle EC well (from what I can tell).\n\n\nQ: I’m overwhelmed/confused by “the code :’) it’s quite a bit to take in”\nA: Yes! It’s a lot! This is why we have group mates on the case study. I encourage everyone to sit with the code after class and then work through it together as you complete the case study!\n\n\nQ: For oral fluid you mentioned looking more into why there’s that big dip in specificity and that we should look more into that on Friday with eda but would that be slightly guided because I have no idea where to start with that.\nA: I would make some plots that specifically look at the data/numbers there to figure out what could be leading to that drop at that particular time window.\n\n\nQ: Why are specificity graphs so high?  A: Good question - this is generally b/c people who didn’t smoke have values very close to zero across compounds…so they will rarely be above the cutoff, making this very effective at identifying individuals who did not smoke\n\n\nQ: What is the dplyr::select notation, like is it a way to use select from dplyr without librarying first?\nA: Yes!\n\n\nQ: Also separate topic, but do we have information on impairment so we can account for that with recent use?  A: Great question - impairment is very hard to define here. We (the researchers) have data on self-reported high and what the police officers determined, but y’all don’t have that data. So, we’re using knowledge from other studies (see 11-cs01-data notes) to understand what we know on impoairment but only focusing on detecting recent use here.\n\n\nQ: I am unable to locate where to sign up for groups for the final project\nA: This form was just released (sorry for delay). link to survey\n\n\nQ: I think I need more time to digest how the code works together to produce the visuals that we saw.\nA: I agree. I think I could balance and give more time in class…but I will say this is an exercise I want groups to work through together!\n\n\n\n\nDue Dates:\n\nNo class This Th; No Lab this Fri (Happy Thanksgiving!)\nCS01 due Monday 11/27\n\ngroup work survey due Tues 11/28\n\n\n. . .\nNotes:\n\nBe sure you watch the video from last Thursday on Canvas\nAny questions about CS01?\n\n\n\n\n\nmachine learning intro\n(re)introduce tidymodels\nworked example: ML in tidymodels\n\n\n\n\n\nThe package itself has some worked examples: https://www.tidymodels.org/start/models/\nThere’s a whole book (written by the developer of tidymodels) that covers the tidymodels package: https://www.tmwr.org/\n\n\n\n\n\n“Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: pre-processing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret. The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do.” - Max Kuhn (tidymodels developer)\n\n. . .\nBenefits:\n\nStandardized workflow/format/notation across different types of machine learning algorithms\nCan easily modify pre-processing, algorithm choice, and hyper-parameter tuning making optimization easy\n\n\n\n\nThe main packages (and their roles):\n\n\n\n\n\n\nIn intro stats, you should have learned the central dogma of statistics: we sample from a population\n\n. . .\nThe data from the sample are used to make an inference about the population:\n\n. . .\nFor prediction, we have a similar sampling problem:\n\n. . .\nBut now we are trying to build a rule that can be used to predict a single observation’s value of some characteristic using characteristics of the other observations.\n\n\n\n\nThe goal is to:\nbuild a machine learning algorithm\n. . .\nthat uses features as input\n. . .\nand predicts an outcome variable\n. . .\nin the situation where we do not know the outcome variable.\n\n\n\nTypically, you use data where you have both the input and output data to train a machine learning algorithm.\n. . .\nWhat you need:\n\n\nA data set to train from.\nAn algorithm or set of algorithms you can use to try values of \\(f\\).\nA distance metric \\(d\\) for measuring how close \\(Y\\) is to \\(\\hat{Y}\\).\nA definition of what a “good” distance is.\n\n\n\n\n\nHow these packages fit together for carrying out machine learning:\n\n\n\n\n\n\n\n\n\nCan you describe the basics of machine learning?\nCan you describe the goals of and general steps in tidymodels?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#qa",
    "href": "content/lectures/14-tidymodels.html#qa",
    "title": "14-tidymodels",
    "section": "",
    "text": "Q: I had a question about the presentations for the final projects; since it is due during finals week, is it a live presentation in class or do we submit a video? If it is a live presentation, do we present during our designated final day/time on webreg?\nA: Video submission!\n\n\nQ: I also wanted to mention that the mid/pre-course extra credit surveys doesn’t reflect a change in grade on canvas. (For ex. if i put a 0 or 100 for E.C my grade stays the same).\nA: Correct - I add these in at the end. Canvas can do many things, but it doesn’t handle EC well (from what I can tell).\n\n\nQ: I’m overwhelmed/confused by “the code :’) it’s quite a bit to take in”\nA: Yes! It’s a lot! This is why we have group mates on the case study. I encourage everyone to sit with the code after class and then work through it together as you complete the case study!\n\n\nQ: For oral fluid you mentioned looking more into why there’s that big dip in specificity and that we should look more into that on Friday with eda but would that be slightly guided because I have no idea where to start with that.\nA: I would make some plots that specifically look at the data/numbers there to figure out what could be leading to that drop at that particular time window.\n\n\nQ: Why are specificity graphs so high?  A: Good question - this is generally b/c people who didn’t smoke have values very close to zero across compounds…so they will rarely be above the cutoff, making this very effective at identifying individuals who did not smoke\n\n\nQ: What is the dplyr::select notation, like is it a way to use select from dplyr without librarying first?\nA: Yes!\n\n\nQ: Also separate topic, but do we have information on impairment so we can account for that with recent use?  A: Great question - impairment is very hard to define here. We (the researchers) have data on self-reported high and what the police officers determined, but y’all don’t have that data. So, we’re using knowledge from other studies (see 11-cs01-data notes) to understand what we know on impoairment but only focusing on detecting recent use here.\n\n\nQ: I am unable to locate where to sign up for groups for the final project\nA: This form was just released (sorry for delay). link to survey\n\n\nQ: I think I need more time to digest how the code works together to produce the visuals that we saw.\nA: I agree. I think I could balance and give more time in class…but I will say this is an exercise I want groups to work through together!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#course-announcements",
    "href": "content/lectures/14-tidymodels.html#course-announcements",
    "title": "14-tidymodels",
    "section": "",
    "text": "Due Dates:\n\nNo class This Th; No Lab this Fri (Happy Thanksgiving!)\nCS01 due Monday 11/27\n\ngroup work survey due Tues 11/28\n\n\n. . .\nNotes:\n\nBe sure you watch the video from last Thursday on Canvas\nAny questions about CS01?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#agenda",
    "href": "content/lectures/14-tidymodels.html#agenda",
    "title": "14-tidymodels",
    "section": "",
    "text": "machine learning intro\n(re)introduce tidymodels\nworked example: ML in tidymodels",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#suggested-resources",
    "href": "content/lectures/14-tidymodels.html#suggested-resources",
    "title": "14-tidymodels",
    "section": "",
    "text": "The package itself has some worked examples: https://www.tidymodels.org/start/models/\nThere’s a whole book (written by the developer of tidymodels) that covers the tidymodels package: https://www.tmwr.org/",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#tidymodels-philosophy",
    "href": "content/lectures/14-tidymodels.html#tidymodels-philosophy",
    "title": "14-tidymodels",
    "section": "",
    "text": "“Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: pre-processing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret. The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do.” - Max Kuhn (tidymodels developer)\n\n. . .\nBenefits:\n\nStandardized workflow/format/notation across different types of machine learning algorithms\nCan easily modify pre-processing, algorithm choice, and hyper-parameter tuning making optimization easy",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#tidymodels-ecosystem",
    "href": "content/lectures/14-tidymodels.html#tidymodels-ecosystem",
    "title": "14-tidymodels",
    "section": "",
    "text": "The main packages (and their roles):",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#machine-learning-intro",
    "href": "content/lectures/14-tidymodels.html#machine-learning-intro",
    "title": "14-tidymodels",
    "section": "",
    "text": "In intro stats, you should have learned the central dogma of statistics: we sample from a population\n\n. . .\nThe data from the sample are used to make an inference about the population:\n\n. . .\nFor prediction, we have a similar sampling problem:\n\n. . .\nBut now we are trying to build a rule that can be used to predict a single observation’s value of some characteristic using characteristics of the other observations.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#ml-the-goal",
    "href": "content/lectures/14-tidymodels.html#ml-the-goal",
    "title": "14-tidymodels",
    "section": "",
    "text": "The goal is to:\nbuild a machine learning algorithm\n. . .\nthat uses features as input\n. . .\nand predicts an outcome variable\n. . .\nin the situation where we do not know the outcome variable.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#classic-ml",
    "href": "content/lectures/14-tidymodels.html#classic-ml",
    "title": "14-tidymodels",
    "section": "",
    "text": "Typically, you use data where you have both the input and output data to train a machine learning algorithm.\n. . .\nWhat you need:\n\n\nA data set to train from.\nAn algorithm or set of algorithms you can use to try values of \\(f\\).\nA distance metric \\(d\\) for measuring how close \\(Y\\) is to \\(\\hat{Y}\\).\nA definition of what a “good” distance is.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#tidymodels-for-ml",
    "href": "content/lectures/14-tidymodels.html#tidymodels-for-ml",
    "title": "14-tidymodels",
    "section": "",
    "text": "How these packages fit together for carrying out machine learning:",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/14-tidymodels.html#recap",
    "href": "content/lectures/14-tidymodels.html#recap",
    "title": "14-tidymodels",
    "section": "",
    "text": "Can you describe the basics of machine learning?\nCan you describe the goals of and general steps in tidymodels?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "14-tidymodels"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#agenda",
    "href": "content/lectures/00-welcome-slides.html#agenda",
    "title": "00-welcome",
    "section": "Agenda",
    "text": "Agenda\n\nDescribe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-r",
    "href": "content/lectures/00-welcome-slides.html#what-is-r",
    "title": "00-welcome",
    "section": "What is R?",
    "text": "What is R?\n : R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-data-science",
    "href": "content/lectures/00-welcome-slides.html#what-is-data-science",
    "title": "00-welcome",
    "section": "What is data science?",
    "text": "What is data science?\n: Data science is the scientific process of using data to answer interesting questions and/or solve important problems.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#practical-data-science-in-r",
    "href": "content/lectures/00-welcome-slides.html#practical-data-science-in-r",
    "title": "00-welcome",
    "section": "Practical Data Science in R",
    "text": "Practical Data Science in R\n\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#who-am-i",
    "href": "content/lectures/00-welcome-slides.html#who-am-i",
    "title": "00-welcome",
    "section": "Who am I?",
    "text": "Who am I?\nShannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com    MOS 0204     Tu/Th 2-3:20PM (Lab: Fri 3-3:50PM)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#who-all-is-involved",
    "href": "content/lectures/00-welcome-slides.html#who-all-is-involved",
    "title": "00-welcome",
    "section": "Who all is involved?",
    "text": "Who all is involved?\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11A-12P\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTime TBD\nLocation TBD\n\n\nIAs\nShenova Davis\n\nTime TBD\nLocation TBD",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#course-staff",
    "href": "content/lectures/00-welcome-slides.html#course-staff",
    "title": "00-welcome",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\nKunal Rustagi (TA)\nShenova Davis (IA)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-this-course",
    "href": "content/lectures/00-welcome-slides.html#what-is-this-course",
    "title": "00-welcome",
    "section": "What is this course?",
    "text": "What is this course?\nEverything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#soi-dont-have-to-know-how-to-program-already",
    "href": "content/lectures/00-welcome-slides.html#soi-dont-have-to-know-how-to-program-already",
    "title": "00-welcome",
    "section": "So…I don’t have to know how to program already?",
    "text": "So…I don’t have to know how to program already?\n\n\n\n\nNope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-general-plan",
    "href": "content/lectures/00-welcome-slides.html#the-general-plan",
    "title": "00-welcome",
    "section": "The General Plan",
    "text": "The General Plan\n\nWeeks 1-4: Learn to program in the tidyverse in R\nWeeks 5-10: Communication, Data Analysis, Statistics, & Case Studies (two Case Studies)\n\n\nNote: This course is back-loaded. But, that’s when group work happens.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-nitty-gritty",
    "href": "content/lectures/00-welcome-slides.html#the-nitty-gritty",
    "title": "00-welcome",
    "section": "The Nitty Gritty",
    "text": "The Nitty Gritty\n\nLectureIn-personWaitlistLab & OHMaterials\n\n\nClass Meetings\n\nInteractive\nLectures & lots of learn-by-doing\nBring your laptop to class every day\n\n\n\nIn-person, synchronous learning\n\nI will be teaching (so long as I’m healthy and have child care) in person.\nLectures and lab will be podcast.\nAttendance will be incentivized using a daily participation survey.\nIf you’re not feeling well, please stay home. I will do the same.\nExam will be take-home.\n\n\n\nThe (Dreaded) Waitlist\n\nCourse enrollment is supposed to be 50 for this course\nThere are 72 people currently enrolled\nI don’t control the waitlist (cogsadvising@ucsd.edu does)\nI’d anticipate our staff adding 3-5 people from the waitlist (but cannot guarantee this)\n\n\n\nLab & Office Hours\n\nOffice hours begin week 1\n\nProf: Tu: 3:30-4:30 (drop-in); W 11-12 (10 min slots; appt.)\n\nLab begins week 1 (next Friday)\n\nit’s not in a computer lab, so you’ll need to bring your own\ndetails about labs covered on Tues and in lab\ntypically labs will be released Monday and due Friday\n\nI will hang out after class today for questions/concerns from students\n\n\n\nCourse Materials\n\nTextbooks are free and available online\nCourse platforms:\n\nWebsite : schedule, policies, due dates, etc.\nGitHub : retrieving assignments, labs, exams, etc.\ndatahub : completing assignments, labs, exams etc.\nCanvas : grades, course-specific links\nPiazza : Q&A",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#diversity-inclusion",
    "href": "content/lectures/00-welcome-slides.html#diversity-inclusion",
    "title": "00-welcome",
    "section": "Diversity & Inclusion:",
    "text": "Diversity & Inclusion:\nGoal: every student be well-served by this course\n\nPhilosophy: The diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding.\n\n\nPlan: Present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture)\n\n\nBut… if I ever fall short or if you ever have suggestions for improvement, please do share with me! There is also an anonymous Google Form if you’re more comfortable there.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#a-new-ish-course",
    "href": "content/lectures/00-welcome-slides.html#a-new-ish-course",
    "title": "00-welcome",
    "section": "A new-ish course!",
    "text": "A new-ish course!\n\nOffered twice previously\nIf something doesn’t make sense, tell me!\nIf you’ve got feedback/suggestions, I’m all ears!\n\n\nChanges since last iteration (based on feedback):\n\nspread out second half\nlikely changing the heaviness of a case study\nadd in communication to public portion\none fewer HW assignments",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-to-get-help",
    "href": "content/lectures/00-welcome-slides.html#how-to-get-help",
    "title": "00-welcome",
    "section": "How to get help",
    "text": "How to get help\n\nLab\nOffice Hours\nPiazza\n\n\nA few (Piazza) guidelines:\n1. No duplicates.\n2. Public posts are best.\n3. Posts should include your question, what you've tried so far, & resources used.\n4. Helping others is encouraged.\n5. No assignment code in public posts.\n6. We're not robots.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-r-community",
    "href": "content/lectures/00-welcome-slides.html#the-r-community",
    "title": "00-welcome",
    "section": " The R Community",
    "text": "The R Community\n\n\n\nR Rollercoaster\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#academic-integrity",
    "href": "content/lectures/00-welcome-slides.html#academic-integrity",
    "title": "00-welcome",
    "section": "Academic integrity",
    "text": "Academic integrity\nDon’t cheat.\n\nTeamwork is allowed, but you should be able to answer “Yes” to each of the following:\n\nCan I explain each piece of code and each analysis carried out in what I’m submitting?\nCould I reproduce this code/analysis on my own?\n\n\n\nThe Internet is a great resource. Cite your sources.\n\n\nTeamwork is not allowed on your midterm. It is open-notes and open-Google/ChatGPT. You cannot discuss the questions on the exam with anyone.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#when-to-can-i-use-chatgptllms",
    "href": "content/lectures/00-welcome-slides.html#when-to-can-i-use-chatgptllms",
    "title": "00-welcome",
    "section": "When To (Can I) Use ChatGPT/LLMs?",
    "text": "When To (Can I) Use ChatGPT/LLMs?\nFor anything in this course.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-to-use-chatgptllms",
    "href": "content/lectures/00-welcome-slides.html#how-to-use-chatgptllms",
    "title": "00-welcome",
    "section": "How To Use ChatGPT/LLMs",
    "text": "How To Use ChatGPT/LLMs\nProbably never first or right away.\n\nTo learn: Think first. Try first. Then use external resources.\n\n\nAlways read/think about/understand the output.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#chatgpt-what-to-avoid",
    "href": "content/lectures/00-welcome-slides.html#chatgpt-what-to-avoid",
    "title": "00-welcome",
    "section": "ChatGPT: What to Avoid",
    "text": "ChatGPT: What to Avoid\n\n\nOver-reliance (thwarts learning)\nHaving to look everything up (wastes time)\nLeaving tasks to the last minute (can lead to bad decisions/academic integrity issues)\nTaking the output without thinking (thwarts learning; limits critical thinking practice)\nUsing it right away for brainstorming ideas (limits ideas generated)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#course-components",
    "href": "content/lectures/00-welcome-slides.html#course-components",
    "title": "00-welcome",
    "section": "Course components:",
    "text": "Course components:\n\n\nLabs (8): Individual submission; graded on effort\nHomework (3): Individual submission; graded on correctness\nExam (1): Individual completion & submission, take-home midterm\nCase Studies (2): Team submission, technical analysis report\nFinal Project (1) : Team submission, due Tues of finals week",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#grading",
    "href": "content/lectures/00-welcome-slides.html#grading",
    "title": "00-welcome",
    "section": "Grading",
    "text": "Grading\nYour final grade will be comprised of the following:\n\n\n\nAssignment (#)\n% of grade\n\n\n\n\nLabs (8)\n16%\n\n\nHomework (3)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal project* (1)\n17%\n\n\n\n* indicates group submission",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#latemissed-work-policy",
    "href": "content/lectures/00-welcome-slides.html#latemissed-work-policy",
    "title": "00-welcome",
    "section": "Late/missed work policy",
    "text": "Late/missed work policy\n\nHomework and case study projects: accepted up to 3 days (72 hours) after the assigned deadline for a 25% deduction\nNo late deadlines for labs, the exam, or the final project\n\n\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#datahub",
    "href": "content/lectures/00-welcome-slides.html#datahub",
    "title": "00-welcome",
    "section": "Datahub",
    "text": "Datahub\nDatahub is a platform hosted by UCSD that gives students access to computational resources.\nThis means that while you’ll be typing on your keyboard, you’ll be using UCSD’s computers in this class.\nWebsite: https://datahub.ucsd.edu/\n\nLaunch Environment\nWhen working on “stuff” for this course, select the COGS 137 environment.\n ## Datahub Usage\nQ: Do I have to use datahub?\nA: Nope. You could download and install all the packages we use and complete the course locally! However, many packages have already been installed for you on datahub, so it will be a tiny bit more work up front…but you won’t be dependent on the internet/datahub!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#toolkit",
    "href": "content/lectures/00-welcome-slides.html#toolkit",
    "title": "00-welcome",
    "section": "Toolkit",
    "text": "Toolkit\n\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub\nThe Internet (Google/ChatGPT/etc.)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-and-rstudio",
    "href": "content/lectures/00-welcome-slides.html#r-and-rstudio",
    "title": "00-welcome",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR/RStudioTourTryR packages\n\n\nR & RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integreated development environment, IDE)\n\n\n\n\n[DEMO]\n\nConcepts introduced:\n\nConsole\nUsing R as a calculator\nEnvironment\nLoading and viewing a data frame\nAccessing a variable in a data frame\nR functions\n\n\n\nYour Turn\n\nLogin to datahub\nCarry out a mathematical operation in the console\nView the airquality dataframe\nAccess a column from the airquality dataframe\nCalculate the median for one of the numeric columns\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\nPackages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data 1\nAs of Sept 2023, there are ~19,941 R packages available on CRAN (the Comprehensive R Archive Network)2\nWe’re going to work with a small (but important) subset of these!\n\n\n\n\nWickham and Bryan, R PackagesCRAN contributed packages",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-the-tidyverse",
    "href": "content/lectures/00-welcome-slides.html#what-is-the-tidyverse",
    "title": "00-welcome",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\n\n\n\n\n\ntidyverse.org\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying philosophy and a common syntax.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#rstudio-projects3",
    "href": "content/lectures/00-welcome-slides.html#rstudio-projects3",
    "title": "00-welcome",
    "section": "RStudio Projects1",
    "text": "RStudio Projects1\n\nBuilt-in functionality to keep all files for a single project organized\n\nRStudio Projects Documentation",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-markdown",
    "href": "content/lectures/00-welcome-slides.html#r-markdown",
    "title": "00-welcome",
    "section": "R Markdown",
    "text": "R Markdown\n\nFully reproducible reports – each time you knit, the document is executed from top to bottom\nSimple markdown syntax for text\nCode goes in chunks, defined by three backticks, narrative goes outside of chunks",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-markdown-tips",
    "href": "content/lectures/00-welcome-slides.html#r-markdown-tips",
    "title": "00-welcome",
    "section": "R Markdown tips",
    "text": "R Markdown tips\n\nKeep the R Markdown cheat sheet and Markdown Quick Reference (Help -&gt; Markdown Quick Reference) handy, we’ll refer to it often as the course progresses\nThe workspace of your R Markdown document is separate from the Console\n\n\n\n[DEMO]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-will-we-use-r-markdown",
    "href": "content/lectures/00-welcome-slides.html#how-will-we-use-r-markdown",
    "title": "00-welcome",
    "section": "How will we use R Markdown?",
    "text": "How will we use R Markdown?\n\nEvery lab / midterm / project / homework / notes / etc. is an R Markdown document\nYou’ll always have a template R Markdown document to start with\nThe amount of scaffolding in the template will decrease over the quarter",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#collaboration-git-github",
    "href": "content/lectures/00-welcome-slides.html#collaboration-git-github",
    "title": "00-welcome",
    "section": "Collaboration: Git & GitHub",
    "text": "Collaboration: Git & GitHub\n\nThe statistical programming language we’ll use is R\nThe software we use to interface with R is RStudio\nBut how do I get you the course materials that you can build on for your assignments?\n\nI’m not going to email you documents, that would be a mess!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#version-control",
    "href": "content/lectures/00-welcome-slides.html#version-control",
    "title": "00-welcome",
    "section": "Version control",
    "text": "Version control\n\nWe introduced GitHub as a platform for collaboration\nBut it’s much more than that…\nIt’s actually designed for version control",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#versioning",
    "href": "content/lectures/00-welcome-slides.html#versioning",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\n\nLego versions",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#versioning-1",
    "href": "content/lectures/00-welcome-slides.html#versioning-1",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\nwith human readable messages\n\nLego versions with commit messages",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#why-do-we-need-version-control",
    "href": "content/lectures/00-welcome-slides.html#why-do-we-need-version-control",
    "title": "00-welcome",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\nPhD Comics",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#git-and-github-tips",
    "href": "content/lectures/00-welcome-slides.html#git-and-github-tips",
    "title": "00-welcome",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\nGit is a version control system – like “Track Changes” feature Google Docs…but optimized for code. GitHub is the home for your Git-based projects on the internet – like Drive with additional features for code.\n\n\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\n\n\nWe will be doing Git things and interfacing with GitHub through RStudio, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\n\n\n\n\nResource: happygitwithr.com: book for working with git in R; Some content is beyond the scope of this course, but it’s a good resource",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#lets-take-a-tour-git-github",
    "href": "content/lectures/00-welcome-slides.html#lets-take-a-tour-git-github",
    "title": "00-welcome",
    "section": "Let’s take a tour – Git / GitHub",
    "text": "Let’s take a tour – Git / GitHub\nWe’ll cover this time permitting, you’ll see it again in lab this week\nConcepts introduced:\n\nConnect an R project to Github repository\nWorking with a local and remote repository\nCommitting, Pushing and Pulling\n\nThere is a bit more of GitHub that we’ll use in this class, but for today this is enough.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#documentation",
    "href": "content/lectures/00-welcome-slides.html#documentation",
    "title": "00-welcome",
    "section": "Documentation",
    "text": "Documentation\nConsider ggplot2 (a package we’ll learn a lot)\n\n\nOfficial documentation (CRAN): https://cran.r-project.org/web/packages/ggplot2/index.html\nCode (Github): https://github.com/tidyverse/ggplot2\nDocumentation: https://ggplot2.tidyverse.org/reference/index.html\nSpecific Function: https://ggplot2.tidyverse.org/reference/geom_point.html",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#chatgpt-what-it-could-look-like",
    "href": "content/lectures/00-welcome-slides.html#chatgpt-what-it-could-look-like",
    "title": "00-welcome",
    "section": "ChatGPT: What it could look like",
    "text": "ChatGPT: What it could look like\nImagine: You’ve been asked to carry out a number of wrangling operations on a dataset and make a plot…\n\n[DEMO]",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#additional-help",
    "href": "content/lectures/00-welcome-slides.html#additional-help",
    "title": "00-welcome",
    "section": "Additional help",
    "text": "Additional help\n\nclassmates\ncourse staff (OH, Piazza, class, lab)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#recap",
    "href": "content/lectures/00-welcome-slides.html#recap",
    "title": "00-welcome",
    "section": "Recap",
    "text": "Recap\nCan you answer these questions?\n\nWhat is R vs RStudio?\nWhat are RStudio Projects?\nWhat is version control, and why do we care?\nWhat is git vs GitHub (and do I need to care)?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#additional-git-resources",
    "href": "content/lectures/00-welcome-slides.html#additional-git-resources",
    "title": "00-welcome",
    "section": "Additional git Resources",
    "text": "Additional git Resources\nVersion Control (git and GitHub):\n\nGetting Started with git\nGitHub Guide\nGitHub Desktop App Tutorial\nGit Command Line Resource\nUsing git from the command line\n\nInstalling and using git (Part 1), by COGS 108 TA Ganesh (youtube, 22min tutorial)\nmerge conflicts and branching (Part 2), by IA Shubham Kulkarni (youtube, 8min tutorial)\n\nUsing git with GitHub Desktop, by COGS 108 TA Sidharth Suresh (youtube, 13min tutorial)\nGIT & GITHUB TUTORIAL, from edureka!\n\nwith notes from COGS 18/108 TA Holly(Yueying) Dong",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#slides-to-pdf",
    "href": "content/lectures/00-welcome-slides.html#slides-to-pdf",
    "title": "00-welcome",
    "section": "Slides to PDF",
    "text": "Slides to PDF\n\nToggle into Print View using the Esc key (or using the Navigation Menu)\nOpen the in-browser print dialog (CTRL/CMD+P).\nChange the Destination setting to Save as PDF.\nChange the Layout to Landscape.\nChange the Margins to None.\nEnable the Background graphics option.\nClick Save 🎉\n\n\n\nInstructions from quarto documentation",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster &lt;- read_sheet('10kG09t5Uvjy2zLt4sToHvveBRnqYXTwfaAhPpgEFr3s')\n\nggplot(roster, aes(x = College)) +\n  geom_bar() +\n  labs(title = \"COGS 137\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nNote: This code will not run for you because you don’t have access to the roster for this course.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class-1",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class-1",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |&gt;\n  mutate(major = substr(Major, 1, 2)) |&gt;\n  ggplot(aes(fct_infreq(major))) + \n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Major\") +\n  theme_bw(base_size = 12) + \n  theme(plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class-2",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class-2",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |&gt;\n  ggplot(aes(fct_relevel(Level, \"SO\", \"JR\", \"SR\"))) +\n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Level\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\nWarning: 1 unknown level in `f`: SO\n1 unknown level in `f`: SO",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#id-like-to-know-more",
    "href": "content/lectures/00-welcome-slides.html#id-like-to-know-more",
    "title": "00-welcome",
    "section": "I’d like to know more!",
    "text": "I’d like to know more!\n(required)Student Survey - complete by Tuesday at 11:59 PM.\nThis is required and completion will be used for CAA/#finaid. DO complete this even if you’re on the waitlist, please.\n\n(optional) Daily Post-Lecture Feedback\n\nopportunity to reflect on learning\nopportunity to ask questions (I will read and answer these.)\nopportunity for extra credit on final project\n\n\n\n\n\n\nhttps://cogs137.github.io/website/\n\n\n\n\nNote: Links to both surveys are also on Canvas. I will try to remind you at the end of lecture, but I’ll probably forget. Feel free to remind me/one another!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html",
    "href": "content/lectures/00-welcome.html",
    "title": "00-welcome",
    "section": "",
    "text": "Practical Data Science in R\nPlease take one green sticky and one pink sticky as they come around. If you’re able, try and save these. We’ll use them most classes. (But, I’ll always have extra!)\n\n\n\n\n\n\n\n\nDescribe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub\n\n\n\n\n : R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data.\n\n\n\n: Data science is the scientific process of using data to answer interesting questions and/or solve important problems.\n\n\n\n\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports\n\n\n\n\n\nShannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com    MOS 0204     Tu/Th 2-3:20PM (Lab: Fri 3-3:50PM)\n\n\n\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11A-12P\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTime TBD\nLocation TBD\n\n\nIAs\nShenova Davis\n\nTime TBD\nLocation TBD\n\n\n\n\n\n\n\n\n\nKunal Rustagi (TA)\nShenova Davis (IA)\n\n\n\n\n\n\n\n\n\n\n\n\nEverything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!\n\n\n\n\n\n\n\n\n\nNope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#agenda",
    "href": "content/lectures/00-welcome.html#agenda",
    "title": "00-welcome",
    "section": "",
    "text": "Describe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#what-is-r",
    "href": "content/lectures/00-welcome.html#what-is-r",
    "title": "00-welcome",
    "section": "",
    "text": ": R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#what-is-data-science",
    "href": "content/lectures/00-welcome.html#what-is-data-science",
    "title": "00-welcome",
    "section": "",
    "text": ": Data science is the scientific process of using data to answer interesting questions and/or solve important problems.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#practical-data-science-in-r",
    "href": "content/lectures/00-welcome.html#practical-data-science-in-r",
    "title": "00-welcome",
    "section": "",
    "text": "Program at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#who-am-i",
    "href": "content/lectures/00-welcome.html#who-am-i",
    "title": "00-welcome",
    "section": "",
    "text": "Shannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com    MOS 0204     Tu/Th 2-3:20PM (Lab: Fri 3-3:50PM)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#who-all-is-involved",
    "href": "content/lectures/00-welcome.html#who-all-is-involved",
    "title": "00-welcome",
    "section": "",
    "text": "Instructor\nShannon Ellis\nsellis@ucsd.edu\nWed 11A-12P\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nKunal Rustagi\n\nTime TBD\nLocation TBD\n\n\nIAs\nShenova Davis\n\nTime TBD\nLocation TBD",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#course-staff",
    "href": "content/lectures/00-welcome.html#course-staff",
    "title": "00-welcome",
    "section": "",
    "text": "Kunal Rustagi (TA)\nShenova Davis (IA)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#what-is-this-course",
    "href": "content/lectures/00-welcome.html#what-is-this-course",
    "title": "00-welcome",
    "section": "",
    "text": "Everything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#soi-dont-have-to-know-how-to-program-already",
    "href": "content/lectures/00-welcome.html#soi-dont-have-to-know-how-to-program-already",
    "title": "00-welcome",
    "section": "",
    "text": "Nope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-general-plan",
    "href": "content/lectures/00-welcome.html#the-general-plan",
    "title": "00-welcome",
    "section": "The General Plan",
    "text": "The General Plan\n\nWeeks 1-4: Learn to program in the tidyverse in R\nWeeks 5-10: Communication, Data Analysis, Statistics, & Case Studies (two Case Studies)\n\n. . .\nNote: This course is back-loaded. But, that’s when group work happens.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-nitty-gritty",
    "href": "content/lectures/00-welcome.html#the-nitty-gritty",
    "title": "00-welcome",
    "section": "The Nitty Gritty",
    "text": "The Nitty Gritty\n\nLectureIn-personWaitlistLab & OHMaterials\n\n\nClass Meetings\n\nInteractive\nLectures & lots of learn-by-doing\nBring your laptop to class every day\n\n\n\nIn-person, synchronous learning\n\nI will be teaching (so long as I’m healthy and have child care) in person.\nLectures and lab will be podcast.\nAttendance will be incentivized using a daily participation survey.\nIf you’re not feeling well, please stay home. I will do the same.\nExam will be take-home.\n\n\n\nThe (Dreaded) Waitlist\n\nCourse enrollment is supposed to be 50 for this course\nThere are 72 people currently enrolled\nI don’t control the waitlist (cogsadvising@ucsd.edu does)\nI’d anticipate our staff adding 3-5 people from the waitlist (but cannot guarantee this)\n\n\n\nLab & Office Hours\n\nOffice hours begin week 1\n\nProf: Tu: 3:30-4:30 (drop-in); W 11-12 (10 min slots; appt.)\n\nLab begins week 1 (next Friday)\n\nit’s not in a computer lab, so you’ll need to bring your own\ndetails about labs covered on Tues and in lab\ntypically labs will be released Monday and due Friday\n\nI will hang out after class today for questions/concerns from students\n\n\n\nCourse Materials\n\nTextbooks are free and available online\nCourse platforms:\n\nWebsite : schedule, policies, due dates, etc.\nGitHub : retrieving assignments, labs, exams, etc.\ndatahub : completing assignments, labs, exams etc.\nCanvas : grades, course-specific links\nPiazza : Q&A",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#diversity-inclusion",
    "href": "content/lectures/00-welcome.html#diversity-inclusion",
    "title": "00-welcome",
    "section": "Diversity & Inclusion:",
    "text": "Diversity & Inclusion:\nGoal: every student be well-served by this course\n. . .\nPhilosophy: The diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding.\n. . .\nPlan: Present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture)\n. . .\nBut… if I ever fall short or if you ever have suggestions for improvement, please do share with me! There is also an anonymous Google Form if you’re more comfortable there.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#a-new-ish-course",
    "href": "content/lectures/00-welcome.html#a-new-ish-course",
    "title": "00-welcome",
    "section": "A new-ish course!",
    "text": "A new-ish course!\n\nOffered twice previously\nIf something doesn’t make sense, tell me!\nIf you’ve got feedback/suggestions, I’m all ears!\n\n. . .\nChanges since last iteration (based on feedback):\n\nspread out second half\nlikely changing the heaviness of a case study\nadd in communication to public portion\none fewer HW assignments",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-to-get-help",
    "href": "content/lectures/00-welcome.html#how-to-get-help",
    "title": "00-welcome",
    "section": "How to get help",
    "text": "How to get help\n\nLab\nOffice Hours\nPiazza\n\n. . .\nA few (Piazza) guidelines:\n1. No duplicates.\n2. Public posts are best.\n3. Posts should include your question, what you've tried so far, & resources used.\n4. Helping others is encouraged.\n5. No assignment code in public posts.\n6. We're not robots.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-r-community",
    "href": "content/lectures/00-welcome.html#the-r-community",
    "title": "00-welcome",
    "section": " The R Community",
    "text": "The R Community\n\n\n\nR Rollercoaster\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#academic-integrity",
    "href": "content/lectures/00-welcome.html#academic-integrity",
    "title": "00-welcome",
    "section": "Academic integrity",
    "text": "Academic integrity\nDon’t cheat.\n. . .\nTeamwork is allowed, but you should be able to answer “Yes” to each of the following:\n\nCan I explain each piece of code and each analysis carried out in what I’m submitting?\nCould I reproduce this code/analysis on my own?\n\n. . .\nThe Internet is a great resource. Cite your sources.\n. . .\nTeamwork is not allowed on your midterm. It is open-notes and open-Google/ChatGPT. You cannot discuss the questions on the exam with anyone.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#when-to-can-i-use-chatgptllms",
    "href": "content/lectures/00-welcome.html#when-to-can-i-use-chatgptllms",
    "title": "00-welcome",
    "section": "When To (Can I) Use ChatGPT/LLMs?",
    "text": "When To (Can I) Use ChatGPT/LLMs?\nFor anything in this course.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-to-use-chatgptllms",
    "href": "content/lectures/00-welcome.html#how-to-use-chatgptllms",
    "title": "00-welcome",
    "section": "How To Use ChatGPT/LLMs",
    "text": "How To Use ChatGPT/LLMs\nProbably never first or right away.\n. . .\nTo learn: Think first. Try first. Then use external resources.\n. . .\nAlways read/think about/understand the output.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#chatgpt-what-to-avoid",
    "href": "content/lectures/00-welcome.html#chatgpt-what-to-avoid",
    "title": "00-welcome",
    "section": "ChatGPT: What to Avoid",
    "text": "ChatGPT: What to Avoid\n\n\nOver-reliance (thwarts learning)\nHaving to look everything up (wastes time)\nLeaving tasks to the last minute (can lead to bad decisions/academic integrity issues)\nTaking the output without thinking (thwarts learning; limits critical thinking practice)\nUsing it right away for brainstorming ideas (limits ideas generated)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#course-components",
    "href": "content/lectures/00-welcome.html#course-components",
    "title": "00-welcome",
    "section": "Course components:",
    "text": "Course components:\n\n\nLabs (8): Individual submission; graded on effort\nHomework (3): Individual submission; graded on correctness\nExam (1): Individual completion & submission, take-home midterm\nCase Studies (2): Team submission, technical analysis report\nFinal Project (1) : Team submission, due Tues of finals week",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#grading",
    "href": "content/lectures/00-welcome.html#grading",
    "title": "00-welcome",
    "section": "Grading",
    "text": "Grading\nYour final grade will be comprised of the following:\n\n\n\nAssignment (#)\n% of grade\n\n\n\n\nLabs (8)\n16%\n\n\nHomework (3)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal project* (1)\n17%\n\n\n\n* indicates group submission",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#latemissed-work-policy",
    "href": "content/lectures/00-welcome.html#latemissed-work-policy",
    "title": "00-welcome",
    "section": "Late/missed work policy",
    "text": "Late/missed work policy\n\nHomework and case study projects: accepted up to 3 days (72 hours) after the assigned deadline for a 25% deduction\nNo late deadlines for labs, the exam, or the final project\n\n. . .\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#datahub",
    "href": "content/lectures/00-welcome.html#datahub",
    "title": "00-welcome",
    "section": "Datahub",
    "text": "Datahub\nDatahub is a platform hosted by UCSD that gives students access to computational resources.\nThis means that while you’ll be typing on your keyboard, you’ll be using UCSD’s computers in this class.\nWebsite: https://datahub.ucsd.edu/\n. . .\nLaunch Environment\nWhen working on “stuff” for this course, select the COGS 137 environment.\n ## Datahub Usage\nQ: Do I have to use datahub?\nA: Nope. You could download and install all the packages we use and complete the course locally! However, many packages have already been installed for you on datahub, so it will be a tiny bit more work up front…but you won’t be dependent on the internet/datahub!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#toolkit",
    "href": "content/lectures/00-welcome.html#toolkit",
    "title": "00-welcome",
    "section": "Toolkit",
    "text": "Toolkit\n\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub\nThe Internet (Google/ChatGPT/etc.)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-and-rstudio",
    "href": "content/lectures/00-welcome.html#r-and-rstudio",
    "title": "00-welcome",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR/RStudioTourTryR packages\n\n\nR & RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integreated development environment, IDE)\n\n\n\n\n[DEMO]\n\nConcepts introduced:\n\nConsole\nUsing R as a calculator\nEnvironment\nLoading and viewing a data frame\nAccessing a variable in a data frame\nR functions\n\n\n\nYour Turn\n\nLogin to datahub\nCarry out a mathematical operation in the console\nView the airquality dataframe\nAccess a column from the airquality dataframe\nCalculate the median for one of the numeric columns\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\nPackages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data 1\nAs of Sept 2023, there are ~19,941 R packages available on CRAN (the Comprehensive R Archive Network)2\nWe’re going to work with a small (but important) subset of these!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "href": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "title": "00-welcome",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\n\n\n\n\n\ntidyverse.org\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying philosophy and a common syntax.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#rstudio-projects3",
    "href": "content/lectures/00-welcome.html#rstudio-projects3",
    "title": "00-welcome",
    "section": "RStudio Projects3",
    "text": "RStudio Projects3\n\nBuilt-in functionality to keep all files for a single project organized",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown",
    "href": "content/lectures/00-welcome.html#r-markdown",
    "title": "00-welcome",
    "section": "R Markdown",
    "text": "R Markdown\n\nFully reproducible reports – each time you knit, the document is executed from top to bottom\nSimple markdown syntax for text\nCode goes in chunks, defined by three backticks, narrative goes outside of chunks",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown-tips",
    "href": "content/lectures/00-welcome.html#r-markdown-tips",
    "title": "00-welcome",
    "section": "R Markdown tips",
    "text": "R Markdown tips\n\nKeep the R Markdown cheat sheet and Markdown Quick Reference (Help -&gt; Markdown Quick Reference) handy, we’ll refer to it often as the course progresses\nThe workspace of your R Markdown document is separate from the Console\n\n\n\n[DEMO]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "href": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "title": "00-welcome",
    "section": "How will we use R Markdown?",
    "text": "How will we use R Markdown?\n\nEvery lab / midterm / project / homework / notes / etc. is an R Markdown document\nYou’ll always have a template R Markdown document to start with\nThe amount of scaffolding in the template will decrease over the quarter",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#collaboration-git-github",
    "href": "content/lectures/00-welcome.html#collaboration-git-github",
    "title": "00-welcome",
    "section": "Collaboration: Git & GitHub",
    "text": "Collaboration: Git & GitHub\n\nThe statistical programming language we’ll use is R\nThe software we use to interface with R is RStudio\nBut how do I get you the course materials that you can build on for your assignments?\n\nI’m not going to email you documents, that would be a mess!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#version-control",
    "href": "content/lectures/00-welcome.html#version-control",
    "title": "00-welcome",
    "section": "Version control",
    "text": "Version control\n\nWe introduced GitHub as a platform for collaboration\nBut it’s much more than that…\nIt’s actually designed for version control",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning",
    "href": "content/lectures/00-welcome.html#versioning",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\n\n\n\nLego versions",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning-1",
    "href": "content/lectures/00-welcome.html#versioning-1",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\nwith human readable messages\n\n\n\nLego versions with commit messages",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "href": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "title": "00-welcome",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\nPhD Comics",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#git-and-github-tips",
    "href": "content/lectures/00-welcome.html#git-and-github-tips",
    "title": "00-welcome",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\nGit is a version control system – like “Track Changes” feature Google Docs…but optimized for code. GitHub is the home for your Git-based projects on the internet – like Drive with additional features for code.\n\n. . .\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n. . .\n\nWe will be doing Git things and interfacing with GitHub through RStudio, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\n\n\n\nResource: happygitwithr.com: book for working with git in R; Some content is beyond the scope of this course, but it’s a good resource",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "href": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "title": "00-welcome",
    "section": "Let’s take a tour – Git / GitHub",
    "text": "Let’s take a tour – Git / GitHub\nWe’ll cover this time permitting, you’ll see it again in lab this week\nConcepts introduced:\n\nConnect an R project to Github repository\nWorking with a local and remote repository\nCommitting, Pushing and Pulling\n\nThere is a bit more of GitHub that we’ll use in this class, but for today this is enough.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#documentation",
    "href": "content/lectures/00-welcome.html#documentation",
    "title": "00-welcome",
    "section": "Documentation",
    "text": "Documentation\nConsider ggplot2 (a package we’ll learn a lot)\n\n\nOfficial documentation (CRAN): https://cran.r-project.org/web/packages/ggplot2/index.html\nCode (Github): https://github.com/tidyverse/ggplot2\nDocumentation: https://ggplot2.tidyverse.org/reference/index.html\nSpecific Function: https://ggplot2.tidyverse.org/reference/geom_point.html",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#chatgpt-what-it-could-look-like",
    "href": "content/lectures/00-welcome.html#chatgpt-what-it-could-look-like",
    "title": "00-welcome",
    "section": "ChatGPT: What it could look like",
    "text": "ChatGPT: What it could look like\nImagine: You’ve been asked to carry out a number of wrangling operations on a dataset and make a plot…\n\n[DEMO]",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#additional-help",
    "href": "content/lectures/00-welcome.html#additional-help",
    "title": "00-welcome",
    "section": "Additional help",
    "text": "Additional help\n\nclassmates\ncourse staff (OH, Piazza, class, lab)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#recap",
    "href": "content/lectures/00-welcome.html#recap",
    "title": "00-welcome",
    "section": "Recap",
    "text": "Recap\nCan you answer these questions?\n\nWhat is R vs RStudio?\nWhat are RStudio Projects?\nWhat is version control, and why do we care?\nWhat is git vs GitHub (and do I need to care)?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#additional-git-resources",
    "href": "content/lectures/00-welcome.html#additional-git-resources",
    "title": "00-welcome",
    "section": "Additional git Resources",
    "text": "Additional git Resources\n\nVersion Control (git and GitHub):\n\nGetting Started with git\nGitHub Guide\nGitHub Desktop App Tutorial\nGit Command Line Resource\nUsing git from the command line\n\nInstalling and using git (Part 1), by COGS 108 TA Ganesh (youtube, 22min tutorial)\nmerge conflicts and branching (Part 2), by IA Shubham Kulkarni (youtube, 8min tutorial)\n\nUsing git with GitHub Desktop, by COGS 108 TA Sidharth Suresh (youtube, 13min tutorial)\nGIT & GITHUB TUTORIAL, from edureka!\n\nwith notes from COGS 18/108 TA Holly(Yueying) Dong",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#slides-to-pdf",
    "href": "content/lectures/00-welcome.html#slides-to-pdf",
    "title": "00-welcome",
    "section": "Slides to PDF",
    "text": "Slides to PDF\n\nToggle into Print View using the Esc key (or using the Navigation Menu)\nOpen the in-browser print dialog (CTRL/CMD+P).\nChange the Destination setting to Save as PDF.\nChange the Layout to Landscape.\nChange the Margins to None.\nEnable the Background graphics option.\nClick Save 🎉\n\n\n\nInstructions from quarto documentation",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class",
    "href": "content/lectures/00-welcome.html#whos-in-this-class",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster &lt;- read_sheet('10kG09t5Uvjy2zLt4sToHvveBRnqYXTwfaAhPpgEFr3s')\n\nggplot(roster, aes(x = College)) +\n  geom_bar() +\n  labs(title = \"COGS 137\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nNote: This code will not run for you because you don’t have access to the roster for this course.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |&gt;\n  mutate(major = substr(Major, 1, 2)) |&gt;\n  ggplot(aes(fct_infreq(major))) + \n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Major\") +\n  theme_bw(base_size = 12) + \n  theme(plot.title.position = \"plot\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |&gt;\n  ggplot(aes(fct_relevel(Level, \"SO\", \"JR\", \"SR\"))) +\n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Level\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\nWarning: 1 unknown level in `f`: SO\n1 unknown level in `f`: SO",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#id-like-to-know-more",
    "href": "content/lectures/00-welcome.html#id-like-to-know-more",
    "title": "00-welcome",
    "section": "I’d like to know more!",
    "text": "I’d like to know more!\n(required)Student Survey - complete by Tuesday at 11:59 PM.\nThis is required and completion will be used for CAA/#finaid. DO complete this even if you’re on the waitlist, please.\n. . .\n(optional) Daily Post-Lecture Feedback\n\nopportunity to reflect on learning\nopportunity to ask questions (I will read and answer these.)\nopportunity for extra credit on final project\n\n\n\n\n\nNote: Links to both surveys are also on Canvas. I will try to remind you at the end of lecture, but I’ll probably forget. Feel free to remind me/one another!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/00-welcome.html#footnotes",
    "href": "content/lectures/00-welcome.html#footnotes",
    "title": "00-welcome",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWickham and Bryan, R Packages↩︎\nCRAN contributed packages↩︎\nRStudio Projects Documentation↩︎",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "00-welcome"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#qa",
    "href": "content/lectures/13-cs01-analysis-slides.html#qa",
    "title": "13-cs01-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: How extensive does our extension component need to be?\nA: A bit hard to answer in certain terms. We’ll discuss some examples today to hopefully set expectaions well. To explain in writing here, the most typical extension is students using the data provided to ask and answer a question not directly presented in class. Thus, simply generating a visualization not presented in class would NOT be sufficient. At the other end, finding external data on the topic and analyzing that data, while certainly allowed, would far exceed expectations. In between those extremes is what we expect: add significantly to the analysis, beyond what was presented in class.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#course-announcements",
    "href": "content/lectures/13-cs01-analysis-slides.html#course-announcements",
    "title": "13-cs01-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nHW03 (MLR) due Mon 11/20\nProject Proposal (it will be a Google Form) due 11/20\nCS01 Deadlines:\n\nLab06 due Friday - cs01-focused\nReport & “General Communication” due 11/27\nsurvey about how working with group went - due 11/28\n\n\n\nNotes:\nMidterm scores & Feedback posted\n\noverall, did very well\n\navg: 13.85/15 (92%)\n6 perfect scores\n\nanswer key on course website\n\nI am behind on emails and Piazza posts.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#mid-course-survey-summary",
    "href": "content/lectures/13-cs01-analysis-slides.html#mid-course-survey-summary",
    "title": "13-cs01-analysis",
    "section": "Mid-course Survey Summary",
    "text": "Mid-course Survey Summary\n\nN=73 (~75%)\nPacing workload (so far) about right\nCourse notes most helpful in the course overall\nAlso helpful: completing labs, doing homework,\nMany are not checking labs against answer keys; most are not doing suggested readings\nOf those that attend lecture, most find it helpful",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#mid-course-time-spent",
    "href": "content/lectures/13-cs01-analysis-slides.html#mid-course-time-spent",
    "title": "13-cs01-analysis",
    "section": "Mid-course: Time Spent",
    "text": "Mid-course: Time Spent",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#mid-course-what-would-you-change",
    "href": "content/lectures/13-cs01-analysis-slides.html#mid-course-what-would-you-change",
    "title": "13-cs01-analysis",
    "section": "Mid-course: What would you change?",
    "text": "Mid-course: What would you change?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#agenda",
    "href": "content/lectures/13-cs01-analysis-slides.html#agenda",
    "title": "13-cs01-analysis",
    "section": "Agenda",
    "text": "Agenda\n\nDebugging/Understanding Code Strategies\nSensitivity & Specificity\nCross-compound correlations\nExtensions",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#summary-figuring-out-whats-going-on-in-code",
    "href": "content/lectures/13-cs01-analysis-slides.html#summary-figuring-out-whats-going-on-in-code",
    "title": "13-cs01-analysis",
    "section": "Summary: Figuring out what’s going on in code",
    "text": "Summary: Figuring out what’s going on in code\nSuggestions (as discussed in class):\n\n\nLook up documentation (i.e. ?...) / Google the function\nRun it on different input; see how output changing\nRun the code line-by-line, understanding output at each step\nAsk ChatGPT",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#packages",
    "href": "content/lectures/13-cs01-analysis-slides.html#packages",
    "title": "13-cs01-analysis",
    "section": "Packages",
    "text": "Packages\nThree additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)\nlibrary(cowplot)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#the-data",
    "href": "content/lectures/13-cs01-analysis-slides.html#the-data",
    "title": "13-cs01-analysis",
    "section": "The Data",
    "text": "The Data\nReading in the data from the end of data wrangling notes:\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nAnd the functions…\n\nsource(\"src/cs01_functions.R\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#sensitivity-specificity",
    "href": "content/lectures/13-cs01-analysis-slides.html#sensitivity-specificity",
    "title": "13-cs01-analysis",
    "section": "Sensitivity & Specificity",
    "text": "Sensitivity & Specificity\nSensitivity | the ability of a test to correctly identify patients with a disease/trait/condition. \\[TP/(TP + FN)\\]\n\nSpecificity | the ability of a test to correctly identify people without the disease/trait/condition. \\[TN/(TN + FP)\\]\n\n\n❓ For this analysis, do you care more about sensitivity? about specificity? equally about both?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#what-is-a-tp-here-tn-fp-fn",
    "href": "content/lectures/13-cs01-analysis-slides.html#what-is-a-tp-here-tn-fp-fn",
    "title": "13-cs01-analysis",
    "section": "What is a TP here? TN? FP? FN?",
    "text": "What is a TP here? TN? FP? FN?\nPost-smoking (cutoff &gt; 0)\n\n\nTP = THC group, value &gt;= cutoff\nFN = THC group, value &lt; cutoff\nFP = Placebo group, value &gt;= cutoff\nTN = Placebo group, value &lt; cutoff\n\n\n\nPost-smoking (cutoff == 0)\nCannot be a TP or FP if zero…\n\nTP = THC group, value &gt; cutoff),\nFN = THC group, value &lt;= cutoff),\nFP = Placebo group, value &gt; cutoff),\nTN = Placebo group, value &lt; cutoff)\n\n\n\nPre-smoking\nCannot be a TP or FN before consuming…\n\nTP = 0\nFN = 0\nFP = value &gt;= cutoff\nTN = value &lt; cutoff",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#roc",
    "href": "content/lectures/13-cs01-analysis-slides.html#roc",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\nReceiver-Operator Characteristic (ROC) Curve: TPR (Sensitivity) vs FPR (1-Specificity)\n\nImage Credit: By cmglee, MartinThoma - Roc-draft-xkcd-style.svg, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=109730045",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculating-sensitivity-and-specificity",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculating-sensitivity-and-specificity",
    "title": "13-cs01-analysis",
    "section": "Calculating Sensitivity and Specificity",
    "text": "Calculating Sensitivity and Specificity\n\nCalculateRunApplyDo it!\n\n\n\nmake_calculations &lt;- function(dataset, dataset_removedups, split, compound, \n                              start = start, stop = stop, tpt_use = tpt_use){\n  ## remove NAs\n  df &lt;- dataset_removedups %&gt;% \n    dplyr::select(treatment, compound, timepoint_use) %&gt;%\n    rename(compound = 2) %&gt;%\n    filter(complete.cases(.))\n  if(nrow(df)&gt;0){\n    if(stop &lt;= 0){\n      output &lt;- df %&gt;% \n        summarise(TP = 0,\n                  FN = 0,\n                  FP = sum(compound &gt;= split),\n                  TN = sum(compound &lt; split)) \n    }else{\n      if(split == 0){\n        output_pre &lt;- df %&gt;% \n          filter(tpt_use == \"pre-smoking\") %&gt;%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound &gt;= split),\n                    TN = sum(compound &lt; split)) \n        \n        output &lt;- df %&gt;% \n          filter(tpt_use != \"pre-smoking\") %&gt;%\n          summarise(TP = sum(treatment != \"Placebo\" & compound &gt; split),\n                    FN = sum(treatment != \"Placebo\" & compound &lt;= split),\n                    FP = sum(treatment == \"Placebo\" & compound &gt; split),\n                    TN = sum(treatment == \"Placebo\" & compound &lt; split))\n        \n        output &lt;- output + output_pre\n      }else{\n        ## calculate values if pre-smoking\n        output_pre &lt;- df %&gt;% \n          filter(tpt_use == \"pre-smoking\") %&gt;%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound &gt;= split),\n                    TN = sum(compound &lt; split)) \n        \n        output &lt;- df %&gt;% \n          filter(tpt_use != \"pre-smoking\") %&gt;%\n          summarise(TP = sum(treatment != \"Placebo\" & compound &gt;= split),\n                    FN = sum(treatment != \"Placebo\" & compound &lt; split),\n                    FP = sum(treatment == \"Placebo\" & compound &gt;= split),\n                    TN = sum(treatment == \"Placebo\" & compound &lt; split))\n        \n        output &lt;- output + output_pre\n      }\n    }\n  }\n  # clean things up; make calculations on above values\n  output &lt;- output %&gt;%\n    mutate(detection_limit = split,\n           compound = compound,\n           time_start = start,\n           time_stop = stop,\n           time_window = tpt_use,\n           NAs = nrow(dataset) - nrow(df),\n           N = nrow(dataset_removedups),\n           N_removed = nrow(dataset) - nrow(dataset_removedups),\n           Sensitivity = (TP/(TP + FN)), \n           Specificity = (TN /(TN + FP)),\n           PPV = (TP/(TP+FP)),\n           NPV = (TN/(TN + FN)),\n           Efficiency = ((TP + TN)/(TP + TN + FP + FN))*100\n    )\n  \n  return(output)\n}\n\n\n\n\ndetermine what cutoff values to try\ncarry out above function on those cutoffs\n\n\nsens_spec &lt;- function(dataset, compound, start, stop, tpt_use, \n                      lowest_value = 0.5, splits = NULL, ...){\n  # if it's not all NAs...\n  if(sum(is.na(dataset[,compound])) != nrow(dataset)){\n    # specify what splits should be used for calculations\n    if(is.null(splits)){\n      limits &lt;- dataset[is.finite(rowSums(dataset[,compound])),compound]\n      ## define lower and upper limits\n      lower = min(limits, na.rm=TRUE)\n      upper = max(limits, na.rm=TRUE)\n      ## determine splits to use for calculations\n      tosplit = pull(limits[,1])[limits[,1]&gt;0]\n      ## only split if there are detectable limits:\n      if(length(tosplit)&gt;=1){\n        splits = c(lowest_value, quantile(tosplit, probs=seq(0, 1, by = 0.01), na.rm=TRUE))\n        splits = unique(splits)\n      }else{\n        splits = 0\n      }\n    }else{\n      splits = splits\n    }\n    # filter to include timepoint of interest\n    dataset &lt;- dataset %&gt;% \n      filter(time_from_start &gt; start & time_from_start &lt;= stop & !is.na(timepoint_use))\n    dataset_removedups &lt;- dataset %&gt;%\n      filter(!is.na(timepoint_use)) %&gt;% \n      group_by(timepoint_use) %&gt;% \n      distinct(id, .keep_all = TRUE) %&gt;% \n      ungroup()\n\n    ## create empty output variable which we'll fill in\n    ## iterate through each possible dose and calculate\n    output &lt;- map_dfr(as.list(splits), ~make_calculations(dataset, \n                                                          dataset_removedups, \n                                                          split = .x,\n                                                          compound,\n                                                          start = start,\n                                                          stop = stop, \n                                                          tpt_use = tpt_use))\n  }\n  \n  return(output)\n}\n\n\n\nMap the above for each matrix…\n\nsens_spec_cpd &lt;- function(dataset, cpd, timepoints, splits = NULL){\n  args2 &lt;- list(start = timepoints$start, \n                stop = timepoints$stop, \n                tpt_use = timepoints$timepoint)\n  out &lt;- args2 %&gt;% \n    pmap_dfr(sens_spec, dataset, compound = cpd, splits = splits)\n  return(out)\n}\n\n\n\nThis takes a few minutes to run… (reminder: cache=TRUE)\n\noutput_WB &lt;- map_dfr(compounds_WB, \n                     ~sens_spec_cpd(dataset = WB, cpd = all_of(.x), \n                                    timepoints = timepoints_WB)) %&gt;% clean_gluc()\n\noutput_BR &lt;- map_dfr(compounds_BR, \n                     ~sens_spec_cpd(dataset = BR,  cpd = all_of(.x),\n                                    timepoints = timepoints_BR))  %&gt;% clean_gluc()\n\noutput_OF &lt;- map_dfr(compounds_OF, \n                     ~sens_spec_cpd(dataset = OF, cpd = all_of(.x),\n                                    timepoints = timepoints_OF))  %&gt;% clean_gluc()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#roc-1",
    "href": "content/lectures/13-cs01-analysis-slides.html#roc-1",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\n\nCodeCalculatePlot\n\n\n\nss_plot &lt;- function(output, tpts=8, tissue){\n  to_include = output %&gt;%\n    group_by(compound) %&gt;% \n    summarize(mean_detection = mean(detection_limit)) %&gt;% \n    filter(mean_detection &gt; 0)\n  \n  output &lt;-  output %&gt;% \n    mutate(iszero = ifelse(time_start&lt;0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %&gt;%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %&gt;%\n    clean_gluc() %&gt;% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output &lt;- output %&gt;%  mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#A2DDED', '#86BEDC', '#6C9FCA', \n                  '#547EB9', '#3F5EA8', '#2D4096', '#1E2385',\n                  '#181173', '#180762', '#180051')\n  values = c(blue_colors[1:tpts])\n  \n  print(ggplot(output, aes(x = detection_limit, y = Sensitivity, group = fct_inorder(legend))) + \n          geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n          geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n          facet_grid(~compound, scales = \"free_x\") +\n          labs(x = 'Detection Limit',\n               y = 'Sensitivity') +\n          ylim(0,1) +\n          scale_color_manual(values = values, name = 'Time Window') +\n          theme_classic(base_size = 12) + \n          theme(axis.title = element_text(size=16), \n                panel.grid = element_blank(),\n                strip.background = element_blank(),\n                strip.text.x = element_text(size = 12))  \n  )\n  print(\n    ggplot(output, aes(x = detection_limit, y = Specificity, group = fct_inorder(legend))) + \n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound, scales = \"free_x\") +\n      ylim(0,100) +\n      labs(title = tissue,\n           x = 'Detection Limit',\n           y = 'Specificity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12))\n  )\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12))\n  )\n}\n\nroc_plot &lt;- function(output, tpts=8, tissue){\n  to_include = output %&gt;%\n    group_by(compound) %&gt;% \n    summarize(mean_detection = mean(detection_limit)) %&gt;% \n    filter(mean_detection &gt; 0)\n  \n  output &lt;-  output %&gt;% \n    mutate(iszero = ifelse(time_start&lt;0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %&gt;%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %&gt;%\n    clean_gluc() %&gt;% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output &lt;- output %&gt;% mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#86BEDC', \n                  '#547EB9', '#2D4096',\n                  '#181173', '#180051')\n  values = c(blue_colors[1:tpts])\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12) )\n  )\n}\n\n\n\n\nss1_a &lt;- ss_plot(output_WB, tpts = length(unique(output_WB$time_start)), tissue = \"Blood\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nss2_a &lt;- ss_plot(output_OF, tpts = length(unique(output_OF$time_start)), tissue = \"Oral Fluid\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nss3_a &lt;- roc_plot(output_BR, tpts = length(unique(output_BR$time_start)), tissue = \"Breath\")\n\n\n\n\n\n\n\n\n\n\n\nbottom_row &lt;- plot_grid(ss2_a, ss3_a, labels = c('B', 'C'), label_size = 12, ncol = 2, rel_widths = c(0.66, .33))\nplot_grid(ss1_a, bottom_row, labels = c('A', ''), label_size = 12, ncol = 1)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculate-thc",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculate-thc",
    "title": "13-cs01-analysis",
    "section": "Calculate: THC",
    "text": "Calculate: THC\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOFBR\n\n\n\ncutoffs = c(0.5, 1, 2, 5, 10)\nWB_THC &lt;- sens_spec_cpd(dataset = WB, cpd = 'thc',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nWB_THC\n\n# A tibble: 50 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;           &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    81   108             0.5 THC            -400         0\n 2     0     0    61   128             1   THC            -400         0\n 3     0     0    45   144             2   THC            -400         0\n 4     0     0    10   179             5   THC            -400         0\n 5     0     0     1   188            10   THC            -400         0\n 6   124     2    28    33             0.5 THC               0        30\n 7   123     3    22    39             1   THC               0        30\n 8   119     7    15    46             2   THC               0        30\n 9   106    20     4    57             5   THC               0        30\n10   101    25     0    61            10   THC               0        30\n# ℹ 40 more rows\n# ℹ 9 more variables: time_window &lt;chr&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;\n\n\n\n\n\nOF_THC &lt;- sens_spec_cpd(dataset = OF, cpd = 'thc',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nOF_THC\n\n# A tibble: 40 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;           &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    35   157             0.5 THC            -400         0\n 2     0     0    20   172             1   THC            -400         0\n 3     0     0     9   183             2   THC            -400         0\n 4     0     0     0   192             5   THC            -400         0\n 5     0     0     0   192            10   THC            -400         0\n 6   129     0    39    24             0.5 THC               0        30\n 7   129     0    30    33             1   THC               0        30\n 8   128     1    19    44             2   THC               0        30\n 9   128     1     3    60             5   THC               0        30\n10   125     4     1    62            10   THC               0        30\n# ℹ 30 more rows\n# ℹ 9 more variables: time_window &lt;chr&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;\n\n\n\n\nWhy is there no calculation for breath with these cutoffs?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#cutoffs",
    "href": "content/lectures/13-cs01-analysis-slides.html#cutoffs",
    "title": "13-cs01-analysis",
    "section": "Cutoffs",
    "text": "Cutoffs\n\nCodeWBOF\n\n\n\nplot_cutoffs &lt;- function(dataset, timepoint_use_variable, tissue, labels = c(\"A\", \"B\"), vertline, cpd, x_labels){\n    col_val = c(\"#D9D9D9\", \"#BDBDBD\", \"#969696\", \"#636363\", \"#252525\")\n    lines = rep(\"solid\", 5)\n    \n  df_ss &lt;- dataset %&gt;% \n    mutate(time_window = fct_relevel(as.factor(time_window), \n                                     levels(timepoint_use_variable)),\n           detection_limit = as.factor(detection_limit),\n           Sensitivity =  round(Sensitivity*100,0),\n           Specificity =  round(Specificity*100,0),\n           my_label = paste0(time_window, ' N=', N),\n           my_label =  gsub(\" \", \"\\n\", my_label),\n           my_label = fct_relevel(as.factor(my_label), x_labels)) #%&gt;%          \n    \n    p1 &lt;- df_ss %&gt;% \n    ggplot(aes(x = my_label, y = Sensitivity, \n               colour = detection_limit)) + \n    geom_line(size = 1.2, aes(group = detection_limit, \n                              linetype = detection_limit)) + \n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point(show.legend=FALSE) + \n    ylim(0,100) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values=lines) +\n      scale_color_manual(values = col_val, name = \"Cutoff \\n (ng/mL)\",\n                         guide = guide_legend(override.aes = list(linetype = c(1),\n                                                                  shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme( axis.title = element_text(size=16),\n           axis.text = element_text(size=10),\n           legend.position = c(0.08, 0.4),\n           panel.grid = element_blank(),\n           strip.background = element_blank()\n           ) +\n      guides(linetype = FALSE) +\n    labs(x = \"Time Window\", \n         y = \"Sensitivity\", \n         title = paste0(tissue,\": \", cpd) )\n \n  p2 &lt;- df_ss %&gt;% \n    ggplot(aes(x = my_label, y = Specificity,\n               group = detection_limit, \n               colour = detection_limit, \n               linetype = detection_limit)) + \n    geom_line(size = 1.2) +\n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point() + \n    ylim(0,100) +\n    scale_color_manual(values = col_val) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values = lines, \n                          guide = guide_legend(override.aes = list(linetype = \"solid\",\n                                                                   shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme(axis.title = element_text(size=16),\n          axis.text = element_text(size=10),\n          legend.position = \"none\", \n          panel.grid = element_blank(),\n          strip.background = element_blank()) +\n    labs(x = \"Time Window\", \n         y = \"Specificity\",\n         title = \"\" )\n  \n  title &lt;- ggdraw() + \n    draw_label(\n      tissue,\n      x = 0.05,\n      hjust = 0\n    )\n  \n  plot_row &lt;- plot_grid(p1, p2, labels = labels, label_size = 12)\n  \n  plot_grid(\n    title, plot_row,\n    ncol = 1,\n    # rel_heights values control vertical title margins\n    rel_heights = c(0.1, 1)\n  )\n  \n  return(list(plot_row, df_ss))\n\n}\n\n\n\n\nblood_levels &lt;- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(dataset=WB_THC, \n             timepoint_use_variable=WB$timepoint_use, \n             tissue=\"Blood\", \n             vertline=levels(WB$timepoint_use)[5], \n             cpd=\"THC\", \n             x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    81   108 0.5             THC            -400         0\n 2     0     0    61   128 1               THC            -400         0\n 3     0     0    45   144 2               THC            -400         0\n 4     0     0    10   179 5               THC            -400         0\n 5     0     0     1   188 10              THC            -400         0\n 6   124     2    28    33 0.5             THC               0        30\n 7   123     3    22    39 1               THC               0        30\n 8   119     7    15    46 2               THC               0        30\n 9   106    20     4    57 5               THC               0        30\n10   101    25     0    61 10              THC               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;\n\n\n\n\n\nof_levels &lt;- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_THC, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"THC\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    35   157 0.5             THC            -400         0\n 2     0     0    20   172 1               THC            -400         0\n 3     0     0     9   183 2               THC            -400         0\n 4     0     0     0   192 5               THC            -400         0\n 5     0     0     0   192 10              THC            -400         0\n 6   129     0    39    24 0.5             THC               0        30\n 7   129     0    30    33 1               THC               0        30\n 8   128     1    19    44 2               THC               0        30\n 9   128     1     3    60 5               THC               0        30\n10   125     4     1    62 10              THC               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculate-cbn",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculate-cbn",
    "title": "13-cs01-analysis",
    "section": "Calculate: CBN",
    "text": "Calculate: CBN\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOF\n\n\n\nWB_CBN =  sens_spec_cpd(dataset = WB, cpd = 'cbn',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nblood_levels &lt;- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(WB_CBN, WB$timepoint_use, tissue = \"Blood\", vertline=levels(WB$timepoint_use)[5], cpd=\"CBN\", x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0     1   188 0.5             CBN            -400         0\n 2     0     0     0   189 1               CBN            -400         0\n 3     0     0     0   189 2               CBN            -400         0\n 4     0     0     0   189 5               CBN            -400         0\n 5     0     0     0   189 10              CBN            -400         0\n 6   106    20     7    54 0.5             CBN               0        30\n 7    97    29     0    61 1               CBN               0        30\n 8    82    44     0    61 2               CBN               0        30\n 9    40    86     0    61 5               CBN               0        30\n10     9   117     0    61 10              CBN               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;\n\n\n\n\n\nOF_CBN =  sens_spec_cpd(dataset = OF, cpd = 'cbn',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nof_levels &lt;- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_CBN, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"CBN\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0     5   187 0.5             CBN            -400         0\n 2     0     0     1   191 1               CBN            -400         0\n 3     0     0     1   191 2               CBN            -400         0\n 4     0     0     1   191 5               CBN            -400         0\n 5     0     0     0   192 10              CBN            -400         0\n 6   127     2    41    22 0.5             CBN               0        30\n 7   125     4    32    31 1               CBN               0        30\n 8   122     7    18    45 2               CBN               0        30\n 9   116    13     7    56 5               CBN               0        30\n10   107    22     3    60 10              CBN               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#compound-correlations",
    "href": "content/lectures/13-cs01-analysis-slides.html#compound-correlations",
    "title": "13-cs01-analysis",
    "section": "Compound Correlations",
    "text": "Compound Correlations\n\nCodePlot\n\n\n\nggplotRegression &lt;- function (x, y, xlab, ylab, x_text, y_text,  y_text2, title) {\n  fit &lt;- lm(y ~ x)\n  if(max(fit$model[,1],na.rm=TRUE)!=0){\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), \n                                               digits=2)),\n               vjust=1, hjust=0, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))),\n        vjust=1, hjust=0, size=4.5) + \n      theme_minimal(base_size=14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n  } else{\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      scale_y_continuous(limits = c(0,3)) +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), digits=2)),vjust=1, hjust=1, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))), vjust=1, hjust=1,size=4.5) + \n      theme_minimal(base_size = 14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n    \n    \n  }\n}\n\n\n\n\nwb_reg &lt;- ggplotRegression(WB$thc, WB$cbn, xlab = 'THC (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 150, y_text = 7, y_text2 = 5, title = \"Blood\")\n\nof_reg &lt;- ggplotRegression(OF$thc, OF$cbn, xlab = 'THC  (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 12500, y_text = 750, y_text2 = 500, title = \"Oral Fluid\")\n\nplot_grid(wb_reg, of_reg, labels = 'AUTO', label_size = 12, ncol = 2, scale = 1)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#possible-extensions",
    "href": "content/lectures/13-cs01-analysis-slides.html#possible-extensions",
    "title": "13-cs01-analysis",
    "section": "Possible Extensions",
    "text": "Possible Extensions\nOur current question asks for a single compound…and you’ll need to decide that.\n\n…but you could imagine a world where more than one compound or more than one matrix could be measured at the roadside.\n\n\nSo:\n\n\ncombination of the oral fluid and blood that would better predict recent use? (For example if an officer stopped a driver and got a high oral fluid, but could not get a blood sample for a couple of hours and got a relatively low result would this predict recent use better than blood (or OF) alone?\nIs there a ratio of OF/blood that predicts recent use?\nMachine learning model to determine optimal combination of measurements/cutoffs to detect recent use?\n\n\n\n\nThings to keep in mind:\n\nsome matrices are easier to get at the roadside\ntime from use matters (trying to detect recent use)\nwe may not care equally about sensitivity and specificity",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#cs01-what-to-do-now",
    "href": "content/lectures/13-cs01-analysis-slides.html#cs01-what-to-do-now",
    "title": "13-cs01-analysis",
    "section": "cs01: what to do now?",
    "text": "cs01: what to do now?\n\nCommunicate with your group!\nDiscuss possible extensions\nMake a plan; figure out who’s doing what; set deadlines\nImplement the plan!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#what-has-to-be-done",
    "href": "content/lectures/13-cs01-analysis-slides.html#what-has-to-be-done",
    "title": "13-cs01-analysis",
    "section": "What has to be done:",
    "text": "What has to be done:\n\n\nQuestion | include in Rmd; add extension if applicable\nBackground | summarize and add to what was discussed in classed\nData\n\nDescribe data & variables\nData wrangling | likely copy + paste from notes; add explanation as you go\n\nAnalysis\n\nEDA | likely borrowing parts from notes and adding more in; be sure to include interpretations of output & guide the reader\nAnalysis | likely borrowing most/all from class; interpretations/guiding reader/contextualizing is essential\nExtension | must be completed\n\nConclusion | summarize\nProofread | ensure it makes sense from top to bottom\nGeneral Audience communication (submit on Canvas; 1 submission per group)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#collaborating-on-github",
    "href": "content/lectures/13-cs01-analysis-slides.html#collaborating-on-github",
    "title": "13-cs01-analysis",
    "section": "Collaborating on GitHub",
    "text": "Collaborating on GitHub\n\nBe sure to pull changes every time you sit down to work\nAvoid working on the same part of the same file as another teammate OR work in separate files and combine at the end\npush your changes once you’re ready to add them to the group",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#recap",
    "href": "content/lectures/13-cs01-analysis-slides.html#recap",
    "title": "13-cs01-analysis",
    "section": "Recap",
    "text": "Recap\n\nCan you describe sensitivity? Specificity?\nCan you explain how TP, TN, FP, and FN were calculated/defined in this experiment?\nCan you describe the code used to carry out the calculations?\nCan you interpret the results from these data?\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html",
    "href": "content/lectures/13-cs01-analysis.html",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Q: How extensive does our extension component need to be?\nA: A bit hard to answer in certain terms. We’ll discuss some examples today to hopefully set expectaions well. To explain in writing here, the most typical extension is students using the data provided to ask and answer a question not directly presented in class. Thus, simply generating a visualization not presented in class would NOT be sufficient. At the other end, finding external data on the topic and analyzing that data, while certainly allowed, would far exceed expectations. In between those extremes is what we expect: add significantly to the analysis, beyond what was presented in class.\n\n\n\n\nDue Dates:\n\nHW03 (MLR) due Mon 11/20\nProject Proposal (it will be a Google Form) due 11/20\nCS01 Deadlines:\n\nLab06 due Friday - cs01-focused\nReport & “General Communication” due 11/27\nsurvey about how working with group went - due 11/28\n\n\n. . .\nNotes:\nMidterm scores & Feedback posted\n\noverall, did very well\n\navg: 13.85/15 (92%)\n6 perfect scores\n\nanswer key on course website\n\nI am behind on emails and Piazza posts.\n\n\n\n\nN=73 (~75%)\nPacing workload (so far) about right\nCourse notes most helpful in the course overall\nAlso helpful: completing labs, doing homework,\nMany are not checking labs against answer keys; most are not doing suggested readings\nOf those that attend lecture, most find it helpful\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging/Understanding Code Strategies\nSensitivity & Specificity\nCross-compound correlations\nExtensions\n\n\n\n\nSuggestions (as discussed in class):\n\n\nLook up documentation (i.e. ?...) / Google the function\nRun it on different input; see how output changing\nRun the code line-by-line, understanding output at each step\nAsk ChatGPT",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#qa",
    "href": "content/lectures/13-cs01-analysis.html#qa",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Q: How extensive does our extension component need to be?\nA: A bit hard to answer in certain terms. We’ll discuss some examples today to hopefully set expectaions well. To explain in writing here, the most typical extension is students using the data provided to ask and answer a question not directly presented in class. Thus, simply generating a visualization not presented in class would NOT be sufficient. At the other end, finding external data on the topic and analyzing that data, while certainly allowed, would far exceed expectations. In between those extremes is what we expect: add significantly to the analysis, beyond what was presented in class.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#course-announcements",
    "href": "content/lectures/13-cs01-analysis.html#course-announcements",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Due Dates:\n\nHW03 (MLR) due Mon 11/20\nProject Proposal (it will be a Google Form) due 11/20\nCS01 Deadlines:\n\nLab06 due Friday - cs01-focused\nReport & “General Communication” due 11/27\nsurvey about how working with group went - due 11/28\n\n\n. . .\nNotes:\nMidterm scores & Feedback posted\n\noverall, did very well\n\navg: 13.85/15 (92%)\n6 perfect scores\n\nanswer key on course website\n\nI am behind on emails and Piazza posts.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#mid-course-survey-summary",
    "href": "content/lectures/13-cs01-analysis.html#mid-course-survey-summary",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "N=73 (~75%)\nPacing workload (so far) about right\nCourse notes most helpful in the course overall\nAlso helpful: completing labs, doing homework,\nMany are not checking labs against answer keys; most are not doing suggested readings\nOf those that attend lecture, most find it helpful",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#agenda",
    "href": "content/lectures/13-cs01-analysis.html#agenda",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Debugging/Understanding Code Strategies\nSensitivity & Specificity\nCross-compound correlations\nExtensions",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#summary-figuring-out-whats-going-on-in-code",
    "href": "content/lectures/13-cs01-analysis.html#summary-figuring-out-whats-going-on-in-code",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Suggestions (as discussed in class):\n\n\nLook up documentation (i.e. ?...) / Google the function\nRun it on different input; see how output changing\nRun the code line-by-line, understanding output at each step\nAsk ChatGPT",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#packages",
    "href": "content/lectures/13-cs01-analysis.html#packages",
    "title": "13-cs01-analysis",
    "section": "Packages",
    "text": "Packages\nThree additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)\nlibrary(cowplot)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#the-data",
    "href": "content/lectures/13-cs01-analysis.html#the-data",
    "title": "13-cs01-analysis",
    "section": "The Data",
    "text": "The Data\nReading in the data from the end of data wrangling notes:\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nAnd the functions…\n\nsource(\"src/cs01_functions.R\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#sensitivity-specificity",
    "href": "content/lectures/13-cs01-analysis.html#sensitivity-specificity",
    "title": "13-cs01-analysis",
    "section": "Sensitivity & Specificity",
    "text": "Sensitivity & Specificity\nSensitivity | the ability of a test to correctly identify patients with a disease/trait/condition. \\[TP/(TP + FN)\\]\n. . .\nSpecificity | the ability of a test to correctly identify people without the disease/trait/condition. \\[TN/(TN + FP)\\]\n. . .\n❓ For this analysis, do you care more about sensitivity? about specificity? equally about both?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#what-is-a-tp-here-tn-fp-fn",
    "href": "content/lectures/13-cs01-analysis.html#what-is-a-tp-here-tn-fp-fn",
    "title": "13-cs01-analysis",
    "section": "What is a TP here? TN? FP? FN?",
    "text": "What is a TP here? TN? FP? FN?\nPost-smoking (cutoff &gt; 0)\n\n\nTP = THC group, value &gt;= cutoff\nFN = THC group, value &lt; cutoff\nFP = Placebo group, value &gt;= cutoff\nTN = Placebo group, value &lt; cutoff\n\n\n. . .\nPost-smoking (cutoff == 0)\nCannot be a TP or FP if zero…\n\nTP = THC group, value &gt; cutoff),\nFN = THC group, value &lt;= cutoff),\nFP = Placebo group, value &gt; cutoff),\nTN = Placebo group, value &lt; cutoff)\n\n. . .\nPre-smoking\nCannot be a TP or FN before consuming…\n\nTP = 0\nFN = 0\nFP = value &gt;= cutoff\nTN = value &lt; cutoff",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#roc",
    "href": "content/lectures/13-cs01-analysis.html#roc",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\nReceiver-Operator Characteristic (ROC) Curve: TPR (Sensitivity) vs FPR (1-Specificity)\n\nImage Credit: By cmglee, MartinThoma - Roc-draft-xkcd-style.svg, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=109730045",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculating-sensitivity-and-specificity",
    "href": "content/lectures/13-cs01-analysis.html#calculating-sensitivity-and-specificity",
    "title": "13-cs01-analysis",
    "section": "Calculating Sensitivity and Specificity",
    "text": "Calculating Sensitivity and Specificity\n\nCalculateRunApplyDo it!\n\n\n\nmake_calculations &lt;- function(dataset, dataset_removedups, split, compound, \n                              start = start, stop = stop, tpt_use = tpt_use){\n  ## remove NAs\n  df &lt;- dataset_removedups %&gt;% \n    dplyr::select(treatment, compound, timepoint_use) %&gt;%\n    rename(compound = 2) %&gt;%\n    filter(complete.cases(.))\n  if(nrow(df)&gt;0){\n    if(stop &lt;= 0){\n      output &lt;- df %&gt;% \n        summarise(TP = 0,\n                  FN = 0,\n                  FP = sum(compound &gt;= split),\n                  TN = sum(compound &lt; split)) \n    }else{\n      if(split == 0){\n        output_pre &lt;- df %&gt;% \n          filter(tpt_use == \"pre-smoking\") %&gt;%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound &gt;= split),\n                    TN = sum(compound &lt; split)) \n        \n        output &lt;- df %&gt;% \n          filter(tpt_use != \"pre-smoking\") %&gt;%\n          summarise(TP = sum(treatment != \"Placebo\" & compound &gt; split),\n                    FN = sum(treatment != \"Placebo\" & compound &lt;= split),\n                    FP = sum(treatment == \"Placebo\" & compound &gt; split),\n                    TN = sum(treatment == \"Placebo\" & compound &lt; split))\n        \n        output &lt;- output + output_pre\n      }else{\n        ## calculate values if pre-smoking\n        output_pre &lt;- df %&gt;% \n          filter(tpt_use == \"pre-smoking\") %&gt;%\n          summarise(TP = 0,\n                    FN = 0,\n                    FP = sum(compound &gt;= split),\n                    TN = sum(compound &lt; split)) \n        \n        output &lt;- df %&gt;% \n          filter(tpt_use != \"pre-smoking\") %&gt;%\n          summarise(TP = sum(treatment != \"Placebo\" & compound &gt;= split),\n                    FN = sum(treatment != \"Placebo\" & compound &lt; split),\n                    FP = sum(treatment == \"Placebo\" & compound &gt;= split),\n                    TN = sum(treatment == \"Placebo\" & compound &lt; split))\n        \n        output &lt;- output + output_pre\n      }\n    }\n  }\n  # clean things up; make calculations on above values\n  output &lt;- output %&gt;%\n    mutate(detection_limit = split,\n           compound = compound,\n           time_start = start,\n           time_stop = stop,\n           time_window = tpt_use,\n           NAs = nrow(dataset) - nrow(df),\n           N = nrow(dataset_removedups),\n           N_removed = nrow(dataset) - nrow(dataset_removedups),\n           Sensitivity = (TP/(TP + FN)), \n           Specificity = (TN /(TN + FP)),\n           PPV = (TP/(TP+FP)),\n           NPV = (TN/(TN + FN)),\n           Efficiency = ((TP + TN)/(TP + TN + FP + FN))*100\n    )\n  \n  return(output)\n}\n\n\n\n\ndetermine what cutoff values to try\ncarry out above function on those cutoffs\n\n\nsens_spec &lt;- function(dataset, compound, start, stop, tpt_use, \n                      lowest_value = 0.5, splits = NULL, ...){\n  # if it's not all NAs...\n  if(sum(is.na(dataset[,compound])) != nrow(dataset)){\n    # specify what splits should be used for calculations\n    if(is.null(splits)){\n      limits &lt;- dataset[is.finite(rowSums(dataset[,compound])),compound]\n      ## define lower and upper limits\n      lower = min(limits, na.rm=TRUE)\n      upper = max(limits, na.rm=TRUE)\n      ## determine splits to use for calculations\n      tosplit = pull(limits[,1])[limits[,1]&gt;0]\n      ## only split if there are detectable limits:\n      if(length(tosplit)&gt;=1){\n        splits = c(lowest_value, quantile(tosplit, probs=seq(0, 1, by = 0.01), na.rm=TRUE))\n        splits = unique(splits)\n      }else{\n        splits = 0\n      }\n    }else{\n      splits = splits\n    }\n    # filter to include timepoint of interest\n    dataset &lt;- dataset %&gt;% \n      filter(time_from_start &gt; start & time_from_start &lt;= stop & !is.na(timepoint_use))\n    dataset_removedups &lt;- dataset %&gt;%\n      filter(!is.na(timepoint_use)) %&gt;% \n      group_by(timepoint_use) %&gt;% \n      distinct(id, .keep_all = TRUE) %&gt;% \n      ungroup()\n\n    ## create empty output variable which we'll fill in\n    ## iterate through each possible dose and calculate\n    output &lt;- map_dfr(as.list(splits), ~make_calculations(dataset, \n                                                          dataset_removedups, \n                                                          split = .x,\n                                                          compound,\n                                                          start = start,\n                                                          stop = stop, \n                                                          tpt_use = tpt_use))\n  }\n  \n  return(output)\n}\n\n\n\nMap the above for each matrix…\n\nsens_spec_cpd &lt;- function(dataset, cpd, timepoints, splits = NULL){\n  args2 &lt;- list(start = timepoints$start, \n                stop = timepoints$stop, \n                tpt_use = timepoints$timepoint)\n  out &lt;- args2 %&gt;% \n    pmap_dfr(sens_spec, dataset, compound = cpd, splits = splits)\n  return(out)\n}\n\n\n\nThis takes a few minutes to run… (reminder: cache=TRUE)\n\noutput_WB &lt;- map_dfr(compounds_WB, \n                     ~sens_spec_cpd(dataset = WB, cpd = all_of(.x), \n                                    timepoints = timepoints_WB)) %&gt;% clean_gluc()\n\noutput_BR &lt;- map_dfr(compounds_BR, \n                     ~sens_spec_cpd(dataset = BR,  cpd = all_of(.x),\n                                    timepoints = timepoints_BR))  %&gt;% clean_gluc()\n\noutput_OF &lt;- map_dfr(compounds_OF, \n                     ~sens_spec_cpd(dataset = OF, cpd = all_of(.x),\n                                    timepoints = timepoints_OF))  %&gt;% clean_gluc()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#roc-1",
    "href": "content/lectures/13-cs01-analysis.html#roc-1",
    "title": "13-cs01-analysis",
    "section": "ROC",
    "text": "ROC\n\nCodeCalculatePlot\n\n\n\nss_plot &lt;- function(output, tpts=8, tissue){\n  to_include = output %&gt;%\n    group_by(compound) %&gt;% \n    summarize(mean_detection = mean(detection_limit)) %&gt;% \n    filter(mean_detection &gt; 0)\n  \n  output &lt;-  output %&gt;% \n    mutate(iszero = ifelse(time_start&lt;0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %&gt;%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %&gt;%\n    clean_gluc() %&gt;% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output &lt;- output %&gt;%  mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#A2DDED', '#86BEDC', '#6C9FCA', \n                  '#547EB9', '#3F5EA8', '#2D4096', '#1E2385',\n                  '#181173', '#180762', '#180051')\n  values = c(blue_colors[1:tpts])\n  \n  print(ggplot(output, aes(x = detection_limit, y = Sensitivity, group = fct_inorder(legend))) + \n          geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n          geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n          facet_grid(~compound, scales = \"free_x\") +\n          labs(x = 'Detection Limit',\n               y = 'Sensitivity') +\n          ylim(0,1) +\n          scale_color_manual(values = values, name = 'Time Window') +\n          theme_classic(base_size = 12) + \n          theme(axis.title = element_text(size=16), \n                panel.grid = element_blank(),\n                strip.background = element_blank(),\n                strip.text.x = element_text(size = 12))  \n  )\n  print(\n    ggplot(output, aes(x = detection_limit, y = Specificity, group = fct_inorder(legend))) + \n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound, scales = \"free_x\") +\n      ylim(0,100) +\n      labs(title = tissue,\n           x = 'Detection Limit',\n           y = 'Specificity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12))\n  )\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12))\n  )\n}\n\nroc_plot &lt;- function(output, tpts=8, tissue){\n  to_include = output %&gt;%\n    group_by(compound) %&gt;% \n    summarize(mean_detection = mean(detection_limit)) %&gt;% \n    filter(mean_detection &gt; 0)\n  \n  output &lt;-  output %&gt;% \n    mutate(iszero = ifelse(time_start&lt;0,TRUE,FALSE),\n           Sensitivity = round(Sensitivity*100,0),\n           Specificity = round(Specificity*100,0)) %&gt;%\n    filter(compound %in% to_include$compound,\n           time_window != \"pre-smoking\") %&gt;%\n    clean_gluc() %&gt;% \n    mutate(compound = fct_relevel(as.factor(compound), \"THC\"))\n  \n  output &lt;- output %&gt;% mutate(\n    legend = paste0(time_window,' (N=', N,')'))\n  \n  blue_colors = c('#C2F8FF', '#86BEDC', \n                  '#547EB9', '#2D4096',\n                  '#181173', '#180051')\n  values = c(blue_colors[1:tpts])\n  print(\n    ggplot(output, aes(x=(100-Specificity), y = Sensitivity, group = fct_inorder(legend))) +\n      geom_point(aes(color=fct_inorder(legend)), size = 0.9, show.legend = FALSE) +\n      geom_path(aes(color=fct_inorder(legend)), size=1.2) + \n      facet_grid(~compound) +\n      xlim(0, 100) +\n      ylim(0, 100) +\n      labs(title = tissue,\n           x = '(100-Specificity)',\n           y = 'Sensitivity') +\n      scale_color_manual(values = values, name = 'Time Window') +\n      theme_classic(base_size = 12) + \n      theme(axis.title = element_text(size=16),\n            panel.grid = element_blank(),\n            strip.background = element_blank(),\n            strip.text.x = element_text(size = 12),\n            axis.text = element_text(size=12) )\n  )\n}\n\n\n\n\nss1_a &lt;- ss_plot(output_WB, tpts = length(unique(output_WB$time_start)), tissue = \"Blood\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nss2_a &lt;- ss_plot(output_OF, tpts = length(unique(output_OF$time_start)), tissue = \"Oral Fluid\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nss3_a &lt;- roc_plot(output_BR, tpts = length(unique(output_BR$time_start)), tissue = \"Breath\")\n\n\n\n\n\n\n\n\n\n\n\nbottom_row &lt;- plot_grid(ss2_a, ss3_a, labels = c('B', 'C'), label_size = 12, ncol = 2, rel_widths = c(0.66, .33))\nplot_grid(ss1_a, bottom_row, labels = c('A', ''), label_size = 12, ncol = 1)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculate-thc",
    "href": "content/lectures/13-cs01-analysis.html#calculate-thc",
    "title": "13-cs01-analysis",
    "section": "Calculate: THC",
    "text": "Calculate: THC\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOFBR\n\n\n\ncutoffs = c(0.5, 1, 2, 5, 10)\nWB_THC &lt;- sens_spec_cpd(dataset = WB, cpd = 'thc',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nWB_THC\n\n# A tibble: 50 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;           &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    81   108             0.5 THC            -400         0\n 2     0     0    61   128             1   THC            -400         0\n 3     0     0    45   144             2   THC            -400         0\n 4     0     0    10   179             5   THC            -400         0\n 5     0     0     1   188            10   THC            -400         0\n 6   124     2    28    33             0.5 THC               0        30\n 7   123     3    22    39             1   THC               0        30\n 8   119     7    15    46             2   THC               0        30\n 9   106    20     4    57             5   THC               0        30\n10   101    25     0    61            10   THC               0        30\n# ℹ 40 more rows\n# ℹ 9 more variables: time_window &lt;chr&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;\n\n\n\n\n\nOF_THC &lt;- sens_spec_cpd(dataset = OF, cpd = 'thc',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nOF_THC\n\n# A tibble: 40 × 17\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;           &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    35   157             0.5 THC            -400         0\n 2     0     0    20   172             1   THC            -400         0\n 3     0     0     9   183             2   THC            -400         0\n 4     0     0     0   192             5   THC            -400         0\n 5     0     0     0   192            10   THC            -400         0\n 6   129     0    39    24             0.5 THC               0        30\n 7   129     0    30    33             1   THC               0        30\n 8   128     1    19    44             2   THC               0        30\n 9   128     1     3    60             5   THC               0        30\n10   125     4     1    62            10   THC               0        30\n# ℹ 30 more rows\n# ℹ 9 more variables: time_window &lt;chr&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;\n\n\n\n\nWhy is there no calculation for breath with these cutoffs?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#cutoffs",
    "href": "content/lectures/13-cs01-analysis.html#cutoffs",
    "title": "13-cs01-analysis",
    "section": "Cutoffs",
    "text": "Cutoffs\n\nCodeWBOF\n\n\n\nplot_cutoffs &lt;- function(dataset, timepoint_use_variable, tissue, labels = c(\"A\", \"B\"), vertline, cpd, x_labels){\n    col_val = c(\"#D9D9D9\", \"#BDBDBD\", \"#969696\", \"#636363\", \"#252525\")\n    lines = rep(\"solid\", 5)\n    \n  df_ss &lt;- dataset %&gt;% \n    mutate(time_window = fct_relevel(as.factor(time_window), \n                                     levels(timepoint_use_variable)),\n           detection_limit = as.factor(detection_limit),\n           Sensitivity =  round(Sensitivity*100,0),\n           Specificity =  round(Specificity*100,0),\n           my_label = paste0(time_window, ' N=', N),\n           my_label =  gsub(\" \", \"\\n\", my_label),\n           my_label = fct_relevel(as.factor(my_label), x_labels)) #%&gt;%          \n    \n    p1 &lt;- df_ss %&gt;% \n    ggplot(aes(x = my_label, y = Sensitivity, \n               colour = detection_limit)) + \n    geom_line(size = 1.2, aes(group = detection_limit, \n                              linetype = detection_limit)) + \n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point(show.legend=FALSE) + \n    ylim(0,100) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values=lines) +\n      scale_color_manual(values = col_val, name = \"Cutoff \\n (ng/mL)\",\n                         guide = guide_legend(override.aes = list(linetype = c(1),\n                                                                  shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme( axis.title = element_text(size=16),\n           axis.text = element_text(size=10),\n           legend.position = c(0.08, 0.4),\n           panel.grid = element_blank(),\n           strip.background = element_blank()\n           ) +\n      guides(linetype = FALSE) +\n    labs(x = \"Time Window\", \n         y = \"Sensitivity\", \n         title = paste0(tissue,\": \", cpd) )\n \n  p2 &lt;- df_ss %&gt;% \n    ggplot(aes(x = my_label, y = Specificity,\n               group = detection_limit, \n               colour = detection_limit, \n               linetype = detection_limit)) + \n    geom_line(size = 1.2) +\n    geom_vline(xintercept=vertline, linetype = 'dotted') +\n    geom_point() + \n    ylim(0,100) +\n    scale_color_manual(values = col_val) +\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +\n    scale_linetype_manual(values = lines, \n                          guide = guide_legend(override.aes = list(linetype = \"solid\",\n                                                                   shape = rep(NA, length(lines))) )) +\n    theme_classic() +\n    theme(axis.title = element_text(size=16),\n          axis.text = element_text(size=10),\n          legend.position = \"none\", \n          panel.grid = element_blank(),\n          strip.background = element_blank()) +\n    labs(x = \"Time Window\", \n         y = \"Specificity\",\n         title = \"\" )\n  \n  title &lt;- ggdraw() + \n    draw_label(\n      tissue,\n      x = 0.05,\n      hjust = 0\n    )\n  \n  plot_row &lt;- plot_grid(p1, p2, labels = labels, label_size = 12)\n  \n  plot_grid(\n    title, plot_row,\n    ncol = 1,\n    # rel_heights values control vertical title margins\n    rel_heights = c(0.1, 1)\n  )\n  \n  return(list(plot_row, df_ss))\n\n}\n\n\n\n\nblood_levels &lt;- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(dataset=WB_THC, \n             timepoint_use_variable=WB$timepoint_use, \n             tissue=\"Blood\", \n             vertline=levels(WB$timepoint_use)[5], \n             cpd=\"THC\", \n             x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    81   108 0.5             THC            -400         0\n 2     0     0    61   128 1               THC            -400         0\n 3     0     0    45   144 2               THC            -400         0\n 4     0     0    10   179 5               THC            -400         0\n 5     0     0     1   188 10              THC            -400         0\n 6   124     2    28    33 0.5             THC               0        30\n 7   123     3    22    39 1               THC               0        30\n 8   119     7    15    46 2               THC               0        30\n 9   106    20     4    57 5               THC               0        30\n10   101    25     0    61 10              THC               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;\n\n\n\n\n\nof_levels &lt;- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_THC, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"THC\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0    35   157 0.5             THC            -400         0\n 2     0     0    20   172 1               THC            -400         0\n 3     0     0     9   183 2               THC            -400         0\n 4     0     0     0   192 5               THC            -400         0\n 5     0     0     0   192 10              THC            -400         0\n 6   129     0    39    24 0.5             THC               0        30\n 7   129     0    30    33 1               THC               0        30\n 8   128     1    19    44 2               THC               0        30\n 9   128     1     3    60 5               THC               0        30\n10   125     4     1    62 10              THC               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculate-cbn",
    "href": "content/lectures/13-cs01-analysis.html#calculate-cbn",
    "title": "13-cs01-analysis",
    "section": "Calculate: CBN",
    "text": "Calculate: CBN\nReminder: Currently, states have laws on the books from zero tolerance (detection of any level) to 5ng/mL\n\nWBOF\n\n\n\nWB_CBN =  sens_spec_cpd(dataset = WB, cpd = 'cbn',\n                        timepoints = timepoints_WB,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nblood_levels &lt;- c(\"pre-smoking\\nN=189\", \"0-30\\nmin\\nN=187\", \"31-70\\nmin\\nN=165\",\n                  \"71-100\\nmin\\nN=157\", \"101-180\\nmin\\nN=168\", \"181-210\\nmin\\nN=103\",\n                  \"211-240\\nmin\\nN=127\", \"241-270\\nmin\\nN=137\", \"271-300\\nmin\\nN=120\",\n                  \"301+\\nmin\\nN=88\")\n\nplot_cutoffs(WB_CBN, WB$timepoint_use, tissue = \"Blood\", vertline=levels(WB$timepoint_use)[5], cpd=\"CBN\", x_labels=blood_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 50 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0     1   188 0.5             CBN            -400         0\n 2     0     0     0   189 1               CBN            -400         0\n 3     0     0     0   189 2               CBN            -400         0\n 4     0     0     0   189 5               CBN            -400         0\n 5     0     0     0   189 10              CBN            -400         0\n 6   106    20     7    54 0.5             CBN               0        30\n 7    97    29     0    61 1               CBN               0        30\n 8    82    44     0    61 2               CBN               0        30\n 9    40    86     0    61 5               CBN               0        30\n10     9   117     0    61 10              CBN               0        30\n# ℹ 40 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;\n\n\n\n\n\nOF_CBN =  sens_spec_cpd(dataset = OF, cpd = 'cbn',\n                        timepoints = timepoints_OF,\n                        splits = cutoffs) %&gt;% clean_gluc()\n\nof_levels &lt;- c(\"pre-smoking\\nN=192\", \"0-30\\nmin\\nN=192\", \"31-90\\nmin\\nN=117\",\n               \"91-180\\nmin\\nN=99\", \"181-210\\nmin\\nN=102\", \"211-240\\nmin\\nN=83\",\n               \"241-270\\nmin\\nN=90\",  \"271+\\nmin\\nN=76\")\n\nplot_cutoffs(OF_CBN, OF$timepoint_use, tissue = \"Oral Fluid\", labels = c(\"A\", \"B\"), vertline=levels(OF$timepoint_use)[4], cpd=\"CBN\", x_labels=of_levels)\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n# A tibble: 40 × 18\n      TP    FN    FP    TN detection_limit compound time_start time_stop\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;           &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     0     0     5   187 0.5             CBN            -400         0\n 2     0     0     1   191 1               CBN            -400         0\n 3     0     0     1   191 2               CBN            -400         0\n 4     0     0     1   191 5               CBN            -400         0\n 5     0     0     0   192 10              CBN            -400         0\n 6   127     2    41    22 0.5             CBN               0        30\n 7   125     4    32    31 1               CBN               0        30\n 8   122     7    18    45 2               CBN               0        30\n 9   116    13     7    56 5               CBN               0        30\n10   107    22     3    60 10              CBN               0        30\n# ℹ 30 more rows\n# ℹ 10 more variables: time_window &lt;fct&gt;, NAs &lt;int&gt;, N &lt;int&gt;, N_removed &lt;int&gt;,\n#   Sensitivity &lt;dbl&gt;, Specificity &lt;dbl&gt;, PPV &lt;dbl&gt;, NPV &lt;dbl&gt;,\n#   Efficiency &lt;dbl&gt;, my_label &lt;fct&gt;",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#compound-correlations",
    "href": "content/lectures/13-cs01-analysis.html#compound-correlations",
    "title": "13-cs01-analysis",
    "section": "Compound Correlations",
    "text": "Compound Correlations\n\nCodePlot\n\n\n\nggplotRegression &lt;- function (x, y, xlab, ylab, x_text, y_text,  y_text2, title) {\n  fit &lt;- lm(y ~ x)\n  if(max(fit$model[,1],na.rm=TRUE)!=0){\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), \n                                               digits=2)),\n               vjust=1, hjust=0, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))),\n        vjust=1, hjust=0, size=4.5) + \n      theme_minimal(base_size=14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n  } else{\n    ggplot(fit$model, aes_string(x = names(fit$model)[2], \n                                 y = names(fit$model)[1])) + \n      geom_point() +\n      scale_y_continuous(limits = c(0,3)) +\n      stat_smooth(method = \"lm\", col = \"#B73239\", size = 1.5, se = FALSE) +\n      annotate(\"text\", x=x_text, y=y_text, \n               label = paste(\"R^2 == \", format(signif(summary(fit)$adj.r.squared, 5), digits=2)),vjust=1, hjust=1, parse=TRUE,size=4.5) +\n      labs(x = xlab, \n           y = ylab, \n           title = title ) +\n      annotate(\"text\", x=x_text, y=y_text2, label = paste(\n        \"y = \", format(signif(fit$coef[[2]], 5),digits=2),\n        \"x + \",\n        format(signif(fit$coef[[1]],5 ),digits=2),\n        paste0(\"\\nN = \", length(x))), vjust=1, hjust=1,size=4.5) + \n      theme_minimal(base_size = 14) +\n      theme(panel.grid = element_blank(),\n            axis.line = element_line(size = 0.5, linetype = \"solid\",\n                                     colour = \"black\"),\n            legend.position=\"none\") \n    \n    \n  }\n}\n\n\n\n\nwb_reg &lt;- ggplotRegression(WB$thc, WB$cbn, xlab = 'THC (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 150, y_text = 7, y_text2 = 5, title = \"Blood\")\n\nof_reg &lt;- ggplotRegression(OF$thc, OF$cbn, xlab = 'THC  (ng/mL)', ylab = 'CBN  (ng/mL)', x_text= 12500, y_text = 750, y_text2 = 500, title = \"Oral Fluid\")\n\nplot_grid(wb_reg, of_reg, labels = 'AUTO', label_size = 12, ncol = 2, scale = 1)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#possible-extensions",
    "href": "content/lectures/13-cs01-analysis.html#possible-extensions",
    "title": "13-cs01-analysis",
    "section": "Possible Extensions",
    "text": "Possible Extensions\nOur current question asks for a single compound…and you’ll need to decide that.\n. . .\n…but you could imagine a world where more than one compound or more than one matrix could be measured at the roadside.\n. . .\nSo:\n\n\ncombination of the oral fluid and blood that would better predict recent use? (For example if an officer stopped a driver and got a high oral fluid, but could not get a blood sample for a couple of hours and got a relatively low result would this predict recent use better than blood (or OF) alone?\nIs there a ratio of OF/blood that predicts recent use?\nMachine learning model to determine optimal combination of measurements/cutoffs to detect recent use?\n\n\n. . .\nThings to keep in mind:\n\nsome matrices are easier to get at the roadside\ntime from use matters (trying to detect recent use)\nwe may not care equally about sensitivity and specificity",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#cs01-what-to-do-now",
    "href": "content/lectures/13-cs01-analysis.html#cs01-what-to-do-now",
    "title": "13-cs01-analysis",
    "section": "cs01: what to do now?",
    "text": "cs01: what to do now?\n\nCommunicate with your group!\nDiscuss possible extensions\nMake a plan; figure out who’s doing what; set deadlines\nImplement the plan!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#what-has-to-be-done",
    "href": "content/lectures/13-cs01-analysis.html#what-has-to-be-done",
    "title": "13-cs01-analysis",
    "section": "What has to be done:",
    "text": "What has to be done:\n\n\nQuestion | include in Rmd; add extension if applicable\nBackground | summarize and add to what was discussed in classed\nData\n\nDescribe data & variables\nData wrangling | likely copy + paste from notes; add explanation as you go\n\nAnalysis\n\nEDA | likely borrowing parts from notes and adding more in; be sure to include interpretations of output & guide the reader\nAnalysis | likely borrowing most/all from class; interpretations/guiding reader/contextualizing is essential\nExtension | must be completed\n\nConclusion | summarize\nProofread | ensure it makes sense from top to bottom\nGeneral Audience communication (submit on Canvas; 1 submission per group)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#collaborating-on-github",
    "href": "content/lectures/13-cs01-analysis.html#collaborating-on-github",
    "title": "13-cs01-analysis",
    "section": "Collaborating on GitHub",
    "text": "Collaborating on GitHub\n\nBe sure to pull changes every time you sit down to work\nAvoid working on the same part of the same file as another teammate OR work in separate files and combine at the end\npush your changes once you’re ready to add them to the group",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#recap",
    "href": "content/lectures/13-cs01-analysis.html#recap",
    "title": "13-cs01-analysis",
    "section": "Recap",
    "text": "Recap\n\nCan you describe sensitivity? Specificity?\nCan you explain how TP, TN, FP, and FN were calculated/defined in this experiment?\nCan you describe the code used to carry out the calculations?\nCan you interpret the results from these data?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "13-cs01-analysis"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#qa",
    "href": "content/lectures/11-cs01-data-slides.html#qa",
    "title": "11-cs01-data",
    "section": "Q&A",
    "text": "Q&A\n\nQ: How much time are we expected to spend on the case studies?\nA: That’s hard to say. I would recommend spending a bit of time after each lecture ensuring I understand the code presented. It will eventually be included in your final report, so you’ll need to understand/describe/explain it. After the case study has been presented, I would expect a few hours from each group member to complete the extension and write the report. Last year students reported typically spending 4-6h on case studies (with a big range around that median).\n\n\nQ: For the general project plan how much time should we budget towards working on this?\nA: Students report spending ~10h on their final project\n\n\nQ: Are we allowed to work with some of our case study partners for a final project?\nA: Absolutely! My hope is through the case studies students will get to know one another a bit and hopefully want to work together again!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#course-announcements",
    "href": "content/lectures/11-cs01-data-slides.html#course-announcements",
    "title": "11-cs01-data",
    "section": "Course Announcements",
    "text": "Course Announcements\n\n💻 Midterm is due next Monday at 11:59PM (released Friday 5PM; practice answer keys are on website)\n❓ Mid-course survey will open with midterm - please complete after finishing midterm; will have a week to complete\n🧪 Lab is for midterm review; Lab05 & HW03 will be released next Monday",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#agenda",
    "href": "content/lectures/11-cs01-data-slides.html#agenda",
    "title": "11-cs01-data",
    "section": "Agenda",
    "text": "Agenda\n\nBackground\nData Intro\nPaper Results\nWrangle",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#motor-vehicle-accidents-mvas",
    "href": "content/lectures/11-cs01-data-slides.html#motor-vehicle-accidents-mvas",
    "title": "11-cs01-data",
    "section": "Motor Vehicle Accidents (MVAs)",
    "text": "Motor Vehicle Accidents (MVAs)\n\n2/3 of US trauma center admissions are due to MVAs\n~60% of such patients testing positive for drugs or alcohol\nAlcohol and cannabis are most frequently detected\n\nSource: https://academic.oup.com/clinchem/article/59/3/478/5621997",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#legalization-of-marijuana",
    "href": "content/lectures/11-cs01-data-slides.html#legalization-of-marijuana",
    "title": "11-cs01-data",
    "section": "Legalization of Marijuana",
    "text": "Legalization of Marijuana\n\nFederally illegal in the US\nDecriminalized in many states\nMedically available in 15 states\nLegal for recreational use in 24 states (including CA)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#increased-roadside-surveys",
    "href": "content/lectures/11-cs01-data-slides.html#increased-roadside-surveys",
    "title": "11-cs01-data",
    "section": "Increased roadside surveys",
    "text": "Increased roadside surveys\n\n\n25% increase in use nationwide from 2002 to 2015 (survey)\nTHC detection in drivers increased by 48% from 2007 to 2014\nIncreased prevalence of consumption -&gt; possible intoxication -&gt; possible impaired driving -&gt; public health concern",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#dui-of-alcohol-duia",
    "href": "content/lectures/11-cs01-data-slides.html#dui-of-alcohol-duia",
    "title": "11-cs01-data",
    "section": "DUI of Alcohol (DUIA)",
    "text": "DUI of Alcohol (DUIA)\n\n\nThe science is there. Don’t do it.\nDUIA has decreased since the 1970s\n\n% of nighttime, weekend drivers testing over the legal limit (BAC &gt; 0.08 g/dL) decreased from 7.5% (1973) to 2.2% (2007) link",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#dui-of-cannabis",
    "href": "content/lectures/11-cs01-data-slides.html#dui-of-cannabis",
    "title": "11-cs01-data",
    "section": "DUI of Cannabis",
    "text": "DUI of Cannabis\n\nIn a 2007 survey, 16.3% of nighttime drivers were drug-positive link\n\n8.6% of these tested positive for THC\n\nExperimental and cognitive studies suggest cannabis-induced impairment increases risk of motor vehicle crashes:\n\n\n\nEvidence suggests recent smoking and/or blood THC concentrations 2–5 ng/mL are associated with substantial driving impairment, particularly in occasional smokers.link",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#roadside-detection",
    "href": "content/lectures/11-cs01-data-slides.html#roadside-detection",
    "title": "11-cs01-data",
    "section": "Roadside Detection",
    "text": "Roadside Detection\n\nper se laws: “a driver is deemed to have committed an offense if THC is detected at or above a pre-determined cutoff” link\n\n\n\nDefining cutoffs for safe driving is difficult\nTHC concentration differs by:\n\n“smoking topography” (time to smoke; number of puffs)\nfrequency of use\nroute of ingestion\n\n\n\n\nAs of 2021…link\n\n\n19 states have per se or zero tolerance cannabis laws\nStates with per se laws (Illinois, Montana, Nevada, Ohio, Pennsylvania, Washington and West Virginia), cutoffs range from 1 to 5 ng/mL THC in whole blood.\nIn 3 states, per se limits also apply to THC metabolites\nColorado: “reasonable inference” - blood contained &gt;5 ng/mL THC at the time of the offense\n3 states zero tolerance for THC; 8 states for THC and metabolites",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#metabolism",
    "href": "content/lectures/11-cs01-data-slides.html#metabolism",
    "title": "11-cs01-data",
    "section": "Metabolism",
    "text": "Metabolism\n\n\npeak blood concentrations occur during smoking, then drop rapidly link\nsubjective ‘high’ persists for several hours, varies greatly between individuals\nTHC concentrations remain detectable in frequent users longer than occasional users link\nTHC and certain metabolites can be detected in blood for weeks to months after use and do not necessarily indicate impairment",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#detection",
    "href": "content/lectures/11-cs01-data-slides.html#detection",
    "title": "11-cs01-data",
    "section": "Detection",
    "text": "Detection\nVarious approaches:\n\nDetect impairment (officers detect DUIC)\nDetect recent use (test for compounds)\nCombine recent use + impairment\n\n\nFocus here: Can we identify a biomarker of recent use?\n\nrecent use: defined here as within 3h\ntesting THC and metabolites in blood, oral fluid (OF), and breath",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#aside-case-study-report",
    "href": "content/lectures/11-cs01-data-slides.html#aside-case-study-report",
    "title": "11-cs01-data",
    "section": "Aside: Case Study Report",
    "text": "Aside: Case Study Report\n\nYour Case study will need a background section\nIt can use/summarize/paraphrase the information here (you should cite the source, not me)\nBut, you’re not limited to this information\nYou are allowed/encouraged to dig deeper, include what’s most important, add to, remove, etc.\nThere are a lot of citations in this section - go ahead and peruse them/others/use references in these papers",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#participants",
    "href": "content/lectures/11-cs01-data-slides.html#participants",
    "title": "11-cs01-data",
    "section": "Participants",
    "text": "Participants\n\nplacebo-controlled, double-blinded, randomized study\n\n\n\nrecruited:\n\nvolunteers 21-55y/o\nhad a driver’s license\nself-reported cannabis use &gt;= 4x in the past month\n\n\n\n\n\nParticipants were:\n\ncompensated\nmedically evaluated (for safety)\nasked to refrain from use for 2d prior to participation\nexclusion criteria: OF THC concentration ≥5 ng/mL on day of study (n=7)\n\n\n\n\n\nStudy included 191 participants",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#demographics",
    "href": "content/lectures/11-cs01-data-slides.html#demographics",
    "title": "11-cs01-data",
    "section": "Demographics",
    "text": "Demographics\n\nSource: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#experimental-design",
    "href": "content/lectures/11-cs01-data-slides.html#experimental-design",
    "title": "11-cs01-data",
    "section": "Experimental Design",
    "text": "Experimental Design\nParticipants were:\n\n\nrandomly assigned to receive a cigarette containing placebo (0.02%), or 5.9% or 13.4% THC\nBlood, OF and breath were collected prior to smoking\nsmoked a 700 mg cigarette ad libitum within 10 min, with a minimum of four puffs.\nAfter smoking, 4 additional OF and breath and 8 blood collections were completed at time points up to ∼6h from the start of smoking.\nParticipants ate and drank water between collections, although not within 10 min of OF collection.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#timeline",
    "href": "content/lectures/11-cs01-data-slides.html#timeline",
    "title": "11-cs01-data",
    "section": "Timeline",
    "text": "Timeline\n\nSource: Fitzgerald et al.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#consumption",
    "href": "content/lectures/11-cs01-data-slides.html#consumption",
    "title": "11-cs01-data",
    "section": "Consumption",
    "text": "Consumption\n Source: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#topography",
    "href": "content/lectures/11-cs01-data-slides.html#topography",
    "title": "11-cs01-data",
    "section": "Topography",
    "text": "Topography\n Source: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#subjective-highness",
    "href": "content/lectures/11-cs01-data-slides.html#subjective-highness",
    "title": "11-cs01-data",
    "section": "Subjective Highness",
    "text": "Subjective Highness\n\nSource: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#our-datasets",
    "href": "content/lectures/11-cs01-data-slides.html#our-datasets",
    "title": "11-cs01-data",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#the-data-1",
    "href": "content/lectures/11-cs01-data-slides.html#the-data-1",
    "title": "11-cs01-data",
    "section": "The Data",
    "text": "The Data\nYou’ll have access once your groups/repos are created…(today I want people to follow along; there will be time to try on your own soon!)\n\nWB &lt;- read_csv(\"data/Blood.csv\")\nBR &lt;- read_csv(\"data/Breath.csv\")\nOF &lt;- read_csv(\"data/OF.csv\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-wb",
    "href": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-wb",
    "title": "11-cs01-data",
    "section": "First Look at the data (WB)",
    "text": "First Look at the data (WB)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-of",
    "href": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-of",
    "title": "11-cs01-data",
    "section": "First Look at the data (OF)",
    "text": "First Look at the data (OF)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-br",
    "href": "content/lectures/11-cs01-data-slides.html#first-look-at-the-data-br",
    "title": "11-cs01-data",
    "section": "First Look at the data (BR)",
    "text": "First Look at the data (BR)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-1-pre-smoking",
    "href": "content/lectures/11-cs01-data-slides.html#fig-1-pre-smoking",
    "title": "11-cs01-data",
    "section": "Fig 1: Pre-smoking",
    "text": "Fig 1: Pre-smoking",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-2-sensitivity-and-specificity",
    "href": "content/lectures/11-cs01-data-slides.html#fig-2-sensitivity-and-specificity",
    "title": "11-cs01-data",
    "section": "Fig 2: Sensitivity and Specificity",
    "text": "Fig 2: Sensitivity and Specificity",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-3-cross-compound-relationship",
    "href": "content/lectures/11-cs01-data-slides.html#fig-3-cross-compound-relationship",
    "title": "11-cs01-data",
    "section": "Fig 3: Cross-compound relationship",
    "text": "Fig 3: Cross-compound relationship",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-4-cutoffs",
    "href": "content/lectures/11-cs01-data-slides.html#fig-4-cutoffs",
    "title": "11-cs01-data",
    "section": "Fig 4: Cutoffs",
    "text": "Fig 4: Cutoffs",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#fig-5-youden",
    "href": "content/lectures/11-cs01-data-slides.html#fig-5-youden",
    "title": "11-cs01-data",
    "section": "Fig 5: Youden",
    "text": "Fig 5: Youden\n\n\n…and if there’s time PPV and Accuracy post 3h",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#what-came-after",
    "href": "content/lectures/11-cs01-data-slides.html#what-came-after",
    "title": "11-cs01-data",
    "section": "What Came After",
    "text": "What Came After\n Source: Fiztgerald et al.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#oral-fluid",
    "href": "content/lectures/11-cs01-data-slides.html#oral-fluid",
    "title": "11-cs01-data",
    "section": "Oral Fluid",
    "text": "Oral Fluid\n\nOF &lt;- OF |&gt;\n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |&gt;  \n  janitor::clean_names() |&gt;\n  rename(thcoh = x11_oh_thc,\n         thcv = thc_v)\n\n❓ What’s this accomplishing?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#whole-blood",
    "href": "content/lectures/11-cs01-data-slides.html#whole-blood",
    "title": "11-cs01-data",
    "section": "Whole Blood",
    "text": "Whole Blood\n\nWB &lt;- WB |&gt; \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\")) |&gt; \n  janitor::clean_names() |&gt;\n  rename(thcoh = x11_oh_thc,\n         thccooh = thc_cooh,\n         thccooh_gluc = thc_cooh_gluc,\n         thcv = thc_v)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#breath",
    "href": "content/lectures/11-cs01-data-slides.html#breath",
    "title": "11-cs01-data",
    "section": "Breath",
    "text": "Breath\n\nBR &lt;- BR |&gt; \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |&gt; \n  janitor::clean_names() |&gt; \n  rename(thc = thc_pg_pad)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#question-1",
    "href": "content/lectures/11-cs01-data-slides.html#question-1",
    "title": "11-cs01-data",
    "section": "Question",
    "text": "Question\n❓ We’re doing very similar things across three similar (albeit different) datasets. What would be a better approach?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#storing-compounds",
    "href": "content/lectures/11-cs01-data-slides.html#storing-compounds",
    "title": "11-cs01-data",
    "section": "Storing compounds",
    "text": "Storing compounds\nWe’ll need these later in our functions\n\n# whole blood\ncompounds_WB &lt;-  as.list(colnames(Filter(function(x) !all(is.na(x)), WB[6:13])))\n\n# breath\ncompounds_BR &lt;-  as.list(colnames(Filter(function(x) !all(is.na(x)), BR[6])))\n\n# oral fluid\ncompounds_OF &lt;-  as.list(colnames(Filter(function(x) !all(is.na(x)), OF[6:12])))\n\n\n\n# to get a sense of output\ncompounds_WB\n\n[[1]]\n[1] \"cbn\"\n\n[[2]]\n[1] \"cbd\"\n\n[[3]]\n[1] \"thc\"\n\n[[4]]\n[1] \"thcoh\"\n\n[[5]]\n[1] \"thccooh\"\n\n[[6]]\n[1] \"thccooh_gluc\"\n\n[[7]]\n[1] \"cbg\"\n\n[[8]]\n[1] \"thcv\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#storing-timepoints",
    "href": "content/lectures/11-cs01-data-slides.html#storing-timepoints",
    "title": "11-cs01-data",
    "section": "Storing timepoints",
    "text": "Storing timepoints\n\ntimepoints_WB = tibble(start = c(-400, 0, 30, 70, 100, 180, 210, 240, 270, 300), \n                       stop = c(0, 30, 70, 100, 180, 210, 240, 270, 300, max(WB$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-70 min\",\n                                     \"71-100 min\",\"101-180 min\",\"181-210 min\",\n                                     \"211-240 min\",\"241-270 min\",\n                                     \"271-300 min\", \"301+ min\") )\n\n\n\ntimepoints_WB\n\n# A tibble: 10 × 3\n   start  stop timepoint  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n 1  -400     0 pre-smoking\n 2     0    30 0-30 min   \n 3    30    70 31-70 min  \n 4    70   100 71-100 min \n 5   100   180 101-180 min\n 6   180   210 181-210 min\n 7   210   240 211-240 min\n 8   240   270 241-270 min\n 9   270   300 271-300 min\n10   300   382 301+ min   \n\n\n\n\n…and in BR and OF\n\ntimepoints_BR = tibble(start = c(-400, 0, 40, 90, 180, 210, 240, 270), \n                       stop = c(0, 40, 90, 180, 210, 240, 270, \n                                max(BR$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-40 min\",\"41-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\"))\ntimepoints_OF = tibble(start = c(-400, 0, 30, 90, 180, 210, 240, 270), \n                       stop = c(0, 30, 90, 180, 210, 240, 270, \n                                max(OF$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\") )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#first-udf-assign_timepoint",
    "href": "content/lectures/11-cs01-data-slides.html#first-udf-assign_timepoint",
    "title": "11-cs01-data",
    "section": "First UDF: assign_timepoint",
    "text": "First UDF: assign_timepoint\n\nassign_timepoint &lt;- function(x, timepoints){\n  if(!is.na(x)){ \n    timepoints$timepoint[x &gt; timepoints$start & x &lt;= timepoints$stop]\n  }else{\n    NA\n  }\n}\n\n🧠 What’s a UDF? What do you think this is doing?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#timepoints-to-use",
    "href": "content/lectures/11-cs01-data-slides.html#timepoints-to-use",
    "title": "11-cs01-data",
    "section": "Timepoints to use",
    "text": "Timepoints to use\n\n WB &lt;- WB |&gt; \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_WB),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_WB$timepoint))\n\n# let's get a sense for what this did\nlevels(WB$timepoint_use)\n\n [1] \"pre-smoking\" \"0-30 min\"    \"31-70 min\"   \"71-100 min\"  \"101-180 min\"\n [6] \"181-210 min\" \"211-240 min\" \"241-270 min\" \"271-300 min\" \"301+ min\"   \n\n\nNote: map_* allow you to apply a function across multiple “things” (here: across all rows in a dataframe)\n❓What do you think the above is doing?\n\n\nOF &lt;- OF |&gt; \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_OF),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_OF$timepoint))\n\nBR &lt;- BR |&gt; \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_BR),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_BR$timepoint))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#drop-duplicates",
    "href": "content/lectures/11-cs01-data-slides.html#drop-duplicates",
    "title": "11-cs01-data",
    "section": "Drop Duplicates",
    "text": "Drop Duplicates\n\n drop_dups &lt;- function(dataset){\n  out &lt;- dataset |&gt; \n    filter(!is.na(timepoint_use)) |&gt; \n    group_by(timepoint_use) |&gt; \n    distinct(id, .keep_all = TRUE) |&gt; \n    ungroup()\n  return(out)\n} \n\n❓What do you think the above is doing?\n\n\nWB_dups &lt;- drop_dups(WB)\nOF_dups &lt;- drop_dups(OF)\nBR_dups &lt;- drop_dups(BR)\n\n❓What would you do to try to understand what this has done?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#saving-intermediate-files",
    "href": "content/lectures/11-cs01-data-slides.html#saving-intermediate-files",
    "title": "11-cs01-data",
    "section": "Saving Intermediate Files",
    "text": "Saving Intermediate Files\nCleaned/wrangled files as CSVs:\n\nwrite_csv(WB, \"data/WB_clean.csv\")\nwrite_csv(BR, \"data/BR_clean.csv\")\nwrite_csv(OF, \"data/OF_clean.csv\")\n\nNote: can lose “type” of object (factor levels)\n\n(Alt) Save as RData:\n\nsave(compounds_WB, compounds_BR, compounds_OF, file=\"data/compounds.RData\")\nsave(timepoints_WB, timepoints_BR, timepoints_OF, file=\"data/timepoints.RData\")\nsave(WB, BR, OF, WB_dups, BR_dups, OF_dups, file=\"data/data_clean.RData\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#recap",
    "href": "content/lectures/11-cs01-data-slides.html#recap",
    "title": "11-cs01-data",
    "section": "Recap",
    "text": "Recap\n\nCould you summarize/explain background presented?\nCould you summarize the experiment that was done?\nCould you describe the datasets? (variables, observations, values, etc.)\nDo you understand/could you explain the wrangling that was done?\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html",
    "href": "content/lectures/11-cs01-data.html",
    "title": "11-cs01-data",
    "section": "",
    "text": "Q: How much time are we expected to spend on the case studies?\nA: That’s hard to say. I would recommend spending a bit of time after each lecture ensuring I understand the code presented. It will eventually be included in your final report, so you’ll need to understand/describe/explain it. After the case study has been presented, I would expect a few hours from each group member to complete the extension and write the report. Last year students reported typically spending 4-6h on case studies (with a big range around that median).\n\n\nQ: For the general project plan how much time should we budget towards working on this?\nA: Students report spending ~10h on their final project\n\n\nQ: Are we allowed to work with some of our case study partners for a final project?\nA: Absolutely! My hope is through the case studies students will get to know one another a bit and hopefully want to work together again!\n\n\n\n\n\n💻 Midterm is due next Monday at 11:59PM (released Friday 5PM; practice answer keys are on website)\n❓ Mid-course survey will open with midterm - please complete after finishing midterm; will have a week to complete\n🧪 Lab is for midterm review; Lab05 & HW03 will be released next Monday\n\n\n\n\n\nBackground\nData Intro\nPaper Results\nWrangle",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#qa",
    "href": "content/lectures/11-cs01-data.html#qa",
    "title": "11-cs01-data",
    "section": "",
    "text": "Q: How much time are we expected to spend on the case studies?\nA: That’s hard to say. I would recommend spending a bit of time after each lecture ensuring I understand the code presented. It will eventually be included in your final report, so you’ll need to understand/describe/explain it. After the case study has been presented, I would expect a few hours from each group member to complete the extension and write the report. Last year students reported typically spending 4-6h on case studies (with a big range around that median).\n\n\nQ: For the general project plan how much time should we budget towards working on this?\nA: Students report spending ~10h on their final project\n\n\nQ: Are we allowed to work with some of our case study partners for a final project?\nA: Absolutely! My hope is through the case studies students will get to know one another a bit and hopefully want to work together again!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#course-announcements",
    "href": "content/lectures/11-cs01-data.html#course-announcements",
    "title": "11-cs01-data",
    "section": "",
    "text": "💻 Midterm is due next Monday at 11:59PM (released Friday 5PM; practice answer keys are on website)\n❓ Mid-course survey will open with midterm - please complete after finishing midterm; will have a week to complete\n🧪 Lab is for midterm review; Lab05 & HW03 will be released next Monday",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#agenda",
    "href": "content/lectures/11-cs01-data.html#agenda",
    "title": "11-cs01-data",
    "section": "",
    "text": "Background\nData Intro\nPaper Results\nWrangle",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#motor-vehicle-accidents-mvas",
    "href": "content/lectures/11-cs01-data.html#motor-vehicle-accidents-mvas",
    "title": "11-cs01-data",
    "section": "Motor Vehicle Accidents (MVAs)",
    "text": "Motor Vehicle Accidents (MVAs)\n\n2/3 of US trauma center admissions are due to MVAs\n~60% of such patients testing positive for drugs or alcohol\nAlcohol and cannabis are most frequently detected\n\nSource: https://academic.oup.com/clinchem/article/59/3/478/5621997",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#legalization-of-marijuana",
    "href": "content/lectures/11-cs01-data.html#legalization-of-marijuana",
    "title": "11-cs01-data",
    "section": "Legalization of Marijuana",
    "text": "Legalization of Marijuana\n\nFederally illegal in the US\nDecriminalized in many states\nMedically available in 15 states\nLegal for recreational use in 24 states (including CA)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#increased-roadside-surveys",
    "href": "content/lectures/11-cs01-data.html#increased-roadside-surveys",
    "title": "11-cs01-data",
    "section": "Increased roadside surveys",
    "text": "Increased roadside surveys\n\n\n25% increase in use nationwide from 2002 to 2015 (survey)\nTHC detection in drivers increased by 48% from 2007 to 2014\nIncreased prevalence of consumption -&gt; possible intoxication -&gt; possible impaired driving -&gt; public health concern",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#dui-of-alcohol-duia",
    "href": "content/lectures/11-cs01-data.html#dui-of-alcohol-duia",
    "title": "11-cs01-data",
    "section": "DUI of Alcohol (DUIA)",
    "text": "DUI of Alcohol (DUIA)\n\n\nThe science is there. Don’t do it.\nDUIA has decreased since the 1970s\n\n% of nighttime, weekend drivers testing over the legal limit (BAC &gt; 0.08 g/dL) decreased from 7.5% (1973) to 2.2% (2007) link",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#dui-of-cannabis",
    "href": "content/lectures/11-cs01-data.html#dui-of-cannabis",
    "title": "11-cs01-data",
    "section": "DUI of Cannabis",
    "text": "DUI of Cannabis\n\nIn a 2007 survey, 16.3% of nighttime drivers were drug-positive link\n\n8.6% of these tested positive for THC\n\nExperimental and cognitive studies suggest cannabis-induced impairment increases risk of motor vehicle crashes:\n\n. . .\n\nEvidence suggests recent smoking and/or blood THC concentrations 2–5 ng/mL are associated with substantial driving impairment, particularly in occasional smokers.link",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#roadside-detection",
    "href": "content/lectures/11-cs01-data.html#roadside-detection",
    "title": "11-cs01-data",
    "section": "Roadside Detection",
    "text": "Roadside Detection\n\nper se laws: “a driver is deemed to have committed an offense if THC is detected at or above a pre-determined cutoff” link\n\n. . .\n\nDefining cutoffs for safe driving is difficult\nTHC concentration differs by:\n\n“smoking topography” (time to smoke; number of puffs)\nfrequency of use\nroute of ingestion\n\n\n. . .\nAs of 2021…link\n\n\n19 states have per se or zero tolerance cannabis laws\nStates with per se laws (Illinois, Montana, Nevada, Ohio, Pennsylvania, Washington and West Virginia), cutoffs range from 1 to 5 ng/mL THC in whole blood.\nIn 3 states, per se limits also apply to THC metabolites\nColorado: “reasonable inference” - blood contained &gt;5 ng/mL THC at the time of the offense\n3 states zero tolerance for THC; 8 states for THC and metabolites",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#metabolism",
    "href": "content/lectures/11-cs01-data.html#metabolism",
    "title": "11-cs01-data",
    "section": "Metabolism",
    "text": "Metabolism\n\n\npeak blood concentrations occur during smoking, then drop rapidly link\nsubjective ‘high’ persists for several hours, varies greatly between individuals\nTHC concentrations remain detectable in frequent users longer than occasional users link\nTHC and certain metabolites can be detected in blood for weeks to months after use and do not necessarily indicate impairment",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#detection",
    "href": "content/lectures/11-cs01-data.html#detection",
    "title": "11-cs01-data",
    "section": "Detection",
    "text": "Detection\nVarious approaches:\n\nDetect impairment (officers detect DUIC)\nDetect recent use (test for compounds)\nCombine recent use + impairment\n\n. . .\nFocus here: Can we identify a biomarker of recent use?\n\nrecent use: defined here as within 3h\ntesting THC and metabolites in blood, oral fluid (OF), and breath",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#aside-case-study-report",
    "href": "content/lectures/11-cs01-data.html#aside-case-study-report",
    "title": "11-cs01-data",
    "section": "Aside: Case Study Report",
    "text": "Aside: Case Study Report\n\nYour Case study will need a background section\nIt can use/summarize/paraphrase the information here (you should cite the source, not me)\nBut, you’re not limited to this information\nYou are allowed/encouraged to dig deeper, include what’s most important, add to, remove, etc.\nThere are a lot of citations in this section - go ahead and peruse them/others/use references in these papers",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#participants",
    "href": "content/lectures/11-cs01-data.html#participants",
    "title": "11-cs01-data",
    "section": "Participants",
    "text": "Participants\n\nplacebo-controlled, double-blinded, randomized study\n\n. . .\n\nrecruited:\n\nvolunteers 21-55y/o\nhad a driver’s license\nself-reported cannabis use &gt;= 4x in the past month\n\n\n. . .\n\nParticipants were:\n\ncompensated\nmedically evaluated (for safety)\nasked to refrain from use for 2d prior to participation\nexclusion criteria: OF THC concentration ≥5 ng/mL on day of study (n=7)\n\n\n. . .\n\nStudy included 191 participants",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#demographics",
    "href": "content/lectures/11-cs01-data.html#demographics",
    "title": "11-cs01-data",
    "section": "Demographics",
    "text": "Demographics\n\nSource: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#experimental-design",
    "href": "content/lectures/11-cs01-data.html#experimental-design",
    "title": "11-cs01-data",
    "section": "Experimental Design",
    "text": "Experimental Design\nParticipants were:\n\n\nrandomly assigned to receive a cigarette containing placebo (0.02%), or 5.9% or 13.4% THC\nBlood, OF and breath were collected prior to smoking\nsmoked a 700 mg cigarette ad libitum within 10 min, with a minimum of four puffs.\nAfter smoking, 4 additional OF and breath and 8 blood collections were completed at time points up to ∼6h from the start of smoking.\nParticipants ate and drank water between collections, although not within 10 min of OF collection.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#timeline",
    "href": "content/lectures/11-cs01-data.html#timeline",
    "title": "11-cs01-data",
    "section": "Timeline",
    "text": "Timeline\n\nSource: Fitzgerald et al.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#consumption",
    "href": "content/lectures/11-cs01-data.html#consumption",
    "title": "11-cs01-data",
    "section": "Consumption",
    "text": "Consumption\n Source: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#topography",
    "href": "content/lectures/11-cs01-data.html#topography",
    "title": "11-cs01-data",
    "section": "Topography",
    "text": "Topography\n Source: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#subjective-highness",
    "href": "content/lectures/11-cs01-data.html#subjective-highness",
    "title": "11-cs01-data",
    "section": "Subjective Highness",
    "text": "Subjective Highness\n\nSource: Hoffman et al.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#our-datasets",
    "href": "content/lectures/11-cs01-data.html#our-datasets",
    "title": "11-cs01-data",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n. . .\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#the-data-1",
    "href": "content/lectures/11-cs01-data.html#the-data-1",
    "title": "11-cs01-data",
    "section": "The Data",
    "text": "The Data\nYou’ll have access once your groups/repos are created…(today I want people to follow along; there will be time to try on your own soon!)\n\nWB &lt;- read_csv(\"data/Blood.csv\")\nBR &lt;- read_csv(\"data/Breath.csv\")\nOF &lt;- read_csv(\"data/OF.csv\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-wb",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-wb",
    "title": "11-cs01-data",
    "section": "First Look at the data (WB)",
    "text": "First Look at the data (WB)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-of",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-of",
    "title": "11-cs01-data",
    "section": "First Look at the data (OF)",
    "text": "First Look at the data (OF)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-look-at-the-data-br",
    "href": "content/lectures/11-cs01-data.html#first-look-at-the-data-br",
    "title": "11-cs01-data",
    "section": "First Look at the data (BR)",
    "text": "First Look at the data (BR)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-1-pre-smoking",
    "href": "content/lectures/11-cs01-data.html#fig-1-pre-smoking",
    "title": "11-cs01-data",
    "section": "Fig 1: Pre-smoking",
    "text": "Fig 1: Pre-smoking",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-2-sensitivity-and-specificity",
    "href": "content/lectures/11-cs01-data.html#fig-2-sensitivity-and-specificity",
    "title": "11-cs01-data",
    "section": "Fig 2: Sensitivity and Specificity",
    "text": "Fig 2: Sensitivity and Specificity",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-3-cross-compound-relationship",
    "href": "content/lectures/11-cs01-data.html#fig-3-cross-compound-relationship",
    "title": "11-cs01-data",
    "section": "Fig 3: Cross-compound relationship",
    "text": "Fig 3: Cross-compound relationship",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-4-cutoffs",
    "href": "content/lectures/11-cs01-data.html#fig-4-cutoffs",
    "title": "11-cs01-data",
    "section": "Fig 4: Cutoffs",
    "text": "Fig 4: Cutoffs",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#fig-5-youden",
    "href": "content/lectures/11-cs01-data.html#fig-5-youden",
    "title": "11-cs01-data",
    "section": "Fig 5: Youden",
    "text": "Fig 5: Youden\n\n. . .\n…and if there’s time PPV and Accuracy post 3h",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#what-came-after",
    "href": "content/lectures/11-cs01-data.html#what-came-after",
    "title": "11-cs01-data",
    "section": "What Came After",
    "text": "What Came After\n Source: Fiztgerald et al.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#oral-fluid",
    "href": "content/lectures/11-cs01-data.html#oral-fluid",
    "title": "11-cs01-data",
    "section": "Oral Fluid",
    "text": "Oral Fluid\n\nOF &lt;- OF |&gt;\n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |&gt;  \n  janitor::clean_names() |&gt;\n  rename(thcoh = x11_oh_thc,\n         thcv = thc_v)\n\n❓ What’s this accomplishing?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#whole-blood",
    "href": "content/lectures/11-cs01-data.html#whole-blood",
    "title": "11-cs01-data",
    "section": "Whole Blood",
    "text": "Whole Blood\n\nWB &lt;- WB |&gt; \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\")) |&gt; \n  janitor::clean_names() |&gt;\n  rename(thcoh = x11_oh_thc,\n         thccooh = thc_cooh,\n         thccooh_gluc = thc_cooh_gluc,\n         thcv = thc_v)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#breath",
    "href": "content/lectures/11-cs01-data.html#breath",
    "title": "11-cs01-data",
    "section": "Breath",
    "text": "Breath\n\nBR &lt;- BR |&gt; \n  mutate(Treatment = fct_recode(Treatment, \n                                \"5.9% THC (low dose)\" = \"5.90%\",\n                                \"13.4% THC (high dose)\" = \"13.40%\"),\n         Treatment = fct_relevel(Treatment, \"Placebo\", \"5.9% THC (low dose)\"),\n         Group = fct_recode(Group, \n                            \"Occasional user\" = \"Not experienced user\",\n                            \"Frequent user\" = \"Experienced user\" )) |&gt; \n  janitor::clean_names() |&gt; \n  rename(thc = thc_pg_pad)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#question-1",
    "href": "content/lectures/11-cs01-data.html#question-1",
    "title": "11-cs01-data",
    "section": "Question",
    "text": "Question\n❓ We’re doing very similar things across three similar (albeit different) datasets. What would be a better approach?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#storing-compounds",
    "href": "content/lectures/11-cs01-data.html#storing-compounds",
    "title": "11-cs01-data",
    "section": "Storing compounds",
    "text": "Storing compounds\nWe’ll need these later in our functions\n\n# whole blood\ncompounds_WB &lt;-  as.list(colnames(Filter(function(x) !all(is.na(x)), WB[6:13])))\n\n# breath\ncompounds_BR &lt;-  as.list(colnames(Filter(function(x) !all(is.na(x)), BR[6])))\n\n# oral fluid\ncompounds_OF &lt;-  as.list(colnames(Filter(function(x) !all(is.na(x)), OF[6:12])))\n\n. . .\n\n# to get a sense of output\ncompounds_WB\n\n[[1]]\n[1] \"cbn\"\n\n[[2]]\n[1] \"cbd\"\n\n[[3]]\n[1] \"thc\"\n\n[[4]]\n[1] \"thcoh\"\n\n[[5]]\n[1] \"thccooh\"\n\n[[6]]\n[1] \"thccooh_gluc\"\n\n[[7]]\n[1] \"cbg\"\n\n[[8]]\n[1] \"thcv\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#storing-timepoints",
    "href": "content/lectures/11-cs01-data.html#storing-timepoints",
    "title": "11-cs01-data",
    "section": "Storing timepoints",
    "text": "Storing timepoints\n\ntimepoints_WB = tibble(start = c(-400, 0, 30, 70, 100, 180, 210, 240, 270, 300), \n                       stop = c(0, 30, 70, 100, 180, 210, 240, 270, 300, max(WB$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-70 min\",\n                                     \"71-100 min\",\"101-180 min\",\"181-210 min\",\n                                     \"211-240 min\",\"241-270 min\",\n                                     \"271-300 min\", \"301+ min\") )\n\n. . .\n\ntimepoints_WB\n\n# A tibble: 10 × 3\n   start  stop timepoint  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n 1  -400     0 pre-smoking\n 2     0    30 0-30 min   \n 3    30    70 31-70 min  \n 4    70   100 71-100 min \n 5   100   180 101-180 min\n 6   180   210 181-210 min\n 7   210   240 211-240 min\n 8   240   270 241-270 min\n 9   270   300 271-300 min\n10   300   382 301+ min   \n\n\n. . .\n…and in BR and OF\n\ntimepoints_BR = tibble(start = c(-400, 0, 40, 90, 180, 210, 240, 270), \n                       stop = c(0, 40, 90, 180, 210, 240, 270, \n                                max(BR$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-40 min\",\"41-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\"))\ntimepoints_OF = tibble(start = c(-400, 0, 30, 90, 180, 210, 240, 270), \n                       stop = c(0, 30, 90, 180, 210, 240, 270, \n                                max(OF$time_from_start, na.rm = TRUE)), \n                       timepoint = c(\"pre-smoking\",\"0-30 min\",\"31-90 min\",\n                                     \"91-180 min\", \"181-210 min\", \"211-240 min\",\n                                     \"241-270 min\", \"271+ min\") )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#first-udf-assign_timepoint",
    "href": "content/lectures/11-cs01-data.html#first-udf-assign_timepoint",
    "title": "11-cs01-data",
    "section": "First UDF: assign_timepoint",
    "text": "First UDF: assign_timepoint\n\nassign_timepoint &lt;- function(x, timepoints){\n  if(!is.na(x)){ \n    timepoints$timepoint[x &gt; timepoints$start & x &lt;= timepoints$stop]\n  }else{\n    NA\n  }\n}\n\n🧠 What’s a UDF? What do you think this is doing?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#timepoints-to-use",
    "href": "content/lectures/11-cs01-data.html#timepoints-to-use",
    "title": "11-cs01-data",
    "section": "Timepoints to use",
    "text": "Timepoints to use\n\n WB &lt;- WB |&gt; \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_WB),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_WB$timepoint))\n\n# let's get a sense for what this did\nlevels(WB$timepoint_use)\n\n [1] \"pre-smoking\" \"0-30 min\"    \"31-70 min\"   \"71-100 min\"  \"101-180 min\"\n [6] \"181-210 min\" \"211-240 min\" \"241-270 min\" \"271-300 min\" \"301+ min\"   \n\n\nNote: map_* allow you to apply a function across multiple “things” (here: across all rows in a dataframe)\n❓What do you think the above is doing?\n. . .\n\nOF &lt;- OF |&gt; \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_OF),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_OF$timepoint))\n\nBR &lt;- BR |&gt; \n  mutate(timepoint_use = map_chr(time_from_start, \n                                 assign_timepoint, \n                                 timepoints=timepoints_BR),\n         timepoint_use = fct_relevel(timepoint_use, timepoints_BR$timepoint))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#drop-duplicates",
    "href": "content/lectures/11-cs01-data.html#drop-duplicates",
    "title": "11-cs01-data",
    "section": "Drop Duplicates",
    "text": "Drop Duplicates\n\n drop_dups &lt;- function(dataset){\n  out &lt;- dataset |&gt; \n    filter(!is.na(timepoint_use)) |&gt; \n    group_by(timepoint_use) |&gt; \n    distinct(id, .keep_all = TRUE) |&gt; \n    ungroup()\n  return(out)\n} \n\n❓What do you think the above is doing?\n. . .\n\nWB_dups &lt;- drop_dups(WB)\nOF_dups &lt;- drop_dups(OF)\nBR_dups &lt;- drop_dups(BR)\n\n❓What would you do to try to understand what this has done?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#saving-intermediate-files",
    "href": "content/lectures/11-cs01-data.html#saving-intermediate-files",
    "title": "11-cs01-data",
    "section": "Saving Intermediate Files",
    "text": "Saving Intermediate Files\nCleaned/wrangled files as CSVs:\n\nwrite_csv(WB, \"data/WB_clean.csv\")\nwrite_csv(BR, \"data/BR_clean.csv\")\nwrite_csv(OF, \"data/OF_clean.csv\")\n\nNote: can lose “type” of object (factor levels)\n. . .\n(Alt) Save as RData:\n\nsave(compounds_WB, compounds_BR, compounds_OF, file=\"data/compounds.RData\")\nsave(timepoints_WB, timepoints_BR, timepoints_OF, file=\"data/timepoints.RData\")\nsave(WB, BR, OF, WB_dups, BR_dups, OF_dups, file=\"data/data_clean.RData\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#recap",
    "href": "content/lectures/11-cs01-data.html#recap",
    "title": "11-cs01-data",
    "section": "Recap",
    "text": "Recap\n\nCould you summarize/explain background presented?\nCould you summarize the experiment that was done?\nCould you describe the datasets? (variables, observations, values, etc.)\nDo you understand/could you explain the wrangling that was done?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "11-cs01-data"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#qa",
    "href": "content/lectures/06-analysis-slides.html#qa",
    "title": "06-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I was most confused about the differences between facet_wrap and facet_grid. I didn’t understand what made them so different\nA: You’re right that they’re very similar and can even at times be used/coerced to do what the other does. Basically, if you want a 2D comparison (variable1 x variable2), facet_grid. If you just want to break down the plots by a single variable, facet_wrap.\n\n\nQ: Will future HW assignments also include things that were not explicitly covered in class? For example, HW1 which was due last night, included concepts and materials that were just covered today.\nA: Nope. That was my fault. We were behind where I intended to be. This will not be an issue going forward.\n\n\nQ: can we plot data from 2 separate data frames somehow?\nA: Not typically in ggplot2. Instead, data would have to be wrangled into the tidy format (single dataset) first.\n\n\nQ: does filling= mean seperate and color by the value passed in, and we don’t need to assign color again? What if we don’t want color?\nA: Fill indicates how to fill the plot with color. If you don’t want color, you wouldn’t use fill\n\n\nQ: How we would do the second suggestion in how to visualize the brexit data differently?\nA: We’ll do this together today!\n\n\nQ: for datasets that are included in a library (like penguins one), could we read it into a df to see it in the environment? or is that redundant and we should only view it, use functions to see var names, etc\nA: That would be ok!\n\n\nQ: I was wondering where I can find all available themes.\nA: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n\nQ: I have heard about plotly before. I was wondering what will be its difference between this and ggplot?\nA: There is a ggplotly package! Plotly enables interaction on top of ggplot2 packages",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#course-announcements",
    "href": "content/lectures/06-analysis-slides.html#course-announcements",
    "title": "06-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 03 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\n\nNotes:\n\nHow To: Chunk output in console\n\n\n\n\nCase Studies Discussion\n\nBiomarkers of recent marijuana usage (Linear Regression)\nVaping in American Youth (Logistic Regression)\nRight-to-Carry Laws Effect on Violent Crime (Multiple Linear Regression)\nPredicting Annual Air Pollution (Machine Learning)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#ad-css-masters",
    "href": "content/lectures/06-analysis-slides.html#ad-css-masters",
    "title": "06-analysis",
    "section": "[ad] CSS Masters",
    "text": "[ad] CSS Masters",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#agenda",
    "href": "content/lectures/06-analysis-slides.html#agenda",
    "title": "06-analysis",
    "section": "Agenda",
    "text": "Agenda\n\nDiscuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#suggested-reading",
    "href": "content/lectures/06-analysis-slides.html#suggested-reading",
    "title": "06-analysis",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nIntroduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#what-is-eda",
    "href": "content/lectures/06-analysis-slides.html#what-is-eda",
    "title": "06-analysis",
    "section": "What is EDA?",
    "text": "What is EDA?\n\nExploratory data analysis (EDA) is an aproach to analyzing data sets to summarize and understand its main characteristics.\nOften, this is visual….but the visuals do not have to be perfect. (Save that for communication)\nCalculating summary statistics is also part of EDA.\n\n\n\nData tidying/wrangling/manipulation/transformation typically happens before this stage of the analysis.\n\n\n\nThe Goal: KNOW YOUR DATA!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#consider-no.-of-variables-involved",
    "href": "content/lectures/06-analysis-slides.html#consider-no.-of-variables-involved",
    "title": "06-analysis",
    "section": "Consider: No. of variables involved",
    "text": "Consider: No. of variables involved\n\nUnivariate data analysis - distribution of single variable\nBivariate data analysis - relationship between two variables\nMultivariate data analysis - relationship between many variables at once, usually focusing on the relationship between two while conditioning for others",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#consider-types-of-variables",
    "href": "content/lectures/06-analysis-slides.html#consider-types-of-variables",
    "title": "06-analysis",
    "section": "Consider: Types of variables",
    "text": "Consider: Types of variables\n\nNumerical variables can be classified as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.\nIf the variable is categorical, we can determine if it is ordinal based on whether or not the levels have a natural ordering.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#data-visualization",
    "href": "content/lectures/06-analysis-slides.html#data-visualization",
    "title": "06-analysis",
    "section": "Data visualization",
    "text": "Data visualization\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\n\nData visualization is the creation and study of the visual representation of data.\nThere are many tools for visualizing data (R is one of them), and many approaches/systems within R for making data visualizations (ggplot2 is what we’ll continue to use).\nEDA will involve making plots/visualizing your data",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#paris-paintings",
    "href": "content/lectures/06-analysis-slides.html#paris-paintings",
    "title": "06-analysis",
    "section": "Paris Paintings",
    "text": "Paris Paintings\n\npp &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nSource: Printed catalogs of 28 auction sales in Paris, 1764 - 1780\nData curators Sandra van Ginhoven and Hilary Coe Cronheim (who were PhD students in the Duke Art, Law, and Markets Initiative at the time of putting together this dataset) translated and tabulated the catalogues\n3393 paintings, their prices, and descriptive details from sales catalogues over 60 variables",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auctions-today",
    "href": "content/lectures/06-analysis-slides.html#auctions-today",
    "title": "06-analysis",
    "section": "Auctions today",
    "text": "Auctions today\n\n\n\n\n\n\n\n\n\n\n\nSource: Sothebys",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auctions-back-in-the-day",
    "href": "content/lectures/06-analysis-slides.html#auctions-back-in-the-day",
    "title": "06-analysis",
    "section": "Auctions back in the day",
    "text": "Auctions back in the day\n\nSource: Pierre-Antoine de Machy, Public Sale at the Hôtel Bullion, Musée Carnavalet, Paris (18th century)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#paris-auction-market",
    "href": "content/lectures/06-analysis-slides.html#paris-auction-market",
    "title": "06-analysis",
    "section": "Paris auction market",
    "text": "Paris auction market",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse",
    "href": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse",
    "title": "06-analysis",
    "section": "Depart pour la chasse",
    "text": "Depart pour la chasse",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auction-catalogue-text",
    "href": "content/lectures/06-analysis-slides.html#auction-catalogue-text",
    "title": "06-analysis",
    "section": "Auction catalogue text",
    "text": "Auction catalogue text\n\n\n\n\nTwo paintings very rich in composition, of a beautiful execution, and whose merit is very remarkable, each 17 inches 3 lines high, 23 inches wide; the first, painted on wood, comes from the Cabinet of Madame la Comtesse de Verrue; it represents a departure for the hunt: it shows in the front a child on a white horse, a man who gives the horn to gather the dogs, a falconer and other figures nicely distributed across the width of the painting; two horses drinking from a fountain; on the right in the corner a lovely country house topped by a terrace, on which people are at the table, others who play instruments; trees and fabriques pleasantly enrich the background.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse-as-data",
    "href": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse-as-data",
    "title": "06-analysis",
    "section": "Depart pour la chasse as Data",
    "text": "Depart pour la chasse as Data",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#section",
    "href": "content/lectures/06-analysis-slides.html#section",
    "title": "06-analysis",
    "section": "",
    "text": "pp |&gt;\n  filter(name == \"R1777-89a\") |&gt;\n  glimpse()\n\nRows: 1\nColumns: 61\n$ name              &lt;chr&gt; \"R1777-89a\"\n$ sale              &lt;chr&gt; \"R1777\"\n$ lot               &lt;chr&gt; \"89\"\n$ position          &lt;dbl&gt; 0.3755274\n$ dealer            &lt;chr&gt; \"R\"\n$ year              &lt;dbl&gt; 1777\n$ origin_author     &lt;chr&gt; \"D/FL\"\n$ origin_cat        &lt;chr&gt; \"D/FL\"\n$ school_pntg       &lt;chr&gt; \"D/FL\"\n$ diff_origin       &lt;dbl&gt; 0\n$ logprice          &lt;dbl&gt; 8.575462\n$ price             &lt;dbl&gt; 5300\n$ count             &lt;dbl&gt; 1\n$ subject           &lt;chr&gt; \"D\\x8epart pour la chasse\"\n$ authorstandard    &lt;chr&gt; \"Wouwerman, Philips\"\n$ artistliving      &lt;dbl&gt; 0\n$ authorstyle       &lt;chr&gt; NA\n$ author            &lt;chr&gt; \"Philippe Wouwermans\"\n$ winningbidder     &lt;chr&gt; \"Langlier, Jacques for Poullain, Antoine\"\n$ winningbiddertype &lt;chr&gt; \"DC\"\n$ endbuyer          &lt;chr&gt; \"C\"\n$ Interm            &lt;dbl&gt; 1\n$ type_intermed     &lt;chr&gt; \"D\"\n$ Height_in         &lt;dbl&gt; 17.25\n$ Width_in          &lt;dbl&gt; 23\n$ Surface_Rect      &lt;dbl&gt; 396.75\n$ Diam_in           &lt;dbl&gt; NA\n$ Surface_Rnd       &lt;dbl&gt; NA\n$ Shape             &lt;chr&gt; \"squ_rect\"\n$ Surface           &lt;dbl&gt; 396.75\n$ material          &lt;chr&gt; \"bois\"\n$ mat               &lt;chr&gt; \"b\"\n$ materialCat       &lt;chr&gt; \"wood\"\n$ quantity          &lt;dbl&gt; 1\n$ nfigures          &lt;dbl&gt; 0\n$ engraved          &lt;dbl&gt; 0\n$ original          &lt;dbl&gt; 0\n$ prevcoll          &lt;dbl&gt; 1\n$ othartist         &lt;dbl&gt; 0\n$ paired            &lt;dbl&gt; 1\n$ figures           &lt;dbl&gt; 0\n$ finished          &lt;dbl&gt; 0\n$ lrgfont           &lt;dbl&gt; 0\n$ relig             &lt;dbl&gt; 0\n$ landsALL          &lt;dbl&gt; 1\n$ lands_sc          &lt;dbl&gt; 0\n$ lands_elem        &lt;dbl&gt; 1\n$ lands_figs        &lt;dbl&gt; 1\n$ lands_ment        &lt;dbl&gt; 0\n$ arch              &lt;dbl&gt; 1\n$ mytho             &lt;dbl&gt; 0\n$ peasant           &lt;dbl&gt; 0\n$ othgenre          &lt;dbl&gt; 0\n$ singlefig         &lt;dbl&gt; 0\n$ portrait          &lt;dbl&gt; 0\n$ still_life        &lt;dbl&gt; 0\n$ discauth          &lt;dbl&gt; 0\n$ history           &lt;dbl&gt; 0\n$ allegory          &lt;dbl&gt; 0\n$ pastorale         &lt;dbl&gt; 0\n$ other             &lt;dbl&gt; 0",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-numerical-data",
    "href": "content/lectures/06-analysis-slides.html#visualizing-numerical-data",
    "title": "06-analysis",
    "section": "Visualizing numerical data",
    "text": "Visualizing numerical data\nDescribing shapes of numerical distributions\n\nshape:\n\nskewness: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail)\nmodality: unimodal, bimodal, multimodal, uniform\n\ncenter: mean (mean), median (median), mode (not always useful)\nspread: range (range), standard deviation (sd), inter-quartile range (IQR)\nunusual observations",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#histograms",
    "href": "content/lectures/06-analysis-slides.html#histograms",
    "title": "06-analysis",
    "section": "Histograms",
    "text": "Histograms\n\nHeightsWidthsPrices\n\n\n\nggplot(data = pp, aes(x = Height_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Height, in inches\", y = NULL)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = Width_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Width, in inches\", y = NULL)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 100) +\n  labs(x = \"Price\", y = NULL)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#density-plots",
    "href": "content/lectures/06-analysis-slides.html#density-plots",
    "title": "06-analysis",
    "section": "Density plots",
    "text": "Density plots\n\nggplot(data = pp, mapping = aes(x = Height_in)) +\n  geom_density()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#side-by-side-box-plots",
    "href": "content/lectures/06-analysis-slides.html#side-by-side-box-plots",
    "title": "06-analysis",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = pp, mapping = aes(y = Height_in, x = as.factor(landsALL))) +\n  geom_boxplot()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-categorical-data",
    "href": "content/lectures/06-analysis-slides.html#visualizing-categorical-data",
    "title": "06-analysis",
    "section": "Visualizing categorical data",
    "text": "Visualizing categorical data\n\ncount/proportion of values\nunusual observations",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#bar-plots",
    "href": "content/lectures/06-analysis-slides.html#bar-plots",
    "title": "06-analysis",
    "section": "Bar plots",
    "text": "Bar plots\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL))) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#segmented-bar-plots-counts",
    "href": "content/lectures/06-analysis-slides.html#segmented-bar-plots-counts",
    "title": "06-analysis",
    "section": "Segmented bar plots, counts",
    "text": "Segmented bar plots, counts\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#segmented-bar-plots-proportions",
    "href": "content/lectures/06-analysis-slides.html#segmented-bar-plots-proportions",
    "title": "06-analysis",
    "section": "Segmented bar plots, proportions",
    "text": "Segmented bar plots, proportions\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#your-turn",
    "href": "content/lectures/06-analysis-slides.html#your-turn",
    "title": "06-analysis",
    "section": "Your Turn",
    "text": "Your Turn\n❓ Which of the previous two bar plots is a more useful representation for visualizing the relationship between landscape and painting material?\n\n❓ What else would you want to do/know to complete EDA?\n\n\n🧠 Try to answer at least one thing you’d want to know from the dataset.\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#modelling-1",
    "href": "content/lectures/06-analysis-slides.html#modelling-1",
    "title": "06-analysis",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we focus on linear models (but remember there are other types of models too!)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#packages",
    "href": "content/lectures/06-analysis-slides.html#packages",
    "title": "06-analysis",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nYou’re familiar with the tidyverse:\n\n\nlibrary(tidyverse)\n\n\nThe broom package takes the messy output of built-in functions in R, such as lm, and turns them into tidy data frames.\n\n\nlibrary(broom)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#modelling-the-relationship-between-variables",
    "href": "content/lectures/06-analysis-slides.html#modelling-the-relationship-between-variables",
    "title": "06-analysis",
    "section": "Modelling the relationship between variables",
    "text": "Modelling the relationship between variables\nEDA: Prices\n❗ Describe the distribution of prices of paintings.\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 1000)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#models-as-functions",
    "href": "content/lectures/06-analysis-slides.html#models-as-functions",
    "title": "06-analysis",
    "section": "Models as functions",
    "text": "Models as functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs.\n\nPlug in the inputs and receive back the output\nExample: the formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\), when \\(x\\) is \\(5\\), the output \\(y\\) is \\(22\\)\n\n\ny = 3 * 5 + 7 = 22",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#height-as-a-function-of-width",
    "href": "content/lectures/06-analysis-slides.html#height-as-a-function-of-width",
    "title": "06-analysis",
    "section": "Height as a function of width",
    "text": "Height as a function of width\n❗ Describe the relationship between height and width of paintings.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # lm for linear model",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-1",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-1",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… without the measure of uncertainty around the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) # lm for linear model",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-2",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-2",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… with different cosmetic choices for the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, \n              col = \"pink\",      # color\n              lty = 2,           # line type\n              linewidth = 3)     # line weight",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#vocabulary",
    "href": "content/lectures/06-analysis-slides.html#vocabulary",
    "title": "06-analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis (dependent variable)\n\n\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis (independent variables)\n\n\n\n\nPredicted value: Output of the function model function\n\nThe model function gives the typical value of the response variable conditioning on the explanatory variables\n\n\n\n\n\nResiduals: Show how far each case is from its model value\n\nResidual = Observed value - Predicted value\nTells how far above/below the model function each case is",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#residuals",
    "href": "content/lectures/06-analysis-slides.html#residuals",
    "title": "06-analysis",
    "section": "Residuals",
    "text": "Residuals\n❓ What does a negative residual mean? Which paintings on the plot have have negative residuals?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#section-1",
    "href": "content/lectures/06-analysis-slides.html#section-1",
    "title": "06-analysis",
    "section": "",
    "text": "The plot below displays the relationship between height and width of paintings. It uses a lower alpha level for the points than the previous plots we looked at.\n\n❓ What feature is apparent in this plot that was not (as) apparent in the previous plots? What might be the reason for this feature?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#landscape-paintings",
    "href": "content/lectures/06-analysis-slides.html#landscape-paintings",
    "title": "06-analysis",
    "section": "Landscape paintings",
    "text": "Landscape paintings\n\nLandscape painting is the depiction in art of landscapes – natural scenery such as mountains, valleys, trees, rivers, and forests, especially where the main subject is a wide view – with its elements arranged into a coherent composition.1\n\nLandscape paintings tend to be wider than longer.\n\nPortrait painting is a genre in painting, where the intent is to depict a human subject.2\n\nPortrait paintings tend to be longer than wider.\n\n\nSource: Wikipedia, Landscape paintingSource: Wikipedia, Portait painting",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#multiple-explanatory-variables",
    "href": "content/lectures/06-analysis-slides.html#multiple-explanatory-variables",
    "title": "06-analysis",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\n❓ How, if at all, does the relationship between width and height of paintings vary by whether or not they have any landscape elements?\n\nggplot(data = pp, aes(x = Width_in, y = Height_in, \n                      color = factor(landsALL))) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"landscape\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#models---upsides-and-downsides",
    "href": "content/lectures/06-analysis-slides.html#models---upsides-and-downsides",
    "title": "06-analysis",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modelling over simple visual inspection of data.\n\n\n\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#variation-around-the-model",
    "href": "content/lectures/06-analysis-slides.html#variation-around-the-model",
    "title": "06-analysis",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\n\nStatistics is the explanation of variation in the context of what remains unexplained.\n\n\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#how-do-we-use-models",
    "href": "content/lectures/06-analysis-slides.html#how-do-we-use-models",
    "title": "06-analysis",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nExplanation: Characterize the relationship between \\(y\\) and \\(x\\) via slopes for numerical explanatory variables or differences for categorical explanatory variables (Inference)\nPrediction: Plug in \\(x\\), get the predicted \\(y\\) (Machine Learning)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html",
    "href": "content/lectures/06-analysis.html",
    "title": "06-analysis",
    "section": "",
    "text": "Q: I was most confused about the differences between facet_wrap and facet_grid. I didn’t understand what made them so different\nA: You’re right that they’re very similar and can even at times be used/coerced to do what the other does. Basically, if you want a 2D comparison (variable1 x variable2), facet_grid. If you just want to break down the plots by a single variable, facet_wrap.\n\n\nQ: Will future HW assignments also include things that were not explicitly covered in class? For example, HW1 which was due last night, included concepts and materials that were just covered today.\nA: Nope. That was my fault. We were behind where I intended to be. This will not be an issue going forward.\n\n\nQ: can we plot data from 2 separate data frames somehow?\nA: Not typically in ggplot2. Instead, data would have to be wrangled into the tidy format (single dataset) first.\n\n\nQ: does filling= mean seperate and color by the value passed in, and we don’t need to assign color again? What if we don’t want color?\nA: Fill indicates how to fill the plot with color. If you don’t want color, you wouldn’t use fill\n\n\nQ: How we would do the second suggestion in how to visualize the brexit data differently?\nA: We’ll do this together today!\n\n\nQ: for datasets that are included in a library (like penguins one), could we read it into a df to see it in the environment? or is that redundant and we should only view it, use functions to see var names, etc\nA: That would be ok!\n\n\nQ: I was wondering where I can find all available themes.\nA: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n\nQ: I have heard about plotly before. I was wondering what will be its difference between this and ggplot?\nA: There is a ggplotly package! Plotly enables interaction on top of ggplot2 packages\n\n\n\n\nDue Dates:\n\nLab 03 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\n. . .\nNotes:\n\nHow To: Chunk output in console\n\n. . .\n\nCase Studies Discussion\n\nBiomarkers of recent marijuana usage (Linear Regression)\nVaping in American Youth (Logistic Regression)\nRight-to-Carry Laws Effect on Violent Crime (Multiple Linear Regression)\nPredicting Annual Air Pollution (Machine Learning)\n\n\n\n\n\n\n\n\n\n\nDiscuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)\n\n\n\n\n\n\nIntroduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#qa",
    "href": "content/lectures/06-analysis.html#qa",
    "title": "06-analysis",
    "section": "",
    "text": "Q: I was most confused about the differences between facet_wrap and facet_grid. I didn’t understand what made them so different\nA: You’re right that they’re very similar and can even at times be used/coerced to do what the other does. Basically, if you want a 2D comparison (variable1 x variable2), facet_grid. If you just want to break down the plots by a single variable, facet_wrap.\n\n\nQ: Will future HW assignments also include things that were not explicitly covered in class? For example, HW1 which was due last night, included concepts and materials that were just covered today.\nA: Nope. That was my fault. We were behind where I intended to be. This will not be an issue going forward.\n\n\nQ: can we plot data from 2 separate data frames somehow?\nA: Not typically in ggplot2. Instead, data would have to be wrangled into the tidy format (single dataset) first.\n\n\nQ: does filling= mean seperate and color by the value passed in, and we don’t need to assign color again? What if we don’t want color?\nA: Fill indicates how to fill the plot with color. If you don’t want color, you wouldn’t use fill\n\n\nQ: How we would do the second suggestion in how to visualize the brexit data differently?\nA: We’ll do this together today!\n\n\nQ: for datasets that are included in a library (like penguins one), could we read it into a df to see it in the environment? or is that redundant and we should only view it, use functions to see var names, etc\nA: That would be ok!\n\n\nQ: I was wondering where I can find all available themes.\nA: https://ggplot2.tidyverse.org/reference/ggtheme.html\n\n\nQ: I have heard about plotly before. I was wondering what will be its difference between this and ggplot?\nA: There is a ggplotly package! Plotly enables interaction on top of ggplot2 packages",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#course-announcements",
    "href": "content/lectures/06-analysis.html#course-announcements",
    "title": "06-analysis",
    "section": "",
    "text": "Due Dates:\n\nLab 03 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\n. . .\nNotes:\n\nHow To: Chunk output in console\n\n. . .\n\nCase Studies Discussion\n\nBiomarkers of recent marijuana usage (Linear Regression)\nVaping in American Youth (Logistic Regression)\nRight-to-Carry Laws Effect on Violent Crime (Multiple Linear Regression)\nPredicting Annual Air Pollution (Machine Learning)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#agenda",
    "href": "content/lectures/06-analysis.html#agenda",
    "title": "06-analysis",
    "section": "",
    "text": "Discuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#suggested-reading",
    "href": "content/lectures/06-analysis.html#suggested-reading",
    "title": "06-analysis",
    "section": "",
    "text": "Introduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#what-is-eda",
    "href": "content/lectures/06-analysis.html#what-is-eda",
    "title": "06-analysis",
    "section": "What is EDA?",
    "text": "What is EDA?\n\nExploratory data analysis (EDA) is an aproach to analyzing data sets to summarize and understand its main characteristics.\nOften, this is visual….but the visuals do not have to be perfect. (Save that for communication)\nCalculating summary statistics is also part of EDA.\n\n. . .\n\nData tidying/wrangling/manipulation/transformation typically happens before this stage of the analysis.\n\n. . .\nThe Goal: KNOW YOUR DATA!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "href": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "title": "06-analysis",
    "section": "Consider: No. of variables involved",
    "text": "Consider: No. of variables involved\n\nUnivariate data analysis - distribution of single variable\nBivariate data analysis - relationship between two variables\nMultivariate data analysis - relationship between many variables at once, usually focusing on the relationship between two while conditioning for others",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-types-of-variables",
    "href": "content/lectures/06-analysis.html#consider-types-of-variables",
    "title": "06-analysis",
    "section": "Consider: Types of variables",
    "text": "Consider: Types of variables\n\nNumerical variables can be classified as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.\nIf the variable is categorical, we can determine if it is ordinal based on whether or not the levels have a natural ordering.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#data-visualization",
    "href": "content/lectures/06-analysis.html#data-visualization",
    "title": "06-analysis",
    "section": "Data visualization",
    "text": "Data visualization\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\n\nData visualization is the creation and study of the visual representation of data.\nThere are many tools for visualizing data (R is one of them), and many approaches/systems within R for making data visualizations (ggplot2 is what we’ll continue to use).\nEDA will involve making plots/visualizing your data",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-paintings",
    "href": "content/lectures/06-analysis.html#paris-paintings",
    "title": "06-analysis",
    "section": "Paris Paintings",
    "text": "Paris Paintings\n\npp &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nSource: Printed catalogs of 28 auction sales in Paris, 1764 - 1780\nData curators Sandra van Ginhoven and Hilary Coe Cronheim (who were PhD students in the Duke Art, Law, and Markets Initiative at the time of putting together this dataset) translated and tabulated the catalogues\n3393 paintings, their prices, and descriptive details from sales catalogues over 60 variables",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-today",
    "href": "content/lectures/06-analysis.html#auctions-today",
    "title": "06-analysis",
    "section": "Auctions today",
    "text": "Auctions today\n\n\n\n\n\n\n\n\n\n\n\nSource: Sothebys",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "href": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "title": "06-analysis",
    "section": "Auctions back in the day",
    "text": "Auctions back in the day\n\n\n\n\n\n\n\n\n\nSource: Pierre-Antoine de Machy, Public Sale at the Hôtel Bullion, Musée Carnavalet, Paris (18th century)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-auction-market",
    "href": "content/lectures/06-analysis.html#paris-auction-market",
    "title": "06-analysis",
    "section": "Paris auction market",
    "text": "Paris auction market",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "title": "06-analysis",
    "section": "Depart pour la chasse",
    "text": "Depart pour la chasse",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#auction-catalogue-text",
    "href": "content/lectures/06-analysis.html#auction-catalogue-text",
    "title": "06-analysis",
    "section": "Auction catalogue text",
    "text": "Auction catalogue text\n\n\n\n\nTwo paintings very rich in composition, of a beautiful execution, and whose merit is very remarkable, each 17 inches 3 lines high, 23 inches wide; the first, painted on wood, comes from the Cabinet of Madame la Comtesse de Verrue; it represents a departure for the hunt: it shows in the front a child on a white horse, a man who gives the horn to gather the dogs, a falconer and other figures nicely distributed across the width of the painting; two horses drinking from a fountain; on the right in the corner a lovely country house topped by a terrace, on which people are at the table, others who play instruments; trees and fabriques pleasantly enrich the background.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "title": "06-analysis",
    "section": "Depart pour la chasse as Data",
    "text": "Depart pour la chasse as Data",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#section",
    "href": "content/lectures/06-analysis.html#section",
    "title": "06-analysis",
    "section": "",
    "text": "pp |&gt;\n  filter(name == \"R1777-89a\") |&gt;\n  glimpse()\n\nRows: 1\nColumns: 61\n$ name              &lt;chr&gt; \"R1777-89a\"\n$ sale              &lt;chr&gt; \"R1777\"\n$ lot               &lt;chr&gt; \"89\"\n$ position          &lt;dbl&gt; 0.3755274\n$ dealer            &lt;chr&gt; \"R\"\n$ year              &lt;dbl&gt; 1777\n$ origin_author     &lt;chr&gt; \"D/FL\"\n$ origin_cat        &lt;chr&gt; \"D/FL\"\n$ school_pntg       &lt;chr&gt; \"D/FL\"\n$ diff_origin       &lt;dbl&gt; 0\n$ logprice          &lt;dbl&gt; 8.575462\n$ price             &lt;dbl&gt; 5300\n$ count             &lt;dbl&gt; 1\n$ subject           &lt;chr&gt; \"D\\x8epart pour la chasse\"\n$ authorstandard    &lt;chr&gt; \"Wouwerman, Philips\"\n$ artistliving      &lt;dbl&gt; 0\n$ authorstyle       &lt;chr&gt; NA\n$ author            &lt;chr&gt; \"Philippe Wouwermans\"\n$ winningbidder     &lt;chr&gt; \"Langlier, Jacques for Poullain, Antoine\"\n$ winningbiddertype &lt;chr&gt; \"DC\"\n$ endbuyer          &lt;chr&gt; \"C\"\n$ Interm            &lt;dbl&gt; 1\n$ type_intermed     &lt;chr&gt; \"D\"\n$ Height_in         &lt;dbl&gt; 17.25\n$ Width_in          &lt;dbl&gt; 23\n$ Surface_Rect      &lt;dbl&gt; 396.75\n$ Diam_in           &lt;dbl&gt; NA\n$ Surface_Rnd       &lt;dbl&gt; NA\n$ Shape             &lt;chr&gt; \"squ_rect\"\n$ Surface           &lt;dbl&gt; 396.75\n$ material          &lt;chr&gt; \"bois\"\n$ mat               &lt;chr&gt; \"b\"\n$ materialCat       &lt;chr&gt; \"wood\"\n$ quantity          &lt;dbl&gt; 1\n$ nfigures          &lt;dbl&gt; 0\n$ engraved          &lt;dbl&gt; 0\n$ original          &lt;dbl&gt; 0\n$ prevcoll          &lt;dbl&gt; 1\n$ othartist         &lt;dbl&gt; 0\n$ paired            &lt;dbl&gt; 1\n$ figures           &lt;dbl&gt; 0\n$ finished          &lt;dbl&gt; 0\n$ lrgfont           &lt;dbl&gt; 0\n$ relig             &lt;dbl&gt; 0\n$ landsALL          &lt;dbl&gt; 1\n$ lands_sc          &lt;dbl&gt; 0\n$ lands_elem        &lt;dbl&gt; 1\n$ lands_figs        &lt;dbl&gt; 1\n$ lands_ment        &lt;dbl&gt; 0\n$ arch              &lt;dbl&gt; 1\n$ mytho             &lt;dbl&gt; 0\n$ peasant           &lt;dbl&gt; 0\n$ othgenre          &lt;dbl&gt; 0\n$ singlefig         &lt;dbl&gt; 0\n$ portrait          &lt;dbl&gt; 0\n$ still_life        &lt;dbl&gt; 0\n$ discauth          &lt;dbl&gt; 0\n$ history           &lt;dbl&gt; 0\n$ allegory          &lt;dbl&gt; 0\n$ pastorale         &lt;dbl&gt; 0\n$ other             &lt;dbl&gt; 0",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "href": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "title": "06-analysis",
    "section": "Visualizing numerical data",
    "text": "Visualizing numerical data\nDescribing shapes of numerical distributions\n\nshape:\n\nskewness: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail)\nmodality: unimodal, bimodal, multimodal, uniform\n\ncenter: mean (mean), median (median), mode (not always useful)\nspread: range (range), standard deviation (sd), inter-quartile range (IQR)\nunusual observations",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#histograms",
    "href": "content/lectures/06-analysis.html#histograms",
    "title": "06-analysis",
    "section": "Histograms",
    "text": "Histograms\n\nHeightsWidthsPrices\n\n\n\nggplot(data = pp, aes(x = Height_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Height, in inches\", y = NULL)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = Width_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Width, in inches\", y = NULL)\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 100) +\n  labs(x = \"Price\", y = NULL)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#density-plots",
    "href": "content/lectures/06-analysis.html#density-plots",
    "title": "06-analysis",
    "section": "Density plots",
    "text": "Density plots\n\nggplot(data = pp, mapping = aes(x = Height_in)) +\n  geom_density()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "href": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "title": "06-analysis",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = pp, mapping = aes(y = Height_in, x = as.factor(landsALL))) +\n  geom_boxplot()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "href": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "title": "06-analysis",
    "section": "Visualizing categorical data",
    "text": "Visualizing categorical data\n\ncount/proportion of values\nunusual observations",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#bar-plots",
    "href": "content/lectures/06-analysis.html#bar-plots",
    "title": "06-analysis",
    "section": "Bar plots",
    "text": "Bar plots\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL))) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "title": "06-analysis",
    "section": "Segmented bar plots, counts",
    "text": "Segmented bar plots, counts\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "title": "06-analysis",
    "section": "Segmented bar plots, proportions",
    "text": "Segmented bar plots, proportions\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#your-turn",
    "href": "content/lectures/06-analysis.html#your-turn",
    "title": "06-analysis",
    "section": "Your Turn",
    "text": "Your Turn\n❓ Which of the previous two bar plots is a more useful representation for visualizing the relationship between landscape and painting material?\n. . .\n❓ What else would you want to do/know to complete EDA?\n. . .\n🧠 Try to answer at least one thing you’d want to know from the dataset.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-1",
    "href": "content/lectures/06-analysis.html#modelling-1",
    "title": "06-analysis",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we focus on linear models (but remember there are other types of models too!)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#packages",
    "href": "content/lectures/06-analysis.html#packages",
    "title": "06-analysis",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nYou’re familiar with the tidyverse:\n\n\nlibrary(tidyverse)\n\n\nThe broom package takes the messy output of built-in functions in R, such as lm, and turns them into tidy data frames.\n\n\nlibrary(broom)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "href": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "title": "06-analysis",
    "section": "Modelling the relationship between variables",
    "text": "Modelling the relationship between variables\nEDA: Prices\n❗ Describe the distribution of prices of paintings.\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 1000)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#models-as-functions",
    "href": "content/lectures/06-analysis.html#models-as-functions",
    "title": "06-analysis",
    "section": "Models as functions",
    "text": "Models as functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs.\n\nPlug in the inputs and receive back the output\nExample: the formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\), when \\(x\\) is \\(5\\), the output \\(y\\) is \\(22\\)\n\n\ny = 3 * 5 + 7 = 22",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "href": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "title": "06-analysis",
    "section": "Height as a function of width",
    "text": "Height as a function of width\n❗ Describe the relationship between height and width of paintings.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # lm for linear model",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… without the measure of uncertainty around the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) # lm for linear model",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… with different cosmetic choices for the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, \n              col = \"pink\",      # color\n              lty = 2,           # line type\n              linewidth = 3)     # line weight",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#vocabulary",
    "href": "content/lectures/06-analysis.html#vocabulary",
    "title": "06-analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis (dependent variable)\n\n. . .\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis (independent variables)\n\n. . .\n\nPredicted value: Output of the function model function\n\nThe model function gives the typical value of the response variable conditioning on the explanatory variables\n\n\n. . .\n\nResiduals: Show how far each case is from its model value\n\nResidual = Observed value - Predicted value\nTells how far above/below the model function each case is\n\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#residuals",
    "href": "content/lectures/06-analysis.html#residuals",
    "title": "06-analysis",
    "section": "Residuals",
    "text": "Residuals\n❓ What does a negative residual mean? Which paintings on the plot have have negative residuals?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#section-1",
    "href": "content/lectures/06-analysis.html#section-1",
    "title": "06-analysis",
    "section": "",
    "text": "The plot below displays the relationship between height and width of paintings. It uses a lower alpha level for the points than the previous plots we looked at.\n\n\n\n\n\n\n\n\n\n❓ What feature is apparent in this plot that was not (as) apparent in the previous plots? What might be the reason for this feature?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#landscape-paintings",
    "href": "content/lectures/06-analysis.html#landscape-paintings",
    "title": "06-analysis",
    "section": "Landscape paintings",
    "text": "Landscape paintings\n\nLandscape painting is the depiction in art of landscapes – natural scenery such as mountains, valleys, trees, rivers, and forests, especially where the main subject is a wide view – with its elements arranged into a coherent composition.1\n\nLandscape paintings tend to be wider than longer.\n\nPortrait painting is a genre in painting, where the intent is to depict a human subject.2\n\nPortrait paintings tend to be longer than wider.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "href": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "title": "06-analysis",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\n❓ How, if at all, does the relationship between width and height of paintings vary by whether or not they have any landscape elements?\n\nggplot(data = pp, aes(x = Width_in, y = Height_in, \n                      color = factor(landsALL))) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"landscape\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "href": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "title": "06-analysis",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modelling over simple visual inspection of data.\n\n. . .\n\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#variation-around-the-model",
    "href": "content/lectures/06-analysis.html#variation-around-the-model",
    "title": "06-analysis",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\n. . .\nStatistics is the explanation of variation in the context of what remains unexplained.\n. . .\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#how-do-we-use-models",
    "href": "content/lectures/06-analysis.html#how-do-we-use-models",
    "title": "06-analysis",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nExplanation: Characterize the relationship between \\(y\\) and \\(x\\) via slopes for numerical explanatory variables or differences for categorical explanatory variables (Inference)\nPrediction: Plug in \\(x\\), get the predicted \\(y\\) (Machine Learning)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/06-analysis.html#footnotes",
    "href": "content/lectures/06-analysis.html#footnotes",
    "title": "06-analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: Wikipedia, Landscape painting↩︎\nSource: Wikipedia, Portait painting↩︎",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "06-analysis"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#qa-tu-117",
    "href": "content/lectures/12-cs01-eda-slides.html#qa-tu-117",
    "title": "12-cs01-eda",
    "section": "Q&A (Tu 11/7)",
    "text": "Q&A (Tu 11/7)\n\nQ: was curious about the sensitivity/cut-offs/specificity, but we mainly discussed it in class already; was still slightly confused about it?\nA: We’ll discuss this in detail later this week and early next week!\n\n\nQ: Are Youden’s indices related to ROC curves?\nA: Related, yes! We’ll discuss both soon!\n\n\nQ: I wasn’t sure how to interpret some of the visuals towards the end of lecture.\nA: That’s OK! We’ll be recreating these and discussing them more as we do this case study in class.\n\n\nQ: Did they actually use intravenous blood draws or just thumb pricks because multiple intravenous would not be fun.\nA: It was venous blood from the arm. This unfunness is one of the reasons participants were compensated.\n\n\nQ: Did this study (or other studies on THC) impairment end up influencing any legislation at the local or state level?\nA: Great question! The state is currently reviewing these and other study’s data. The state was definitely aware of this study and waited (im)patiently while we analyzed and worked to publish.\n\n\nQ: How long should our reports be?\nA: It’s hard to say. We’ll discuss an example today so you have a sense!\n\n\nQ: Regarding the final project, will there be a Google form that we can fill out that will help us form groups if we can’t form one ourselves?  A: Yup - I’d say try to find a group using Piazza, in class, or during lab. However, if you’re unable, when you fill out the form to indicate your group next week, you’ll select that you’d like to be placed into a group.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#course-announcements-tu-117",
    "href": "content/lectures/12-cs01-eda-slides.html#course-announcements-tu-117",
    "title": "12-cs01-eda",
    "section": "Course Announcements (Tu 11/7)",
    "text": "Course Announcements (Tu 11/7)\nDue Dates:\n\n🧪 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n\nNotes:\n\nCS01 Groups have been sent out\n\nemail for contact\nGitHub repo &lt;- please accept and open; make sure you have access\ngroup mate feedback is required\nif you made changes to repo yesterday, be sure to pull to get data in your repo\n\nFinal Project: can use Piazza to help find group mates\n\n\n\n\n\n\n\n\nImportant\n\n\nThe CS01 data are data for you only. My collaborator is excited that y’all will be working on this…but these are still research data, so please do not share with others or post publicly.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#qa-th-119",
    "href": "content/lectures/12-cs01-eda-slides.html#qa-th-119",
    "title": "12-cs01-eda",
    "section": "Q&A (Th 11/9)",
    "text": "Q&A (Th 11/9)\n\nQ: Are you allowed to share the average midterm scores for the past few quarters?\nA: IIRC, they were in the mid-high 80%s\n\n\nQ: When describing the dataset in our CSs, would it be okay to format it as a data card rather than a paragraph explanation of the variables structure? Additionally, would it be okay to store this as its own read.me in the repo or should it be a part of the main report?\nA: Yes - like the data card idea. And a detailed README in the repo is great. A short description should still be included in the report and can point to the readme.\n\n\nQ: When should we cite in our case study? It is just whenever we look up and use code from the internet?\nA: There AND any time you get information elsewhere that’s not general knowledge. For example, in your background section, you’ll likely cite a bunch of sources.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#course-announcements-th-119",
    "href": "content/lectures/12-cs01-eda-slides.html#course-announcements-th-119",
    "title": "12-cs01-eda",
    "section": "Course Announcements (Th 11/9)",
    "text": "Course Announcements (Th 11/9)\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n✅ HW02 Scores/Feedback Posted",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#functions-in-r",
    "href": "content/lectures/12-cs01-eda-slides.html#functions-in-r",
    "title": "12-cs01-eda",
    "section": "Functions in R",
    "text": "Functions in R\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice” -Hadley\n\n\nfunction_name &lt;- function(input){\n  # operations using input\n}\n\n\nFor example…\n\ndouble_value &lt;- function(val){\n  val * 2\n}\n\n\n\nTo use/execute:\n\ndouble_value(3)\n\n[1] 6\n\n\n\n\nIn what we’ve done so far, we’ve seen functions that operate on and return the whole dataframe (DF in DF out) (drop_dups) and those that carry out operations on each row of a dataframe with a number of inputs (i.e. assign_timepoint; these require the function to be map-ed)\nAdditional resource: https://r4ds.had.co.nz/functions.html",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#agenda",
    "href": "content/lectures/12-cs01-eda-slides.html#agenda",
    "title": "12-cs01-eda",
    "section": "Agenda",
    "text": "Agenda\n\nPrevious Projects\nExploring the Data",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#example-case-study",
    "href": "content/lectures/12-cs01-eda-slides.html#example-case-study",
    "title": "12-cs01-eda",
    "section": "Example Case Study",
    "text": "Example Case Study\nSee & Discuss: https://cogs137.github.io/website/content/cs/cs-example.html",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#feedback-scores",
    "href": "content/lectures/12-cs01-eda-slides.html#feedback-scores",
    "title": "12-cs01-eda",
    "section": "Feedback & Scores",
    "text": "Feedback & Scores\nFeedback to other students here\n\n\nCommon comments:\n\ncontext/explanation/guidance/lacking\nmissing citations\nfailure to introduce/describe the data\nmaking statements without evidence\nneed to edit for cohesiveness, story, clarity\n\n\n\nYou cannot see the projects, but can read all of the comments and see the associated score. Also, note that the same row is not the same group.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#an-example-rubric",
    "href": "content/lectures/12-cs01-eda-slides.html#an-example-rubric",
    "title": "12-cs01-eda",
    "section": "An (Example) Rubric",
    "text": "An (Example) Rubric\nThis is NOT the rubric for your case study, but it will be similar:",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#notes",
    "href": "content/lectures/12-cs01-eda-slides.html#notes",
    "title": "12-cs01-eda",
    "section": "Notes",
    "text": "Notes\n\n\nLots of code/plots will be provided here\nYou are free to include any of it in your own case study (no attribution needed)\nYou probably should NOT include all of them in your final report\nFor any of the “basic” plots you include in your report, you’ll want to clean them up/improve their design.\nYour final report should be polished from start to finish",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#packages",
    "href": "content/lectures/12-cs01-eda-slides.html#packages",
    "title": "12-cs01-eda",
    "section": "Packages",
    "text": "Packages\nTwo additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#our-datasets",
    "href": "content/lectures/12-cs01-eda-slides.html#our-datasets",
    "title": "12-cs01-eda",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#the-data",
    "href": "content/lectures/12-cs01-eda-slides.html#the-data",
    "title": "12-cs01-eda",
    "section": "The Data",
    "text": "The Data\nReading in the .RData we wrote at the end of the last set of notes…(using load)\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nThis reads the objects stored in these files into your Environment for use.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#what-to-do-with-all-of-these-functions",
    "href": "content/lectures/12-cs01-eda-slides.html#what-to-do-with-all-of-these-functions",
    "title": "12-cs01-eda",
    "section": "What to do with all of these functions?",
    "text": "What to do with all of these functions?\nFor example…we discussed this function in the last set of notes.\n\n drop_dups &lt;- function(dataset){\n  out &lt;- dataset |&gt; \n    filter(!is.na(timepoint_use)) |&gt; \n    group_by(timepoint_use) |&gt; \n    distinct(id, .keep_all=TRUE) |&gt; \n    ungroup()\n  return(out)\n } \n\n\nWe’re going to have a lot of functions throughout…like this helper function to clean up names\n\n# helper function to clean up name of two compounds\nclean_gluc &lt;- function(df){\n  df &lt;- df |&gt; \n    mutate(compound=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))),\n           compound=gsub('THCOH', '11-OH-THC', compound))\n  return(df)\n}\n\n\n\nFunctions can/should be stored in a separate .R file, probably in a src/ directory.\n\n\nTo have access to the functions in that file…\n\nsource(\"path/to/file\")\n\n\n\n\nsource(\"src/cs01_functions.R\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#single-variable-basic-plots",
    "href": "content/lectures/12-cs01-eda-slides.html#single-variable-basic-plots",
    "title": "12-cs01-eda",
    "section": "Single Variable (basic) plots",
    "text": "Single Variable (basic) plots\nFor a single compound…\n\nggplot(WB, aes(x=thc)) + geom_histogram()\n\n\n\n\n\n\n\n\n\nBut, with wide data, that’s not easy to do for all compounds, so you may want to pivot those data….\n\nWB_long &lt;- WB |&gt; \n  pivot_longer(6:13) |&gt;\n  rename(\"fluid\"=\"fluid_type\")\n\n\n\nDistribtions across all compounds (WB):\n\nggplot(WB_long, aes(x=value)) + \n  geom_histogram() +\n  facet_wrap(~name)\n\n\n\n\n\n\n\n\n\n\nNow the same for OF and BR:\n\nOF_long &lt;- OF |&gt; pivot_longer(6:12)\nBR_long &lt;- BR |&gt; pivot_longer(6)\n\n\n\nCombining long datasets:\n\ndf_full &lt;- bind_rows(WB_long, OF_long, BR_long)\n\n\n\nPlotting some of these data…\n\ndf_full |&gt;\n  mutate(group_compound=paste0(fluid,\": \", name)) |&gt;\nggplot(aes(x=value)) + \n  geom_histogram() + \n  facet_wrap(~group_compound, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#two-variables-at-a-time",
    "href": "content/lectures/12-cs01-eda-slides.html#two-variables-at-a-time",
    "title": "12-cs01-eda",
    "section": "Two variables at a time",
    "text": "Two variables at a time",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#thc-frequency",
    "href": "content/lectures/12-cs01-eda-slides.html#thc-frequency",
    "title": "12-cs01-eda",
    "section": "THC & Frequency",
    "text": "THC & Frequency\n\ndf_full |&gt; \n  filter(name==\"thc\") |&gt;\n  ggplot(aes(x=group, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#thc-treatment-group",
    "href": "content/lectures/12-cs01-eda-slides.html#thc-treatment-group",
    "title": "12-cs01-eda",
    "section": "THC & Treatment Group",
    "text": "THC & Treatment Group\n\ndf_full |&gt; \n  filter(name==\"thc\") |&gt;\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#focus-on-a-specific-timepoint",
    "href": "content/lectures/12-cs01-eda-slides.html#focus-on-a-specific-timepoint",
    "title": "12-cs01-eda",
    "section": "Focus on a specific timepoint…",
    "text": "Focus on a specific timepoint…\n\ndf_full |&gt; \n  filter(name==\"thc\", timepoint==\"T2A\") |&gt;\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#at-this-point",
    "href": "content/lectures/12-cs01-eda-slides.html#at-this-point",
    "title": "12-cs01-eda",
    "section": "At this point…",
    "text": "At this point…\nWe start to get a sense of the data with these quick and dirty plots, but we’re really only scratching the surface of what’s going on in these data.\n\nThese data require a lot of exploration due to the number of compounds, multiple matrices, and data over time aspects.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#compounds-across-time",
    "href": "content/lectures/12-cs01-eda-slides.html#compounds-across-time",
    "title": "12-cs01-eda",
    "section": "Compounds across time",
    "text": "Compounds across time\n\nCodeExecute (WB)Plots (WB)OFBR\n\n\n\ncompound_scatterplot_group &lt;- function(dataset, compound, timepoints){\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      dataset |&gt; \n        filter(!is.na(time_from_start)) |&gt;\n        ggplot(aes_string(x=\"time_from_start\", \n                          y=compound,\n                          color=\"group\")) + \n        geom_point() +\n        geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                   linetype=\"dashed\", \n                   color=\"gray28\") +\n        scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_y_continuous(limits=c(0,3)) +\n        theme_classic() +\n        theme(legend.position=\"bottom\",\n              legend.title=element_blank()) +\n        labs(x='Time From Start (min)',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        dataset |&gt; \n          filter(!is.na(time_from_start)) |&gt;\n          ggplot(aes_string(x=\"time_from_start\", \n                            y=compound,\n                            color=\"group\")) + \n          geom_point() +\n          geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                     linetype=\"dashed\", \n                     color=\"gray28\")  +\n          scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          theme_classic() +\n          theme(legend.position=\"bottom\",\n                legend.title=element_blank()) +\n          labs(x='Time From Start (min)',\n               y=gsub('GLUC', 'gluc', gsub(\"_\", \"-\", toupper(compound))))\n      )\n    }\n}\n\n\n\n\nscatter_wb &lt;- map(compounds_WB, ~ compound_scatterplot_group( \n    dataset=WB_dups, \n    compound=.x, \n    timepoints=timepoints_WB))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_of &lt;- map(compounds_OF, ~ compound_scatterplot_group( \n    dataset=OF_dups, \n    compound=.x, \n    timepoints=timepoints_OF))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_br &lt;- map(compounds_BR, ~ compound_scatterplot_group( \n    dataset=BR_dups, \n    compound=.x, \n    timepoints=timepoints_BR))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#pairplots",
    "href": "content/lectures/12-cs01-eda-slides.html#pairplots",
    "title": "12-cs01-eda",
    "section": "Pairplots",
    "text": "Pairplots\n\nWBOFBR\n\n\n\npairs(WB[,unlist(compounds_WB)], \n      pch=19, \n      cex=0.3, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(WB[,unlist(compounds_WB)])))))\n\n\n\n\n\n\n\n\n\n\n\npairs(OF[,unlist(compounds_OF)], \n      pch=19, \n      cex=0.4, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(OF[,unlist(compounds_OF)])))))\n\n\n\n\n\n\n\n\n\n\n❓ Why is there no pairplot for Breath?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#group-differences-frequency-of-use",
    "href": "content/lectures/12-cs01-eda-slides.html#group-differences-frequency-of-use",
    "title": "12-cs01-eda",
    "section": "Group Differences: Frequency of Use",
    "text": "Group Differences: Frequency of Use\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_group_only &lt;- function(dataset, compounds, tissue, legend=TRUE, y_lab=TRUE){\n  timepoint_to_use=levels(dataset$timepoint_use)[1]\n  df &lt;- dataset |&gt; \n    filter(timepoint_use == timepoint_to_use) |&gt;\n    select(group, all_of(compounds)) \n  df &lt;- df |&gt; \n    gather(compound, value, -group) |&gt; \n    clean_gluc() |&gt; \n    group_by(compound) |&gt; \n    mutate(y_max=1.2*max(value)) |&gt; \n    group_by(compound, group) |&gt; \n    mutate(n=n(),\n           my_label=paste0(group, ' N=', n),\n           my_label= gsub(\" \", \"\\n\", my_label))\n  \n  if(tissue == \"Blood\"){\n    df$compound=factor(df$compound, levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  y_pos &lt;- df |&gt; \n    group_by(compound) |&gt; \n    summarize(y.position=mean(y_max))\n  \n  stat.test &lt;- df |&gt;\n    group_by(compound) |&gt;\n    t_test(value ~ my_label) |&gt;\n    adjust_pvalue(method=\"bonferroni\") |&gt;\n    add_significance()\n  test &lt;- stat.test |&gt;\n    left_join(y_pos) |&gt;\n    mutate(p.adj.signif=ifelse(p.adj.signif=='?', 'ns', p.adj.signif),\n           p.adj=ifelse(p.adj &lt; 0.001, \"&lt;0.001\", p.adj))\n\n  if(legend){\n    leg_position='right'\n  }else{\n    leg_position='none'\n  }\n  \n  if(y_lab){\n    y_text=\"Concentration (ng/mL)\"\n  }else{\n    y_text=''\n  }\n  \n  medianFunction &lt;- function(x){\n    return(data.frame(y=round(median(x),1),label=round(median(x,na.rm=T),1)))}\n  \n  p2 &lt;- ggplot(df, aes(x=my_label, y=value, fill=my_label)) + \n    geom_jitter(position=position_jitter(width=.3, height=0), size=0.8, color=\"gray65\")  +\n    geom_boxplot(outlier.shape=NA, alpha=0.6) +\n    stat_summary(fun=\"median\", geom=\"point\", shape=19, size=3, fill=\"black\") + \n    stat_summary(fun.data=medianFunction, geom =\"text\", color=\"black\", size=3.5, vjust=-0.65) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    geom_blank(aes(y=y_max)) + \n    scale_y_continuous(limits=c(0, NA), expand=expansion(mult=c(0, 0.1))) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    theme_classic() +\n    theme(text=element_text(size=14),\n          legend.position=leg_position,\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x='',\n         y=y_text) \n  \n  ann_text &lt;- test |&gt; \n    select(compound, p.adj, value=y.position, my_label=group1) |&gt;\n    filter(p.adj &lt; 0.05) |&gt; \n    mutate(x1=1, x2=2)\n  \n  if(tissue == \"Whole Blood\"){\n    ann_text$compound=factor(ann_text$compound, \n                               levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  p2 + geom_text(data=ann_text, label=ann_text$p.adj, nudge_x=0.5) +\n    geom_segment(data=ann_text, aes(x=x1, xend=x2,\n                                    y=value - (0.04 * value), \n                                    yend=value - (0.04*value)))\n  \n}\n\n\n\n\ncompound_boxplot_group_only(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_group_only(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_group_only(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#group-differences-treatment",
    "href": "content/lectures/12-cs01-eda-slides.html#group-differences-treatment",
    "title": "12-cs01-eda",
    "section": "Group Differences: Treatment",
    "text": "Group Differences: Treatment\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_treatment &lt;- function(dataset, compounds, tissue){\n  timepoint_to_use=levels(dataset$timepoint_use)[2]\n  df &lt;- dataset |&gt; \n    filter(timepoint_use == timepoint_to_use) |&gt;\n    select(treatment, group, compounds)\n  df &lt;- df |&gt; \n    gather(compound, value, -treatment, -group) |&gt; \n    clean_gluc()\n  \n  df |&gt; \n    ggplot(aes(x=treatment, y=value, fill=group)) + \n    # geom_jitter(color=\"gray36\") +\n    geom_boxplot(outlier.size=0.1) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    scale_x_discrete(labels=function(x) str_wrap(x, width=11)) +\n    theme_classic(base_size=10) +\n    theme(legend.position=\"bottom\",\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x=\"Treatment\",\n         y=\"Measurement (ng/mL)\")\n  \n}\n\n\n\n\ncompound_boxplot_treatment(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_treatment(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_treatment(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#compound-summaries",
    "href": "content/lectures/12-cs01-eda-slides.html#compound-summaries",
    "title": "12-cs01-eda",
    "section": "Compound Summaries",
    "text": "Compound Summaries\n\nCodeWBOFBR\n\n\n\nT2A_plot &lt;- function(dataset, compound, timepoint_use=2){\n  timepoint_to_use=levels(factor(dataset$timepoint_use))[timepoint_use]\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n             aes_string(x=\"group\", \n                        y=compound, \n                        fill=\"group\")) + \n        geom_boxplot(outlier.size=0.1) +\n        scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n        scale_y_continuous(limits=c(0,3)) +\n        facet_grid(~treatment) + \n        theme_classic() +\n        theme(legend.position=\"none\",\n              panel.grid=element_blank(),\n              strip.background=element_blank(),\n              plot.title.position=\"plot\") +\n        labs(title=paste0('Timepoint: ',\n                          levels(dataset$timepoint_use)[timepoint_use],\n                          ' post-smoking'),\n             x='Group',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n               aes_string(x=\"group\", \n                          y=compound, \n                          fill=\"group\")) + \n          geom_boxplot(outlier.shape=NA, alpha=0.8) +\n          scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n          facet_grid(~treatment) + \n          theme_classic() +\n          theme(legend.position=\"none\",\n                panel.grid=element_blank(),\n                strip.background=element_blank(),\n                plot.title.position=\"plot\") +\n          labs(title=paste0('Timepoint: ',\n                            levels(dataset$timepoint_use)[timepoint_use],\n                            ' post-smoking'),\n               x='Group',\n               y=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))))\n      )\n    }\n}\n\n\n\n\npost_wb &lt;- map(compounds_WB, ~ T2A_plot( \n    dataset=WB_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_of &lt;- map(compounds_OF, ~ T2A_plot( \n    dataset=OF_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_br &lt;- map(compounds_BR, ~ T2A_plot( \n    dataset=BR_dups, \n    compound=.x))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#hw02-part-ii",
    "href": "content/lectures/12-cs01-eda-slides.html#hw02-part-ii",
    "title": "12-cs01-eda",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#chess-players",
    "href": "content/lectures/12-cs01-eda-slides.html#chess-players",
    "title": "12-cs01-eda",
    "section": "Chess Players",
    "text": "Chess Players\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval &lt;- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n\nchess &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/chess-transfers/transfers.csv\")\n\nchess_trans &lt;- chess |&gt;\n  count(Federation) |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 10)\n\nchess_trans &lt;- chess_trans |&gt;\n  mutate(rank = row_number()) |&gt;\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nchess_trans &lt;- chess |&gt;\n  count(Federation) |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 10)\n\nchess_trans &lt;- chess_trans |&gt;\n  mutate(rank = row_number()) |&gt;\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nggplot(chess_trans, aes(y = reorder(Federation, n), x = n)) +\n  geom_bar(stat = \"identity\",fill = \"#1c9099\") +\n  geom_text(aes(x = -3, y = Federation, label = n), size = 4) + #add count at the left side of the bars\n  labs(title = bquote(bold(\"More players transfer to the U.S. than to any other country\")),\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       x = \"NUMBER OF TRANSFERS\",\n       y = \"COUNTRY\") +\n  scale_fill_identity() +\n  theme_minimal() + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank()) + \n  theme(axis.text.y = element_text(size = 10, angle = 0, hjust = 0), #align to the left\n        plot.title = element_text(size=18)) #change font size",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#kushi-fandango",
    "href": "content/lectures/12-cs01-eda-slides.html#kushi-fandango",
    "title": "12-cs01-eda",
    "section": "Kushi: Fandango",
    "text": "Kushi: Fandango\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Kushi also got font working and stars on x-axis that I'd have to spend more time to get working \nfandango_score_comparison &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv\")\n\nall_scores_comp &lt;- fandango_score_comparison |&gt;\n  select(FILM, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars) |&gt;\n  pivot_longer(c(RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars), names_to = \"Site\", values_to = \"Score\")\n\ncounts_of_scores &lt;- all_scores_comp |&gt;\n  group_by(Site, Score) |&gt;\n  summarize(Count = n())\n\ncounts_of_scores &lt;- counts_of_scores |&gt;\n  group_by(Site) |&gt;\n  mutate(Total = sum(Count))\n\nscores_with_percents &lt;- counts_of_scores |&gt;\n  mutate(Percent = Count / Total * 100)\n\n# making percents 0 for any scores that don't have any values\n\nsites &lt;- tibble(Site = c(\"RT_norm_round\", \"RT_user_norm_round\", \"Metacritic_norm_round\", \"Metacritic_user_norm_round\", \"IMDB_norm_round\", \"Fandango_Stars\"))\n\nratings &lt;- tibble(Score = seq(0, 5, by = 0.5))\n\nall_scores &lt;- expand.grid(Site = sites$Site, Score = ratings$Score)\n\nall_scores &lt;- all_scores |&gt;\n  full_join(scores_with_percents, by = c(\"Site\", \"Score\")) |&gt;\n  mutate(Percent = case_when(is.na(Count) ~ 0,\n                             TRUE ~ Percent))\n\nscores &lt;- ggplot(all_scores, aes(x = Score, y = Percent, color = Site)) +\n  geom_line()  +\n  geom_hline(yintercept = 0, size = 0.7, color = \"black\") +\n  labs(x = NULL, y = NULL,\n       title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distrubution of 146 films in theaters in 2015 that\\n had 30+ reviews on Fandango.com\") +\n  scale_y_continuous(labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")) +\n  scale_x_continuous(labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +\n  scale_color_manual(values = c(\"Fandango_Stars\" = \"#fa6d54\",\n                                \"IMDB_norm_round\" = \"#e5c66a\",\n                                \"Metacritic_user_norm_round\" = \"#aeca91\",\n                                \"RT_user_norm_round\" = \"#76bde0\",\n                                \"Metacritic_norm_round\" = \"#b87eb5\",\n                                \"RT_norm_round\" = \"#a3a3a3\")) +\n  geom_ribbon(data = filter(all_scores, Site != \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent), alpha = 0.1) +\n  geom_ribbon(data = filter(all_scores, Site == \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent, fill = \"#fa6d54\", alpha = 0.24)) +\n  guides(color = \"none\", fill = \"none\", alpha = \"none\")\n\nscores + \n  annotate(\"text\", x = 4.9, y = 35, label = \"Fandango\", size = 5, color = \"#fa6d54\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.9, y = 37, label = \"IMDb users\", size = 5, color = \"#e5c66a\") +\n  annotate(\"text\", x = 2.7, y = 27, label = \"Metacritic\\nusers\", size = 5, color = \"#aeca91\") +\n  annotate(\"text\", x = 2.2, y = 20, label = \"Rotten\\nTomatoes\\nusers\", size = 5, color = \"#76bde0\") +\n  annotate(\"text\", x = 1.5, y = 16, label = \"Metacritic\", size = 5, color = \"#b87eb5\") +\n  annotate(\"text\", x = 0.7, y = 13, label = \"Rotten\\nTomatoes\", size = 5, color = \"#a3a3a3\") +\n  theme(#text = element_text(family = \"NimbusSan\"), \n        plot.title = element_text(face = \"bold\", size = 25, hjust = -0.1), \n        plot.subtitle = element_text(size = 15, hjust = -0.1),\n        plot.background = element_rect(fill = \"#f0f0f0\"),\n        panel.background = element_rect(fill = \"#f0f0f0\"),\n        panel.grid.major = element_line(color = \"gray75\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text = element_text(size = 14)\n        )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#markus-congress",
    "href": "content/lectures/12-cs01-eda-slides.html#markus-congress",
    "title": "12-cs01-eda",
    "section": "Markus: Congress",
    "text": "Markus: Congress\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncongress_data &lt;- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\")\n\ncongress_ages =\n  congress_data |&gt;\n  select(congress, chamber, age_years) |&gt;\n  mutate(congress_year = case_when(TRUE ~ 1787 + (2 * as.integer(congress)))) |&gt;\n  group_by(congress_year, chamber) |&gt;\n  mutate(mean_age = case_when(TRUE ~ mean(age_years))) |&gt;\n  mutate(chamber = fct_recode(chamber,\n                              SENATE = \"Senate\",\n                              HOUSE = \"House\",))\n\nggplot(data = congress_ages,\n       mapping = aes(y = mean_age,\n                     x = congress_year,\n                     color = fct_rev(chamber),\n                     )) +\n  geom_step(size = 1) + \n  guides() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       caption = \"Data is based on all members who served in either the Senate or House in each Congress, which is notated\\nby the year in which it was seated. Any member who served in bothchambers in the same Congress was\\nassigned to the chamber in which they cast more votes.\\n FiveThirtyEight\\nSOURCES: BIOGRAPHICAL DIRECTORY OF THE U.S. CONGRESS, U.S. HOUSE OF REPRESENTATIVES,\\nU.S. SENATE, UNITEDSTATES GITHUB, VOTEVIEW.COM\",\n       y = NULL,\n       x = NULL\n  ) +\n  scale_color_manual(values=c(\"#6b4ddd\",\"#29ae53\")) +\n  theme_minimal(base_size = 13) + \n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(face = \"bold\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.title=element_blank(),\n        legend.position = \"top\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#voter-demographics",
    "href": "content/lectures/12-cs01-eda-slides.html#voter-demographics",
    "title": "12-cs01-eda",
    "section": "Voter Demographics",
    "text": "Voter Demographics\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggpubr)\n\nvoters = read.csv(\"data/nonvoters_data.csv\")\n\n\n#creating subplots\nrace = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Race\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())+\n    guides(fill = guide_legend(override.aes = list(shape = 16, key_width = 1, key_height = 1))) #trying to change the shape of the legend key\n\nincome = ggplot(data = voters, mapping = aes(y = factor(income_cat), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Income\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\nage = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Age\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\neducation = ggplot(data = voters, mapping = aes(y = factor(educ), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Education\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\npartyID = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"party ID\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\n#combining subplots\nria = ggarrange(race, income, age, ncol = 3, common.legend = TRUE, legend = \"top\") +\n  theme(plot.margin = margin(0.5, 0,-2, -1,\"cm\")) \n\nep = ggarrange(education, partyID) +\n  theme(plot.margin = margin(2, 4, -2.5, 1, \"cm\"))\n\nvoterplot = ggarrange(ria, ep, nrow = 3) \n\n\n#titles\nvoterplot = annotate_figure(annotate_figure(voterplot, \n  top = text_grob(\"Demographic information of survey respondants, by voting history\")),\n  top = text_grob(\"Those who always vote and those who sometimes vote aren't that different\", face = \"bold\")\n)\n\n\nvoterplot",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#fouls",
    "href": "content/lectures/12-cs01-eda-slides.html#fouls",
    "title": "12-cs01-eda",
    "section": "Fouls",
    "text": "Fouls\n\nOriginalPlot ICode IPlot (Banso)Code (Banso)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_csv_file &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/foul-balls/foul-balls.csv\"\nfoulballs &lt;- read.csv(url(raw_csv_file))\n\nfoulballs &lt;- foulballs |&gt;\n  mutate(over90 = case_when(\n    exit_velocity &lt; 90 ~ \"no\",\n    exit_velocity &gt;= 90 ~ \"yes\"\n  ))\n\nggplot(foulballs,\n       aes (y = used_zone)) +\n  geom_bar(aes(fill = over90),\n           position = position_stack(reverse = TRUE),\n           show.legend = FALSE) +\n  scale_fill_manual(labels = c(\"&lt; 90 mph\", \"≥ 90 mph\", \"Unknown exit velocity\"),\n                    values = c(\"#97c16d\", \"#63abb0\"), na.value = \"#d3d3d3\") +\n  scale_y_discrete(limits = rev(unique(foulballs$used_zone)))+\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"#ececec\"),\n        panel.background = element_rect(fill = \"white\"),\n        axis.text.x = element_text(color = \"#9e9e9e\"),\n        plot.caption = element_text(color = \"#b5b5b5\"),\n        axis.line.y = element_line(colour = \"#343434\"),\n        axis.title.y = element_text(angle = 0, vjust = 0.12, color = \"#343434\", size = 9),\n        axis.ticks = element_blank(),\n        legend.title = element_blank()) +\n  labs(title = \"The hardest-hit fouls seem to land in unprotected areas\",\n       subtitle = str_wrap(\"Foul balls by the stadium zone they landed in and their exit velocity, among 906 fouls hit this season in the most foul-heavy day at the 10 MLB stadiums that produced the most fouls as of June 5\", 85),\n       x = \"\", y = \"Zone\",\n       caption = \"SOURCE: BASEBALL SAVANT\") +\n  annotate(\"text\", x = 75, y = 1, label = \"&lt; 90 mph\", col = \"white\") +\n  annotate(\"text\", x = 140, y = 3.3, label = \"≥ 90 mph\", col = \"#63abb0\") +\n  annotate(\"text\", x = 215, y = 1, label = \"Unknown exit velocity\", col = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nball_data &lt;- foulballs |&gt;\n  mutate(category = case_when(\n    exit_velocity &lt; 90 ~ \"&lt;90\",\n    exit_velocity &gt;= 90 ~ \"&gt;=90\",\n    is.na(exit_velocity) ~ \"Unknown\"\n                           ))\nmy_plot &lt;- ggplot(ball_data, aes(y = fct_rev(as.character(used_zone)), fill = category)) +\n  geom_bar(position = position_stack(reverse = TRUE)) +\n  labs(title = \"The hardest-hit fouls seem \\nto land in unprotected areas\",\n       subtitle = \"The 906 foul balls hit this season from \\nthe most foul-heavy day at each of the \\n10 MLB stadiums that produced the \\nthe most fouls as of June 5, by zone where \\nthe balls landed and their exit velocities\",\n       y = \"Zone\", x = NULL) +\n  scale_fill_manual(values = c(\"Unknown\" = \"#DEDEDE\", \"&lt;90\" = \"#9ECE88\", \"&gt;=90\" = \"#17AFAD\")) +\n  scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250),position = \"top\") +\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major.x = element_line(size = 0.5, linetype=\"solid\", color=\"#CECECE\"),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(color = \"black\", face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(size = 10, margin = margin(b = 20))\n)\n\nmy_plot",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#colin-nba-raptor-ratings",
    "href": "content/lectures/12-cs01-eda-slides.html#colin-nba-raptor-ratings",
    "title": "12-cs01-eda",
    "section": "Colin: NBA RAPTOR ratings",
    "text": "Colin: NBA RAPTOR ratings\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\nraptor &lt;- read.csv(\"data/latest_RAPTOR_by_player.csv\")\n\nraptor_rounded &lt;- raptor |&gt; \n  filter(mp &gt;= 1137) |&gt;\n  mutate(across(where(is.numeric), round, 1))\n\nraptor_plot &lt;- raptor_rounded |&gt;\n  ggplot(aes(x=raptor_offense, y = raptor_defense)) + \n  \n  # Annotations\n    # The colored rectangles for the 1 & 3 quadrants\n    annotate(\"rect\", xmin=0, xmax=10, ymin=0, ymax=10, fill = '#c5ecee', alpha = .85) + \n    annotate(\"rect\", xmin=-10, xmax=0, ymin=-10, ymax=0, fill = '#fecada', alpha = .85) +\n    \n    # The 3rd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -7.8, ymax = -6.8, fill = '#fd97b6') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = -7.4, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = -9, label = \" -  defense\") +\n    \n    # The 1st quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 8.4, ymax = 9.4, fill = '#8cdadf') +\n    annotate(\"text\", x = 7.5, y = 8.8, label = \" +  offense\") +\n    annotate(\"text\", x = 7.55, y = 7.1, label = \" +  defense\") +\n  \n    # The 2nd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 8.4, ymax = 9.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = 8.8, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = 7.1, label = \" +  defense\") +\n  \n    # The 4th quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -7.8, ymax = -6.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = 7.55, y = -7.4, label = \" +  offense\") +\n    annotate(\"text\", x = 7.5, y = -9, label = \" -  defense\") +\n  \n  geom_point(shape= 21, colour = \"black\", fill = \"white\", size = 4) + \n  \n  labs(\n    x = \"Offensive RAPTOR rating\", \n    y = \"Defensive RAPTOR rating\",\n    title = paste0('Nikola Jokic is the Best NBA Player Based on Overall RAPTOR Rating',\n            '&lt;br&gt;',\n            '&lt;sup&gt;',\n            'An Analytical Approach to the 2022 - 2023 NBA Season','&lt;/sup&gt;')\n    ) + \n    \n  # Theme settings\n  theme_light() + \n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(), \n    panel.grid.major = element_line(color = \"#cdcdcd\", linewidth = 0.5), \n    plot.margin = margin(l = 100, r = 100, b = 20, t = 10),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(color = \"#cdcdcd\", size = 12),\n    axis.title.x = element_text(margin = margin(t=3)),\n    axis.title.y = element_text(margin = margin(r=3)),\n    ) + \n  coord_fixed(ratio = 1) \n\n# interactive\n# ggplotly(raptor_plot, text=player_name, hoverinfo='text') \n\nraptor_plot",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#biden-approval",
    "href": "content/lectures/12-cs01-eda-slides.html#biden-approval",
    "title": "12-cs01-eda",
    "section": "Biden Approval",
    "text": "Biden Approval\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval &lt;- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n#getting only data from beginning of COVID to Jan 19, 2021 and getting rid of the \"all\" category in the party column \nyear_20 &lt;- \n  COVID_approval |&gt;\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |&gt;\n  filter(party != \"all\", \n         end_year &lt;= 2020)|&gt;\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n\nyear_21 &lt;-\n  COVID_approval |&gt;\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |&gt;\n  filter(party != \"all\", \n         end_year == 2021 & end_month == 1 & end_day &lt;= 19) |&gt;\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n       #  if (end_year == 2021 & end_month == 1 & end_day &lt;= 19))\n\nselect_COVID_approval &lt;-\n  full_join(year_20, year_21)\n\nggplot(select_COVID_approval, aes(x = as.Date(end_date),\n                           y = approve,\n                           color = party)) +\n  geom_smooth(aes(group = party), span = 0.05, se = FALSE) + \n  scale_color_manual(values = c(\"D\" = \"#008fd5\", \n                               \"R\" = \"#ff2700\",\n                               \"I\" = \"#a55330\")) +\n  \n  #Important Dates\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-02-29\"), y = 95, label=\"First U.S. \\n Death \\n Reported\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-05-20\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-05-20\"), y = 95, label=\"U.S. Deaths \\n surpass \\n 100,000\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype=3) + #Trump Diagnosed with COVID-19\n    annotate(\"text\", x= as.Date(\"2020-10-02\"), y = 95, label=\"Trump \\n Diagnosed \\n with \\n COVID-19\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-11-07\"), y = 95, label=\"Biden \\n declared \\n election \\n winner\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2021-01-19\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2021-01-19\"), y = 95, label=\"Biden \\n sworn \\n into \\n office\", size=3, color=\"black\") +\n  \n  labs(title = \"Approval of Trump’s response varies widely by party\",\n       subtitle = \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s \\n handling of the coronavirus outbreak\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5 ), \n        plot.subtitle = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.minor = element_line(colour=\"gray\")\n        )",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#recap",
    "href": "content/lectures/12-cs01-eda-slides.html#recap",
    "title": "12-cs01-eda",
    "section": "Recap",
    "text": "Recap\n\nCan you explain/describe the plots generated in the context of these data?\nCan you generate EDA plots of your own for these data\nCan you understand/work through the more complicated code provided (even if you couldn’t have come up with it on your own)\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html",
    "href": "content/lectures/12-cs01-eda.html",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Q: was curious about the sensitivity/cut-offs/specificity, but we mainly discussed it in class already; was still slightly confused about it?\nA: We’ll discuss this in detail later this week and early next week!\n\n\nQ: Are Youden’s indices related to ROC curves?\nA: Related, yes! We’ll discuss both soon!\n\n\nQ: I wasn’t sure how to interpret some of the visuals towards the end of lecture.\nA: That’s OK! We’ll be recreating these and discussing them more as we do this case study in class.\n\n\nQ: Did they actually use intravenous blood draws or just thumb pricks because multiple intravenous would not be fun.\nA: It was venous blood from the arm. This unfunness is one of the reasons participants were compensated.\n\n\nQ: Did this study (or other studies on THC) impairment end up influencing any legislation at the local or state level?\nA: Great question! The state is currently reviewing these and other study’s data. The state was definitely aware of this study and waited (im)patiently while we analyzed and worked to publish.\n\n\nQ: How long should our reports be?\nA: It’s hard to say. We’ll discuss an example today so you have a sense!\n\n\nQ: Regarding the final project, will there be a Google form that we can fill out that will help us form groups if we can’t form one ourselves?  A: Yup - I’d say try to find a group using Piazza, in class, or during lab. However, if you’re unable, when you fill out the form to indicate your group next week, you’ll select that you’d like to be placed into a group.\n\n\n\n\nDue Dates:\n\n🧪 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n\nNotes:\n\nCS01 Groups have been sent out\n\nemail for contact\nGitHub repo &lt;- please accept and open; make sure you have access\ngroup mate feedback is required\nif you made changes to repo yesterday, be sure to pull to get data in your repo\n\nFinal Project: can use Piazza to help find group mates\n\n. . .\n\n\n\n\n\n\nImportant\n\n\n\nThe CS01 data are data for you only. My collaborator is excited that y’all will be working on this…but these are still research data, so please do not share with others or post publicly.\n\n\n\n\n\n\nQ: Are you allowed to share the average midterm scores for the past few quarters?\nA: IIRC, they were in the mid-high 80%s\n\n\nQ: When describing the dataset in our CSs, would it be okay to format it as a data card rather than a paragraph explanation of the variables structure? Additionally, would it be okay to store this as its own read.me in the repo or should it be a part of the main report?\nA: Yes - like the data card idea. And a detailed README in the repo is great. A short description should still be included in the report and can point to the readme.\n\n\nQ: When should we cite in our case study? It is just whenever we look up and use code from the internet?\nA: There AND any time you get information elsewhere that’s not general knowledge. For example, in your background section, you’ll likely cite a bunch of sources.\n\n\n\n\nDue Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n✅ HW02 Scores/Feedback Posted\n\n\n\n\n\n“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice” -Hadley\n\n\nfunction_name &lt;- function(input){\n  # operations using input\n}\n\n. . .\nFor example…\n\ndouble_value &lt;- function(val){\n  val * 2\n}\n\n. . .\nTo use/execute:\n\ndouble_value(3)\n\n[1] 6\n\n\n. . .\nIn what we’ve done so far, we’ve seen functions that operate on and return the whole dataframe (DF in DF out) (drop_dups) and those that carry out operations on each row of a dataframe with a number of inputs (i.e. assign_timepoint; these require the function to be map-ed)\nAdditional resource: https://r4ds.had.co.nz/functions.html\n\n\n\n\nPrevious Projects\nExploring the Data",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#qa-tu-117",
    "href": "content/lectures/12-cs01-eda.html#qa-tu-117",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Q: was curious about the sensitivity/cut-offs/specificity, but we mainly discussed it in class already; was still slightly confused about it?\nA: We’ll discuss this in detail later this week and early next week!\n\n\nQ: Are Youden’s indices related to ROC curves?\nA: Related, yes! We’ll discuss both soon!\n\n\nQ: I wasn’t sure how to interpret some of the visuals towards the end of lecture.\nA: That’s OK! We’ll be recreating these and discussing them more as we do this case study in class.\n\n\nQ: Did they actually use intravenous blood draws or just thumb pricks because multiple intravenous would not be fun.\nA: It was venous blood from the arm. This unfunness is one of the reasons participants were compensated.\n\n\nQ: Did this study (or other studies on THC) impairment end up influencing any legislation at the local or state level?\nA: Great question! The state is currently reviewing these and other study’s data. The state was definitely aware of this study and waited (im)patiently while we analyzed and worked to publish.\n\n\nQ: How long should our reports be?\nA: It’s hard to say. We’ll discuss an example today so you have a sense!\n\n\nQ: Regarding the final project, will there be a Google form that we can fill out that will help us form groups if we can’t form one ourselves?  A: Yup - I’d say try to find a group using Piazza, in class, or during lab. However, if you’re unable, when you fill out the form to indicate your group next week, you’ll select that you’d like to be placed into a group.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#course-announcements-tu-117",
    "href": "content/lectures/12-cs01-eda.html#course-announcements-tu-117",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Due Dates:\n\n🧪 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n\nNotes:\n\nCS01 Groups have been sent out\n\nemail for contact\nGitHub repo &lt;- please accept and open; make sure you have access\ngroup mate feedback is required\nif you made changes to repo yesterday, be sure to pull to get data in your repo\n\nFinal Project: can use Piazza to help find group mates\n\n. . .\n\n\n\n\n\n\nImportant\n\n\n\nThe CS01 data are data for you only. My collaborator is excited that y’all will be working on this…but these are still research data, so please do not share with others or post publicly.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#qa-th-119",
    "href": "content/lectures/12-cs01-eda.html#qa-th-119",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Q: Are you allowed to share the average midterm scores for the past few quarters?\nA: IIRC, they were in the mid-high 80%s\n\n\nQ: When describing the dataset in our CSs, would it be okay to format it as a data card rather than a paragraph explanation of the variables structure? Additionally, would it be okay to store this as its own read.me in the repo or should it be a part of the main report?\nA: Yes - like the data card idea. And a detailed README in the repo is great. A short description should still be included in the report and can point to the readme.\n\n\nQ: When should we cite in our case study? It is just whenever we look up and use code from the internet?\nA: There AND any time you get information elsewhere that’s not general knowledge. For example, in your background section, you’ll likely cite a bunch of sources.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#course-announcements-th-119",
    "href": "content/lectures/12-cs01-eda.html#course-announcements-th-119",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Due Dates:\n\n🔬 No Lab this week (holiday) - all students will receive full credit for Lab05 (MLR)\n❓ Mid-course survey “due” (for EC) Friday\n💻 HW03 (MLR) due Mon 11/20\n✅ HW02 Scores/Feedback Posted",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#functions-in-r",
    "href": "content/lectures/12-cs01-eda.html#functions-in-r",
    "title": "12-cs01-eda",
    "section": "",
    "text": "“You should consider writing a function whenever you’ve copied and pasted a block of code more than twice” -Hadley\n\n\nfunction_name &lt;- function(input){\n  # operations using input\n}\n\n. . .\nFor example…\n\ndouble_value &lt;- function(val){\n  val * 2\n}\n\n. . .\nTo use/execute:\n\ndouble_value(3)\n\n[1] 6\n\n\n. . .\nIn what we’ve done so far, we’ve seen functions that operate on and return the whole dataframe (DF in DF out) (drop_dups) and those that carry out operations on each row of a dataframe with a number of inputs (i.e. assign_timepoint; these require the function to be map-ed)\nAdditional resource: https://r4ds.had.co.nz/functions.html",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#agenda",
    "href": "content/lectures/12-cs01-eda.html#agenda",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Previous Projects\nExploring the Data",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#example-case-study",
    "href": "content/lectures/12-cs01-eda.html#example-case-study",
    "title": "12-cs01-eda",
    "section": "Example Case Study",
    "text": "Example Case Study\nSee & Discuss: https://cogs137.github.io/website/content/cs/cs-example.html",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#feedback-scores",
    "href": "content/lectures/12-cs01-eda.html#feedback-scores",
    "title": "12-cs01-eda",
    "section": "Feedback & Scores",
    "text": "Feedback & Scores\nFeedback to other students here\n\n\nYou cannot see the projects, but can read all of the comments and see the associated score. Also, note that the same row is not the same group.\n. . .\nCommon comments:\n\ncontext/explanation/guidance/lacking\nmissing citations\nfailure to introduce/describe the data\nmaking statements without evidence\nneed to edit for cohesiveness, story, clarity",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#an-example-rubric",
    "href": "content/lectures/12-cs01-eda.html#an-example-rubric",
    "title": "12-cs01-eda",
    "section": "An (Example) Rubric",
    "text": "An (Example) Rubric\nThis is NOT the rubric for your case study, but it will be similar:",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#notes",
    "href": "content/lectures/12-cs01-eda.html#notes",
    "title": "12-cs01-eda",
    "section": "Notes",
    "text": "Notes\n\n\nLots of code/plots will be provided here\nYou are free to include any of it in your own case study (no attribution needed)\nYou probably should NOT include all of them in your final report\nFor any of the “basic” plots you include in your report, you’ll want to clean them up/improve their design.\nYour final report should be polished from start to finish",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#packages",
    "href": "content/lectures/12-cs01-eda.html#packages",
    "title": "12-cs01-eda",
    "section": "Packages",
    "text": "Packages\nTwo additional packages required for these notes:\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(rstatix)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#our-datasets",
    "href": "content/lectures/12-cs01-eda.html#our-datasets",
    "title": "12-cs01-eda",
    "section": "Our Datasets",
    "text": "Our Datasets\nThree matrices:\n\nBlood (WB): 8 compounds; 190 participants\nOral Fluid (OF): 7 compounds; 192 participants\nBreath (BR): 1 compound; 191 participants\n\n. . .\nVariables:\n\nID | participants identifier\nTreatment | placebo, 5.90%, 13.40%\nGroup | Occasional user, Frequent user\nTimepoint | indicator of which point in the timeline participant’s collection occurred\ntime.from.start | number of minutes from consumption\n& measurements for individual compounds",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#the-data",
    "href": "content/lectures/12-cs01-eda.html#the-data",
    "title": "12-cs01-eda",
    "section": "The Data",
    "text": "The Data\nReading in the .RData we wrote at the end of the last set of notes…(using load)\n\nload(\"data/compounds.RData\")\nload(\"data/timepoints.RData\")\nload(\"data/data_clean.RData\")\n\nThis reads the objects stored in these files into your Environment for use.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#what-to-do-with-all-of-these-functions",
    "href": "content/lectures/12-cs01-eda.html#what-to-do-with-all-of-these-functions",
    "title": "12-cs01-eda",
    "section": "What to do with all of these functions?",
    "text": "What to do with all of these functions?\nFor example…we discussed this function in the last set of notes.\n\n drop_dups &lt;- function(dataset){\n  out &lt;- dataset |&gt; \n    filter(!is.na(timepoint_use)) |&gt; \n    group_by(timepoint_use) |&gt; \n    distinct(id, .keep_all=TRUE) |&gt; \n    ungroup()\n  return(out)\n } \n\n. . .\nWe’re going to have a lot of functions throughout…like this helper function to clean up names\n\n# helper function to clean up name of two compounds\nclean_gluc &lt;- function(df){\n  df &lt;- df |&gt; \n    mutate(compound=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))),\n           compound=gsub('THCOH', '11-OH-THC', compound))\n  return(df)\n}\n\n. . .\nFunctions can/should be stored in a separate .R file, probably in a src/ directory.\n. . .\nTo have access to the functions in that file…\n\nsource(\"path/to/file\")\n\n. . .\n\nsource(\"src/cs01_functions.R\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#single-variable-basic-plots",
    "href": "content/lectures/12-cs01-eda.html#single-variable-basic-plots",
    "title": "12-cs01-eda",
    "section": "Single Variable (basic) plots",
    "text": "Single Variable (basic) plots\nFor a single compound…\n\nggplot(WB, aes(x=thc)) + geom_histogram()\n\n\n\n\n\n\n\n\n. . .\nBut, with wide data, that’s not easy to do for all compounds, so you may want to pivot those data….\n\nWB_long &lt;- WB |&gt; \n  pivot_longer(6:13) |&gt;\n  rename(\"fluid\"=\"fluid_type\")\n\n. . .\nDistribtions across all compounds (WB):\n\nggplot(WB_long, aes(x=value)) + \n  geom_histogram() +\n  facet_wrap(~name)\n\n\n\n\n\n\n\n\n. . .\nNow the same for OF and BR:\n\nOF_long &lt;- OF |&gt; pivot_longer(6:12)\nBR_long &lt;- BR |&gt; pivot_longer(6)\n\n. . .\nCombining long datasets:\n\ndf_full &lt;- bind_rows(WB_long, OF_long, BR_long)\n\n. . .\nPlotting some of these data…\n\ndf_full |&gt;\n  mutate(group_compound=paste0(fluid,\": \", name)) |&gt;\nggplot(aes(x=value)) + \n  geom_histogram() + \n  facet_wrap(~group_compound, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#two-variables-at-a-time",
    "href": "content/lectures/12-cs01-eda.html#two-variables-at-a-time",
    "title": "12-cs01-eda",
    "section": "Two variables at a time",
    "text": "Two variables at a time",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#thc-frequency",
    "href": "content/lectures/12-cs01-eda.html#thc-frequency",
    "title": "12-cs01-eda",
    "section": "THC & Frequency",
    "text": "THC & Frequency\n\ndf_full |&gt; \n  filter(name==\"thc\") |&gt;\n  ggplot(aes(x=group, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#thc-treatment-group",
    "href": "content/lectures/12-cs01-eda.html#thc-treatment-group",
    "title": "12-cs01-eda",
    "section": "THC & Treatment Group",
    "text": "THC & Treatment Group\n\ndf_full |&gt; \n  filter(name==\"thc\") |&gt;\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#focus-on-a-specific-timepoint",
    "href": "content/lectures/12-cs01-eda.html#focus-on-a-specific-timepoint",
    "title": "12-cs01-eda",
    "section": "Focus on a specific timepoint…",
    "text": "Focus on a specific timepoint…\n\ndf_full |&gt; \n  filter(name==\"thc\", timepoint==\"T2A\") |&gt;\n  ggplot(aes(x=treatment, y=value)) + \n  geom_boxplot() +\n  facet_wrap(~fluid, scales=\"free\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#at-this-point",
    "href": "content/lectures/12-cs01-eda.html#at-this-point",
    "title": "12-cs01-eda",
    "section": "At this point…",
    "text": "At this point…\nWe start to get a sense of the data with these quick and dirty plots, but we’re really only scratching the surface of what’s going on in these data.\n. . .\nThese data require a lot of exploration due to the number of compounds, multiple matrices, and data over time aspects.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#compounds-across-time",
    "href": "content/lectures/12-cs01-eda.html#compounds-across-time",
    "title": "12-cs01-eda",
    "section": "Compounds across time",
    "text": "Compounds across time\n\nCodeExecute (WB)Plots (WB)OFBR\n\n\n\ncompound_scatterplot_group &lt;- function(dataset, compound, timepoints){\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      dataset |&gt; \n        filter(!is.na(time_from_start)) |&gt;\n        ggplot(aes_string(x=\"time_from_start\", \n                          y=compound,\n                          color=\"group\")) + \n        geom_point() +\n        geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                   linetype=\"dashed\", \n                   color=\"gray28\") +\n        scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_y_continuous(limits=c(0,3)) +\n        theme_classic() +\n        theme(legend.position=\"bottom\",\n              legend.title=element_blank()) +\n        labs(x='Time From Start (min)',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        dataset |&gt; \n          filter(!is.na(time_from_start)) |&gt;\n          ggplot(aes_string(x=\"time_from_start\", \n                            y=compound,\n                            color=\"group\")) + \n          geom_point() +\n          geom_vline(data=timepoints, aes(xintercept=as.numeric(stop)), \n                     linetype=\"dashed\", \n                     color=\"gray28\")  +\n          scale_color_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          theme_classic() +\n          theme(legend.position=\"bottom\",\n                legend.title=element_blank()) +\n          labs(x='Time From Start (min)',\n               y=gsub('GLUC', 'gluc', gsub(\"_\", \"-\", toupper(compound))))\n      )\n    }\n}\n\n\n\n\nscatter_wb &lt;- map(compounds_WB, ~ compound_scatterplot_group( \n    dataset=WB_dups, \n    compound=.x, \n    timepoints=timepoints_WB))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_of &lt;- map(compounds_OF, ~ compound_scatterplot_group( \n    dataset=OF_dups, \n    compound=.x, \n    timepoints=timepoints_OF))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter_br &lt;- map(compounds_BR, ~ compound_scatterplot_group( \n    dataset=BR_dups, \n    compound=.x, \n    timepoints=timepoints_BR))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#pairplots",
    "href": "content/lectures/12-cs01-eda.html#pairplots",
    "title": "12-cs01-eda",
    "section": "Pairplots",
    "text": "Pairplots\n\nWBOFBR\n\n\n\npairs(WB[,unlist(compounds_WB)], \n      pch=19, \n      cex=0.3, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(WB[,unlist(compounds_WB)])))))\n\n\n\n\n\n\n\n\n\n\n\npairs(OF[,unlist(compounds_OF)], \n      pch=19, \n      cex=0.4, \n      cex.labels=0.6,\n      labels=gsub('GLUC','gluc',gsub(\"_\",\"-\",toupper(colnames(OF[,unlist(compounds_OF)])))))\n\n\n\n\n\n\n\n\n\n\n❓ Why is there no pairplot for Breath?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#group-differences-frequency-of-use",
    "href": "content/lectures/12-cs01-eda.html#group-differences-frequency-of-use",
    "title": "12-cs01-eda",
    "section": "Group Differences: Frequency of Use",
    "text": "Group Differences: Frequency of Use\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_group_only &lt;- function(dataset, compounds, tissue, legend=TRUE, y_lab=TRUE){\n  timepoint_to_use=levels(dataset$timepoint_use)[1]\n  df &lt;- dataset |&gt; \n    filter(timepoint_use == timepoint_to_use) |&gt;\n    select(group, all_of(compounds)) \n  df &lt;- df |&gt; \n    gather(compound, value, -group) |&gt; \n    clean_gluc() |&gt; \n    group_by(compound) |&gt; \n    mutate(y_max=1.2*max(value)) |&gt; \n    group_by(compound, group) |&gt; \n    mutate(n=n(),\n           my_label=paste0(group, ' N=', n),\n           my_label= gsub(\" \", \"\\n\", my_label))\n  \n  if(tissue == \"Blood\"){\n    df$compound=factor(df$compound, levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  y_pos &lt;- df |&gt; \n    group_by(compound) |&gt; \n    summarize(y.position=mean(y_max))\n  \n  stat.test &lt;- df |&gt;\n    group_by(compound) |&gt;\n    t_test(value ~ my_label) |&gt;\n    adjust_pvalue(method=\"bonferroni\") |&gt;\n    add_significance()\n  test &lt;- stat.test |&gt;\n    left_join(y_pos) |&gt;\n    mutate(p.adj.signif=ifelse(p.adj.signif=='?', 'ns', p.adj.signif),\n           p.adj=ifelse(p.adj &lt; 0.001, \"&lt;0.001\", p.adj))\n\n  if(legend){\n    leg_position='right'\n  }else{\n    leg_position='none'\n  }\n  \n  if(y_lab){\n    y_text=\"Concentration (ng/mL)\"\n  }else{\n    y_text=''\n  }\n  \n  medianFunction &lt;- function(x){\n    return(data.frame(y=round(median(x),1),label=round(median(x,na.rm=T),1)))}\n  \n  p2 &lt;- ggplot(df, aes(x=my_label, y=value, fill=my_label)) + \n    geom_jitter(position=position_jitter(width=.3, height=0), size=0.8, color=\"gray65\")  +\n    geom_boxplot(outlier.shape=NA, alpha=0.6) +\n    stat_summary(fun=\"median\", geom=\"point\", shape=19, size=3, fill=\"black\") + \n    stat_summary(fun.data=medianFunction, geom =\"text\", color=\"black\", size=3.5, vjust=-0.65) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    geom_blank(aes(y=y_max)) + \n    scale_y_continuous(limits=c(0, NA), expand=expansion(mult=c(0, 0.1))) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    theme_classic() +\n    theme(text=element_text(size=14),\n          legend.position=leg_position,\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x='',\n         y=y_text) \n  \n  ann_text &lt;- test |&gt; \n    select(compound, p.adj, value=y.position, my_label=group1) |&gt;\n    filter(p.adj &lt; 0.05) |&gt; \n    mutate(x1=1, x2=2)\n  \n  if(tissue == \"Whole Blood\"){\n    ann_text$compound=factor(ann_text$compound, \n                               levels=c(\"THC\",\"11-OH-THC\",\"THCCOOH\",\"THCCOOH-gluc\")) \n  }\n  \n  p2 + geom_text(data=ann_text, label=ann_text$p.adj, nudge_x=0.5) +\n    geom_segment(data=ann_text, aes(x=x1, xend=x2,\n                                    y=value - (0.04 * value), \n                                    yend=value - (0.04*value)))\n  \n}\n\n\n\n\ncompound_boxplot_group_only(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_group_only(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_group_only(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#group-differences-treatment",
    "href": "content/lectures/12-cs01-eda.html#group-differences-treatment",
    "title": "12-cs01-eda",
    "section": "Group Differences: Treatment",
    "text": "Group Differences: Treatment\n\nCodeWBOFBR\n\n\n\ncompound_boxplot_treatment &lt;- function(dataset, compounds, tissue){\n  timepoint_to_use=levels(dataset$timepoint_use)[2]\n  df &lt;- dataset |&gt; \n    filter(timepoint_use == timepoint_to_use) |&gt;\n    select(treatment, group, compounds)\n  df &lt;- df |&gt; \n    gather(compound, value, -treatment, -group) |&gt; \n    clean_gluc()\n  \n  df |&gt; \n    ggplot(aes(x=treatment, y=value, fill=group)) + \n    # geom_jitter(color=\"gray36\") +\n    geom_boxplot(outlier.size=0.1) +\n    scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n    facet_wrap(~compound, scales=\"free_y\", ncol=4) +\n    scale_x_discrete(labels=function(x) str_wrap(x, width=11)) +\n    theme_classic(base_size=10) +\n    theme(legend.position=\"bottom\",\n          legend.title=element_blank(),\n          panel.grid=element_blank(),\n          strip.background=element_blank()) +\n    labs(title=tissue,\n         x=\"Treatment\",\n         y=\"Measurement (ng/mL)\")\n  \n}\n\n\n\n\ncompound_boxplot_treatment(WB_dups, compounds=unlist(compounds_WB), tissue=\"Whole Blood\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_treatment(OF_dups, compounds=unlist(compounds_OF), tissue=\"Oral Fluid\")\n\n\n\n\n\n\n\n\n\n\n\ncompound_boxplot_treatment(BR_dups, compounds=unlist(compounds_BR), tissue=\"Breath\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#compound-summaries",
    "href": "content/lectures/12-cs01-eda.html#compound-summaries",
    "title": "12-cs01-eda",
    "section": "Compound Summaries",
    "text": "Compound Summaries\n\nCodeWBOFBR\n\n\n\nT2A_plot &lt;- function(dataset, compound, timepoint_use=2){\n  timepoint_to_use=levels(factor(dataset$timepoint_use))[timepoint_use]\n  if(max(dataset[,compound],na.rm=TRUE)==0){\n    print(\n      ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n             aes_string(x=\"group\", \n                        y=compound, \n                        fill=\"group\")) + \n        geom_boxplot(outlier.size=0.1) +\n        scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n        scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n        scale_y_continuous(limits=c(0,3)) +\n        facet_grid(~treatment) + \n        theme_classic() +\n        theme(legend.position=\"none\",\n              panel.grid=element_blank(),\n              strip.background=element_blank(),\n              plot.title.position=\"plot\") +\n        labs(title=paste0('Timepoint: ',\n                          levels(dataset$timepoint_use)[timepoint_use],\n                          ' post-smoking'),\n             x='Group',\n             y=gsub('GLUC', 'gluc',gsub(\"_\", \"-\", toupper(compound))))\n    )}else{\n      print(\n        ggplot(subset(dataset, timepoint_use==timepoint_to_use), \n               aes_string(x=\"group\", \n                          y=compound, \n                          fill=\"group\")) + \n          geom_boxplot(outlier.shape=NA, alpha=0.8) +\n          scale_fill_manual(values=c(\"#19831C\", \"#A27FC9\")) +\n          scale_x_discrete(labels=function(x) str_wrap(x, width=10)) +\n          facet_grid(~treatment) + \n          theme_classic() +\n          theme(legend.position=\"none\",\n                panel.grid=element_blank(),\n                strip.background=element_blank(),\n                plot.title.position=\"plot\") +\n          labs(title=paste0('Timepoint: ',\n                            levels(dataset$timepoint_use)[timepoint_use],\n                            ' post-smoking'),\n               x='Group',\n               y=gsub('GLUC', 'gluc',gsub(\"_\",\"-\",toupper(compound))))\n      )\n    }\n}\n\n\n\n\npost_wb &lt;- map(compounds_WB, ~ T2A_plot( \n    dataset=WB_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_of &lt;- map(compounds_OF, ~ T2A_plot( \n    dataset=OF_dups, \n    compound=.x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npost_br &lt;- map(compounds_BR, ~ T2A_plot( \n    dataset=BR_dups, \n    compound=.x))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#hw02-part-ii",
    "href": "content/lectures/12-cs01-eda.html#hw02-part-ii",
    "title": "12-cs01-eda",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#chess-players",
    "href": "content/lectures/12-cs01-eda.html#chess-players",
    "title": "12-cs01-eda",
    "section": "Chess Players",
    "text": "Chess Players\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval &lt;- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n\nchess &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/chess-transfers/transfers.csv\")\n\nchess_trans &lt;- chess |&gt;\n  count(Federation) |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 10)\n\nchess_trans &lt;- chess_trans |&gt;\n  mutate(rank = row_number()) |&gt;\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nchess_trans &lt;- chess |&gt;\n  count(Federation) |&gt;\n  arrange(desc(n)) |&gt;\n  slice_head(n = 10)\n\nchess_trans &lt;- chess_trans |&gt;\n  mutate(rank = row_number()) |&gt;\n  mutate(Federation = case_when(\n    Federation == \"USA\" ~ \"United States\",\n    Federation == \"GER\" ~ \"Germany\",\n    Federation == \"CAN\" ~ \"Canada\",\n    Federation == \"ESP\" ~ \"Spain\",\n    Federation == \"RUS\" ~ \"Russia\",\n    Federation == \"FRA\" ~ \"France\",\n    Federation == \"BIH\" ~ \"Bosnia and Herzegovina\",\n    Federation == \"CRO\" ~ \"Croatia\",\n    Federation == \"TUR\" ~ \"Turkey\",\n    Federation == \"AUT\" ~ \"Austria\",\n    \n    TRUE ~ Federation  # Keep the name as-is for other cases\n  ))\n\nggplot(chess_trans, aes(y = reorder(Federation, n), x = n)) +\n  geom_bar(stat = \"identity\",fill = \"#1c9099\") +\n  geom_text(aes(x = -3, y = Federation, label = n), size = 4) + #add count at the left side of the bars\n  labs(title = bquote(bold(\"More players transfer to the U.S. than to any other country\")),\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       x = \"NUMBER OF TRANSFERS\",\n       y = \"COUNTRY\") +\n  scale_fill_identity() +\n  theme_minimal() + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank()) + \n  theme(axis.text.y = element_text(size = 10, angle = 0, hjust = 0), #align to the left\n        plot.title = element_text(size=18)) #change font size",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#kushi-fandango",
    "href": "content/lectures/12-cs01-eda.html#kushi-fandango",
    "title": "12-cs01-eda",
    "section": "Kushi: Fandango",
    "text": "Kushi: Fandango\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Kushi also got font working and stars on x-axis that I'd have to spend more time to get working \nfandango_score_comparison &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/fandango_score_comparison.csv\")\n\nall_scores_comp &lt;- fandango_score_comparison |&gt;\n  select(FILM, RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars) |&gt;\n  pivot_longer(c(RT_norm_round, RT_user_norm_round, Metacritic_norm_round, Metacritic_user_norm_round, IMDB_norm_round, Fandango_Stars), names_to = \"Site\", values_to = \"Score\")\n\ncounts_of_scores &lt;- all_scores_comp |&gt;\n  group_by(Site, Score) |&gt;\n  summarize(Count = n())\n\ncounts_of_scores &lt;- counts_of_scores |&gt;\n  group_by(Site) |&gt;\n  mutate(Total = sum(Count))\n\nscores_with_percents &lt;- counts_of_scores |&gt;\n  mutate(Percent = Count / Total * 100)\n\n# making percents 0 for any scores that don't have any values\n\nsites &lt;- tibble(Site = c(\"RT_norm_round\", \"RT_user_norm_round\", \"Metacritic_norm_round\", \"Metacritic_user_norm_round\", \"IMDB_norm_round\", \"Fandango_Stars\"))\n\nratings &lt;- tibble(Score = seq(0, 5, by = 0.5))\n\nall_scores &lt;- expand.grid(Site = sites$Site, Score = ratings$Score)\n\nall_scores &lt;- all_scores |&gt;\n  full_join(scores_with_percents, by = c(\"Site\", \"Score\")) |&gt;\n  mutate(Percent = case_when(is.na(Count) ~ 0,\n                             TRUE ~ Percent))\n\nscores &lt;- ggplot(all_scores, aes(x = Score, y = Percent, color = Site)) +\n  geom_line()  +\n  geom_hline(yintercept = 0, size = 0.7, color = \"black\") +\n  labs(x = NULL, y = NULL,\n       title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distrubution of 146 films in theaters in 2015 that\\n had 30+ reviews on Fandango.com\") +\n  scale_y_continuous(labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")) +\n  scale_x_continuous(labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +\n  scale_color_manual(values = c(\"Fandango_Stars\" = \"#fa6d54\",\n                                \"IMDB_norm_round\" = \"#e5c66a\",\n                                \"Metacritic_user_norm_round\" = \"#aeca91\",\n                                \"RT_user_norm_round\" = \"#76bde0\",\n                                \"Metacritic_norm_round\" = \"#b87eb5\",\n                                \"RT_norm_round\" = \"#a3a3a3\")) +\n  geom_ribbon(data = filter(all_scores, Site != \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent), alpha = 0.1) +\n  geom_ribbon(data = filter(all_scores, Site == \"Fandango_Stars\"), aes(ymin = 0, ymax = Percent, fill = \"#fa6d54\", alpha = 0.24)) +\n  guides(color = \"none\", fill = \"none\", alpha = \"none\")\n\nscores + \n  annotate(\"text\", x = 4.9, y = 35, label = \"Fandango\", size = 5, color = \"#fa6d54\", fontface = \"bold\") +\n  annotate(\"text\", x = 2.9, y = 37, label = \"IMDb users\", size = 5, color = \"#e5c66a\") +\n  annotate(\"text\", x = 2.7, y = 27, label = \"Metacritic\\nusers\", size = 5, color = \"#aeca91\") +\n  annotate(\"text\", x = 2.2, y = 20, label = \"Rotten\\nTomatoes\\nusers\", size = 5, color = \"#76bde0\") +\n  annotate(\"text\", x = 1.5, y = 16, label = \"Metacritic\", size = 5, color = \"#b87eb5\") +\n  annotate(\"text\", x = 0.7, y = 13, label = \"Rotten\\nTomatoes\", size = 5, color = \"#a3a3a3\") +\n  theme(#text = element_text(family = \"NimbusSan\"), \n        plot.title = element_text(face = \"bold\", size = 25, hjust = -0.1), \n        plot.subtitle = element_text(size = 15, hjust = -0.1),\n        plot.background = element_rect(fill = \"#f0f0f0\"),\n        panel.background = element_rect(fill = \"#f0f0f0\"),\n        panel.grid.major = element_line(color = \"gray75\", size = 0.2),\n        panel.grid.minor = element_blank(),\n        axis.text = element_text(size = 14)\n        )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#markus-congress",
    "href": "content/lectures/12-cs01-eda.html#markus-congress",
    "title": "12-cs01-eda",
    "section": "Markus: Congress",
    "text": "Markus: Congress\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncongress_data &lt;- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-demographics/data_aging_congress.csv\")\n\ncongress_ages =\n  congress_data |&gt;\n  select(congress, chamber, age_years) |&gt;\n  mutate(congress_year = case_when(TRUE ~ 1787 + (2 * as.integer(congress)))) |&gt;\n  group_by(congress_year, chamber) |&gt;\n  mutate(mean_age = case_when(TRUE ~ mean(age_years))) |&gt;\n  mutate(chamber = fct_recode(chamber,\n                              SENATE = \"Senate\",\n                              HOUSE = \"House\",))\n\nggplot(data = congress_ages,\n       mapping = aes(y = mean_age,\n                     x = congress_year,\n                     color = fct_rev(chamber),\n                     )) +\n  geom_step(size = 1) + \n  guides() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       caption = \"Data is based on all members who served in either the Senate or House in each Congress, which is notated\\nby the year in which it was seated. Any member who served in bothchambers in the same Congress was\\nassigned to the chamber in which they cast more votes.\\n FiveThirtyEight\\nSOURCES: BIOGRAPHICAL DIRECTORY OF THE U.S. CONGRESS, U.S. HOUSE OF REPRESENTATIVES,\\nU.S. SENATE, UNITEDSTATES GITHUB, VOTEVIEW.COM\",\n       y = NULL,\n       x = NULL\n  ) +\n  scale_color_manual(values=c(\"#6b4ddd\",\"#29ae53\")) +\n  theme_minimal(base_size = 13) + \n  theme(plot.title.position = \"plot\", \n        plot.title = element_text(face = \"bold\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.title=element_blank(),\n        legend.position = \"top\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#voter-demographics",
    "href": "content/lectures/12-cs01-eda.html#voter-demographics",
    "title": "12-cs01-eda",
    "section": "Voter Demographics",
    "text": "Voter Demographics\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggpubr)\n\nvoters = read.csv(\"data/nonvoters_data.csv\")\n\n\n#creating subplots\nrace = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Race\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())+\n    guides(fill = guide_legend(override.aes = list(shape = 16, key_width = 1, key_height = 1))) #trying to change the shape of the legend key\n\nincome = ggplot(data = voters, mapping = aes(y = factor(income_cat), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\") +\n  ggtitle(\"Income\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\nage = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Age\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\neducation = ggplot(data = voters, mapping = aes(y = factor(educ), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"Education\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\npartyID = ggplot(data = voters, mapping = aes(y = factor(race), fill = factor(voter_category))) +\n  geom_bar(position = \"fill\", show.legend = FALSE) +\n  ggtitle(\"party ID\") +\n  scale_fill_manual(values = c(\"always\" = \"#ffbc00\",\"sporadic\"=\"#e082ad\", \"rarely/never\" =\"#ae4dff\")) +\n  theme_minimal() +\n  theme(axis.title = element_blank(), axis.text.x = element_blank(), axis.ticks = element_blank(), line = element_blank(), panel.border = element_blank(), panel.grid.major = element_blank())\n\n#combining subplots\nria = ggarrange(race, income, age, ncol = 3, common.legend = TRUE, legend = \"top\") +\n  theme(plot.margin = margin(0.5, 0,-2, -1,\"cm\")) \n\nep = ggarrange(education, partyID) +\n  theme(plot.margin = margin(2, 4, -2.5, 1, \"cm\"))\n\nvoterplot = ggarrange(ria, ep, nrow = 3) \n\n\n#titles\nvoterplot = annotate_figure(annotate_figure(voterplot, \n  top = text_grob(\"Demographic information of survey respondants, by voting history\")),\n  top = text_grob(\"Those who always vote and those who sometimes vote aren't that different\", face = \"bold\")\n)\n\n\nvoterplot",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#fouls",
    "href": "content/lectures/12-cs01-eda.html#fouls",
    "title": "12-cs01-eda",
    "section": "Fouls",
    "text": "Fouls\n\nOriginalPlot ICode IPlot (Banso)Code (Banso)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_csv_file &lt;- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/foul-balls/foul-balls.csv\"\nfoulballs &lt;- read.csv(url(raw_csv_file))\n\nfoulballs &lt;- foulballs |&gt;\n  mutate(over90 = case_when(\n    exit_velocity &lt; 90 ~ \"no\",\n    exit_velocity &gt;= 90 ~ \"yes\"\n  ))\n\nggplot(foulballs,\n       aes (y = used_zone)) +\n  geom_bar(aes(fill = over90),\n           position = position_stack(reverse = TRUE),\n           show.legend = FALSE) +\n  scale_fill_manual(labels = c(\"&lt; 90 mph\", \"≥ 90 mph\", \"Unknown exit velocity\"),\n                    values = c(\"#97c16d\", \"#63abb0\"), na.value = \"#d3d3d3\") +\n  scale_y_discrete(limits = rev(unique(foulballs$used_zone)))+\n  scale_x_continuous(expand = expansion(mult = c(0, 0))) +\n  theme(panel.grid.minor.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(color = \"#ececec\"),\n        panel.background = element_rect(fill = \"white\"),\n        axis.text.x = element_text(color = \"#9e9e9e\"),\n        plot.caption = element_text(color = \"#b5b5b5\"),\n        axis.line.y = element_line(colour = \"#343434\"),\n        axis.title.y = element_text(angle = 0, vjust = 0.12, color = \"#343434\", size = 9),\n        axis.ticks = element_blank(),\n        legend.title = element_blank()) +\n  labs(title = \"The hardest-hit fouls seem to land in unprotected areas\",\n       subtitle = str_wrap(\"Foul balls by the stadium zone they landed in and their exit velocity, among 906 fouls hit this season in the most foul-heavy day at the 10 MLB stadiums that produced the most fouls as of June 5\", 85),\n       x = \"\", y = \"Zone\",\n       caption = \"SOURCE: BASEBALL SAVANT\") +\n  annotate(\"text\", x = 75, y = 1, label = \"&lt; 90 mph\", col = \"white\") +\n  annotate(\"text\", x = 140, y = 3.3, label = \"≥ 90 mph\", col = \"#63abb0\") +\n  annotate(\"text\", x = 215, y = 1, label = \"Unknown exit velocity\", col = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nball_data &lt;- foulballs |&gt;\n  mutate(category = case_when(\n    exit_velocity &lt; 90 ~ \"&lt;90\",\n    exit_velocity &gt;= 90 ~ \"&gt;=90\",\n    is.na(exit_velocity) ~ \"Unknown\"\n                           ))\nmy_plot &lt;- ggplot(ball_data, aes(y = fct_rev(as.character(used_zone)), fill = category)) +\n  geom_bar(position = position_stack(reverse = TRUE)) +\n  labs(title = \"The hardest-hit fouls seem \\nto land in unprotected areas\",\n       subtitle = \"The 906 foul balls hit this season from \\nthe most foul-heavy day at each of the \\n10 MLB stadiums that produced the \\nthe most fouls as of June 5, by zone where \\nthe balls landed and their exit velocities\",\n       y = \"Zone\", x = NULL) +\n  scale_fill_manual(values = c(\"Unknown\" = \"#DEDEDE\", \"&lt;90\" = \"#9ECE88\", \"&gt;=90\" = \"#17AFAD\")) +\n  scale_x_continuous(breaks = c(0, 50, 100, 150, 200, 250),position = \"top\") +\n  theme(\n    panel.background = element_rect(fill = \"white\"),\n    panel.grid.major.x = element_line(size = 0.5, linetype=\"solid\", color=\"#CECECE\"),\n    panel.grid.minor = element_blank(),\n    axis.text.y = element_text(color = \"black\", face = \"bold\"),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text(size = 10, margin = margin(b = 20))\n)\n\nmy_plot",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#colin-nba-raptor-ratings",
    "href": "content/lectures/12-cs01-eda.html#colin-nba-raptor-ratings",
    "title": "12-cs01-eda",
    "section": "Colin: NBA RAPTOR ratings",
    "text": "Colin: NBA RAPTOR ratings\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\nraptor &lt;- read.csv(\"data/latest_RAPTOR_by_player.csv\")\n\nraptor_rounded &lt;- raptor |&gt; \n  filter(mp &gt;= 1137) |&gt;\n  mutate(across(where(is.numeric), round, 1))\n\nraptor_plot &lt;- raptor_rounded |&gt;\n  ggplot(aes(x=raptor_offense, y = raptor_defense)) + \n  \n  # Annotations\n    # The colored rectangles for the 1 & 3 quadrants\n    annotate(\"rect\", xmin=0, xmax=10, ymin=0, ymax=10, fill = '#c5ecee', alpha = .85) + \n    annotate(\"rect\", xmin=-10, xmax=0, ymin=-10, ymax=0, fill = '#fecada', alpha = .85) +\n    \n    # The 3rd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -7.8, ymax = -6.8, fill = '#fd97b6') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = -7.4, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = -9, label = \" -  defense\") +\n    \n    # The 1st quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = 8.4, ymax = 9.4, fill = '#8cdadf') +\n    annotate(\"text\", x = 7.5, y = 8.8, label = \" +  offense\") +\n    annotate(\"text\", x = 7.55, y = 7.1, label = \" +  defense\") +\n  \n    # The 2nd quadrant text rectangles\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 6.8, ymax = 7.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=-9.8, xmax=-5.5, ymin = 8.4, ymax = 9.4, fill = '#fd97b6') +\n    annotate(\"text\", x = -7.8, y = 8.8, label = \" -  offense\") +\n    annotate(\"text\", x = -7.8, y = 7.1, label = \" +  defense\") +\n  \n    # The 4th quadrant text rectangles\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -7.8, ymax = -6.8, fill = '#8cdadf') +\n    annotate(\"rect\", xmin=5.5, xmax=9.8, ymin = -9.4, ymax = -8.4, fill = '#fd97b6') +\n    annotate(\"text\", x = 7.55, y = -7.4, label = \" +  offense\") +\n    annotate(\"text\", x = 7.5, y = -9, label = \" -  defense\") +\n  \n  geom_point(shape= 21, colour = \"black\", fill = \"white\", size = 4) + \n  \n  labs(\n    x = \"Offensive RAPTOR rating\", \n    y = \"Defensive RAPTOR rating\",\n    title = paste0('Nikola Jokic is the Best NBA Player Based on Overall RAPTOR Rating',\n            '&lt;br&gt;',\n            '&lt;sup&gt;',\n            'An Analytical Approach to the 2022 - 2023 NBA Season','&lt;/sup&gt;')\n    ) + \n    \n  # Theme settings\n  theme_light() + \n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(), \n    panel.grid.major = element_line(color = \"#cdcdcd\", linewidth = 0.5), \n    plot.margin = margin(l = 100, r = 100, b = 20, t = 10),\n    plot.title = element_text(hjust = 0.5),\n    axis.text = element_text(color = \"#cdcdcd\", size = 12),\n    axis.title.x = element_text(margin = margin(t=3)),\n    axis.title.y = element_text(margin = margin(r=3)),\n    ) + \n  coord_fixed(ratio = 1) \n\n# interactive\n# ggplotly(raptor_plot, text=player_name, hoverinfo='text') \n\nraptor_plot",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#biden-approval",
    "href": "content/lectures/12-cs01-eda.html#biden-approval",
    "title": "12-cs01-eda",
    "section": "Biden Approval",
    "text": "Biden Approval\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID_approval &lt;- read_csv(url(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls.csv\"))\n\n#getting only data from beginning of COVID to Jan 19, 2021 and getting rid of the \"all\" category in the party column \nyear_20 &lt;- \n  COVID_approval |&gt;\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |&gt;\n  filter(party != \"all\", \n         end_year &lt;= 2020)|&gt;\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n\nyear_21 &lt;-\n  COVID_approval |&gt;\n  separate(end_date, into = c(\"end_year\", \"end_month\", \"end_day\"),convert = TRUE) |&gt;\n  filter(party != \"all\", \n         end_year == 2021 & end_month == 1 & end_day &lt;= 19) |&gt;\n  unite(end_date, end_year, end_month, end_day, sep = \"-\")\n       #  if (end_year == 2021 & end_month == 1 & end_day &lt;= 19))\n\nselect_COVID_approval &lt;-\n  full_join(year_20, year_21)\n\nggplot(select_COVID_approval, aes(x = as.Date(end_date),\n                           y = approve,\n                           color = party)) +\n  geom_smooth(aes(group = party), span = 0.05, se = FALSE) + \n  scale_color_manual(values = c(\"D\" = \"#008fd5\", \n                               \"R\" = \"#ff2700\",\n                               \"I\" = \"#a55330\")) +\n  \n  #Important Dates\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-02-29\"), y = 95, label=\"First U.S. \\n Death \\n Reported\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-05-20\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-05-20\"), y = 95, label=\"U.S. Deaths \\n surpass \\n 100,000\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype=3) + #Trump Diagnosed with COVID-19\n    annotate(\"text\", x= as.Date(\"2020-10-02\"), y = 95, label=\"Trump \\n Diagnosed \\n with \\n COVID-19\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2020-11-07\"), y = 95, label=\"Biden \\n declared \\n election \\n winner\", size=3, color=\"black\") +\n  geom_vline(xintercept = as.Date(\"2021-01-19\"), linetype=3) + \n    annotate(\"text\", x= as.Date(\"2021-01-19\"), y = 95, label=\"Biden \\n sworn \\n into \\n office\", size=3, color=\"black\") +\n  \n  labs(title = \"Approval of Trump’s response varies widely by party\",\n       subtitle = \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s \\n handling of the coronavirus outbreak\") +\n  scale_y_continuous(limits = c(0, 100)) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5 ), \n        plot.subtitle = element_text(hjust = 0.5),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position=\"none\",\n        panel.border = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.minor = element_line(colour=\"gray\")\n        )",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#recap",
    "href": "content/lectures/12-cs01-eda.html#recap",
    "title": "12-cs01-eda",
    "section": "Recap",
    "text": "Recap\n\nCan you explain/describe the plots generated in the context of these data?\nCan you generate EDA plots of your own for these data\nCan you understand/work through the more complicated code provided (even if you couldn’t have come up with it on your own)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "12-cs01-eda"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#ad-computing-paths",
    "href": "content/lectures/01-intro-to-r-slides.html#ad-computing-paths",
    "title": "01-intro-to-r",
    "section": "[ad] Computing Paths",
    "text": "[ad] Computing Paths\n\nhttps://computingpaths.ucsd.edu/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#ad-the-basement",
    "href": "content/lectures/01-intro-to-r-slides.html#ad-the-basement",
    "title": "01-intro-to-r",
    "section": "[ad] The Basement",
    "text": "[ad] The Basement",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#ad-calpirg",
    "href": "content/lectures/01-intro-to-r-slides.html#ad-calpirg",
    "title": "01-intro-to-r",
    "section": "[ad] CALPIRG",
    "text": "[ad] CALPIRG\n\nAPPLY NOW: Protect the environment and make social change\nCALPIRG Students is a student organization here that works to protect the environment, make college more affordable, and promote civic engagement. Last Fall we helped nearly 10,000 students register to vote in California and got the UCs to release new policy to phase out single-use plastics to protect our oceans! SIGN UP TO HELP NOW\nNow, we are working to tackle the biggest problem facing our generation - climate change. Coming off another record-setting summer of hot temperatures, it’s clear we need to take strong, swift action to reduce the impacts of climate change. That’s why we are building support from students across the state to call for 100% clean energy UC-wide - for cars, buses, buildings, lights, and more! Fill out this interest form to learn more!\nAs a volunteer or intern with CALPIRG you can:\nWork with the media and help organize events like a Solar-Powered Concert and climate week of action\nIncrease voting accessibility and voter turnout in elections\nBring down the cost of textbooks\nProtect wildlife like whales and sea otters in the Pacific\nAnd more!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#qa",
    "href": "content/lectures/01-intro-to-r-slides.html#qa",
    "title": "01-intro-to-r",
    "section": "Q&A",
    "text": "Q&A\nQ: How are groups formed for the projects?\nA: I form them randomly for the two case studies and students get to choose their own groups for the final project.\nQ: I’m curious about what the workload would be like for the case studies.\nA: We’ll discuss this when the time comes in detail but for now, students are presented with a lot of the starter code for the case studies. You and your group mates have to get teh code running, add explanations, and “extend” the case study, meaning add something meaningful onto what was presented.\nQ: Will this course go into advanced topics in tidyverse?\nA: We will certainly go beyond the basic dplyr verbs and cover multiple packages in the tidyverse, but we won’t be able to cover everything.\nQ: Would it be possible to have access to the third Case Study just to work on our own time?\nA: Yup! I’ll you all in the direction of OpenCaseStudies, which is a resource I’ll be using for 1 of the 2 case studies this quarter, and that we used for all case studies previously.\nQ: How many homework assignments are there? Some slides said 3, some said 4.\nA: Apologies. There are 3. (There were previously 4 but I removed one. Slides have been updated.)\nQ: Could we just sit in for lectures? Can I keep watching lectures?\nA: Yup - this would be fine, so long as everyone enrolled had a seat. And, the podcasts are open to anyone!\nQ: If we decide to do our projects and homework locally, how can we download/install the packages necessary for it?\nA: This is covered in your first lab!\nQ: I’m generally curious on how the language R is used in real world settings after college. What are its specific uses and what are the better ways to learn the language to maximize its utility. Also, how does this language differ from python in the data science realm.\nA: Great question that we’ll cover throughout the course. However, breifly here, R is most heavily used by individuals who do more statistics, who work in biology, psychology or economics, and/or who analyze data regularly.\nQ: What is the advantage of using R over other programming languages to do data science tasks? How much is R used for data science in the real world?\nA: Within the tidyverse and in RStudio, the advantage is the cohesiveness of the tools - once you gain familiarity you can often intuit how to use another tool in the tidyverse. R is used for data science across tons of companies; however, across industries. its use is not as widespread as Python.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#course-announcements",
    "href": "content/lectures/01-intro-to-r-slides.html#course-announcements",
    "title": "01-intro-to-r",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nStudent survey “due” today 11:59 PM\nLab 01 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\nWaitlist (Non)Update: Staff are seeing what options there are. A few people got an email from Kasey Chiang (k4chiang@ucsd.edu) to drop and then enroll. This is legitimate. Follow those instructions.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#agenda",
    "href": "content/lectures/01-intro-to-r-slides.html#agenda",
    "title": "01-intro-to-r",
    "section": "Agenda",
    "text": "Agenda\n\nVariables\nOperators\nData in R\nRMarkdown",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variables-assignment-1",
    "href": "content/lectures/01-intro-to-r-slides.html#variables-assignment-1",
    "title": "01-intro-to-r",
    "section": "Variables & Assignment",
    "text": "Variables & Assignment\nVariables are how we store information so that we can access it later.\n\nVariables are created and stored using the assignment operator &lt;-\n\nfirst_variable &lt;- 3\n\nThe above stores the value 3 in the variable first_variable\n\n\nNote: Other programming languages use = for assignment. R also uses that for assignment, but it is more typical to see &lt;- in R code, so we’ll stick with that.\n\n\nThis means that if we ever want to reference the information stored in that variable later, we can “call” (mean, type in our code) the variable’s name:\n\nfirst_variable\n\n[1] 3",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-type",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-type",
    "title": "01-intro-to-r",
    "section": "Variable Type",
    "text": "Variable Type\n\nEvery variable you create in R will be of a specific type.\n\n\n\nThe type of the variable is determined dynamically on assignment.\n\n\n\n\nDetermining the type of a variable with class():\n\n\nclass(first_variable)\n\n[1] \"numeric\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#basic-variable-types",
    "href": "content/lectures/01-intro-to-r-slides.html#basic-variable-types",
    "title": "01-intro-to-r",
    "section": "Basic Variable Types",
    "text": "Basic Variable Types\n\n\n\n\n\n\n\n\nVariable Type\nExplanation\nExample\n\n\n\n\ncharacter\nstores a string\n\"cogs137\", \"hi!\"\n\n\nnumeric\nstores whole numbers and decimals\n9, 9.29\n\n\ninteger\nspecifies integer\n9L (the L specifies this is an integer)\n\n\nlogical\nBooleans\nTRUE, FALSE\n\n\nlist\nstore multiple elements\nlist(7, \"a\", TRUE)\n\n\n\nNote: There are many more. We’ll get to some but not all in this course.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#logical-character",
    "href": "content/lectures/01-intro-to-r-slides.html#logical-character",
    "title": "01-intro-to-r",
    "section": "logical & character",
    "text": "logical & character\nlogical - Boolean values TRUE and FALSE\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\ncharacter - character strings\n\nclass(\"hello\")\n\n[1] \"character\"\n\nclass('students') # equivalent...but we'll use double quotes!\n\n[1] \"character\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#numeric-double-integer",
    "href": "content/lectures/01-intro-to-r-slides.html#numeric-double-integer",
    "title": "01-intro-to-r",
    "section": "numeric: double & integer",
    "text": "numeric: double & integer\ndouble - floating point numerical values (default numerical type)\n\nclass(1.335)\n\n[1] \"numeric\"\n\nclass(7)\n\n[1] \"numeric\"\n\n\n\ninteger - integer numerical values (indicated with an L)\n\nclass(7L)\n\n[1] \"integer\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#lists",
    "href": "content/lectures/01-intro-to-r-slides.html#lists",
    "title": "01-intro-to-r",
    "section": "lists",
    "text": "lists\nSo far, every variable has been an atomic vector, meaning it only stores a single piece of information.\n\nLists are 1d objects that can contain any combination of R objects\n\n\n\nmylist &lt;- list(\"A\", 7L, TRUE, 18.4)\nmylist\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 18.4\n\n\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#your-turn",
    "href": "content/lectures/01-intro-to-r-slides.html#your-turn",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nDefine variables of each of the following types: character, numeric, integer, logical, list\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#functions",
    "href": "content/lectures/01-intro-to-r-slides.html#functions",
    "title": "01-intro-to-r",
    "section": "Functions",
    "text": "Functions\n\nclass() (and View() & median()) were our first functions…but we’ll show a few more.\n\n\n\nFunctions are (most often) verbs, followed by what they will be applied to in parentheses.\n\n\n\nFunctions are:\n\navailable from base R\navailable from packages you import\ndefined by you\n\n\n\nWe’ll start by getting comfortable with available functions, but in a few days, you’ll learn how to write your own!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#helpful-functions",
    "href": "content/lectures/01-intro-to-r-slides.html#helpful-functions",
    "title": "01-intro-to-r",
    "section": "Helpful Functions",
    "text": "Helpful Functions\n\n\n\nclass() - determine high-level variable type\n\n\nclass(mylist)\n\n[1] \"list\"\n\n\n\nlength()- determine how long an object is\n\n\n# contains 4 elements\nlength(mylist)\n\n[1] 4\n\n\n\n\nstr() - display the structure of an R object\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#coercion",
    "href": "content/lectures/01-intro-to-r-slides.html#coercion",
    "title": "01-intro-to-r",
    "section": "Coercion",
    "text": "Coercion\nR is a dynamically typed language – it will happily convert between the various types without complaint.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"\n\nc(FALSE, 3L)\n\n[1] 0 3\n\nc(1.2, 3L)\n\n[1] 1.2 3.0",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#missing-values",
    "href": "content/lectures/01-intro-to-r-slides.html#missing-values",
    "title": "01-intro-to-r",
    "section": "Missing Values",
    "text": "Missing Values\nR uses NA to represent missing values in its data structures.\n\nclass(NA)\n\n[1] \"logical\"\n\n\n\nOther Special Values\nNaN | Not a number\nInf | Positive infinity\n-Inf | Negative infinity",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#activity",
    "href": "content/lectures/01-intro-to-r-slides.html#activity",
    "title": "01-intro-to-r",
    "section": "Activity",
    "text": "Activity\nWhat is the type of the following vectors? Chat about why they have that type.\n\nc(1, NA+1L, \"C\")\nc(1L / 0, NA)\nc(1:3, 5)\nc(3L, NaN+1L)\nc(NA, TRUE)\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#operators-1",
    "href": "content/lectures/01-intro-to-r-slides.html#operators-1",
    "title": "01-intro-to-r",
    "section": "Operators",
    "text": "Operators\nAt its simplest, R is a calculator. To carry out mathematical operations, R uses operators.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators",
    "href": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\n\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\nx %% y\nmodulus (x mod y) 9%%2 is 1\n\n\nx %/% y\ninteger division 9%/%2 is 4",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators-examples",
    "href": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators-examples",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators: Examples",
    "text": "Arithmetic Operators: Examples\n\n7 + 6  \n\n[1] 13\n\n2 - 3\n\n[1] -1\n\n4 * 2\n\n[1] 8\n\n9 / 2\n\n[1] 4.5",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#reminder",
    "href": "content/lectures/01-intro-to-r-slides.html#reminder",
    "title": "01-intro-to-r",
    "section": "Reminder",
    "text": "Reminder\nOutput can be stored to a variable\n\nmy_addition &lt;- 7 + 6\n\n\n\nmy_addition\n\n[1] 13",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#comparison-operators",
    "href": "content/lectures/01-intro-to-r-slides.html#comparison-operators",
    "title": "01-intro-to-r",
    "section": "Comparison Operators",
    "text": "Comparison Operators\nThese operators return a Boolean.\n\n\n\nOperator\nDescription\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#comparison-operators-examples",
    "href": "content/lectures/01-intro-to-r-slides.html#comparison-operators-examples",
    "title": "01-intro-to-r",
    "section": "Comparison Operators: Examples",
    "text": "Comparison Operators: Examples\n\n4 &lt; 12\n\n[1] TRUE\n\n4 &gt;= 3\n\n[1] TRUE\n\n6 == 6\n\n[1] TRUE\n\n7 != 6\n\n[1] TRUE",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#your-turn-1",
    "href": "content/lectures/01-intro-to-r-slides.html#your-turn-1",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nUse arithmetic and comparison operators to store the value 30 in the variable var_30 and TRUE in the variable true_var.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#packages",
    "href": "content/lectures/01-intro-to-r-slides.html#packages",
    "title": "01-intro-to-r",
    "section": "Packages",
    "text": "Packages\n\nPackages are installed with the install.packages function and loaded with the library function, once per session:\n\n\ninstall.packages(\"package_name\")\nlibrary(package_name)\n\n\nIn this course, most packages we’ll use have been installed for you already on datahub, so you will only have to load the package in (using library).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-sets-in-r",
    "href": "content/lectures/01-intro-to-r-slides.html#data-sets-in-r",
    "title": "01-intro-to-r",
    "section": "Data “sets” in R",
    "text": "Data “sets” in R\n\n“set” is in quotation marks because it is not a formal data class\nA tidy data “set” can be one of the following types:\n\ntibble\ndata.frame\n\nWe’ll often work with tibbles:\n\nreadr package (e.g. read_csv function) loads data as a tibble by default\ntibbles are part of the tidyverse, so they work well with other packages we are using\nthey make minimal assumptions about your data, so are less likely to cause hard to track bugs in your code",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-frames",
    "href": "content/lectures/01-intro-to-r-slides.html#data-frames",
    "title": "01-intro-to-r",
    "section": "Data frames",
    "text": "Data frames\n\nA data frame is the most commonly used data structure in R, they are list of equal length vectors (usually atomic, but can be generic). Each vector is treated as a column and elements of the vectors as rows.\nA tibble is a type of data frame that … makes your life (i.e. data analysis) easier.\nMost often a data frame will be constructed by reading in from a file, but we can create them from scratch.\n\n\ndf &lt;- tibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\nclass(df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(df)\n\nRows: 3\nColumns: 2\n$ x &lt;int&gt; 1, 2, 3\n$ y &lt;chr&gt; \"a\", \"b\", \"c\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-frames-cont.",
    "href": "content/lectures/01-intro-to-r-slides.html#data-frames-cont.",
    "title": "01-intro-to-r",
    "section": "Data frames (cont.)",
    "text": "Data frames (cont.)\n\nattributes(df)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n$names\n[1] \"x\" \"y\"\n\n\n\nColumns (variables) in data frames are accessed with $:\n\ndataframe$var_name\n\n\n\n\nclass(df$x)  # access variable type for column\n\n[1] \"integer\"\n\nclass(df$y)  \n\n[1] \"character\"",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-types",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-types",
    "title": "01-intro-to-r",
    "section": "Variable Types",
    "text": "Variable Types\nData stored in columns can include different kinds of information…which would require a different type (class) of variable to be used in R.\n\n\n\n\nR Data Types:\n\nContinuous: numeric, integer\nDiscrete: factors (we haven’t talked about these yet, but will today!)\n\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-types-cont.",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-types-cont.",
    "title": "01-intro-to-r",
    "section": "Variable Types (cont.)",
    "text": "Variable Types (cont.)\nSometimes data are non-numeric and store words. Even when that is the case, the data can be conveying different information.\n\n\n\n\nR Data Types:\n\nNominal: character\nOrdinal: factors\nBinary: logical OR numeric OR factors 😱\n\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#example-cat-lovers",
    "href": "content/lectures/01-intro-to-r-slides.html#example-cat-lovers",
    "title": "01-intro-to-r",
    "section": "Example: Cat lovers",
    "text": "Example: Cat lovers\nA survey asked respondents their name and number of cats. The instructions said to enter the number of cats as a numerical value.\n\n🚨 There is code ahead that we’re not going to discuss in detail today, but we will in coming lectures.\n\ncat_lovers &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#the-data",
    "href": "content/lectures/01-intro-to-r-slides.html#the-data",
    "title": "01-intro-to-r",
    "section": "The Data",
    "text": "The Data\n\ncat_lovers |&gt;\n  datatable()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#the-question",
    "href": "content/lectures/01-intro-to-r-slides.html#the-question",
    "title": "01-intro-to-r",
    "section": "The Question",
    "text": "The Question\nHow many respondents have a below average number of cats?\n\nGiving it a first shot…\n\ncat_lovers |&gt;\n  summarise(mean = mean(number_of_cats))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean = mean(number_of_cats)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1    NA\n\n\n\n\n💡 maybe there is missing data in the number_of_cats column!\nOh why will you still not work??!!\n\ncat_lovers |&gt;\n  summarise(mean_cats = mean(number_of_cats, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean_cats = mean(number_of_cats, na.rm = TRUE)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n  mean_cats\n      &lt;dbl&gt;\n1        NA\n\n\n\n\n💡What is the type of the number_of_cats variable?",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#take-a-breath-and-look-at-your-data",
    "href": "content/lectures/01-intro-to-r-slides.html#take-a-breath-and-look-at-your-data",
    "title": "01-intro-to-r",
    "section": "Take a breath and look at your data",
    "text": "Take a breath and look at your data\n\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           &lt;chr&gt; \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats &lt;chr&gt; \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     &lt;chr&gt; \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#lets-take-another-look",
    "href": "content/lectures/01-intro-to-r-slides.html#lets-take-another-look",
    "title": "01-intro-to-r",
    "section": "Let’s take another look",
    "text": "Let’s take another look",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#sometimes-you-need-to-babysit-your-respondents",
    "href": "content/lectures/01-intro-to-r-slides.html#sometimes-you-need-to-babysit-your-respondents",
    "title": "01-intro-to-r",
    "section": "Sometimes you need to babysit your respondents",
    "text": "Sometimes you need to babysit your respondents\n\ncat_lovers |&gt;\n  mutate(number_of_cats = case_when(\n    name == \"Ginger Clark\" ~ 2,\n    name == \"Doug Bass\"    ~ 3,\n    TRUE                   ~ as.numeric(number_of_cats))) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `number_of_cats = case_when(...)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 60 × 3\n   name           number_of_cats handedness\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;     \n 1 Bernice Warren              0 left      \n 2 Woodrow Stone               0 left      \n 3 Willie Bass                 1 left      \n 4 Tyrone Estrada              3 left      \n 5 Alex Daniels                3 left      \n 6 Jane Bates                  2 left      \n 7 Latoya Simpson              1 left      \n 8 Darin Woods                 1 left      \n 9 Agnes Cobb                  0 left      \n10 Tabitha Grant               0 left      \n# ℹ 50 more rows",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#always-respect-check-data-types",
    "href": "content/lectures/01-intro-to-r-slides.html#always-respect-check-data-types",
    "title": "01-intro-to-r",
    "section": "Always respect (& check!) data types",
    "text": "Always respect (& check!) data types\n\ncat_lovers |&gt;\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats)) |&gt;\n  summarise(mean_cats = mean(number_of_cats))\n\n# A tibble: 1 × 1\n  mean_cats\n      &lt;dbl&gt;\n1     0.817",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#now-that-we-know-what-were-doing",
    "href": "content/lectures/01-intro-to-r-slides.html#now-that-we-know-what-were-doing",
    "title": "01-intro-to-r",
    "section": "Now that we know what we’re doing…",
    "text": "Now that we know what we’re doing…\n\ncat_lovers &lt;- cat_lovers |&gt;\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats))\n\n… store your data in a variable (here we’re overwriting the old cat_lovers tibble).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#moral-of-the-story",
    "href": "content/lectures/01-intro-to-r-slides.html#moral-of-the-story",
    "title": "01-intro-to-r",
    "section": "Moral of the story",
    "text": "Moral of the story\n\nIf your data does not behave how you expect it to, type coercion upon reading in the data might be the reason.\nGo in and investigate your data, apply the fix, save your data, live happily ever after.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#r-markdown-tour",
    "href": "content/lectures/01-intro-to-r-slides.html#r-markdown-tour",
    "title": "01-intro-to-r",
    "section": "R Markdown: tour",
    "text": "R Markdown: tour\n\n[DEMO]\n\nBefore we move on…\n   What is the Bechdel test?\n\nThe Bechdel test asks whether a work of fiction features at least two women who talk to each other about something other than a man, and there must be two women named characters.\n\n\nConcepts introduced:\n\nKnitting documents\nR Markdown and (some) R syntax",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#github-setup",
    "href": "content/lectures/01-intro-to-r-slides.html#github-setup",
    "title": "01-intro-to-r",
    "section": "GitHub Setup",
    "text": "GitHub Setup\nSee this week’s lab…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#giving-the-demo-a-go",
    "href": "content/lectures/01-intro-to-r-slides.html#giving-the-demo-a-go",
    "title": "01-intro-to-r",
    "section": "Giving the demo a go…",
    "text": "Giving the demo a go…\n\nNavigate to the demo URL (on Canvas)\nAccept the “assignment” (this is NOT graded)\nClone the repo\nEdit the document\nKnit the document\nPush your changes\n\nTry to play around with this after finishing your lab tomorrow!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#recap",
    "href": "content/lectures/01-intro-to-r-slides.html#recap",
    "title": "01-intro-to-r",
    "section": "Recap",
    "text": "Recap\n\nAlways best to think of data as part of a tibble\n\nThis plays nicely with the tidyverse as well\nRows are observations, columns are variables\n\nWhat are the common variable types in R\n\nHow do I create a variable of each type?\nWhen would I use each one?\n\nDo I know how to determine the class/type of a variable?\nCan I explain dynamic typing?\nCan I operate on variables and values using…\n\narithmetic operators?\ncomparison operators?\n\nWhat are dataframes/tibbles? and why are they useful?\nWhat is the difference between installing and loading a package?\nWhat are the components of an R Markdown file?\n\n\n\n\n\nhttps://cogs137.github.io/website/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html",
    "href": "content/lectures/01-intro-to-r.html",
    "title": "01-intro-to-r",
    "section": "",
    "text": "https://computingpaths.ucsd.edu/\n\n\n\n\n\n\n\n\n\n\nAPPLY NOW: Protect the environment and make social change\nCALPIRG Students is a student organization here that works to protect the environment, make college more affordable, and promote civic engagement. Last Fall we helped nearly 10,000 students register to vote in California and got the UCs to release new policy to phase out single-use plastics to protect our oceans! SIGN UP TO HELP NOW\nNow, we are working to tackle the biggest problem facing our generation - climate change. Coming off another record-setting summer of hot temperatures, it’s clear we need to take strong, swift action to reduce the impacts of climate change. That’s why we are building support from students across the state to call for 100% clean energy UC-wide - for cars, buses, buildings, lights, and more! Fill out this interest form to learn more!\nAs a volunteer or intern with CALPIRG you can:\nWork with the media and help organize events like a Solar-Powered Concert and climate week of action\nIncrease voting accessibility and voter turnout in elections\nBring down the cost of textbooks\nProtect wildlife like whales and sea otters in the Pacific\nAnd more!\n\n\n\n\nQ: How are groups formed for the projects?\nA: I form them randomly for the two case studies and students get to choose their own groups for the final project.\nQ: I’m curious about what the workload would be like for the case studies.\nA: We’ll discuss this when the time comes in detail but for now, students are presented with a lot of the starter code for the case studies. You and your group mates have to get teh code running, add explanations, and “extend” the case study, meaning add something meaningful onto what was presented.\nQ: Will this course go into advanced topics in tidyverse?\nA: We will certainly go beyond the basic dplyr verbs and cover multiple packages in the tidyverse, but we won’t be able to cover everything.\nQ: Would it be possible to have access to the third Case Study just to work on our own time?\nA: Yup! I’ll you all in the direction of OpenCaseStudies, which is a resource I’ll be using for 1 of the 2 case studies this quarter, and that we used for all case studies previously.\nQ: How many homework assignments are there? Some slides said 3, some said 4.\nA: Apologies. There are 3. (There were previously 4 but I removed one. Slides have been updated.)\nQ: Could we just sit in for lectures? Can I keep watching lectures?\nA: Yup - this would be fine, so long as everyone enrolled had a seat. And, the podcasts are open to anyone!\nQ: If we decide to do our projects and homework locally, how can we download/install the packages necessary for it?\nA: This is covered in your first lab!\nQ: I’m generally curious on how the language R is used in real world settings after college. What are its specific uses and what are the better ways to learn the language to maximize its utility. Also, how does this language differ from python in the data science realm.\nA: Great question that we’ll cover throughout the course. However, breifly here, R is most heavily used by individuals who do more statistics, who work in biology, psychology or economics, and/or who analyze data regularly.\nQ: What is the advantage of using R over other programming languages to do data science tasks? How much is R used for data science in the real world?\nA: Within the tidyverse and in RStudio, the advantage is the cohesiveness of the tools - once you gain familiarity you can often intuit how to use another tool in the tidyverse. R is used for data science across tons of companies; however, across industries. its use is not as widespread as Python.\n\n\n\nDue Dates:\n\nStudent survey “due” today 11:59 PM\nLab 01 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\nWaitlist (Non)Update: Staff are seeing what options there are. A few people got an email from Kasey Chiang (k4chiang@ucsd.edu) to drop and then enroll. This is legitimate. Follow those instructions.\n\n\n\n\nVariables\nOperators\nData in R\nRMarkdown",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#ad-computing-paths",
    "href": "content/lectures/01-intro-to-r.html#ad-computing-paths",
    "title": "01-intro-to-r",
    "section": "",
    "text": "https://computingpaths.ucsd.edu/",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#ad-calpirg",
    "href": "content/lectures/01-intro-to-r.html#ad-calpirg",
    "title": "01-intro-to-r",
    "section": "",
    "text": "APPLY NOW: Protect the environment and make social change\nCALPIRG Students is a student organization here that works to protect the environment, make college more affordable, and promote civic engagement. Last Fall we helped nearly 10,000 students register to vote in California and got the UCs to release new policy to phase out single-use plastics to protect our oceans! SIGN UP TO HELP NOW\nNow, we are working to tackle the biggest problem facing our generation - climate change. Coming off another record-setting summer of hot temperatures, it’s clear we need to take strong, swift action to reduce the impacts of climate change. That’s why we are building support from students across the state to call for 100% clean energy UC-wide - for cars, buses, buildings, lights, and more! Fill out this interest form to learn more!\nAs a volunteer or intern with CALPIRG you can:\nWork with the media and help organize events like a Solar-Powered Concert and climate week of action\nIncrease voting accessibility and voter turnout in elections\nBring down the cost of textbooks\nProtect wildlife like whales and sea otters in the Pacific\nAnd more!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#qa",
    "href": "content/lectures/01-intro-to-r.html#qa",
    "title": "01-intro-to-r",
    "section": "",
    "text": "Q: How are groups formed for the projects?\nA: I form them randomly for the two case studies and students get to choose their own groups for the final project.\nQ: I’m curious about what the workload would be like for the case studies.\nA: We’ll discuss this when the time comes in detail but for now, students are presented with a lot of the starter code for the case studies. You and your group mates have to get teh code running, add explanations, and “extend” the case study, meaning add something meaningful onto what was presented.\nQ: Will this course go into advanced topics in tidyverse?\nA: We will certainly go beyond the basic dplyr verbs and cover multiple packages in the tidyverse, but we won’t be able to cover everything.\nQ: Would it be possible to have access to the third Case Study just to work on our own time?\nA: Yup! I’ll you all in the direction of OpenCaseStudies, which is a resource I’ll be using for 1 of the 2 case studies this quarter, and that we used for all case studies previously.\nQ: How many homework assignments are there? Some slides said 3, some said 4.\nA: Apologies. There are 3. (There were previously 4 but I removed one. Slides have been updated.)\nQ: Could we just sit in for lectures? Can I keep watching lectures?\nA: Yup - this would be fine, so long as everyone enrolled had a seat. And, the podcasts are open to anyone!\nQ: If we decide to do our projects and homework locally, how can we download/install the packages necessary for it?\nA: This is covered in your first lab!\nQ: I’m generally curious on how the language R is used in real world settings after college. What are its specific uses and what are the better ways to learn the language to maximize its utility. Also, how does this language differ from python in the data science realm.\nA: Great question that we’ll cover throughout the course. However, breifly here, R is most heavily used by individuals who do more statistics, who work in biology, psychology or economics, and/or who analyze data regularly.\nQ: What is the advantage of using R over other programming languages to do data science tasks? How much is R used for data science in the real world?\nA: Within the tidyverse and in RStudio, the advantage is the cohesiveness of the tools - once you gain familiarity you can often intuit how to use another tool in the tidyverse. R is used for data science across tons of companies; however, across industries. its use is not as widespread as Python.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#course-announcements",
    "href": "content/lectures/01-intro-to-r.html#course-announcements",
    "title": "01-intro-to-r",
    "section": "",
    "text": "Due Dates:\n\nStudent survey “due” today 11:59 PM\nLab 01 due Friday (11:59 PM)\nLecture Participation survey “due” after class\n\nWaitlist (Non)Update: Staff are seeing what options there are. A few people got an email from Kasey Chiang (k4chiang@ucsd.edu) to drop and then enroll. This is legitimate. Follow those instructions.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#agenda",
    "href": "content/lectures/01-intro-to-r.html#agenda",
    "title": "01-intro-to-r",
    "section": "",
    "text": "Variables\nOperators\nData in R\nRMarkdown",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variables-assignment-1",
    "href": "content/lectures/01-intro-to-r.html#variables-assignment-1",
    "title": "01-intro-to-r",
    "section": "Variables & Assignment",
    "text": "Variables & Assignment\nVariables are how we store information so that we can access it later.\n. . .\nVariables are created and stored using the assignment operator &lt;-\n\nfirst_variable &lt;- 3\n\nThe above stores the value 3 in the variable first_variable\n. . .\nNote: Other programming languages use = for assignment. R also uses that for assignment, but it is more typical to see &lt;- in R code, so we’ll stick with that.\n. . .\nThis means that if we ever want to reference the information stored in that variable later, we can “call” (mean, type in our code) the variable’s name:\n\nfirst_variable\n\n[1] 3",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-type",
    "href": "content/lectures/01-intro-to-r.html#variable-type",
    "title": "01-intro-to-r",
    "section": "Variable Type",
    "text": "Variable Type\n\nEvery variable you create in R will be of a specific type.\n\n. . .\n\nThe type of the variable is determined dynamically on assignment.\n\n. . .\n\nDetermining the type of a variable with class():\n\n\nclass(first_variable)\n\n[1] \"numeric\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#basic-variable-types",
    "href": "content/lectures/01-intro-to-r.html#basic-variable-types",
    "title": "01-intro-to-r",
    "section": "Basic Variable Types",
    "text": "Basic Variable Types\n\n\n\n\n\n\n\n\nVariable Type\nExplanation\nExample\n\n\n\n\ncharacter\nstores a string\n\"cogs137\", \"hi!\"\n\n\nnumeric\nstores whole numbers and decimals\n9, 9.29\n\n\ninteger\nspecifies integer\n9L (the L specifies this is an integer)\n\n\nlogical\nBooleans\nTRUE, FALSE\n\n\nlist\nstore multiple elements\nlist(7, \"a\", TRUE)\n\n\n\nNote: There are many more. We’ll get to some but not all in this course.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#logical-character",
    "href": "content/lectures/01-intro-to-r.html#logical-character",
    "title": "01-intro-to-r",
    "section": "logical & character",
    "text": "logical & character\nlogical - Boolean values TRUE and FALSE\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n. . .\ncharacter - character strings\n\nclass(\"hello\")\n\n[1] \"character\"\n\nclass('students') # equivalent...but we'll use double quotes!\n\n[1] \"character\"\n\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#numeric-double-integer",
    "href": "content/lectures/01-intro-to-r.html#numeric-double-integer",
    "title": "01-intro-to-r",
    "section": "numeric: double & integer",
    "text": "numeric: double & integer\ndouble - floating point numerical values (default numerical type)\n\nclass(1.335)\n\n[1] \"numeric\"\n\nclass(7)\n\n[1] \"numeric\"\n\n\n. . .\ninteger - integer numerical values (indicated with an L)\n\nclass(7L)\n\n[1] \"integer\"\n\n\n. . .",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#lists",
    "href": "content/lectures/01-intro-to-r.html#lists",
    "title": "01-intro-to-r",
    "section": "lists",
    "text": "lists\nSo far, every variable has been an atomic vector, meaning it only stores a single piece of information.\n. . .\nLists are 1d objects that can contain any combination of R objects\n\n\n\nmylist &lt;- list(\"A\", 7L, TRUE, 18.4)\nmylist\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 18.4\n\n\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#your-turn",
    "href": "content/lectures/01-intro-to-r.html#your-turn",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nDefine variables of each of the following types: character, numeric, integer, logical, list\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#functions",
    "href": "content/lectures/01-intro-to-r.html#functions",
    "title": "01-intro-to-r",
    "section": "Functions",
    "text": "Functions\n\nclass() (and View() & median()) were our first functions…but we’ll show a few more.\n\n. . .\n\nFunctions are (most often) verbs, followed by what they will be applied to in parentheses.\n\n. . .\nFunctions are:\n\navailable from base R\navailable from packages you import\ndefined by you\n\n. . .\nWe’ll start by getting comfortable with available functions, but in a few days, you’ll learn how to write your own!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#helpful-functions",
    "href": "content/lectures/01-intro-to-r.html#helpful-functions",
    "title": "01-intro-to-r",
    "section": "Helpful Functions",
    "text": "Helpful Functions\n\n\n\nclass() - determine high-level variable type\n\n\nclass(mylist)\n\n[1] \"list\"\n\n\n\nlength()- determine how long an object is\n\n\n# contains 4 elements\nlength(mylist)\n\n[1] 4\n\n\n\n\nstr() - display the structure of an R object\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#coercion",
    "href": "content/lectures/01-intro-to-r.html#coercion",
    "title": "01-intro-to-r",
    "section": "Coercion",
    "text": "Coercion\nR is a dynamically typed language – it will happily convert between the various types without complaint.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"\n\nc(FALSE, 3L)\n\n[1] 0 3\n\nc(1.2, 3L)\n\n[1] 1.2 3.0",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#missing-values",
    "href": "content/lectures/01-intro-to-r.html#missing-values",
    "title": "01-intro-to-r",
    "section": "Missing Values",
    "text": "Missing Values\nR uses NA to represent missing values in its data structures.\n\nclass(NA)\n\n[1] \"logical\"\n\n\n. . .\n\nOther Special Values\nNaN | Not a number\nInf | Positive infinity\n-Inf | Negative infinity",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#activity",
    "href": "content/lectures/01-intro-to-r.html#activity",
    "title": "01-intro-to-r",
    "section": "Activity",
    "text": "Activity\nWhat is the type of the following vectors? Chat about why they have that type.\n\nc(1, NA+1L, \"C\")\nc(1L / 0, NA)\nc(1:3, 5)\nc(3L, NaN+1L)\nc(NA, TRUE)\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#operators-1",
    "href": "content/lectures/01-intro-to-r.html#operators-1",
    "title": "01-intro-to-r",
    "section": "Operators",
    "text": "Operators\nAt its simplest, R is a calculator. To carry out mathematical operations, R uses operators.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#arithmetic-operators",
    "href": "content/lectures/01-intro-to-r.html#arithmetic-operators",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\n\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\nx %% y\nmodulus (x mod y) 9%%2 is 1\n\n\nx %/% y\ninteger division 9%/%2 is 4",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#arithmetic-operators-examples",
    "href": "content/lectures/01-intro-to-r.html#arithmetic-operators-examples",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators: Examples",
    "text": "Arithmetic Operators: Examples\n\n7 + 6  \n\n[1] 13\n\n2 - 3\n\n[1] -1\n\n4 * 2\n\n[1] 8\n\n9 / 2\n\n[1] 4.5",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#reminder",
    "href": "content/lectures/01-intro-to-r.html#reminder",
    "title": "01-intro-to-r",
    "section": "Reminder",
    "text": "Reminder\nOutput can be stored to a variable\n\nmy_addition &lt;- 7 + 6\n\n. . .\n\nmy_addition\n\n[1] 13",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#comparison-operators",
    "href": "content/lectures/01-intro-to-r.html#comparison-operators",
    "title": "01-intro-to-r",
    "section": "Comparison Operators",
    "text": "Comparison Operators\nThese operators return a Boolean.\n\n\n\nOperator\nDescription\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#comparison-operators-examples",
    "href": "content/lectures/01-intro-to-r.html#comparison-operators-examples",
    "title": "01-intro-to-r",
    "section": "Comparison Operators: Examples",
    "text": "Comparison Operators: Examples\n\n4 &lt; 12\n\n[1] TRUE\n\n4 &gt;= 3\n\n[1] TRUE\n\n6 == 6\n\n[1] TRUE\n\n7 != 6\n\n[1] TRUE",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#your-turn-1",
    "href": "content/lectures/01-intro-to-r.html#your-turn-1",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nUse arithmetic and comparison operators to store the value 30 in the variable var_30 and TRUE in the variable true_var.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#packages",
    "href": "content/lectures/01-intro-to-r.html#packages",
    "title": "01-intro-to-r",
    "section": "Packages",
    "text": "Packages\n\nPackages are installed with the install.packages function and loaded with the library function, once per session:\n\n\ninstall.packages(\"package_name\")\nlibrary(package_name)\n\n. . .\nIn this course, most packages we’ll use have been installed for you already on datahub, so you will only have to load the package in (using library).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-sets-in-r",
    "href": "content/lectures/01-intro-to-r.html#data-sets-in-r",
    "title": "01-intro-to-r",
    "section": "Data “sets” in R",
    "text": "Data “sets” in R\n\n“set” is in quotation marks because it is not a formal data class\nA tidy data “set” can be one of the following types:\n\ntibble\ndata.frame\n\nWe’ll often work with tibbles:\n\nreadr package (e.g. read_csv function) loads data as a tibble by default\ntibbles are part of the tidyverse, so they work well with other packages we are using\nthey make minimal assumptions about your data, so are less likely to cause hard to track bugs in your code",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-frames",
    "href": "content/lectures/01-intro-to-r.html#data-frames",
    "title": "01-intro-to-r",
    "section": "Data frames",
    "text": "Data frames\n\nA data frame is the most commonly used data structure in R, they are list of equal length vectors (usually atomic, but can be generic). Each vector is treated as a column and elements of the vectors as rows.\nA tibble is a type of data frame that … makes your life (i.e. data analysis) easier.\nMost often a data frame will be constructed by reading in from a file, but we can create them from scratch.\n\n\ndf &lt;- tibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\nclass(df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(df)\n\nRows: 3\nColumns: 2\n$ x &lt;int&gt; 1, 2, 3\n$ y &lt;chr&gt; \"a\", \"b\", \"c\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-frames-cont.",
    "href": "content/lectures/01-intro-to-r.html#data-frames-cont.",
    "title": "01-intro-to-r",
    "section": "Data frames (cont.)",
    "text": "Data frames (cont.)\n\nattributes(df)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n$names\n[1] \"x\" \"y\"\n\n\n. . .\nColumns (variables) in data frames are accessed with $:\n\ndataframe$var_name\n\n. . .\n\nclass(df$x)  # access variable type for column\n\n[1] \"integer\"\n\nclass(df$y)  \n\n[1] \"character\"",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-types",
    "href": "content/lectures/01-intro-to-r.html#variable-types",
    "title": "01-intro-to-r",
    "section": "Variable Types",
    "text": "Variable Types\nData stored in columns can include different kinds of information…which would require a different type (class) of variable to be used in R.\n\n\n\n\nR Data Types:\n\nContinuous: numeric, integer\nDiscrete: factors (we haven’t talked about these yet, but will today!)\n\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-types-cont.",
    "href": "content/lectures/01-intro-to-r.html#variable-types-cont.",
    "title": "01-intro-to-r",
    "section": "Variable Types (cont.)",
    "text": "Variable Types (cont.)\nSometimes data are non-numeric and store words. Even when that is the case, the data can be conveying different information.\n\n\n\n\nR Data Types:\n\nNominal: character\nOrdinal: factors\nBinary: logical OR numeric OR factors 😱\n\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#example-cat-lovers",
    "href": "content/lectures/01-intro-to-r.html#example-cat-lovers",
    "title": "01-intro-to-r",
    "section": "Example: Cat lovers",
    "text": "Example: Cat lovers\nA survey asked respondents their name and number of cats. The instructions said to enter the number of cats as a numerical value.\n. . .\n🚨 There is code ahead that we’re not going to discuss in detail today, but we will in coming lectures.\n\ncat_lovers &lt;- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#the-data",
    "href": "content/lectures/01-intro-to-r.html#the-data",
    "title": "01-intro-to-r",
    "section": "The Data",
    "text": "The Data\n\ncat_lovers |&gt;\n  datatable()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#the-question",
    "href": "content/lectures/01-intro-to-r.html#the-question",
    "title": "01-intro-to-r",
    "section": "The Question",
    "text": "The Question\nHow many respondents have a below average number of cats?\n. . .\nGiving it a first shot…\n\ncat_lovers |&gt;\n  summarise(mean = mean(number_of_cats))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean = mean(number_of_cats)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1    NA\n\n\n. . .\n💡 maybe there is missing data in the number_of_cats column!\nOh why will you still not work??!!\n\ncat_lovers |&gt;\n  summarise(mean_cats = mean(number_of_cats, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean_cats = mean(number_of_cats, na.rm = TRUE)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n  mean_cats\n      &lt;dbl&gt;\n1        NA\n\n\n. . .\n💡What is the type of the number_of_cats variable?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#take-a-breath-and-look-at-your-data",
    "href": "content/lectures/01-intro-to-r.html#take-a-breath-and-look-at-your-data",
    "title": "01-intro-to-r",
    "section": "Take a breath and look at your data",
    "text": "Take a breath and look at your data\n. . .\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           &lt;chr&gt; \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats &lt;chr&gt; \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     &lt;chr&gt; \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#lets-take-another-look",
    "href": "content/lectures/01-intro-to-r.html#lets-take-another-look",
    "title": "01-intro-to-r",
    "section": "Let’s take another look",
    "text": "Let’s take another look",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#sometimes-you-need-to-babysit-your-respondents",
    "href": "content/lectures/01-intro-to-r.html#sometimes-you-need-to-babysit-your-respondents",
    "title": "01-intro-to-r",
    "section": "Sometimes you need to babysit your respondents",
    "text": "Sometimes you need to babysit your respondents\n\ncat_lovers |&gt;\n  mutate(number_of_cats = case_when(\n    name == \"Ginger Clark\" ~ 2,\n    name == \"Doug Bass\"    ~ 3,\n    TRUE                   ~ as.numeric(number_of_cats))) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `number_of_cats = case_when(...)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 60 × 3\n   name           number_of_cats handedness\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;     \n 1 Bernice Warren              0 left      \n 2 Woodrow Stone               0 left      \n 3 Willie Bass                 1 left      \n 4 Tyrone Estrada              3 left      \n 5 Alex Daniels                3 left      \n 6 Jane Bates                  2 left      \n 7 Latoya Simpson              1 left      \n 8 Darin Woods                 1 left      \n 9 Agnes Cobb                  0 left      \n10 Tabitha Grant               0 left      \n# ℹ 50 more rows",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#always-respect-check-data-types",
    "href": "content/lectures/01-intro-to-r.html#always-respect-check-data-types",
    "title": "01-intro-to-r",
    "section": "Always respect (& check!) data types",
    "text": "Always respect (& check!) data types\n\ncat_lovers |&gt;\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats)) |&gt;\n  summarise(mean_cats = mean(number_of_cats))\n\n# A tibble: 1 × 1\n  mean_cats\n      &lt;dbl&gt;\n1     0.817",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#now-that-we-know-what-were-doing",
    "href": "content/lectures/01-intro-to-r.html#now-that-we-know-what-were-doing",
    "title": "01-intro-to-r",
    "section": "Now that we know what we’re doing…",
    "text": "Now that we know what we’re doing…\n\ncat_lovers &lt;- cat_lovers |&gt;\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats))\n\n… store your data in a variable (here we’re overwriting the old cat_lovers tibble).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#moral-of-the-story",
    "href": "content/lectures/01-intro-to-r.html#moral-of-the-story",
    "title": "01-intro-to-r",
    "section": "Moral of the story",
    "text": "Moral of the story\n\nIf your data does not behave how you expect it to, type coercion upon reading in the data might be the reason.\nGo in and investigate your data, apply the fix, save your data, live happily ever after.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#r-markdown-tour",
    "href": "content/lectures/01-intro-to-r.html#r-markdown-tour",
    "title": "01-intro-to-r",
    "section": "R Markdown: tour",
    "text": "R Markdown: tour\n\n[DEMO]\n\nBefore we move on…\n   What is the Bechdel test?\n. . .\nThe Bechdel test asks whether a work of fiction features at least two women who talk to each other about something other than a man, and there must be two women named characters.\n. . .\nConcepts introduced:\n\nKnitting documents\nR Markdown and (some) R syntax",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#github-setup",
    "href": "content/lectures/01-intro-to-r.html#github-setup",
    "title": "01-intro-to-r",
    "section": "GitHub Setup",
    "text": "GitHub Setup\nSee this week’s lab…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#giving-the-demo-a-go",
    "href": "content/lectures/01-intro-to-r.html#giving-the-demo-a-go",
    "title": "01-intro-to-r",
    "section": "Giving the demo a go…",
    "text": "Giving the demo a go…\n\nNavigate to the demo URL (on Canvas)\nAccept the “assignment” (this is NOT graded)\nClone the repo\nEdit the document\nKnit the document\nPush your changes\n\nTry to play around with this after finishing your lab tomorrow!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#recap",
    "href": "content/lectures/01-intro-to-r.html#recap",
    "title": "01-intro-to-r",
    "section": "Recap",
    "text": "Recap\n\nAlways best to think of data as part of a tibble\n\nThis plays nicely with the tidyverse as well\nRows are observations, columns are variables\n\nWhat are the common variable types in R\n\nHow do I create a variable of each type?\nWhen would I use each one?\n\nDo I know how to determine the class/type of a variable?\nCan I explain dynamic typing?\nCan I operate on variables and values using…\n\narithmetic operators?\ncomparison operators?\n\nWhat are dataframes/tibbles? and why are they useful?\nWhat is the difference between installing and loading a package?\nWhat are the components of an R Markdown file?",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "01-intro-to-r"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#qa",
    "href": "content/lectures/04-ggplot2-slides.html#qa",
    "title": "04-ggplot2",
    "section": "Q&A",
    "text": "Q&A\n\nQ: What is the difference between pull and select?\nA: select specifies which columns to display in your resulting dataframe. pull extracts the values from a column and stores them in a vector (not a dataframe)\n\n\nQ: I am a bit confused on factors and what levels mean.\nA: Factors store categorical information. The levels of a factor are all the possible unique values in a variable.\n\n\nQ: how similar is R to numpy/which scenarios are each used in the industry?\nA: Basically, anything data science-y you can do in R, you can also do in python. R has linear algebra/working with matrices built directly into its base installation, so no additional package would be need for numpy-like operations. And, dplyr does very similar things to pandas, but with a more readable and consistent syntax overall.\n\n\nQ: would we ever load just dplyr instead of the entire tidyverse package? is there a big difference?\nA: We’ll always just load tidyverse. The difference is that the tidyverse is quite big, so if you ever wanted to just use dplyr functions, you could load just that. This matters more in development where you’re trying to minimize external dependencies and make code run as fast as possible. For our purposes, there’s no real need to only load dplyr\n\n\nQ: I found the demos to be the most confusing part, because it’s very different understanding slides and applying that to actual coding. Personally, I would prefer if the lecture content were put into recordings, or just uploaded earlier so we could learn it on our own, and then have classes be more focused on data science best practices, applications, etc.\nA: I do really like this idea and would love to run this like this in the. I’m curious what y’all think of this and will add a question like this to the post-course survey to get students’ thoughts.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#course-announcements",
    "href": "content/lectures/04-ggplot2-slides.html#course-announcements",
    "title": "04-ggplot2",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 02 due Friday (11:59 PM)\nHW 01 due Monday (11:59 PM)\nLecture Participation survey “due” after class",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#suggested-reading",
    "href": "content/lectures/04-ggplot2-slides.html#suggested-reading",
    "title": "04-ggplot2",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#ggplot2-in-tidyverse",
    "href": "content/lectures/04-ggplot2-slides.html#ggplot2-in-tidyverse",
    "title": "04-ggplot2",
    "section": "ggplot2 \\(\\in\\) tidyverse",
    "text": "ggplot2 \\(\\in\\) tidyverse\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#data-palmer-penguins",
    "href": "content/lectures/04-ggplot2-slides.html#data-palmer-penguins",
    "title": "04-ggplot2",
    "section": "Data: Palmer Penguins",
    "text": "Data: Palmer Penguins\nMeasurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#the-data",
    "href": "content/lectures/04-ggplot2-slides.html#the-data",
    "title": "04-ggplot2",
    "section": "The Data",
    "text": "The Data\n\npenguins |&gt;\n  datatable()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#a-plot",
    "href": "content/lectures/04-ggplot2-slides.html#a-plot",
    "title": "04-ggplot2",
    "section": "A Plot",
    "text": "A Plot\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section",
    "href": "content/lectures/04-ggplot2-slides.html#section",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame\n\n\n\nggplot(data = penguins)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-1",
    "href": "content/lectures/04-ggplot2-slides.html#section-1",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-2",
    "href": "content/lectures/04-ggplot2-slides.html#section-2",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm))",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-3",
    "href": "content/lectures/04-ggplot2-slides.html#section-3",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) + \n  geom_point()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-4",
    "href": "content/lectures/04-ggplot2-slides.html#section-4",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) + \n  geom_point()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-5",
    "href": "content/lectures/04-ggplot2-slides.html#section-5",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-6",
    "href": "content/lectures/04-ggplot2-slides.html#section-6",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-7",
    "href": "content/lectures/04-ggplot2-slides.html#section-7",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-8",
    "href": "content/lectures/04-ggplot2-slides.html#section-8",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-9",
    "href": "content/lectures/04-ggplot2-slides.html#section-9",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\")",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-10",
    "href": "content/lectures/04-ggplot2-slides.html#section-10",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#coding-out-loud-1",
    "href": "content/lectures/04-ggplot2-slides.html#coding-out-loud-1",
    "title": "04-ggplot2",
    "section": "Coding out loud",
    "text": "Coding out loud\n\nCodePlotNarrative\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\nRepresent each observation with a point and map species to the color of each point.\nTitle the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\nFinally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#argument-names",
    "href": "content/lectures/04-ggplot2-slides.html#argument-names",
    "title": "04-ggplot2",
    "section": "Argument names",
    "text": "Argument names\n\n\n\n\n\n\nTip\n\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm,  \n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\nggplot(penguins, \n       aes(x = bill_depth_mm, \n           y = bill_depth_mm,\n           color = species)) +\n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a basic plot in ggplot2 using different variables than those in the last example (last example: bill_depth_mm & bill_depth_mm).\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#aesthetics-options",
    "href": "content/lectures/04-ggplot2-slides.html#aesthetics-options",
    "title": "04-ggplot2",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#color",
    "href": "content/lectures/04-ggplot2-slides.html#color",
    "title": "04-ggplot2",
    "section": "Color",
    "text": "Color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#shape",
    "href": "content/lectures/04-ggplot2-slides.html#shape",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = island)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#shape-1",
    "href": "content/lectures/04-ggplot2-slides.html#shape-1",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#size",
    "href": "content/lectures/04-ggplot2-slides.html#size",
    "title": "04-ggplot2",
    "section": "Size",
    "text": "Size\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#alpha",
    "href": "content/lectures/04-ggplot2-slides.html#alpha",
    "title": "04-ggplot2",
    "section": "Alpha",
    "text": "Alpha\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g,\n           alpha = flipper_length_mm)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting",
    "href": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting",
    "title": "04-ggplot2",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nMapping: Determine the size, alpha, etc. of points based on the values of a variable in the data\n\ngoes into aes()\n\nSetting: Determine the size, alpha, etc. of points not based on the values of a variable in the data\n\ngoes into geom_*() (this was geom_point() in the previous example, but we’ll learn about other geoms soon!)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting-example",
    "href": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting-example",
    "title": "04-ggplot2",
    "section": "Mapping vs. Setting (example)",
    "text": "Mapping vs. Setting (example)\n\n\nMapping\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm,\n           size = body_mass_g, \n           alpha = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nSetting\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm)) + \n  geom_point(size = 2, alpha = 0.5)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn-1",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn-1",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nEdit the basic plot you created earlier to change something about its aesthetics.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#faceting-1",
    "href": "content/lectures/04-ggplot2-slides.html#faceting-1",
    "title": "04-ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\nSmaller plots that display different subsets of the data\nUseful for exploring conditional relationships and large data\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ island) \n\nWarning: Removed 2 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#various-ways-to-facet",
    "href": "content/lectures/04-ggplot2-slides.html#various-ways-to-facet",
    "title": "04-ggplot2",
    "section": "Various ways to facet",
    "text": "Various ways to facet\n🧠 In the next few slides describe what each plot displays. Think about how the code relates to the output.\n\n\n\n\n\n\nWarning\n\n\nThe plots in the next few slides do not have proper titles, axis labels, etc. because we want you to figure out what’s happening in the plots. But you should always label your plots!",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-11",
    "href": "content/lectures/04-ggplot2-slides.html#section-11",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ sex)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-12",
    "href": "content/lectures/04-ggplot2-slides.html#section-12",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(sex ~ species)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-13",
    "href": "content/lectures/04-ggplot2-slides.html#section-13",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-14",
    "href": "content/lectures/04-ggplot2-slides.html#section-14",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(. ~ species)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-15",
    "href": "content/lectures/04-ggplot2-slides.html#section-15",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species, ncol = 2)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#faceting-summary",
    "href": "content/lectures/04-ggplot2-slides.html#faceting-summary",
    "title": "04-ggplot2",
    "section": "Faceting summary",
    "text": "Faceting summary\n\nfacet_grid():\n\n2d grid\nrows ~ cols\nuse . for no split\n\nfacet_wrap(): 1d ribbon wrapped according to number of rows and columns specified or available plotting area",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#facet-and-color",
    "href": "content/lectures/04-ggplot2-slides.html#facet-and-color",
    "title": "04-ggplot2",
    "section": "Facet and color",
    "text": "Facet and color\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) + \n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#face-and-color-no-legend",
    "href": "content/lectures/04-ggplot2-slides.html#face-and-color-no-legend",
    "title": "04-ggplot2",
    "section": "Face and color, no legend",
    "text": "Face and color, no legend\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#common-geoms",
    "href": "content/lectures/04-ggplot2-slides.html#common-geoms",
    "title": "04-ggplot2",
    "section": "Common geoms",
    "text": "Common geoms\n\n\n\ngeom 1\nDescription 2\n\n\n\n\ngeom_point\nscatterplot\n\n\ngeom_bar\nbarplot\n\n\ngeom_line\nline plot\n\n\ngeom_density\ndensityplot\n\n\ngeom_histogram\nhistogram\n\n\ngeom_boxplot\nboxplot\n\n\n\nggplot2 geoms listed hereWhen each visualization is appropriate here",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn-2",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn-2",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a plot in ggplot2 using a different geom than what you did previously. Customize as much as you can before time is “up.”\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Slides",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html",
    "href": "content/lectures/04-ggplot2.html",
    "title": "04-ggplot2",
    "section": "",
    "text": "Q: What is the difference between pull and select?\nA: select specifies which columns to display in your resulting dataframe. pull extracts the values from a column and stores them in a vector (not a dataframe)\n\n\nQ: I am a bit confused on factors and what levels mean.\nA: Factors store categorical information. The levels of a factor are all the possible unique values in a variable.\n\n\nQ: how similar is R to numpy/which scenarios are each used in the industry?\nA: Basically, anything data science-y you can do in R, you can also do in python. R has linear algebra/working with matrices built directly into its base installation, so no additional package would be need for numpy-like operations. And, dplyr does very similar things to pandas, but with a more readable and consistent syntax overall.\n\n\nQ: would we ever load just dplyr instead of the entire tidyverse package? is there a big difference?\nA: We’ll always just load tidyverse. The difference is that the tidyverse is quite big, so if you ever wanted to just use dplyr functions, you could load just that. This matters more in development where you’re trying to minimize external dependencies and make code run as fast as possible. For our purposes, there’s no real need to only load dplyr\n\n\nQ: I found the demos to be the most confusing part, because it’s very different understanding slides and applying that to actual coding. Personally, I would prefer if the lecture content were put into recordings, or just uploaded earlier so we could learn it on our own, and then have classes be more focused on data science best practices, applications, etc.\nA: I do really like this idea and would love to run this like this in the. I’m curious what y’all think of this and will add a question like this to the post-course survey to get students’ thoughts.\n\n\n\n\nDue Dates:\n\nLab 02 due Friday (11:59 PM)\nHW 01 due Monday (11:59 PM)\nLecture Participation survey “due” after class\n\n\n\n\n\nR4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options\n\n\n\n\n\n\nMeasurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst \n\n\n\n\npenguins |&gt;\n  datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#qa",
    "href": "content/lectures/04-ggplot2.html#qa",
    "title": "04-ggplot2",
    "section": "",
    "text": "Q: What is the difference between pull and select?\nA: select specifies which columns to display in your resulting dataframe. pull extracts the values from a column and stores them in a vector (not a dataframe)\n\n\nQ: I am a bit confused on factors and what levels mean.\nA: Factors store categorical information. The levels of a factor are all the possible unique values in a variable.\n\n\nQ: how similar is R to numpy/which scenarios are each used in the industry?\nA: Basically, anything data science-y you can do in R, you can also do in python. R has linear algebra/working with matrices built directly into its base installation, so no additional package would be need for numpy-like operations. And, dplyr does very similar things to pandas, but with a more readable and consistent syntax overall.\n\n\nQ: would we ever load just dplyr instead of the entire tidyverse package? is there a big difference?\nA: We’ll always just load tidyverse. The difference is that the tidyverse is quite big, so if you ever wanted to just use dplyr functions, you could load just that. This matters more in development where you’re trying to minimize external dependencies and make code run as fast as possible. For our purposes, there’s no real need to only load dplyr\n\n\nQ: I found the demos to be the most confusing part, because it’s very different understanding slides and applying that to actual coding. Personally, I would prefer if the lecture content were put into recordings, or just uploaded earlier so we could learn it on our own, and then have classes be more focused on data science best practices, applications, etc.\nA: I do really like this idea and would love to run this like this in the. I’m curious what y’all think of this and will add a question like this to the post-course survey to get students’ thoughts.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#course-announcements",
    "href": "content/lectures/04-ggplot2.html#course-announcements",
    "title": "04-ggplot2",
    "section": "",
    "text": "Due Dates:\n\nLab 02 due Friday (11:59 PM)\nHW 01 due Monday (11:59 PM)\nLecture Participation survey “due” after class",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#suggested-reading",
    "href": "content/lectures/04-ggplot2.html#suggested-reading",
    "title": "04-ggplot2",
    "section": "",
    "text": "R4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#ggplot2-in-tidyverse",
    "href": "content/lectures/04-ggplot2.html#ggplot2-in-tidyverse",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#data-palmer-penguins",
    "href": "content/lectures/04-ggplot2.html#data-palmer-penguins",
    "title": "04-ggplot2",
    "section": "",
    "text": "Measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#the-data",
    "href": "content/lectures/04-ggplot2.html#the-data",
    "title": "04-ggplot2",
    "section": "",
    "text": "penguins |&gt;\n  datatable()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#a-plot",
    "href": "content/lectures/04-ggplot2.html#a-plot",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section",
    "href": "content/lectures/04-ggplot2.html#section",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame\n\n\nggplot(data = penguins)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-1",
    "href": "content/lectures/04-ggplot2.html#section-1",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-2",
    "href": "content/lectures/04-ggplot2.html#section-2",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm))",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-3",
    "href": "content/lectures/04-ggplot2.html#section-3",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) + \n  geom_point()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-4",
    "href": "content/lectures/04-ggplot2.html#section-4",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) + \n  geom_point()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-5",
    "href": "content/lectures/04-ggplot2.html#section-5",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-6",
    "href": "content/lectures/04-ggplot2.html#section-6",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-7",
    "href": "content/lectures/04-ggplot2.html#section-7",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-8",
    "href": "content/lectures/04-ggplot2.html#section-8",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-9",
    "href": "content/lectures/04-ggplot2.html#section-9",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\")",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-10",
    "href": "content/lectures/04-ggplot2.html#section-10",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "href": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "title": "04-ggplot2",
    "section": "Coding out loud",
    "text": "Coding out loud\n\nCodePlotNarrative\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\nRepresent each observation with a point and map species to the color of each point.\nTitle the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\nFinally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#argument-names",
    "href": "content/lectures/04-ggplot2.html#argument-names",
    "title": "04-ggplot2",
    "section": "Argument names",
    "text": "Argument names\n\n\n\n\n\n\nTip\n\n\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm,  \n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\nggplot(penguins, \n       aes(x = bill_depth_mm, \n           y = bill_depth_mm,\n           color = species)) +\n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn",
    "href": "content/lectures/04-ggplot2.html#your-turn",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a basic plot in ggplot2 using different variables than those in the last example (last example: bill_depth_mm & bill_depth_mm).\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#aesthetics-options",
    "href": "content/lectures/04-ggplot2.html#aesthetics-options",
    "title": "04-ggplot2",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#color",
    "href": "content/lectures/04-ggplot2.html#color",
    "title": "04-ggplot2",
    "section": "Color",
    "text": "Color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape",
    "href": "content/lectures/04-ggplot2.html#shape",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = island)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape-1",
    "href": "content/lectures/04-ggplot2.html#shape-1",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#size",
    "href": "content/lectures/04-ggplot2.html#size",
    "title": "04-ggplot2",
    "section": "Size",
    "text": "Size\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#alpha",
    "href": "content/lectures/04-ggplot2.html#alpha",
    "title": "04-ggplot2",
    "section": "Alpha",
    "text": "Alpha\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g,\n           alpha = flipper_length_mm)) + \n  geom_point() +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "title": "04-ggplot2",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nMapping: Determine the size, alpha, etc. of points based on the values of a variable in the data\n\ngoes into aes()\n\nSetting: Determine the size, alpha, etc. of points not based on the values of a variable in the data\n\ngoes into geom_*() (this was geom_point() in the previous example, but we’ll learn about other geoms soon!)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "title": "04-ggplot2",
    "section": "Mapping vs. Setting (example)",
    "text": "Mapping vs. Setting (example)\n\n\nMapping\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm,\n           size = body_mass_g, \n           alpha = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nSetting\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm)) + \n  geom_point(size = 2, alpha = 0.5)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-1",
    "href": "content/lectures/04-ggplot2.html#your-turn-1",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nEdit the basic plot you created earlier to change something about its aesthetics.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-1",
    "href": "content/lectures/04-ggplot2.html#faceting-1",
    "title": "04-ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\nSmaller plots that display different subsets of the data\nUseful for exploring conditional relationships and large data\n\n. . .\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ island) \n\nWarning: Removed 2 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "href": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "title": "04-ggplot2",
    "section": "Various ways to facet",
    "text": "Various ways to facet\n🧠 In the next few slides describe what each plot displays. Think about how the code relates to the output.\n\n\n\n\n\n\nWarning\n\n\n\nThe plots in the next few slides do not have proper titles, axis labels, etc. because we want you to figure out what’s happening in the plots. But you should always label your plots!",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-11",
    "href": "content/lectures/04-ggplot2.html#section-11",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ sex)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-12",
    "href": "content/lectures/04-ggplot2.html#section-12",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(sex ~ species)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-13",
    "href": "content/lectures/04-ggplot2.html#section-13",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-14",
    "href": "content/lectures/04-ggplot2.html#section-14",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(. ~ species)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-15",
    "href": "content/lectures/04-ggplot2.html#section-15",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species, ncol = 2)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-summary",
    "href": "content/lectures/04-ggplot2.html#faceting-summary",
    "title": "04-ggplot2",
    "section": "Faceting summary",
    "text": "Faceting summary\n\nfacet_grid():\n\n2d grid\nrows ~ cols\nuse . for no split\n\nfacet_wrap(): 1d ribbon wrapped according to number of rows and columns specified or available plotting area",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#facet-and-color",
    "href": "content/lectures/04-ggplot2.html#facet-and-color",
    "title": "04-ggplot2",
    "section": "Facet and color",
    "text": "Facet and color\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) + \n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d()",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "href": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "title": "04-ggplot2",
    "section": "Face and color, no legend",
    "text": "Face and color, no legend\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#common-geoms",
    "href": "content/lectures/04-ggplot2.html#common-geoms",
    "title": "04-ggplot2",
    "section": "Common geoms",
    "text": "Common geoms\n\n\n\ngeom 1\nDescription 2\n\n\n\n\ngeom_point\nscatterplot\n\n\ngeom_bar\nbarplot\n\n\ngeom_line\nline plot\n\n\ngeom_density\ndensityplot\n\n\ngeom_histogram\nhistogram\n\n\ngeom_boxplot\nboxplot",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-2",
    "href": "content/lectures/04-ggplot2.html#your-turn-2",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a plot in ggplot2 using a different geom than what you did previously. Customize as much as you can before time is “up.”\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#footnotes",
    "href": "content/lectures/04-ggplot2.html#footnotes",
    "title": "04-ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nggplot2 geoms listed here↩︎\nWhen each visualization is appropriate here↩︎",
    "crumbs": [
      "Home",
      "Lecture Notes",
      "04-ggplot2"
    ]
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "COGS 137",
    "section": "",
    "text": "Class\nClass will include short lectures as well as interactive activities. The goal of lecture is to introduce the topics and information needed for the course. The goal of your time outside of lecture is to practice with topics that are introduced and deepen your understanding of material presented in class. Since so much of programming and statistical analysis is learned best by doing, we’ll prioritize that throughout the course, both in and outside of the classroom.\n\n\nDiversity & Inclusion\nMy goal is that every student, regardless of their background or perspective, will be well-served by this course. My philosophy is that the diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding. I intend to present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture); however, if I ever fall short or if you ever have suggestions for improvement, please do share with me! This feedback is always welcomed, and I am always in the process of learning and improving to this end. If you would like to provide that feedback anonymously, please use the anonymous Google Form.*\n\nWhat should you call me?\nMost students call me Professor/Prof Ellis, and that’s great! This is how I typically sign emails to students. I’m also totally OK with you addressing me as Shannon or Dr. Ellis.\n\n\nWhat I should call you?\nI should call you by your preferred name, with the correct pronunciation. Please correct me (in the moment or via email/Piazza after the fact…however you’re most comfortable) if I ever make a mistake.\n\n\n\nDisability Access\nStudents requesting accommodations due to a disability should provide a current Authorization for Accommodation (AFA) letter. These letters are issued by the Office for Students with Disabilities (OSD), which is located in University Center 202 behind Center Hall. If you are struggling to get necessary accommodations or want to further discuss your accommodations, please feel free to reach out to Professor Ellis directly.\nContacting the OSD can help you further:\n858.534.4382 (phone)\nosd@ucsd.edu (email)\nhttp://disabilities.ucsd.edu\n\n\nHow to get help\nIt’s great that we have so many ways to communicate, but it can get tricky to figure out who to contact or where your question belongs or when to expect a response. When in doubt, Piazza is great!\nMore specifically, if you have:\n\nquestions about course content - these are awesome! We want everyone to see them and have their questions answered too, so either ask them in class (which is recorded) and/or post these to Piazza!\na technical assignment question - come to office hours (or post to Piazza). Answering technical questions is often best accomplished ‘in person’ where we can discuss the question and talk through ideas. However, if that is not possible, post your question to Piazza. Be as specific as you can in the question you ask. And, for those answering, help your classmates as much as you can without just giving the answer. Help guide them, point them in a direction, provide pseudo code, but do not provide code that answers assignment questions.\nquestions about course logistics - first, check the course website. If you can’t find the answer there, first ask a classmate. If still unsure, post on Piazza.\nquestions about a grade - Post on Piazza with “regrades” tag in a private post to “Instructors & TAs”.\nsomething super cool to share related to class or want to talk about a topic in further depth - feel free to email Professor Ellis (sellis@ucsd.edu) or come to office hours. Please include COGS137 in the email subject line.\nsome feedback about the course you want to share anonymously - If you’ve been offended by an example in class, really liked or disliked a lesson, or wish there were something covered in class that wasn’t but would rather not share this publicly, etc., please fill out the anonymous Google Form*\n\n*This form can be taken down at any time if it’s not being used for its intended purpose; however, you all will be notified should that happen.\n\n\nAcademic integrity\nDon’t cheat.\nYou are generally encouraged to work together and help one another in this course. However, you are personally responsible for the work you submit. A helpful heuristic can be to ask yourself “Can I explain each piece of code and each analysis carried out in what I’m submitting? Could I reproduce this code/analysis on my own?”; you should be able to answer “Yes” to both questions for everything you submit in this course. For labs and assignments, you are allowed and encouraged to work together, but it is your responsibility to ensure you understand everything you’ve submitted. (For the midterm, all work has to be completed individually and communication with other humans about the exam is not allowed; this will be discussed more explicitly beforehand.)\nA note on sharing / reusing code: The Internet is an excellent resource; there will be many times you find helpful information online. You should use available resources (e.g. ChatGPT, Copilot, StackOverflow, etc.), but you must explicitly cite any code you use directly or any code you use as inspiration. This can be done by including the URL/reference to the source directly in your code (as a code comment) or in accompanying text for a given assignment/exam/lab. You should never share code directly (e.g. copy + paste; share an send an answer to a classmate), but you can discuss code and work together on everything other than take-home exams.\nPlease review UCSD’s academic integrity policies here.\nCheating and plagiarism have been and will be strongly penalized. If, for whatever reason, Canvas or DataHub is down or something else prohibits you from being able to turn in an assignment on time, immediately contact Professor Ellis by emailing (sellis@ucsd.edu) your assignment as soon as possible to avoid it being graded as late.\n\n\nCourse components\n\nLecture\nLectures will be your introduction to course topics and material. Lectures will be interactive, and you will be given time to practice with the lecture concepts during class. Attendance is not required, but is encouraged if you’re feeling well. To help incentivize coming to class, there will be a daily participation survey that will open at the end of lecture and close shortly after each Tues/Thurs lecture. Each time you fill out the lecture survey, you get a small % of credit toward your final project Completion of all surveys can provide up to 3.5% extra credit on your final project (not your final course grade).\n\nReadings\nReadings will be assigned for some class days and are best completed prior to the day’s lecture. These are meant to provide background and additional context for the upcoming day’s lecture topics. These can also be a good source after class when studying or reviewing topics discussed in class.\n\n\nPodcast\nIn case you miss class or would like to review the material covered in class, you can view the podcasts here.\n\n\n\nLabs (16%)\nLabs are meant to give you deeper understanding and hands-on experience with the technical and statistical topics introduced during lecture in a low-stakes environment. Lab sections will typically comprise of a short review and explanation of the lab and then time for you to complete the assigned weekly lab. Labs are submitted individually, but you are encouraged to work together during lab. You are free to ask and answer each others’ questions and discuss your work. Instructional staff will be present during lab to help further your understanding.\nLabs are graded for concerted effort. This is because when we learn something new, mistakes are going to happen! In fact, we learn a lot from the mistakes we make during the learning process. If your submission reflects ~50 min of work/effort, you will receive full credit for the week’s lab.\nLab attendance is not required, but is definitely encouraged if you are feeling well. While slides used are shared, lab sessions are not recorded, so being present is the best way to fully engage in the course.\n\n\nHomework (24%)\nAfter practice in lecture and labs, homework assignments are meant to demonstrate your solidified understanding of the course material. These are typically 2-4x longer and more involved than labs. Homework assignments are completed and submitted individually and are marked for correctness. You are allowed to work together on homework assignments, but academic integrity must be upheld.\n\n\nMidterm (15%)\nThere will be a single take-home midterm, and you will have at least 48 hours to complete it. This exam is meant to assess your understanding of the R programming language prior to us moving into focusing on case studies and full analyses. The exam will be completed individually and will be open-notes and open-Internet; however, you will not be permitted to ask questions of one another or instructional staff while completing the take-home exam.\n\n\nTeams\nThere will be two case study mini-projects and a final project. Teams will be randomly assigned for the mini-projects but you will choose your final project groups. (By working with teammates throughout the course, you will also be able to use one another as a resource during labs and assignments.)\n\n\nCase Study Mini-projects (25%)\nStarting week 5, we will transition to a project-based course. This will allow us to use case studies to focus on deepening statistical knowledge and carrying out interesting analyses. In this, specific case studies and statistics topics will be discussed in class. In your teams and for each of the case studies, you will: 1) extend the analysis from class and 2) communicate your findings for both a technical and general audience.\n\n\nFinal Project (20%)\nThe final project will be completed in groups. There will be two different general final projects from which your group can choose, but the idea is that whichever you choose, you will be able to tackle it using and building upon the tools and techniques discussed in class. Briefly here, the two options will be: 1. Create a technical presentation on a statistics topic and/or an R package. 2. Carry out a data analysis.\nEach will require a written technical report, a communication to a general audience, and an oral presentation, but the specific requirements will differ between the two.\nFinal Project groups will have to submit a proposal during week 8 (2%). Final projects (18%) will be due on Tues of finals week at 11:59 PM.\n\n\n\nGrading\nYour final grade will be comprised of the following:\n\n\n\nLabs (8)\n16%\n\n\nHomework (3)\n24%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n25%\n\n\nFinal Project* (Proposal + Project)\n20%\n\n\n\n* indicates group submission\n\nFinal Grades\nTo calculate final grades, I use the standard grading scale and do not round grades up (given the numerous extra credit opportunities offered):\n\n\n\n97-100%\nA+\n\n\n93-96%\nA\n\n\n90-92%\nA-\n\n\n87-89%\nB+\n\n\n83-86%\nB\n\n\n80-82%\nB-\n\n\n77-79%\nC+\n\n\n73-76%\nC\n\n\n70-72%\nC-\n\n\n67-69%\nD+\n\n\n63-66%\nD\n\n\n60-62%\nD-\n\n\n&lt;60%\nF\n\n\n\n\n\n\nLate / missed work\nLate homework assignments and case study projects will be accepted up to 3 days (72 hours) after the assigned deadline. Late submissions will receive a 25% deduction.\nThere are no late deadlines for labs, the exam, or the final project.\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter.\n\n\nRegrade requests\nWe will work hard to grade everyone fairly and return assignments quickly. And, we know you also work hard and want you to receive the grade you’ve earned. Occasionally, grading mistakes do happen, and it’s important to us to correct them. If you think there is a mistake in your grade on an assignment, post privately on Piazza to “Instructors” using the “regrades” tag within 72 hours. This post should include evidence of why you think your answer was correct and should point to the specific part of the assignment in question.\n\n\nProfessionalism\nPlease refrain from texting or using your computer for anything other than coursework during class. Not only is this distracting to you, but it can also be distracting to those around you. (Note that there is no consequence associated with this. I know it can be difficult, but I ask that you try your best!)"
  }
]