[
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "content/labs/08-lab-cs02.html",
    "href": "content/labs/08-lab-cs02.html",
    "title": "Lab 08 - Exploring Case Study 02",
    "section": "",
    "text": "This week in class we’ve been discussing data related to vaping among young people in America. You’ve seen the data in class and maybe have started to work with it, but now is your change to really fully get to understand the dataset. While the case study report will be submitted in groups, this will be submitted individually.The idea is that while you are still encouraged to work together during lab, you and your groupmates may come up with separate ideas. This will allow you to have more ideas when you get together and start working on the case study together."
  },
  {
    "objectID": "content/labs/08-lab-cs02.html#part-1-exploratory-data-analysis-eda",
    "href": "content/labs/08-lab-cs02.html#part-1-exploratory-data-analysis-eda",
    "title": "Lab 08 - Exploring Case Study 02",
    "section": "Part 1: Exploratory Data Analysis (EDA)",
    "text": "Part 1: Exploratory Data Analysis (EDA)\nCreate at least two (2) visualizations or tables that help you learn more about these data beyond what was presented in class. (This is intentionally vague. We want you to look at the data and figure out what would be most helpful to visualize from the provided data. These could be different variables than what we looked at in class. Data could be faceted. Something totally different!) These do not have to be fully polished visualizations, but it should be clear from the visualization and accompanying text what’s to be learned from the visualization."
  },
  {
    "objectID": "content/labs/08-lab-cs02.html#part-2-possible-extensions",
    "href": "content/labs/08-lab-cs02.html#part-2-possible-extensions",
    "title": "Lab 08 - Exploring Case Study 02",
    "section": "Part 2: Possible extensions?",
    "text": "Part 2: Possible extensions?\nThink about the data you have access to, the EDA/analysis presented in class, and the questions we said we’re going to address. What possible extensions to this analysis would you be interested in carrying out? This is a space for brainstorming. Include any possible thoughts you have here, even if they aren’t “good” or you aren’t sure if they are “possible.” This can be used as a jumping off point for when you start discussing analysis extensions with your group."
  },
  {
    "objectID": "content/labs/03-lab-viz.html",
    "href": "content/labs/03-lab-viz.html",
    "title": "Lab 03 - Data Visualization",
    "section": "",
    "text": "A note on expectations: For each exercise, include any relevant output (tables, summary statistics, plots) in your answer along with text to guide the reader. Doing this is easy! Just place any relevant R code in a code chunk, any relevant text outside of code chunks, and hit Knit HTML.\nSome define statistics as the field that focuses on turning information into knowledge. The first step in that process is to summarize and describe raw information - the data. In this lab we explore data on college majors and earnings, specifically the data behind the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While outside the scope of this lab, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-lowest-unemployment-rate",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads |>\n  arrange(unemployment_rate)\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\nNote that your output here likely has a whole bunch of decimal places in the unemployment variable? You likely don’t want all those values to be displayed.\nThere are two ways we can address this problem. One would be to round the unemployment_rate variable in the dataset or we can change the number of digits displayed, without touching the input data.\nBelow are instructions for how you would do both of these:\n\nRound unemployment_rate: We create a new variable with the mutate function. In this case, we’re overwriting the existing unemployment_rate variable, by rounding it to 4 decimal places.For example, the call to mutate would be: mutate(unemployment_rate = round(unemployment_rate, digits = 4))\nChange displayed number of digits without touching data: We can add an option to our R Markdown document to change the displayed number of digits in the output. To do so, add a new chunk, and set:\n\n\noptions(digits = 2)\n\nNote that the digits in options is scientific digits, and in round they are decimal places. If you’re thinking “Wouldn’t it be nice if they were consistent?”, you’re right…\nYou don’t need to do both of these; that would be redundant. The next exercise asks you to choose one.\n\nExercise 1\nWhich of these options, changing the input data or altering the number of digits displayed without touching the input data, is the better option? Explain your reasoning. Then, implement the option you chose."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "href": "content/labs/03-lab-viz.html#which-major-has-the-highest-percentage-of-women",
    "title": "Lab 03 - Data Visualization",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\n\nThe desc function specifies that we want unemployment_rate in descending order.\n\ncollege_recent_grads |>\n  arrange(desc(unemployment_rate)) |>\n  select(rank, major, unemployment_rate)\n\n\nExercise 2\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "content/labs/03-lab-viz.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "Lab 03 - Data Visualization",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\n\n\nA percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nExercise 3\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\n\nExercise 4\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\nWe can also calculate summary statistics for this distribution using the summarise function. Note here that you can calculate multiple summary statistics within a single summarise call:\n\ncollege_recent_grads |>\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n\n\nExercise 5\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nNext, we facet the plot by major category.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram() +\n  facet_wrap( ~ major_category, ncol = 4)\n\n\n\nExercise 6\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\n\nExercise 7\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Also note that we are looking for the highest statistic, so make sure if you arrange to do so in the correct direction.\n\n\nExercise 8\nWhich major category is the least popular in this sample?"
  },
  {
    "objectID": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "href": "content/labs/03-lab-viz.html#all-stem-fields-arent-the-same",
    "title": "Lab 03 - Data Visualization",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories <- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads <- college_recent_grads |>\n  mutate(major_type = case_when(major_category %in% stem_categories ~ \"stem\",\n                                TRUE ~ \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx < y\nless than\n\n\nx > y\ngreater than\n\n\nx <= y\nless than or equal to\n\n\nx >= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads |>\n  filter(\n    major_type == \"stem\",\n    median < 36000\n  )\n\n\nExercise 9\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "content/labs/03-lab-viz.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "Lab 03 - Data Visualization",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nExercise 10\nCreate a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html",
    "href": "content/labs/04-lab-modelling.html",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "",
    "text": "Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously. However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor. The article titled, “Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity” (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings.\n\n\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. link.\nFor this assignment you will analyze the data from this study in order to learn what goes into a positive professor evaluation.\nThe data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin. In addition, six students rated the professors’ physical appearance. (This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#codebook",
    "href": "content/labs/04-lab-modelling.html#codebook",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Codebook",
    "text": "Codebook\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\nscore\nAverage professor evaluation score: (1) very unsatisfactory - (5) excellent\n\n\nrank\nRank of professor: teaching, tenure track, tenure\n\n\nethnicity\nEthnicity of professor: not minority, minority\n\n\ngender\nGender of professor: female, male\n\n\nlanguage\nLanguage of school where professor received education: english or non-english\n\n\nage\nAge of professor\n\n\ncls_perc_eval\nPercent of students in class who completed evaluation\n\n\ncls_did_eval\nNumber of students in class who completed evaluation\n\n\ncls_students\nTotal number of students in class\n\n\ncls_level\nClass level: lower, upper\n\n\ncls_profs\nNumber of professors teaching sections in course in sample: single, multiple\n\n\ncls_credits\nNumber of credits of class: one credit (lab, PE, etc.), multi credit\n\n\nbty_f1lower\nBeauty rating of professor from lower level female: (1) lowest - (10) highest\n\n\nbty_f1upper\nBeauty rating of professor from upper level female: (1) lowest - (10) highest\n\n\nbty_f2upper\nBeauty rating of professor from upper level female: (1) lowest - (10) highest\n\n\nbty_m1lower\nBeauty rating of professor from lower level male: (1) lowest - (10) highest\n\n\nbty_m1upper\nBeauty rating of professor from upper level male: (1) lowest - (10) highest\n\n\nbty_m2upper\nBeauty rating of professor from upper level male: (1) lowest - (10) highest"
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-1-data-manipulation",
    "href": "content/labs/04-lab-modelling.html#part-1-data-manipulation",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 1: Data Manipulation",
    "text": "Part 1: Data Manipulation\n\n\nThe rowwise function is useful for applying mathematical operations to each row.\n\nExercise 1\nCreate a new variable called bty_avg that is the average attractiveness score of the six students for each professor (bty_f1lower through bty_m2upper). Add this new variable to the evals data frame. Do this in one pipe, using the rowwise function. Since rowwise is new to you, incomplete code is given below to guide you in the right direction, however you will need to fill in the blanks.\n\n___ <- evals |>\n  rowwise() |>\n  ___(bty_avg = mean( c( ___ ) )) |>\n  ungroup()\n\nNote that we end the pipeline with ungroup() to remove the effect of the rowwise function from earlier in the pipeline. The rowwise function works a lot like group_by, except it groups the data frame one row at a time so that any operations applied to the data frame is done once per each row. This is helpful for finding the mean beauty score for each row. However in the remainder of the analysis we don’t want to, say, calculate summary statistics for each row, or fit a model for each row. Hence we need to undo the effect of rowwise, which we can do with ungroup."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-2-exploratory-data-analysis",
    "href": "content/labs/04-lab-modelling.html#part-2-exploratory-data-analysis",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 2: Exploratory Data Analysis",
    "text": "Part 2: Exploratory Data Analysis\n\nExercise 2\nVisualize the distribution of score. Is the distribution skewed? What does that tell you about how students rate courses? Is this what you expected to see? Why, or why not? Include any summary statistics and visualizations you use in your response.\n\n\nExercise 3\nVisualize and describe the relationship between score and the new variable you created, bty_avg.\n\n\nHint: See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html.\n\n\nExercise 4\nReplot the scatterplot from Exercise 3, but this time use\ngeom_jitter()? What does “jitter” mean? What was misleading about the initial scatterplot?"
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-3-linear-regression-with-a-numerical-predictor",
    "href": "content/labs/04-lab-modelling.html#part-3-linear-regression-with-a-numerical-predictor",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 3: Linear regression with a numerical predictor",
    "text": "Part 3: Linear regression with a numerical predictor\n\n\nLinear model is in the form \\(\\hat{y} = b_0 + b_1 x\\).\n\nExercise 5\nLet’s see if the apparent trend in the plot is something more than natural variation. Fit a linear model called m_bty to predict average professor evaluation score by average beauty rating (bty_avg). Based on the regression output, write the linear model.\n\n\nExercise 6\nReplot your visualization from Exercise 3, and add the regression line to this plot in orange color. Turn off the shading for the uncertainty of the line.\n\n\nExercise 7\nInterpret the slope of the linear model in context of the data.\n\n\nExercise 8\nInterpret the intercept of the linear model in context of the data. Comment on whether or not the intercept makes sense in this context.\n\n\nExercise 9\nDetermine the \\(R^2\\) of the model and interpret it in context of the data."
  },
  {
    "objectID": "content/labs/04-lab-modelling.html#part-4-linear-regression-with-a-categorical-predictor",
    "href": "content/labs/04-lab-modelling.html#part-4-linear-regression-with-a-categorical-predictor",
    "title": "Lab 04 - Data Modelling course evaluations, Pt 1",
    "section": "Part 4: Linear regression with a categorical predictor",
    "text": "Part 4: Linear regression with a categorical predictor\n\nExercise 10\nFit a new linear model called m_gen to predict average professor evaluation score based on gender of the professor. Based on the regression output, write the linear model and interpret the slope and intercept in context of the data.\n\n\nExercise 11\nWhat is the equation of the line corresponding to male professors? What is it for female professors?\n\n\nExercise 12\nFit a new linear model called m_rank to predict average professor evaluation score based on rank of the professor. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data.\n\n\nSee the course slides on using the forcats package for changing the order of levels.\n\n\nExercise 3\nCreate a new variable called rank_relevel where \"tenure track\" is the baseline level.\n\n\nExercise 14\nFit a new linear model called m_rank_relevel to predict average professor evaluation score based on rank_relevel of the professor. This is the new (releveled) variable you created in Exercise 13. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model.\n\n\nExercise 15\nCreate another new variable called tenure_eligible that labels \"teaching\" faculty as \"no\" and labels \"tenure track\" and \"tenured\" faculty as \"yes\".\n\n\nExercise 16\nFit a new linear model called m_tenure_eligible to predict average professor evaluation score based on tenure_eligibleness of the professor. This is the new (regrouped) variable you created in Exercise 15. Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data. Also determine and interpret the \\(R^2\\) of the model."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html",
    "href": "content/labs/05-lab-mlr.html",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "",
    "text": "In this lab we revisit the professor evaluations data we modeled in an earlier lab. In the modelling lab we modeled evaluation scores using a single predictor at a time. However, this time we use multiple predictors to model evaluation scores.\nIf you don’t remember the data, review the modelling lab’s introduction before continuing to the exercises."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-1-simple-linear-regression",
    "href": "content/labs/05-lab-mlr.html#part-1-simple-linear-regression",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 1: Simple linear regression",
    "text": "Part 1: Simple linear regression\n\nExercise 2\n[Review from linear regression lab] Fit a linear model (one you have fit before): m_bty, predicting average professor evaluation score based on average beauty rating (bty_avg) only. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\)."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-2-multiple-linear-regression",
    "href": "content/labs/05-lab-mlr.html#part-2-multiple-linear-regression",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 2: Multiple linear regression",
    "text": "Part 2: Multiple linear regression\n\nExercise 3\nFit a linear model: m_bty_gen, predicting average professor evaluation score based on average beauty rating (bty_avg) and gender. Write the linear model, and note the \\(R^2\\) and the adjusted \\(R^2\\).\n\n\nExercise 4\nInterpret the slopes and intercept of m_bty_gen in context of the data.\n\n\nExercise 5\nWhat percent of the variability in score is explained by the model m_bty_gen.\n\n\nExercise 6\nWhat is the equation of the line corresponding to just male professors?\n\n\nExercise 7\nFor two professors who received the same beauty rating, which gender tends to have the higher course evaluation score?\n\n\nExercise 8\nHow do the adjusted \\(R^2\\) values of m_bty_gen and m_bty compare? What does this tell us about how useful gender is in explaining the variability in evaluation scores when we already have information on the beauty score of the professor.\n\n\nExercise 9\nCompare the slopes of bty_avg under the two models (m_bty and m_bty_gen). Has the addition of gender to the model changed the parameter estimate (slope) for bty_avg?\n\n\nExercise 10\nCreate a new model called m_bty_rank with gender removed and rank added in. Write the equation of the linear model and interpret the slopes and intercept in context of the data."
  },
  {
    "objectID": "content/labs/05-lab-mlr.html#part-3-the-search-for-the-best-model",
    "href": "content/labs/05-lab-mlr.html#part-3-the-search-for-the-best-model",
    "title": "Lab 05 - Modelling course evaluations, Pt 2",
    "section": "Part 3: The search for the best model",
    "text": "Part 3: The search for the best model\nGoing forward, only consider the following variables as potential predictors: rank, ethnicity, gender, language, age, cls_perc_eval, cls_did_eval, cls_students, cls_level, cls_profs, cls_credits, bty_avg.\n\nExercise 11\nWhich variable, on its own, would you expect to be the worst predictor of evaluation scores? Why? Hint: Think about which variable would you expect to not have any association with the professor’s score.\n\n\nExercise 12\nCheck your suspicions from the previous exercise. Include the model output for that variable in your response.\n\n\nExercise 13\nSuppose you wanted to fit a full model with the variables listed above. If you are already going to include cls_perc_eval and cls_students, which variable should you not include as an additional predictor? Why?\n\n\nExercise 14\nFit a full model with all predictors listed above (except for the one you decided to exclude) in the previous question.\n\n\nExercise 15\nUsing backward-selection (meaning fit all predictors and remove those that are not needed in the model) with adjusted R-squared as the selection criterion, determine the best model. You do not need to show all steps in your answer, just the output for the final model. Also, write out the linear model for predicting score based on the final model you settle on.\n\n\nExercise 16\nInterpret the slopes of one numerical and one categorical predictor based on your final model.\n\n\nExercise 17\nBased on your final model, describe the characteristics of a professor and course at University of Texas at Austin that would be associated with a high evaluation score.\n\n\nExercise 18\nWould you be comfortable generalizing your conclusions to apply to professors generally (at any university)? Why or why not?"
  },
  {
    "objectID": "content/labs/02-lab-wrangling.html",
    "href": "content/labs/02-lab-wrangling.html",
    "title": "Lab 02 - Tooling",
    "section": "",
    "text": "Introduction\nIn the first lab, you got acquainted with RMarkdown documents, knitting, code chunks, and interacting with GitHub; however, most of the required code was provided for you. In this and subsequent labs, there will be less code provided, and it will be up to you to write the requisite code.\nThe goal of this lab is to get you comfortable working with tidy datasets and using dplyr to do so.\n\n\nGetting started\nTo get started, accept the lab02 assignment (link on Canvas), clone the repo (using SSH) into RStudio on datahub. And, then you’re ready to go!\n\n\nPackages\nThe only package required for completion of this lab is tidyverse, as dplyr (which you’ll be using a lot in this lab) is one of the packages in the tidyverse. Be sure to import the tidyverse prior to completing the lab.\n\n\nData\nFor this lab, we’ll be using the storms dataset from the dplyr package, which includes data about a subset of storms from 1975. The description from this dataset states “This data is a subset of the NOAA Atlantic hurricane database best track data, https://www.nhc.noaa.gov/data/#hurdat. The data includes the positions and attributes of 198 tropical storms, measured every six hours during the lifetime of a storm.”\nRemember that you can use ?storms to look up the documentation for the dataset. Be sure to read and understand what information is stored in each variable before proceeding. The instructions below are not very guided and will require that you have read and understood the information in the dataset first.\n\n\nExercises\nFor each of the following, write code using dplyr functions to determine the answers to each of the questions. Your responses should include the code, its output, and a few words that answer the question.\nNote that the final two questions are optional. Definitely give them a try, but it’s OK if you don’t have time to figure them out!\n\nExercise 1\nHow many unique hurricanes are included in this dataset?\n\n\nExercise 2\nWhich tropical storm affected the largest area experiencing tropical storm strength winds? And, what was the maximum sustained wind speed for that storm?\n\n\nExercise 3\nAmong all storms in this dataset, in which month are storms most common? Does this depend on the status of the storm? (In other words, are hurricanes more common in certain months than tropical depressions? or tropical storms?)\n\n\nExercise 4\nYour boss asks for the name, year, and status of all category 5 storms that have happened in the 2000s. Carry out the operations that would deliver what they’re looking for.\n\n\nExercise 5\nFilter these data to only include storms that occurred during your lifetime (your code and results may differ from your classmates!). Among storms that have occurred during your lifetime, what’s the mean and median air pressure across all measurements taken?\n\n\nExercise 6\n(optional challenge) Which decade (of the storms included in the dataset) had the largest number of unique reported storms?\n\n\nExercise 7\n(optional challenge) - Among the subset of storms occurring in your lifetime, which storm lasted the longest? Include your code and explain your answer.\nYay, you’re done! Knit your file, commit all remaining changes to your .Rmd and .html files, use the commit message “Done with Lab 2! 💪”, and push. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo."
  },
  {
    "objectID": "content/labs/06-lab-cs01.html",
    "href": "content/labs/06-lab-cs01.html",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "",
    "text": "This week in class we’ve been discussing data related to right to carry laws and violent crime. You’ve seen the data in class, but you may not have had a chance to really work with the data yourself yet. This lab will allow you to get more comfortable with the data ahead of the associated homework and case study report coming due. While the case study report will be submitted in groups, this will be submitted individually.The idea is that while you are still encouraged to work together during lab, you and your groupmates may come up with separate ideas. This will allow you to have more ideas when you get together and start working on the case study together."
  },
  {
    "objectID": "content/labs/06-lab-cs01.html#part-1-exploratory-data-analysis-eda",
    "href": "content/labs/06-lab-cs01.html#part-1-exploratory-data-analysis-eda",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "Part 1: Exploratory Data Analysis (EDA)",
    "text": "Part 1: Exploratory Data Analysis (EDA)\nCreate at least two (2) visualizations that help you learn more about these data beyond what was presented in class. (This is intentionally vague. We want you to look at the data and figure out what would be most helpful to visualize from the provided data. These could be different variables than what we looked at in class. Data could be faceted. Something totally different!) These do not have to be fully polished visualizations, but it should be clear from the visualization and accompanying text what’s to be learned from the visualization."
  },
  {
    "objectID": "content/labs/06-lab-cs01.html#part-2-possible-extensions",
    "href": "content/labs/06-lab-cs01.html#part-2-possible-extensions",
    "title": "Lab 06 - Exploring Case Study 01",
    "section": "Part 2: Possible extensions?",
    "text": "Part 2: Possible extensions?\nThink about the data you have access to, the EDA/analysis presented in class, and the questions we said we’re going to address. What possible extensions to this analysis would you be interested in carrying out? This is a space for brainstorming. Include any possible thoughts you have here, even if they aren’t “good” or you aren’t sure if they are “possible.” This can be used as a jumping off point for when you start discussing analysis extensions with your group."
  },
  {
    "objectID": "content/labs/07-lab-logistic-regression.html",
    "href": "content/labs/07-lab-logistic-regression.html",
    "title": "Lab 07 - Logistic Regression",
    "section": "",
    "text": "“This experimental data comes from a study that sought to understand the influence of race and gender on job application callback rates. The study monitored job postings in Boston and Chicago for several months during 2001 and 2002 and used this to build up a set of test cases. Over this time period, the researchers randomly generated resumes to go out to a job posting, such as years of experience and education details, to create a realistic-looking resume. They then randomly assigned a name to the resume that would communicate the applicant’s gender and race. The first names chosen for the study were selected so that the names would predominantly be recognized as belonging to black or white individuals. For example, Lakisha was a name that their survey indicated would be interpreted as a black woman, while Greg was a name that would generally be interpreted to be associated with a white male.” (openintro)\n\n\nBertrand, Marianne, and Sendhil Mullainathan. 2003. “Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” https://doi.org/10.3386/w9873."
  },
  {
    "objectID": "content/labs/07-lab-logistic-regression.html#part-1-eda",
    "href": "content/labs/07-lab-logistic-regression.html#part-1-eda",
    "title": "Lab 07 - Logistic Regression",
    "section": "Part 1: EDA",
    "text": "Part 1: EDA\n\nExercise 1\nDo some EDA! The first step in the analysis of a new dataset is getting acquainted with the data. Make summaries of the variables in your dataset, determine which variables are categorical and which are numerical. For numerical variables, are there outliers? What are the observations in this data set? How many observations are there in our dataset? If you aren’t sure or want to take a closer look at the data, make a plot."
  },
  {
    "objectID": "content/labs/07-lab-logistic-regression.html#part-2-single-predictor-logistic-regression",
    "href": "content/labs/07-lab-logistic-regression.html#part-2-single-predictor-logistic-regression",
    "title": "Lab 07 - Logistic Regression",
    "section": "Part 2: Single Predictor Logistic Regression",
    "text": "Part 2: Single Predictor Logistic Regression\nHere, we’ll be modelling the probability of a callback for a given resume \\(i\\). (\\(Y_i\\) will be used to represent whether resume \\(i\\) received a callback (\\(Y_i=1\\)) or not (\\(Y_i=0\\))).\n\nExercise 2\nFit a logistic regression model: m_honors, predicting the probability of a callback (received_callback) given whether the applicant had any honors listed on their resume. Write the logistic regression model out.\n\n\nExercise 3\nIf a resume is randomly selected from the study and it does not have any honors listed, what is the probability it resulted in a callback?\n\n\nExercise 4\nWhat would the probability be if the resume did list some honors?"
  },
  {
    "objectID": "content/labs/07-lab-logistic-regression.html#part-3-multiple-predictors-logistic-regression",
    "href": "content/labs/07-lab-logistic-regression.html#part-3-multiple-predictors-logistic-regression",
    "title": "Lab 07 - Logistic Regression",
    "section": "Part 3: Multiple Predictors Logistic Regression",
    "text": "Part 3: Multiple Predictors Logistic Regression\n\nExercise 5\nFit a logistic regression model m_multiple predicting the probability of a callback (received_callback) given all of the variables specified in the description above. Write the logistic regression model out.\n\n\nExercise 6\nThe race variable had taken only two levels: Black and White. Based on the model results, what does the coefficient of this variable say about callback decisions?\n\n\nExercise 7\nUsing your results from m_multiple, estimate the probability of receiving a callback for a job in Chicago where the candidate lists 14 years experience, no honors, no military experience, includes an email address, and has a first name that implies they are a White male. How do these results differ from someone who has a first name that implies they are a Black male?\n\n\nReminder: Unlike \\(R^2\\) (which we used in multiple linear regression), when using AIC for model selection, models with a lower AIC value are considered to be “better.” Also, AIC provides information about the quality of a model relative to other models, but does not provide information about the overall quality of a model.\n\n\nExercise 8\nConsidering the results from the model you just fit and using a backward elimination strategy, remove any variables from the m_multiple model fit previously to identify the most parsimonious model. Store this in m_resume. Utilize AIC (Akaike information criterion) for model comparison."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html",
    "href": "content/labs/01-lab-intro-r.html",
    "title": "Lab 01 - Tooling",
    "section": "",
    "text": "The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course to learn and practice programming and analyze data.\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\nAn additional goal is to introduce you to git and GitHub, which is the collaboration and version control system that we will be using throughout the course.\n\n\ngit is a version control system (like “Track Changes” features from Microsoft Word on steroids) and GitHub is the home for your Git-based projects on the internet (like DropBox but much, much better).\nAs the labs progress, you are encouraged to explore beyond what the labs say directly; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands.\nYou are encouraged to ask one another questions and work together, but each individual must turn in their own lab each week."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#github-housekeeping",
    "href": "content/labs/01-lab-intro-r.html#github-housekeeping",
    "title": "Lab 01 - Tooling",
    "section": "GitHub Housekeeping",
    "text": "GitHub Housekeeping\n\n\n\n\n\n\nNote\n\n\n\nYour email address is the address tied to your GitHub account and your name should be first and last name.\n\n\nBefore we can get started we need to take care of some required housekeeping. Specifically, we need to configure your git so that RStudio can communicate with GitHub…and so you do not have to type in your username and password every time you want to communicate with GitHub from RStudio. These steps will be demo-ed during lab and you’ll have time to walk through the steps!\n\n\n\n\n\n\nNote\n\n\n\nIf you aren’t able to attend lab and get stuck here, there is a podcast recording from lecture on 9/29 from the COGS 137 Fa21 iteration where you can see these steps demo-ed.\n\n\n\nStep 1: Email and Username\nThe first step requires two pieces of information: your email address and your name.\nTo do so, follow these steps:\n\nGo to the Terminal pane\nType the following two lines of code, replacing the information in the quotation marks with your info:\n\n\ngit config --global user.email \"your email\"\ngit config --global user.name \"your name\"\n\nFor example, for me these are:\n\ngit config --global user.email \"sellis@ucsd.edu\"\ngit config --global user.name \"Shannon Ellis\"\n\nTo confirm that the changes have been implemented, run the following:\n\ngit config --global user.email\ngit config --global user.name\n\n\n\nStep 2: Generate ssh key\nIn the terminal, you’ll want to generate an ssh key by typing:\n\nssh-keygen\n\nAfter hitting enter/return to execute the above, you’ll press return/enter three times to bypass specifying a location and passphrase.\n\n\nStep 3: Copy your ssh key\nFrom the terminal type:\n\ncat ~/.ssh/id_rsa.pub\n\nYou’ll want to highlight and copy the full result of this command. It will start with ssh-rsa and end with dsmlp.login.ucsd.edu.\n\n\nStep 4: Let GitHub know your key\n\nIn your browser, navigate to https://github.com/settings/keys\nClick “New SSH Key”\nSet title to DSMLP\nPaste what you copied in step 3 into the “Key” box\nClick “Add SSH Key”\n\n\n\nStep 5: Finalize\nReturn to the terminal in RStudio and run the following command:\n\nssh git@github.com\n\nYou’ll then see a message like You've successfully authenticated, but GitHub does not provide shell access. At this point, you’re all set!\nThis will be the only time you have to do this. From here on out, you’ll be able to “communicate” with GitHub from RStudio without typing your username/password."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#cloning-the-lab",
    "href": "content/labs/01-lab-intro-r.html#cloning-the-lab",
    "title": "Lab 01 - Tooling",
    "section": "Cloning the lab",
    "text": "Cloning the lab\nEach of your assignments will begin with the following steps. You saw these once in class and, they’re outlined in detail here again. Going forward each lab will start with a “Getting started” section but details will be a bit more sparse than this. You can always refer back to this lab for a detailed list of the steps involved for getting started with an assignment.\nClick on the assignment link for this week’s lab on the Canvas homepage. You will have to Accept before proceeding. Refresh the page and follow the link to the repo created for you. This repo contains a template you can build on to complete your lab.\n\n\n\n\n\n\n\nOn this page on GitHub, click the URL provided for you. This will bring you to your copy of the repo on GitHub. Click on the green <> Code button to clone the repo. Be sure that SSH is selected and copy this URL.\n\n\n\n\n\n\nImportant\n\n\n\nBe sure that any time you are copying a link from GitHub under the <>Code button, you select and use the ‘SSH’ URL as this is what will allow you to not have to type your username and password.\n\n\n\n\n\n\n\n\n\nGo to datahub and open RStudio. Go to File > New Project… and select to create a New Project from Version Control. On the following menu, select Git.\nCopy and paste the URL of your assignment repo into the “Repository URL” dialog box:\n\n\n\n\n\n\n\nHit Create Project. Open up lab-01.Rmd and continue through this lab. Your work/answers for this lab will be submitted in that document."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#packages",
    "href": "content/labs/01-lab-intro-r.html#packages",
    "title": "Lab 01 - Tooling",
    "section": "Packages",
    "text": "Packages\nIn this lab we will work with two packages: datasauRus which contains the dataset, and tidyverse which is a collection of packages for doing data analysis in a “tidy” way.\nThese packages have already been installed for you. However, if they had not, you would need to run install.packages(\"tidyverse\") and install.packages(\"datasauRus\") before proceeding. Note that package installation happens a single time. But, any time you want to use a package (after it’s been installed), it has to be loaded, as we do below:\nIf you’d like to run your code in the Console as well you’ll also need to load the packages there. To do so, run the following in the console.\n\nlibrary(tidyverse) \nlibrary(datasauRus)\n\nYou should be able to Knit your document and see the results.\nNote that the packages are also loaded with the same commands in your R Markdown document."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#yaml",
    "href": "content/labs/01-lab-intro-r.html#yaml",
    "title": "Lab 01 - Tooling",
    "section": "YAML:",
    "text": "YAML:\n\n\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “Yet Another Markup Language”. It is a human-friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\nOpen the R Markdown (Rmd) file in your project, change the author name to your name and knit the document. This will generate an HTML document."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#commiting-changes",
    "href": "content/labs/01-lab-intro-r.html#commiting-changes",
    "title": "Lab 01 - Tooling",
    "section": "Commiting changes:",
    "text": "Commiting changes:\nThen Go to the Git pane in your RStudio on Datahub.\nIf you have made changes to your Rmd file, you should see it listed here. Click on it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. Be sure to also select your HTML document. Once you’re happy with these changes, write “Update author name” in the Commit message box and hit Commit.\n\n\n\n\n\n\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the quarter progresses, we will let you make these decisions."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#pushing-changes",
    "href": "content/labs/01-lab-intro-r.html#pushing-changes",
    "title": "Lab 01 - Tooling",
    "section": "Pushing changes:",
    "text": "Pushing changes:\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only).\nIn order to push your changes to GitHub, click on Push. Go check your repo on GitHub - you’ll see your updated documents there!\n\n\n\n\n\n\nThought exercise\n\n\n\nFor which of the above steps (changing project name, making updates to the document, committing, and pushing changes) do you need to have an internet connection? Discuss with your classmates."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#data",
    "href": "content/labs/01-lab-intro-r.html#data",
    "title": "Lab 01 - Tooling",
    "section": "Data",
    "text": "Data\nThe data frame we will be working with today is called datasaurus_dozen2 and it’s in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualization is important and how summary statistics alone can be misleading. The different datasets are maked by the dataset variable.\nTo find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark before the name of an object will always bring up its help file. This command must be ran in the Console.\n\nExercise 1\nBased on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report. When you’re done, commit your changes with the commit message “Added answer for Ex 1”, and push.\nLet’s take a look at what these datasets are. To do so we can make a frequency table of the dataset variable:\n\ndatasaurus_dozen |>\n  count(dataset)\n\n# A tibble: 13 × 2\n   dataset        n\n   <chr>      <int>\n 1 away         142\n 2 bullseye     142\n 3 circle       142\n 4 dino         142\n 5 dots         142\n 6 h_lines      142\n 7 high_lines   142\n 8 slant_down   142\n 9 slant_up     142\n10 star         142\n11 v_lines      142\n12 wide_lines   142\n13 x_shape      142\n\n\nThe original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice.3 In the paper, the authors simulate a variety of datasets that the same summary statistics to the Datasaurus but have very different distributions."
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#data-visualization-and-summary",
    "href": "content/labs/01-lab-intro-r.html#data-visualization-and-summary",
    "title": "Lab 01 - Tooling",
    "section": "Data visualization and summary",
    "text": "Data visualization and summary\n\nExercise 2\nPlot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset.\nBelow is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Rmd document and successfully knit it and view the results.\nStart with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data.\n\ndino_data <- datasaurus_dozen |>\n  filter(dataset == \"dino\")\n\nThere is a lot going on here, so let’s slow down and unpack it a bit.\nFirst, the pipe operator: |>, takes what comes before it and sends it as the first argument to what comes after it. So here, we’re saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\".\nSecond, the assignment operator: <-, assigns the name dino_data to the filtered data frame.\nNext, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data you’re visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case we want these to be points,m hence geom_point.\n\nggplot(data = dino_data, mapping = aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\nIf this seems like a lot, it is. And you will learn about the philosophy of building data visualizations in layer in detail next week. For now, follow along with the code that is provided.\nFor the second part of this exercises, we need to calculate a summary statistic: the correlation coefficient. Correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear – it’s dinosaurial!\nBut, for illustrative purposes, let’s calculate correlation coefficient between x and y.\n\n\nStart with dino_data and calculate a summary statistic that we will call r as the correlation between x and y.\n\ndino_data |>\n  summarize(r = cor(x, y))\n\n# A tibble: 1 × 1\n        r\n    <dbl>\n1 -0.0645\n\n\nThis is a good place to pause, commit changes with the commit message “Added answer for Ex 2”, and push.\n\n\nExercise 3\nPlot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nThis is another good place to pause, commit changes with the commit message “Added answer for Ex 3”, and push.\n\n\nExercise 4\nPlot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nYou should pause again, commit changes with the commit message “Added answer for Ex 4”, and push.\n\n\nFacet by the dataset variable, placing the plots in a 3 column grid, and don’t add a legend.\n\n\nExercise 5\nFinally, let’s plot all datasets at once. In order to do this we will make use of facetting.\n\nggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset)) +\n  geom_point() +\n  facet_wrap(~ dataset, ncol = 3) +\n  theme(legend.position = \"none\")\n\nAnd we can use the group_by function to generate all the summary correlation coefficients.\n\ndatasaurus_dozen |>\n  group_by(dataset) |>\n  summarize(r = cor(x, y))\n\nYou’re done with the data analysis exercises, but we’d like you to do two more things:\n\n\n\n\n\nFigure 1: ?(caption)"
  },
  {
    "objectID": "content/labs/01-lab-intro-r.html#bonus-exercises",
    "href": "content/labs/01-lab-intro-r.html#bonus-exercises",
    "title": "Lab 01 - Tooling",
    "section": "Bonus Exercises",
    "text": "Bonus Exercises\nComplete these as time permits to further your experience with, comfort in, and understanding of R Markdown documents.\n\nResize your figures\nClick on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the pop up dialogue box go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until you’re happy with the figure sizes. Note that these values get saved in the YAML.\n\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nYou can also use different figure sizes for different figures. To do so click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well.\n\n\n\n\n\n\n\nChange the look of your report\nOnce again click on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the General tab of the pop up dialogue box try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until you’re happy with the look.\n\n\n\n\n\n\nNote\n\n\n\nNot sure how to use emojis on your computer? Maybe a classmate can help? Or you can ask your TA as well!"
  },
  {
    "objectID": "content/hw/hw-01.html",
    "href": "content/hw/hw-01.html",
    "title": "HW 01 - R Basics",
    "section": "",
    "text": "This assignment is meant to get you comfortable with 1) the format of homework assignments in this course and 2) writing R code in RStudio. The specific questions on this assignment will be simpler and should take you less time than those on future assignments. This assignment focuses on variables, operators, datasets, and dplyr basics.\n\nGetting started\nHere are the steps for getting started:\n\nStart by navigating to the hw01 GitHub URL (found on Canvas)\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit your file and take a look at the document generated\nCommit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\n\nYou can of course push multiple to GitHub times throughout the assignment. Your final push at the deadline will be used for grading. (This means even if you made mistakes before that on GitHub, you wouldn’t be penalized for them, so long as the final state of your work is correct).\n\n\n\n\n\n\nImportant\n\n\n\nYou’ll always want to knit your RMarkdown document to HTML and review that HTML document to ensure it includes all the information you want and looks as you intended, as we grade from the knit HTML. Both your .Rmd and .html files should be on GitHub."
  },
  {
    "objectID": "content/hw/hw-03.html",
    "href": "content/hw/hw-03.html",
    "title": "HW 03 - Bike rentals in DC",
    "section": "",
    "text": "Bike sharing systems take traditional bike rentals but automate the entire process (membership, rental, return, etc.). Through these systems, users are able to easily rent a bike from a particular position and return it at another position. There are hundreds of bike-sharing programs around the world comprising hundreds of thousands of bicycles. Today, there exists great interest in these (and other “alternative” transit) systems due to their important role in traffic, environmental, and health issues.\nApart from interesting real world applications of bike sharing systems, the characteristics of data being generated by these systems make them attractive for research. Opposed to other transport services such as bus or subway, the duration of travel, departure and arrival position is explicitly recorded in these systems. This feature turns bike sharing system into a virtual sensor network that can be used for sensing mobility in the city. Hence, it is expected that most of important events in the city could be detected via monitoring these data.\nSource: UCI Machine Learning Repository - Bike Sharing Dataset"
  },
  {
    "objectID": "content/hw/hw-03.html#getting-started",
    "href": "content/hw/hw-03.html#getting-started",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nStart with an assignment link that creates the GitHub repo with starter documents (link on Canvas).\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically commit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\nThis assignment will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading. (This means even if you made mistakes before that submission on GitHub, you won’t be penalized for them, so long as the final state of your work is correct)."
  },
  {
    "objectID": "content/hw/hw-03.html#data",
    "href": "content/hw/hw-03.html#data",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Data",
    "text": "Data\nThe data include daily bike rental counts (by members and casual users) of Capital Bikeshare in Washington, DC in 2011 and 2012 as well as weather information on these days.\nThe original data sources are http://capitalbikeshare.com/system-data and http://www.freemeteo.com.\nThe codebook is below:\n\n\n\n\n\n\n\nVariable name\nDescription\n\n\n\n\ninstant\nrecord index\n\n\ndteday\ndate\n\n\nseason\nseason (1:winter, 2:spring, 3:summer, 4:fall)\n\n\nyr\nyear (0: 2011, 1:2012)\n\n\nmnth\nmonth (1 to 12)\n\n\nholiday\nweather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n\n\nweekday\nday of the week\n\n\nworkingday\nif day is neither weekend nor holiday is 1, otherwise is 0.\n\n\nweathersit\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\n\n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\n\n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n\n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n\ntemp\nNormalized temperature in Celsius. The values are divided by 41 (max)\n\n\natemp\nNormalized feeling temperature in Celsius. The values are divided by 50 (max)\n\n\nhum\nNormalized humidity. The values are divided by 100 (max)\n\n\nwindspeed\nNormalized wind speed. The values are divided by 67 (max)\n\n\ncasual\nCount of casual users\n\n\nregistered\nCount of registered users\n\n\ncnt\nCount of total rental bikes including both casual and registered"
  },
  {
    "objectID": "content/hw/hw-03.html#setup",
    "href": "content/hw/hw-03.html#setup",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Setup",
    "text": "Setup\nYou are free to utilize any packages in this homework. We think you’ll likely use tidyverse, tidymodels (…and maybe olsrr)"
  },
  {
    "objectID": "content/hw/hw-03.html#questions",
    "href": "content/hw/hw-03.html#questions",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Questions",
    "text": "Questions\n\nData wrangling\n\nQuestion 1\nRecode the season variable to be a factor with meaningful level names as outlined in the codebook, with spring as the baseline level.\n\n\nQuestion 2\nRecode the binary variables holiday and workingday to be factors with levels no (0) and yes (1), with no as the baseline level.\n\n\nQuestion 3\nRecode the yr variable to be a factor with levels 2011 and 2012, with 2011 as the baseline level.\n\n\nQuestion 4\nRecode the weathersit variable as 1 - clear, 2 - mist, 3 - light precipitation, and 4 - heavy precipitation, with clear as the baseline.\n\n\nQuestion 5\nCalculate raw temperature, feeling temperature, humidity, and windspeed as their values given in the dataset multiplied by the maximum raw values stated in the codebook for each variable. Instead of writing over the existing variables, create new ones with concise but informative names.\n\n\nQuestion 6\nCheck that the sum of casual and registered adds up to cnt for each record.\n\n\n\nExploratory data analysis\n\nQuestion 7\nRecreate the following visualization, and interpret it in context of the data. Hint: You will need to use one of the variables you created above.\n\n\n\nQuestion 8\nCreate a visualization displaying the relationship between bike rentals and season. Interpret the plot in context of the data.\n\n\n\nModelling\n\nQuestion 9\nFit a linear model predicting total daily bike rentals from daily temperature. Write the linear model, interpret the slope and the intercept in context of the data, and determine and interpret the \\(R^2\\).\n\n\nQuestion 10\nFit another linear model predicting total daily bike rentals from daily feeling temperature. Write the linear model, interpret the slope and the intercept in context of the data, and determine and interpret the \\(R^2\\). Is temperature or feeling temperature a better predictor of bike rentals?\n\n\nQuestion 11\nFit a full model predicting total daily bike rentals from season, year, whether the day is holiday or not, whether the day is a workingday or not, the weather category, temperature, feeling temperature, humidity, and windspeed, as well as the interaction between at least one numerical and one categorical variable.\n\n\nQuestion 12\nPerform backward selection using adjusted \\(R^2\\) as the decision criterion to find the “best” model. Provide the model output for the final model.\n\n\nQuestion 13\nInterpret slope coefficients associated with two of the variables in your final model in context of the data. Note: If one of these is categorical with multiple levels, make sure you interpret all of the slope coefficients associated with the levels of the variable.\n\n\nQuestion 14\nBased on the final model you found in the previous question, discuss what makes for a good day to bike in DC (as measured by rental bikes being more in demand)."
  },
  {
    "objectID": "content/hw/hw-03.html#submission",
    "href": "content/hw/hw-03.html#submission",
    "title": "HW 03 - Bike rentals in DC",
    "section": "Submission",
    "text": "Submission\nBe sure to knit your file to HTML, look at the output HTML file to make sure everything looks as you expected, and then commit and push your final changes to GitHub. We will be grading from the HTML file. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo."
  },
  {
    "objectID": "content/hw/hw-02.html",
    "href": "content/hw/hw-02.html",
    "title": "HW 02 - Data Visualization",
    "section": "",
    "text": "This assignment is meant to get you more comfortable with generating and customizing visualizations in R using ggplot2. The first section will guide you toward the visualizations you’re expected to generate, while the final two sections will be more open-ended. There are multiple distinct visualizations that could be totally “correct” for each question. You may make a different decision than your classmate and could both be correct.\nAlso, note that the final two parts of this assignment will take you way longer than you think they will. Definitely do not wait until the last minute to start this assignment."
  },
  {
    "objectID": "content/hw/hw-02.html#getting-started",
    "href": "content/hw/hw-02.html#getting-started",
    "title": "HW 02 - Data Visualization",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nStart with an assignment link that creates a repo on GitHub with starter documents (link on Canvas).\nClone this repo into RStudio on datahub\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically commit changes (for example, once per each new part)\nPush all your changes back to your GitHub repo\nThis assignment will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading. (This means even if you made mistakes before that submission on GitHub, you won’t be penalized for them, so long as the final state of your work is correct).\n\nImports\nThe following packages must be imported prior to completing this homework: tidyverse and palmerpenguins (Note: If you did not install palmerpenguins during lecture, you’ll have to run install.packages(\"palmerpenguins\") prior to importing it.)\n\n\nGround Rules\nFor this assignment, all visualizations must:\n\nbe completed using ggplot2\nhave an informative title and labeled axes\nfollow good visualization practices (discussed in class)"
  },
  {
    "objectID": "content/hw/hw-02.html#part-i-ggplot2",
    "href": "content/hw/hw-02.html#part-i-ggplot2",
    "title": "HW 02 - Data Visualization",
    "section": "Part I: ggplot2",
    "text": "Part I: ggplot2\nThis first section will continue to use the penguins dataset from the palmerpenguins package that was used during the ggplot2 lecture.\n\nQuestion 1\nGenerate a visualization that will allow readers to determine whether male or female penguins are larger (by mass).\n\n\nQuestion 2\nGenerate a barplot that visualizes how many penguins there are from each species on each island. Each island should be a different panel (in a 1 row x 3 columns visualization), and each chart should visualize the species count.\n\n\nQuestion 3\nGenerate a scatterplot that will allow the viewer to determine whether flipper length has differed over time. Be sure to color the points on this plot by species."
  },
  {
    "objectID": "content/hw/hw-02.html#part-ii-imitation-is-the-highest-form-of-flattery",
    "href": "content/hw/hw-02.html#part-ii-imitation-is-the-highest-form-of-flattery",
    "title": "HW 02 - Data Visualization",
    "section": "Part II: Imitation is the highest form of flattery",
    "text": "Part II: Imitation is the highest form of flattery\nIn class we learned a handful of ways to customize visualizations. Now, it’s your turn to apply what you learned by recreating someone else’s visualization.\n\nQuestion 4\nFor this question, find a visualization somewhere on the Internet and recreate the visualization as close as you can using ggplot2. To make this easier on yourself, you’ll likely want to find a visualization where the data are readily available. (To get started, FiveThirtyEight makes a lot of the data from their articles available and has many charts in their articles. You are not required to recreate a visualization from FiveThirtyEight; however, if you’re not sure where to start, you have this option.) Your answer should include an image of the original visualization, a reference to the original image (this could simply be a URL), and your code + recreation.\nNotes: - To insert an image in an RMarkdown document, you can use the syntax ![alt text](path/to/image.png). - The R/ggplot2 code to create your visualization cannot already exist on the internet. (For example, choosing to recreate a plot from the R Graph Gallery would not be an option b/c all the code is already there and you wouldn’t learn as much.)\n\n\nQuestion 5\nBriefly explain what you learned about ggplot2 in the process of re-creating this visualization.\n\n\nQuestion 6\nExplain how your visualization differs from the original (It’s OK if yours is not a perfect recreation!)"
  },
  {
    "objectID": "content/hw/hw-02.html#part-iii-take-a-sad-plot-and-make-it-better",
    "href": "content/hw/hw-02.html#part-iii-take-a-sad-plot-and-make-it-better",
    "title": "HW 02 - Data Visualization",
    "section": "Part III: Take a sad plot and make it better",
    "text": "Part III: Take a sad plot and make it better\n\nQuestion 7\nThis question was inspired by Alison Hill’s talk. The idea here is that there is a lot of data all around us and a whole bunch of visualizations. Some of them are really excellent, and some could be improved. Choose a visualization you’ve created in the past OR a visualization you’ve found out in the world that could benefit from a redesign and/or significant visual improvement. (This could be the same visualization you recreated above, but for most it will likely be a totally different visualization.) Your answer should include an image of the original visualization, a reference to the original image (this could simply be a URL), and your code + improved version.\nNote: If you’re unsure where to look for visualizations that would benefit from improvement, check out Flowing Data’s Ugly Charts or Reddit’s Data is ugly. You may need to recreate the dataset (meaning store the values from the visualization in a tibble) needed to generate the visualization prior to improving the design.\n\n\nQuestion 8\nBriefly explain what you learned about ggplot2 in the process of re-creating this visualization.\n\n\nQuestion 9\nExplain why you made the design and visualization choices you did for your improved version."
  },
  {
    "objectID": "content/hw/hw-02.html#submission",
    "href": "content/hw/hw-02.html#submission",
    "title": "HW 02 - Data Visualization",
    "section": "Submission",
    "text": "Submission\nBe sure to knit your file to HTML, look at the output HTML file to make sure everything looks as you expected, and then commit and push your final changes to GitHub. We will be grading from the HTML file. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo."
  },
  {
    "objectID": "content/cs/cs-example.html",
    "href": "content/cs/cs-example.html",
    "title": "CS: Example",
    "section": "",
    "text": "library(OCSdata)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(pdftools)\nlibrary(tesseract)\nlibrary(magick)\nlibrary(knitr)\nlibrary(Kendall)\nlibrary(broom)\n\n\n\n\n\n\n\nProf Note\n\n\n\nI would recommend packages be imported after the introduction, but this is very specific and not something anyone would lose credit for."
  },
  {
    "objectID": "content/cs/cs-example.html#introduction",
    "href": "content/cs/cs-example.html#introduction",
    "title": "CS: Example",
    "section": "Introduction",
    "text": "Introduction\nIn this case study, we seek to analyze the youth disconnection among minority groups within the United States. The disconnected youth are the young people who are between the ages of 16 and 24 who are neither working nor in school. Following the aftermath of the Great Recession in 2008, we witnessed a peak youth disconnection rate of 14.7 percent in 2010 which has since been trending downwards to as low as a youth disconnection rate of 11.5 percent in 2017. While analyzing racial and ethnic groups is imperative for viewing any evident disparities, we also intend on analyzing how gender may play a role in affecting the rates of disconnection. Seeing as how we can already see existing disparities between genders within the workforce, we also want to see to what extent, if at all, gender plays a role in youth disconnection among groups.\nThis specific period of young adulthood is one of the most critical in developing the necessary skills and capabilities required of them to be successful in adulthood and their professional careers as youth disconnection during this period stunts development and limits their potential. Measure of America, a nonpartisan project of the nonprofit Social Science Research, claims that “people who experience a period of disconnection as young adults go on to earn less and are less likely to be employed, own a home, or report good health by the time they reach their thirties”. Understanding how these trends among groups arise may be critical for finding solutions to preventing youth disconnection, and analyzing what groups may need more resources in order to prevent further prevent youth disconnection.\n\n\n\n\n\n\nProf Note\n\n\n\nThis introduction is missing citations! Statements that are not your own original thoughts must be cited. See Effective Communication lecture for possible ways to cite. We don’t care what format you use. We do care that you cite others’ work. Also, definitely feel free to find information on the topic beyond what was presented in class."
  },
  {
    "objectID": "content/cs/cs-example.html#questions",
    "href": "content/cs/cs-example.html#questions",
    "title": "CS: Example",
    "section": "Questions",
    "text": "Questions\n\nHow have youth disconnection rates in American youth changed since 2008?\nIn particular, how has this changed for different gender and ethnic groups? Are any groups particularly disconnected?\nDoes removing 2008 (pre-Great Recession time period) data points change our answer?\n\n\n\n\n\n\n\nProf Note\n\n\n\nThe third question here was their extension. Note that this is an ok extension all things considered. This was earlier in the quarter, a more straightforward dataset, there wasn’t as much to do as there is in our dataset. Explaining the why this is their focus in introduction would have been great."
  },
  {
    "objectID": "content/cs/cs-example.html#the-data",
    "href": "content/cs/cs-example.html#the-data",
    "title": "CS: Example",
    "section": "The Data",
    "text": "The Data\nThe data set that we will be using for our case study is provided by two reports from the Measure of America project related to youth disconnect. The data provided will be in the form of images.\n\nData Import\nIn order to begin our analysis, we imported our raw data from the OCSdata library on youth disconnection.\n\n# Data is imported so I get overwrite error when not commented - Adrian\n# Download raw data files\n# load_raw_data(\"ocs-bp-youth-disconnection\", outpath = '.')\n\n\n\n\n\n\n\nProf Note\n\n\n\nReminder that you can control chunk behavior for each chunk within the curly braces. This also applies for the warnings that are displayed later on. Best to supress those in final HTML.\n\n\nSince the raw data is in images, we need to convert to a usable R data set. We started with the major racial ethnic groups.\n\n## Import data for major racial/ethnic groups\nmajor_racial_ethnic_groups <- magick::image_read(\"data/Major_ethnic_groups_screenshot.png\")\nmajor_groups <- magick::image_ocr(major_racial_ethnic_groups)\n\nFor Asian subgroups, we had to import the raw data for 2017 and 2018.\n\n## Import data for Asian subgroups\n\n# 2017 data\nasian_sub_2017 <- image_read(\"data/asian_subgroups_2017.png\")\nasian_sub_2017_A <- image_read(\"data/asian_sub_2017_A.png\")\nasian_sub_2017_B <- image_read(\"data/asian_sub_2017_B.png\")\nasian_sub_2017_C <- image_read(\"data/asian_sub_2017_C.png\")\nasian_sub_2017 <- image_ocr(asian_sub_2017)\nasian_sub_2017_A <- image_ocr(asian_sub_2017_A)\nasian_sub_2017_B <- image_ocr(asian_sub_2017_B)\nasian_sub_2017_C <- image_ocr(asian_sub_2017_C)\n\n# 2018 data\nasian_sub_2018_A <- image_read(\"data/asian_sub_2018_A.png\")\nasian_sub_2018_A <- image_ocr(asian_sub_2018_A)\nasian_sub_2018_B <- image_read(\"data/asian_sub_2018_B.png\")\nasian_sub_2018_B <- image_ocr(asian_sub_2018_B)\n\nFor Latinx subgroups, we also imported 2017 and 2018 data.\n\n## Import data for Latinx subgroups\n\n# 2017 data\nlatinx_imageA <- image_read(\"data/latinx_sub_2017_A.png\")\nlatinx_imageB <- image_read(\"data/latinx_sub_2017_B.png\")\nlatinx_imageC <- image_read(\"data/latinx_sub_2017_C.png\")\nlatinx_sub_2017_A <- image_ocr(latinx_imageA)\nlatinx_sub_2017_B <- image_ocr(latinx_imageB)\nlatinx_sub_2017_C <- image_ocr(latinx_imageC)\n\n# 2018 data\nlatinx_sub_2018 <- image_read(\"data/latinx_subgroups_2018.png\")\nlatinx_sub_2018 <- image_ocr(latinx_sub_2018)\n\nOnce the data import is complete, we saved our data.\n\n# Save data\nsave(\n  major_groups,\n  asian_sub_2017,\n  asian_sub_2017_A,\n  asian_sub_2017_B,\n  asian_sub_2017_C,\n  latinx_sub_2017_A,\n  latinx_sub_2017_B,\n  latinx_sub_2017_C,\n  asian_sub_2018_A,\n  asian_sub_2018_B,\n  latinx_sub_2018,\n  file = \"data/imported_data.rda\")\n\n\n\nData Wrangling\nNow that we have our data properly imported, we need to wrangle our data in R so we can begin our analysis.\n\n\n\n\n\n\nProf Note\n\n\n\nThere is text to guide the viewer, clear code, and code comments as needed. I like this.\n\n\n\nMajor Group Data\nTo begin our data wrangling, we took the imported data that is in a single string and separated the data by new line characters and transformed it into a table.\n\n# Separates string by new line and transforms into a table\nmajor_groups <- major_groups |>\n  stringr::str_split(pattern = \"\\n\") |>\n  unlist() |> \n  tibble::as_tibble()\n\nNow that our data has rows, we created columns Group and Year and assigned the corresponding values according to the data as well as normalizing the capitalization throughout the data set.\n\n# Separate into columns Group and Year\nmajor_groups <- \n  major_groups |>\n  tidyr::separate(col = value, \n                  into = c(\"Group\", \"Years\"), # Set column names\n                  sep = \"(?<=[[:alpha:]])\\\\s(?=[0-9])\")  # Separate after letter string; beginning numerics\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [19].\n\n# Make capitalization format the same\nmajor_groups <- major_groups |> \n  mutate(Group = stringr::str_to_title(Group))\n\nWe have our data in two columns but need to separate our Years column into each individual year. We thus separated the column by every space character, assigned appropriate years for its corresponding values, and made the data values numeric.\n\n# Separate `Years` into columns by individual years\nmajor_groups <- major_groups |> \n  tidyr::separate(col = Years, \n                  into = c(\"2008\", \"2010\", \"2012\", \"2014\", \"2016\", \"2017\"),  # Set column names\n                  sep = \" \") # Separate by spaces\n\n# Remove empty rows\nmajor_groups <- major_groups |> \n  tidyr::drop_na()\n \n# Make data numeric\nmajor_groups <- major_groups |>\n  mutate(\n    across(.cols = -Group,\n           ~ str_remove(string = ., pattern = \"\\\\.\")),  # Remove decimal points from string\n    across(.cols = -Group, as.numeric),  # Convert to numeric\n    across(.cols = -Group, ~ . * 0.1)   # Add decimal point back\n  )\n\nWe have a lot of information in our Group column, so we separated that information and created two new columns, Race_Ethnicity and Gender, to make it easier to analyze our data.\n\n# Create Race_Ethnicity column\nmajor_groups  <- major_groups |>\n  # First, create entries for `All_races`\n  mutate(Race_Ethnicity = dplyr::recode(Group, \"United States\" = \"All_races\",\n                                        \"Female\" = \"All_races\",\n                                        \"Male\" = \"All_races\"),\n  # Second, create entries for all other races by importing data from group column and removing gender  \n         Race_Ethnicity = str_remove(string = Race_Ethnicity,\n                                     pattern = \"Female|Male\"))\n# Create Gender column\nmajor_groups  <- major_groups |>\n  # Extract gender from `Group` column\n  mutate(Gender = str_extract(string = Group, \n                              pattern = \"Female|Male\")) |>\n  # Assign `All' if gender was not in `Group`\n  mutate(Gender = replace_na(Gender, replace = \"All\"))\n\nNow that we have all of our data, we moved the columns containing years into rows and added a new column Percent to log the value contained in the, now former, year column.\n\n# Puts years in rows instead of columns with percent values\nmajor_groups_long <- major_groups |>\n  tidyr::pivot_longer(cols = contains(\"20\"), # any column containing 20\n                      names_to = \"Year\", # Assigns column for year\n                      values_to = \"Percent\", # Assigns column for percent\n                      names_prefix = \"Perc_\") |>\n  dplyr::mutate(Year = as.numeric(Year))\n\n\n\nSubgroup Data\n\nData Wrangling Functions\nWe wrangled the major group data but now have to wrangle the subgroup data, which is a little more complicated. We first created a make_rows function to take the imported raw data and convert it into a table with rows separated by new line characters.\n\n# Create function to separate string into rows\nmake_rows <- function(text){\n  text |>\n  str_split(\"\\n\") |>\n  unlist() |>\n  as_tibble()\n}\n\nWe also created two functions clean_table and clean_table_2018 to clean our subgroup data just like the major group data – separating Group and Years columns, converting data to numeric, and creating Race_Ethnicity and Gender columns.\n\n# Create function to clean 2017 subgroup data \nclean_table <- function(table){\n  table |>\n    separate(col = value,\n             into = c(\"Group\", \"Percentage\"), # Create column names\n             sep =  \"(?<=[[:alpha:]])\\\\s(?=[0-9])\") |> # Split into columns after string of letters\n    drop_na() |> # Remove NA rows\n    # Make percentage data numeric\n    mutate(Group = str_to_title(Group)) |>\n    mutate(Percentage = str_remove(string = Percentage,\n                                   pattern = \"\\\\.\")) |> # Remove decimal points from string\n    separate(Percentage, c(\"Percent\"), sep = \" \") |> # Separate by space\n    mutate(Percent = as.numeric(Percent)) |> # Convert to numeric\n    mutate(Percent = Percent * 0.1) |> # Add back decimal point\n    # Create Race_Ethnicity column\n    mutate(Race_Ethnicity = recode(Group, \n                                   # Create entries for All_races\n                                   \"United States\" = \"All_races\",\n                                   \"Female\" = \"All_races\",\n                                   \"Male\" = \"All_races\")) |>\n    # Create entries for all other races by importing data from group column and removing gender  \n    mutate(Race_Ethnicity = str_remove(string = Race_Ethnicity, \n                                       pattern = \" Female| Male\")) |> \n    # Create Gender column\n    mutate(Gender = str_extract(string = Group,\n                                pattern =\"Female|Male\")) |> # Extract gender from `Group` column\n    mutate(Gender = replace_na(Gender, replace = \"All\"))  # Assign `All' if gender was not in `Group`\n}\n\n# Create function to clean 2018 subgroup data \nclean_table_2018 <- function(table){\n  table |>\n    separate(col = value, \n             into = c(\"Group\", \"Percent\"), # Create column names\n             sep =  \"(?<=[[:alpha:]])\\\\s:\\\\s|\\\\s(?=[0-9])\") |>  # Split into columns after colon\n    mutate(Group = str_remove(string = Group, \n                            pattern = \":\")) |> # Remove colon\n    drop_na() |> # Remove NA rows\n    # Make percentage data numeric\n    mutate(Group = str_to_title(string = Group)) |> \n    mutate(Percent = str_remove(string = Percent, \n                               pattern = \"\\\\.\")) |> # Remove decimal points from string\n    mutate(Percent = as.numeric(Percent)) |> # Convert to numeric\n    mutate(Percent = Percent * 0.1) |> # Add back decimal point\n    # Create Race_Ethnicity column\n    mutate(Race_Ethnicity = str_replace(string = Group,\n                                        pattern = \"Men|Women\", \n                                        replacement = \"missing\")) |> # Replace gender with missing\n    mutate(Race_Ethnicity = na_if(Race_Ethnicity, \"missing\")) |> # Make missing values NA\n    fill(Race_Ethnicity, .direction = \"down\") |> # Fill Race/Ethnicity in missing fields\n    # Create Gender column\n    mutate(Gender = str_extract(string = Group, \n                                pattern = \"Men|Women\")) |> # Extract gender from `Group` column\n    mutate(Gender = replace_na(Gender, replace = \"All\")) # Assign `All' if gender was not in `Group`\n}\n\n\n\nAsian Subgroup Data\nWe ran our make_rows function on our raw data and combined the initially wrangled 2017 data.\n\n## Asian subgroup data\n# 2017 data\n# Apply make_rows function to subgroup data\nasian_sub_2017 <- make_rows(asian_sub_2017)\nasian_sub_2017_A <- make_rows(asian_sub_2017_A)\nasian_sub_2017_B <- make_rows(asian_sub_2017_B)\nasian_sub_2017_C <- make_rows(asian_sub_2017_C)\n\n# Combine data\nasian_sub_2017 <- bind_rows(asian_sub_2017_A, \n                            asian_sub_2017_B,\n                            asian_sub_2017_C)\n\nSimilarly, we applied make_rows to 2018 data.\n\n# 2018 data\n# Combine data\nasian_sub_2018 <- str_c(asian_sub_2018_A, asian_sub_2018_B)\n\n# Apply make_rows function\nasian_sub_2018 <- make_rows(asian_sub_2018)\n\nWe next cleaned both the 2017 and 2018 data using our clean_table and clean_table_2018 functions.\n\n# Apply clean table function to both years data\nasian_sub_2017 <- clean_table(asian_sub_2017)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [17, 22,\n28].\n\nasian_sub_2018 <- clean_table_2018(asian_sub_2018)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 6 rows [4, 8, 15,\n19, 21, 23].\n\n\nThe imported data was missing part of the data, so we manually added it back in as well as created a Year column.\n\n# Add missing data\nasian_sub_2018 <- asian_sub_2018 |>\n  add_row(Group = \"Asian\", Percent = 6.2,\n          Race_Ethnicity = \"Asian\", Gender = \"All\") |>\n  add_row(Group = \"Asian\", Percent = 6.4,\n          Race_Ethnicity = \"Asian\", Gender = \"Men\") |>\n  add_row(Group = \"Asian\", Percent = 6.1,\n          Race_Ethnicity = \"Asian\", Gender = \"Women\")\n\n# Add years to data\nasian_sub_2017 <- asian_sub_2017 |>\n  mutate(Year = 2017)\nasian_sub_2018 <- asian_sub_2018 |>\n  mutate(Year = 2018)\n\nSince the naming convention for males and females different across the 2017 and 2018 data, we standardized the Gender column to only contain Male and Female entries and then combined the data into one data set, and once again moved the years into rows and added a Percent column, just like our major group data.\n\n# Make Male/Female the values for gender across both datasets\nasian_sub_2018 <- asian_sub_2018 |> \n  mutate(across(.cols = c(Gender, Group),\n               ~ str_replace(string = ., \n                             pattern = \"Men\", \n                             replacement = \"Male\")),\n         across(.cols = c(Gender, Group),\n               ~ str_replace(string = ., \n                             pattern = \"Women\", \n                             replacement = \"Female\")))\n\n# Combine 2017 and 2018 data\nasian_subgroups <- bind_rows(asian_sub_2017, asian_sub_2018)\n\n# Add missing categories\nasian_subgroups <- asian_subgroups |> \n  select(-Group) |>\n  pivot_wider(names_from = Year, \n              values_from = Percent) |>\n  pivot_longer(cols = -c(Race_Ethnicity, Gender),\n               names_to = \"Year\",\n               values_to= \"Percent\")\n\n\n\nLatinx Subgroup Data\nTo wrangle our Latinx subgroup data, we followed the same steps as our Asian subgroup data – applying make_rows, clean_table, and clean_table_2018 functions, standardizing the Gender naming convention, combining 2017 and 2018 data, and moving the years into rows while adding a Percent column. For the Latinx data, we also appropriately labeled the Puerto Rican, Dominican, Cuban groups and cleaned the names for Latino/Latina group.\n\n## Latinx subgroup data\n# 2017 data\n# Combine data\nlatinx_sub_2017 <- stringr::str_c(latinx_sub_2017_A,\n                                  latinx_sub_2017_B, \n                                  latinx_sub_2017_C)\n\n# Fix typo\nlatinx_sub_2017 <- latinx_sub_2017 |>\n  str_replace(pattern = \"DR, Cuban Female 15.7\\nPR\", # Identify typo\n              replacement = \"DR, Cuban Male 15.7\\nPR\") # Replace gender\n\n# Apply functions to Latinx data\nlatinx_sub_2017 <- make_rows(latinx_sub_2017)\nlatinx_sub_2017 <- clean_table(table = latinx_sub_2017)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [19].\n\n# 2018 data\n# Clean the data string\nlatinx_sub_2018 <- str_replace_all(string = latinx_sub_2018, \n                                  pattern = \"\\\\s:\\n{2}|\\n{2}\", #remove two newline characters\n                                  replacement = \" \")\n\n# Apply make_rows function\nlatinx_sub_2018 <- make_rows(latinx_sub_2018)\n\n# Apply clean table function\nlatinx_sub_2018 <- clean_table_2018(latinx_sub_2018)\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [12].\n\n# Create function to fix naming issues\nfix_latinx_naming <- function(table){\n  table |>\n    # Appropriately label Puerto Rican, Dominican, Cuban group\n  mutate(Group = str_replace(string = Group,\n                             pattern = \"Pr, Dr, Cuban\",\n                             replacement = \"Puerto Rican, Dominican, Cuban\"),\n          Race_Ethnicity = str_replace(string = Race_Ethnicity,\n                                       pattern = \"Pr, Dr, Cuban\",\n                                       replacement = \"Puerto Rican, Dominican, Cuban\"))\n}\n\n# Apply function to both data sets\nlatinx_sub_2017 <- fix_latinx_naming(latinx_sub_2017)\nlatinx_sub_2018 <- fix_latinx_naming(latinx_sub_2018)\n\n# Add missing data\nlatinx_sub_2018 <- latinx_sub_2018 |>\n  add_row(Group = \"Latinx\", Percent = 12.8,\n          Race_Ethnicity = \"Latinx\", Gender = \"All\") |>\n  add_row(Group = \"Latinx\", Percent = 12.3,\n          Race_Ethnicity = \"Latinx\", Gender = \"Men\") |>\n  add_row(Group = \"Latinx\", Percent = 13.3,\n          Race_Ethnicity = \"Latinx\", Gender = \"Women\")\n\n# Make Male/Female the values for gender across both datasets\nlatinx_sub_2018 <- latinx_sub_2018 |>\n  mutate(across(.cols = c(Gender, Group),\n                ~ str_replace(string = ., pattern = \"Men\", replacement = \"Male\")),\n         across(.cols = c(Gender, Group),\n                ~ str_replace(string = ., pattern = \"Women\", replacement = \"Female\")))\n\n# Add years to data\nlatinx_sub_2017 <- latinx_sub_2017 |>\n  mutate(Year = 2017)\nlatinx_sub_2018 <- latinx_sub_2018 |>\n  mutate(Year = 2018)\n\n# Combine 2017 and 2018 data\nlatinx_subgroups <- bind_rows(latinx_sub_2017, latinx_sub_2018)\n\n# Add missing categories\nlatinx_subgroups <- latinx_subgroups |>\n  select(-Group) |>\n  pivot_wider(names_from = Year,\n              values_from = Percent) |>\n  pivot_longer(cols = -c(Race_Ethnicity, Gender),\n               names_to =\"Year\" ,\n               values_to=\"Percent\")\n\n# Clean up Latinx group names\nlatinx_subgroups <- latinx_subgroups |>\n  # Convert Latino/Latina to Latinx\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latina\", \"Latinx\")) |>\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latino\", \"Latinx\")) |>\n  drop_na()\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\nProf Note\n\n\n\nNote that plots are clear, have titles, it’s clear what’s plotted, and there’s a corresponding interpretation guiding the reader. This is all good! These could be improved by having more informative titles that make clear to the viewer what the take-home message is and by handling years as factors.\n\n\nTo get a glance of our overall data set, we plotted overall youth disconnection over time in the United States.\n\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |>\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n  labs(\n    title = \"Youth Disconnection over Time\",\n    x = \"Year\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nWe are able to visualize that from 2008 to 2010 there was an increase in youth disconnect; however, after 2010, there seems to be a consistent downtrend in youth disconnect.\nTo account for differences in gender, we plotted overall youth disconnection over time by gender.\n\nmajor_groups_long |>\n  filter(Gender != \"All\", Race_Ethnicity == \"All_races\") |>\n  ggplot(aes(x = Year, y = Percent, color = Gender)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n    scale_color_viridis_d() +\n  labs(\n    title = \"Youth Disconnection over Time by Gender\"\n  )\n\n\n\n\nInitially in 2008, females faced more disconnect than males. However, throughout the next years, males consistently faced greater disconnect that females. Both male and female disconnect trended downwards 2010 and after.\nWe then separated major groups by race/ethnicity and visualized youth disconnect over time.\n\nmajor_groups_long |>\n  filter(Gender == \"All\", Group != \"United States\") |>\n  ggplot(aes(x = Year, y = Percent, color = Race_Ethnicity)) +\n    geom_line(size = 0.5) +\n    geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\"\n  )\n\n\n\n\nWe can see that Native Americans faced the most youth disconnect at about 25% and Asian faced the least with about 7.5% throughout all years in our data set. Overall, there is a small downtrend throughout the pass years in all races/ethnicities 2010 and after.\nWe visualized male/female youth disconnection by race/ethnicity across the observed years.\n\nmajor_groups_long |>\n  filter(Gender != \"All\") |> # Filter for only Male and Female\n  # Combine Latino and Latina into Latinx\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latina \", \"Latinx\")) |>\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"Latino \", \"Latinx\")) |>\n  #renaming all races column\n  mutate(Race_Ethnicity = replace(Race_Ethnicity, Race_Ethnicity == \"All_races\", \"All Races\")) |>\n  # Plot scatter line plot of Percent vs Year by Gender and Ethnicity\n  ggplot(aes(x = Year, y = Percent, color = Gender)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n  facet_wrap(~Race_Ethnicity, nrow = 2) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Youth Disconnection over Time by Race/Ethnicity and Gender\"\n  )\n\n\n\n\nGender does not seem to affect the percentage much except in Black, Latinx, and Native American groups where there is a marginal difference. However, the main indicator in determining youth disconnection seems to be stronger aligned to Race/Ethnicity as that is where we see the largest disparity, while gender has smaller differences within a group.\nTaking a deeper dive into the Latinx group, we visualized youth disconnection of the different subgroups within the Latinx group.\n\nlatinx_subgroups |>\n  filter(Gender == \"All\", Race_Ethnicity != \"Latinx\") |> # Filter by overall not specific gender\n  # Plot Percent vs Year by Ethnicity\n  ggplot(aes(x = as.numeric(Year), y = Percent, color = Race_Ethnicity)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection of Latinx Subgroups over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\",\n    x = \"Year\"\n  )\n\n\n\n\nMost groups have a relatively similar youth disconnection percentage, however, the South American group clearly has the lowest rate of youth disconnection at around 8% while Puerto Rico, Cuba, Dominican Republic have the highest with around 14.5% across 2017 and 2018.\nTaking a deeper dive into the Asian group, we visualized youth disconnection of the different subgroups within the Asian group.\n\nasian_subgroups |>\n  filter(Gender == \"All\", Race_Ethnicity != \"All_races\") |> # Filter by overall not specific gender\n  # Plot Percent vs Year by Ethnicity\n  ggplot(aes(x = as.numeric(Year), y = Percent, color = Race_Ethnicity)) +\n  geom_line(size = 0.5) +\n  geom_point(size = 3) +\n    scale_color_viridis_d() +\n    labs(\n    title = \"Youth Disconnection of Asian Subgroups over Time by Race/Ethnicity\",\n    color = \"Race/Ethnicity\",\n    x = \"Year\"\n  )\n\nWarning: Removed 3 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 3 rows containing missing values (`geom_point()`).\n\n\n\n\n\nMost groups are in the 4-7% range for youth disconnection percentage; however, China has the lowest at about 4% and Hmong and Cambodia have the highest at about 13% across 2017 and 2018.\n\n\nData Analysis\nWe graphed youth disconnection over time with a line of best-fit layered on top of it.\n\nmajor_groups_long |>\n  # Scatterplot with linear model for overall youth disconnection over time\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |>\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\") + \n    labs(\n    title = \"Youth Disconnection over Time\",\n    subtitle = \"Includes linear model and standard error\",\n    x = \"Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe line of best-fit exhibits a slight negative relationship, showing there is some correlation with youth disconnection and time; however, there is a decent amount of standard error throughout, which may be due to the 2008 data point defying the trend of the other data points.\nTo test our hypothesis that youth disconnection does not have a decreasing trend over time, we did a Mann-Kendall test for monotonicity.\n\n# M-K test\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\") |>\n  pull(Percent) |>\n  MannKendall() |>\n  tidy()\n\n# A tibble: 1 × 5\n  statistic p.value kendall_score denominator var_kendall_score\n      <dbl>   <dbl>         <dbl>       <dbl>             <dbl>\n1    -0.600   0.133            -9        15.0              28.3\n\n\nWe see there the p-value is 13.3% percent, which is not statistically significant at the traditional 5% level. This means our data is likely not monotonic and we would fail to reject our null hypothesis that youth disconnection is decreasing over time.\nKnowing that the Great Recession occurred in 2008 and could provide an anomaly in our data, we ran a linear model on the data for 2010 and after.\n\n# Scatterplot with linear model for overall youth disconnection over time after 2010\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\", Year != 2008) |>\n  ggplot(aes(x = Year, y = Percent)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Youth Disconnection over Time (2010 and after)\",\n    subtitle = \"Includes linear model and standard error\",\n    x = \"Year\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThis graph seems to show a stronger linear relationship between youth disconnection over time as there is a steeper slope and less standard error than our first linear model visualization.\nAgain, to test our hypothesis that youth disconnection does not have a decreasing trend over time for 2010 and after, we did another Mann-Kendall test.\n\n# M-K test for data after 2008\nmajor_groups_long |>\n  filter(Gender == \"All\", Race_Ethnicity == \"All_races\", Year != 2008) |>\n  pull(Percent) |>\n  MannKendall() |>\n  tidy()\n\n# A tibble: 1 × 5\n  statistic p.value kendall_score denominator var_kendall_score\n      <dbl>   <dbl>         <dbl>       <dbl>             <dbl>\n1        -1  0.0275           -10          10              16.7\n\n\nThis time, the p-value is 2.75% and significant at the 5% level. Thus, this data set is likely monotonic and we would reject the null hypothesis that there is no decreasing trend between youth disconnection and time.\n\n\n\n\n\n\nProf Note\n\n\n\nModel is not fully quantified nor explained in the context of the data. Be sure that if you’re stating numbers, you’re contextualizing them. Be sure if you’re stating things that are true, that you’re quantifying them when possible."
  },
  {
    "objectID": "content/cs/cs-example.html#resultsdiscussion",
    "href": "content/cs/cs-example.html#resultsdiscussion",
    "title": "CS: Example",
    "section": "Results/Discussion",
    "text": "Results/Discussion\nThrough our exploratory data analysis, we observed that there does seem to be a decreasing trend of youth disconnection in the United States over the time frame of our data with the 2008 data being a slight anomaly. The decreasing trend with the exception of the initial 2010 spike remained generally consistent regardless of separation by gender, race/ethnicity, and both as seen through out first four visualizations. Although the trend is fairly consistent, there are additional takeaways from our visualizations.\nRace/ethnicity is a major determinant in the magnitude of youth disconnection rates while gender is a much weaker determinant. Through our third visualization we see that the Native American group has a youth disconnection percentage of about 25%, which is over three time the lowest group, Asian, at about 7.5%. The Black group, although not as high as Native American, has a youth disconnection rate higher than the overall average across the years at about 20% versus 13%. Additionally, every group’s rate is 2-5% different from the nearest group with no two groups being similar to each other. This leads us to believe that although a decreasing trend is apparent across all groups, groups still have differences in how disconnected their youth are. Gender, on the other hand, does not show the same disparity. Our second visualization shows that males may be more disconnected, but when broken down into race/ethnicity in our fourth visualization we see the effect of gender depends greatly on the race/ethnic group. Females are more disconnected in Latinx and Asian groups, while the youth disconnection rate is fairly similar across gender overall and in the Asian and White group. The Black group has the most interesting data where males are fairly more disconnected than females but even in other groups such as Latinx and Native American where there is a difference in female and male disconnection, it is not a major difference.\nIn taking a deeper dive into the Latinx and Asian subgroups in visualizations five and six, it can be seen that there are not big differences in youth disconnection between subgroups except in two cases. The South American subgroup has a substantially lower rate at about 8% versus the 12-15% range that the other Latinx subgroups are in. Oppositely, Hmong and Cambodia groups have a much higher youth disconnection rate at about 13% versus its Asian peers in the 4-7% range.\nTo quantify our observations in our exploratory data analysis, we created a linear model for our major group data and performed a Mann-Kendall test. The linear model shows a decreasing trend in youth disconnection rate over time but it does not seem to be a very strong relationship and has relatively high standard error. When performing the Mann-Kendall test, the p-value was 13.3%, which is not statistically significant at the 5% level, implying that there is not a monotonic trend in the data. We decided to create a second linear model and Mann-Kendall test without the 2008 data since youth disconnection likely increased in 2010 due to the Great Recession that the United States faced. When doing this, a much stronger decreasing trend can be seen in the linear model and the Mann-Kendall test has a p-value of 2.75%, which is statistically significant at the 5%, implying that there is a monotonic trend in the data. In this case, a decreasing trend over time."
  },
  {
    "objectID": "content/cs/cs-example.html#conclusion",
    "href": "content/cs/cs-example.html#conclusion",
    "title": "CS: Example",
    "section": "Conclusion",
    "text": "Conclusion\nWhen taking account our entire data set, we cannot mathematically state that youth disconnection rates have changed over time, although visually and when excluding 2008 data, we can see that youth disconnection has been trending downwards starting in 2010 and after. This trend remains consistent across gender and race/ethnicity; however, there are still differences between groups in how disconnected are youth. Native American and Black are the most disconnected groups with Native American clearly have the highest youth disconnection rate. Although gender is not a major factor in determining youth disconnection in most cases, Black males are notably more disconnected than Black females. All other Latinx groups are more disconnected than the South American group and Hmong and Combodian groups are notably more disconnected than the other Asian groups. In conclusion, youth disconnection rates are experiencing a downward trend in 2010 and after and its magnitude is most heavily influenced by race/ethnicity in most cases."
  },
  {
    "objectID": "content/cs/cs01.html",
    "href": "content/cs/cs01.html",
    "title": "CS01: Right-To-Carry",
    "section": "",
    "text": "This is where you get to put together all you’ve learned so far this quarter into a full data science report! This report will include your analysis from top (the background and question) to bottom (your analysis, interpretation, and conclusions.)\nWe’ll be grading to see that you have: 1) all necessary code for each section of the project. 2) explanatory text that guides the reader from start to finish. 3) polished visualizations that allow the reader to both understand the data you’re working with an your conclusions.\nThis will be submitted and graded as a group. One submission per group."
  },
  {
    "objectID": "content/cs/cs01.html#getting-started",
    "href": "content/cs/cs01.html#getting-started",
    "title": "CS01: Right-To-Carry",
    "section": "Getting started",
    "text": "Getting started\nHere are the steps for getting started:\n\nThis will be completed in cs01 group repository that has been created for you and your group mates.\nMake any changes needed as outlined by the tasks you need to complete for the assignment\nPeriodically knit and commit changes (for example, once per each new part)1\nPush all your changes back to your GitHub repo\nThis case study will be graded from GitHub.\n\nYour final GitHub push prior to the deadline will be used for grading.\n\nImports\nYou are allowed to import whichever packages you like for this case study report."
  },
  {
    "objectID": "content/cs/cs01.html#case-study-report",
    "href": "content/cs/cs01.html#case-study-report",
    "title": "CS01: Right-To-Carry",
    "section": "Case Study Report",
    "text": "Case Study Report\nYour case study can be organized however you see best fit, but we’ll be looking for the following general sections:\n\nTitle\nAuthors\nBackground/Introduction\nQuestion(s)\nData Explanation\nData Import\nData Wrangling\nExploratory Data Analysis\nData Analysis\nResults\nDiscussion of results\nConclusion\n\nNow, you may want to combine some of these sections (i.e. include your results and discussion among your analysis code). That’s totally allowed, but we’ll be looking to see that your report includes sufficient information to understand what you did, why you did it, and what your results are.\n\nExtending the Analysis\nIn addition to getting the code presented in class working, adding explanatory text to your report, and generating polished visualizations, you and your group must “extend the analysis” presented in class in a meaningful way. Now “meaningful” is not a very-easily-measured term. A meaningful extension could be carrying out analysis to answer an additional sub-question beyond what was presented in class, or including a really extensive exploratory data analysis, or generating a really superb set of visualizations to convey your groups’ results, or finding a related dataset and incorporating it into your case study. To determine whether your extension is “meaningful,” you and your group should be able to answer “yes” to the question “Does our extension add something important to this report beyond what was presented in class?”\nThis extension should be included/weaved into your report, meaning it should only be “separated out” as its own section if it makes most sense for the story you’re telling."
  },
  {
    "objectID": "content/cs/cs01.html#group-feedback",
    "href": "content/cs/cs01.html#group-feedback",
    "title": "CS01: Right-To-Carry",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the case study to provide feedback about working with your group mates. This is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary."
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#agenda",
    "href": "content/labslides/03-lab-deck.html#agenda",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Agenda",
    "text": "Agenda\n\nTips:\n\nBriefly review a question regarding sorting.\nReview numeric vs categorical variable types.\n\nLab introduction:\n\nReview FiveThirtyEight article on college majors."
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#lab-intro",
    "href": "content/labslides/03-lab-deck.html#lab-intro",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Lab Intro",
    "text": "Lab Intro\n\nLab instructions posted on the course website.\nThe Economic Guide To Picking A College Major by Ben Casselman"
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#reminders",
    "href": "content/labslides/03-lab-deck.html#reminders",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Reminders",
    "text": "Reminders\n\nStart with library(tidyverse) (includes tidyr, readr, dplyr, etc.)\nClone using ‘SSH’ link from GitHub\nKnit to .html & push both .Rmd and .html to GitHub"
  },
  {
    "objectID": "content/labslides/03-lab-deck.html#tips",
    "href": "content/labslides/03-lab-deck.html#tips",
    "title": "Lab 03: Exploring & Visualizing Data",
    "section": "Tips",
    "text": "Tips\n\nBe ready to troubleshoot your document, since it will likely fail to knit on multiple occasions throughout the process. Read the error message carefully and take note of which line is preventing a successful knit.\nMake sure to keep track of your various chunks and to keep text and code in the right place.\nRemember that your R Markdown file is not aware of your project’s global environment and can only make use of variables, functions, etc. that you have loaded or defined in the document.\nRemind yourself how the pipe operator (|>) works.\nIf you’re unsure how a function works or what its arguments are, type ? in front of it and hit enter (?read_csv for instance). The “Help” tab will open and provide a summary of the function as well as some examples."
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#reminders",
    "href": "content/labslides/02-lab-deck.html#reminders",
    "title": "Lab 02: Wrangling",
    "section": "Reminders",
    "text": "Reminders\n\nStart with library(tidyverse) (includes tidyr, readr, dplyr, etc.)\nClone using ‘SSH’ link from GitHub\nKnit to .html & push both .Rmd and .html to GitHub"
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#starting-a-new-project",
    "href": "content/labslides/02-lab-deck.html#starting-a-new-project",
    "title": "Lab 02: Wrangling",
    "section": "Starting a new project",
    "text": "Starting a new project\n\nGo to Canvas to find the link for today’s lab: lab03-wi23.\nOn GitHub, click on the green Clone or download button, select use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nGo to RStudio on datahub. Create a New Project from Git Repo. You will need to click on the down arrow next to the New Project button to see this option.\nCopy and paste the URL of your assignment repo into the dialog box and hit OK.\nOpen the .Rmd file with your template in it. Be sure to update the author to your name."
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#agenda",
    "href": "content/labslides/02-lab-deck.html#agenda",
    "title": "Lab 02: Wrangling",
    "section": "Agenda",
    "text": "Agenda\n\nLab 02 intro and demos: Introduce the lab, and work through the first question as a class.\nOn your own: Work on the rest of the lab “on your own”, but feel free to check in with classmates as much as you like."
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#dplyr-review",
    "href": "content/labslides/02-lab-deck.html#dplyr-review",
    "title": "Lab 02: Wrangling",
    "section": "dplyr: Review",
    "text": "dplyr: Review\ndplyr provides a “Grammar of Data Manipulation” and is based on the concepts of functions as verbs that manipulate data frames.\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)"
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#the-data",
    "href": "content/labslides/02-lab-deck.html#the-data",
    "title": "Lab 02: Wrangling",
    "section": "The Data",
    "text": "The Data\n\nstorms |>\n  slice(1:20)\n\n# A tibble: 20 × 13\n   name   year month   day  hour   lat  long status        categ…¹  wind press…²\n   <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <fct>           <dbl> <int>   <int>\n 1 Amy    1975     6    27     0  27.5 -79   tropical dep…      NA    25    1013\n 2 Amy    1975     6    27     6  28.5 -79   tropical dep…      NA    25    1013\n 3 Amy    1975     6    27    12  29.5 -79   tropical dep…      NA    25    1013\n 4 Amy    1975     6    27    18  30.5 -79   tropical dep…      NA    25    1013\n 5 Amy    1975     6    28     0  31.5 -78.8 tropical dep…      NA    25    1012\n 6 Amy    1975     6    28     6  32.4 -78.7 tropical dep…      NA    25    1012\n 7 Amy    1975     6    28    12  33.3 -78   tropical dep…      NA    25    1011\n 8 Amy    1975     6    28    18  34   -77   tropical dep…      NA    30    1006\n 9 Amy    1975     6    29     0  34.4 -75.8 tropical sto…      NA    35    1004\n10 Amy    1975     6    29     6  34   -74.8 tropical sto…      NA    40    1002\n11 Amy    1975     6    29    12  33.8 -73.8 tropical sto…      NA    45    1000\n12 Amy    1975     6    29    18  33.8 -72.8 tropical sto…      NA    50     998\n13 Amy    1975     6    30     0  34.3 -71.6 tropical sto…      NA    50     998\n14 Amy    1975     6    30     6  35.6 -70.8 tropical sto…      NA    55     998\n15 Amy    1975     6    30    12  35.9 -70.5 tropical sto…      NA    60     987\n16 Amy    1975     6    30    18  36.2 -70.2 tropical sto…      NA    60     987\n17 Amy    1975     7     1     0  36.2 -69.8 tropical sto…      NA    60     984\n18 Amy    1975     7     1     6  36.2 -69.4 tropical sto…      NA    60     984\n19 Amy    1975     7     1    12  36.2 -68.3 tropical sto…      NA    60     984\n20 Amy    1975     7     1    18  36.7 -67.2 tropical sto…      NA    60     984\n# … with 2 more variables: tropicalstorm_force_diameter <int>,\n#   hurricane_force_diameter <int>, and abbreviated variable names ¹​category,\n#   ²​pressure"
  },
  {
    "objectID": "content/labslides/02-lab-deck.html#the-data-documentation",
    "href": "content/labslides/02-lab-deck.html#the-data-documentation",
    "title": "Lab 02: Wrangling",
    "section": "The Data: Documentation",
    "text": "The Data: Documentation\nFrom the console…\n\n?storms\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#agenda",
    "href": "content/labslides/05-lab-deck.html#agenda",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Agenda",
    "text": "Agenda\n\nDocumentation\nLab 05: Modelling course evaluations\nGetting started with lab\n\nReminder to complete mid-course survey"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#documentation",
    "href": "content/labslides/05-lab-deck.html#documentation",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Documentation",
    "text": "Documentation\nDemo on how to utilize/read/understand R Documentation"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#reminder-data-come-from",
    "href": "content/labslides/05-lab-deck.html#reminder-data-come-from",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Reminder: Data come from…",
    "text": "Reminder: Data come from…\n“Beauty in the classroom: instructors’ pulchritude and putative pedagogical productivity”\nDaniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005. http://www.sciencedirect.com/science/article/pii/S0272775704001165"
  },
  {
    "objectID": "content/labslides/05-lab-deck.html#some-notes-on-this-lab",
    "href": "content/labslides/05-lab-deck.html#some-notes-on-this-lab",
    "title": "Lab 05: Modelling course evals (Pt. 2)",
    "section": "Some notes on this lab",
    "text": "Some notes on this lab\n\nThis is an extension of lab04. It may be worth briefly looking over that lab and/or the answer key to get that fresh in your mind prior to beginning this lab.\nThere are three parts. Ideally, you’d get through Exercise 12. We’ll be looking to see you’ve fit and interpreted models with multiple predictors.\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#agenda",
    "href": "content/labslides/07-lab-deck.html#agenda",
    "title": "Lab 07: Logistic Regression",
    "section": "Agenda",
    "text": "Agenda\n\nLab 07: Modelling resumes\nGetting started with lab"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#data-come-from",
    "href": "content/labslides/07-lab-deck.html#data-come-from",
    "title": "Lab 07: Logistic Regression",
    "section": "Data come from…",
    "text": "Data come from…\n“Are Emily and Greg More Employable than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.”\nBertrand, Marianne, and Sendhil Mullainathan. 2003. https://doi.org/10.3386/w9873."
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#study",
    "href": "content/labslides/07-lab-deck.html#study",
    "title": "Lab 07: Logistic Regression",
    "section": "Study",
    "text": "Study\nGoal: understand the influence of race and gender on job application callback rates\n\nSpecs:\n\nTwo cities: Boston and Chicago\nTime: several months in 2001 and 2002\n\nPlan:\n\nResearchers generated resumes, randomizing years of experience and education details\nThen: randomly assigned a name to the resume that would communicate the applicant’s gender and race\n\n\nthey tested these names and removed those that did not suggest gender and race consistently\ni.e. Lakisha was a name that their survey indicated would be interpreted as a black woman, while Greg was a name that would generally be interpreted to be associated with a white male.”"
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#backwards-elimination-logistic-regression",
    "href": "content/labslides/07-lab-deck.html#backwards-elimination-logistic-regression",
    "title": "Lab 07: Logistic Regression",
    "section": "Backwards elimination (Logistic Regression)",
    "text": "Backwards elimination (Logistic Regression)\n\nStart with full model (including all candidate explanatory variables and all candidate interactions)\nRemove one variable at a time, and select the model with the lowest AIC\nContinue until AIC does not decrease\n\n\n\nYou do NOT have to include every model in your lab…just the final one you settle on."
  },
  {
    "objectID": "content/labslides/07-lab-deck.html#a-note-on-this-lab",
    "href": "content/labslides/07-lab-deck.html#a-note-on-this-lab",
    "title": "Lab 07: Logistic Regression",
    "section": "A note on this lab",
    "text": "A note on this lab\n\nThere are three parts. We’ll be grading to see that you’ve done some EDA and have fit and interpreted at least two models (single and multiple predictors model)\nIt’s OK if you don’t get to backwards elimination\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#agenda",
    "href": "content/labslides/01-lab-deck.html#agenda",
    "title": "Lab 01: Tooling",
    "section": "Agenda",
    "text": "Agenda\n\nLab structure: Lab structure overview.\nLab 01 intro and demos: Introduce the lab, and work through the first section as a class.\nOn your own: Work on the rest of the lab “on your own”, but feel free to check in with classmates as much as you like."
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#lab-strucuture-1",
    "href": "content/labslides/01-lab-deck.html#lab-strucuture-1",
    "title": "Lab 01: Tooling",
    "section": "Lab strucuture",
    "text": "Lab strucuture\n\n5-10 minute introduction (a bit longer today)\nUse the remaining time to work through the lab exercises and fill out your lab report\n\nSubmit: on your own\nWorking: always allowed to work together\n\nLab instructions posted on the course website on the left panel under “Labs”\n\nLet’s go find today’s lab!"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#tips",
    "href": "content/labslides/01-lab-deck.html#tips",
    "title": "Lab 01: Tooling",
    "section": "Tips",
    "text": "Tips\n\nYou do not have to finish the lab in class; you have until midnight to submit. But, you might choose to get through portions that you think will be challenging (which initially might be the coding component) in class when staff can help you on the spot, and leave the narrative writing until later.\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality lab.\nWhen working with others, do not split up lab among classmates, work on it together in its entirety.\nSometimes you may not finish the entire lab…and that’s ok! When this happens or you’re unsure about what you turn in, be sure to go back and check your thoughts/work against the posted answer key."
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#goals",
    "href": "content/labslides/01-lab-deck.html#goals",
    "title": "Lab 01: Tooling",
    "section": "Goals",
    "text": "Goals\n\nIntroduce you to Git and GitHub: collaboration and version control system that we will be using throughout the course\n\nGit is a version control system – like “Track Changes” features from Microsoft Word/Google Docs on steroids\nGitHub is the home for your git-based projects on the internet\nConnect your RStudio on datahub to your GitHub account\n\nIntroduce you to R and RStudio:\n\nR is the name of the programming language itself\nRStudio is a convenient interface"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#getting-started-github-datahub",
    "href": "content/labslides/01-lab-deck.html#getting-started-github-datahub",
    "title": "Lab 01: Tooling",
    "section": "Getting started: GitHub & datahub",
    "text": "Getting started: GitHub & datahub\nFirst, put away computers, and watch me do it:\n\nDemo of the process\nSteps are spelled out in the “GitHub Housekeeping” portion of the lab"
  },
  {
    "objectID": "content/labslides/01-lab-deck.html#getting-started-assignment-retrieval",
    "href": "content/labslides/01-lab-deck.html#getting-started-assignment-retrieval",
    "title": "Lab 01: Tooling",
    "section": "Getting started: Assignment Retrieval",
    "text": "Getting started: Assignment Retrieval\nFirst, put away computers, and watch me do it:\n\nClick on the assignment link on Canvas for today’s lab to create your GitHub repository (which we’ll refer to as “repo” going forward) for the lab. This repo contains a template you can build on to complete your lab.\nOn GitHub, accept the assignment. Click on the clipboard icon to copy the repo URL.\nGo to datahub. and open RStudio. Go to File > New Project… and select to create a New Project from Version Control. On the following menu, select Git.\nCopy and paste the URL of your assignment repo into the “Repository URL” dialog box.\nHit Create Project.\n\nNow it’s your turn! Place a green sticky on your laptop when you’re done with this part (you can continue if you like). Place a pink sticky if you have questions."
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html",
    "href": "content/lab-ans/lab03-viz-ans.html",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-lowest-unemployment-rate",
    "href": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-lowest-unemployment-rate",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\n\ncollege_recent_grads |>\n  arrange(unemployment_rate) |>\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# … with 163 more rows\n\n\nDisplay only 4 decimal places using mutate and round:\n\ncollege_recent_grads |>\n  arrange(unemployment_rate) |>\n  select(rank, major, unemployment_rate) |>\n  mutate(unemployment_rate = round(unemployment_rate, digits = 4))\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   <dbl> <chr>                                                  <dbl>\n 1    53 Mathematics And Computer Science                      0     \n 2    74 Military Technologies                                 0     \n 3    84 Botany                                                0     \n 4   113 Soil Science                                          0     \n 5   121 Educational Administration And Supervision            0     \n 6    15 Engineering Mechanics Physics And Science             0.0063\n 7    20 Court Reporting                                       0.0117\n 8   120 Mathematics Teacher Education                         0.0162\n 9     1 Petroleum Engineering                                 0.0184\n10    65 General Agriculture                                   0.0196\n# … with 163 more rows\n\n\nDisplay 2 scientific digits using options:\n\noptions(digits = 2)\n\n\nExercise 1\nWhich of these options, changing the input data or altering the number of digits displayed without touching the input data, is the better option? Explain your reasoning. Then, implement the option you chose.\nI prefer the options approach because it will enforce consistency each time we display variables, without having to make the call to round each time (without changing the underlying data)."
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-highest-percentage-of-women",
    "href": "content/lab-ans/lab03-viz-ans.html#which-major-has-the-highest-percentage-of-women",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\n\nExercise 2\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding head(3) at the end of the pipeline.\nThe sharewomen column lists the proportion of women in each major. So we can arrange our data in descending order using desc(sharewomen), then select the columns we want: major, total, and sharewomen. We then just display the top 3 majors using head(3).\n\ncollege_recent_grads |>\n  arrange(desc(sharewomen)) |>\n  select(major, total, sharewomen) |>\n  head(3)\n\n# A tibble: 3 × 3\n  major                                         total sharewomen\n  <chr>                                         <dbl>      <dbl>\n1 Early Childhood Education                     37589      0.969\n2 Communication Disorders Sciences And Services 38279      0.968\n3 Medical Assisting Services                    11123      0.928"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "content/lab-ans/lab03-viz-ans.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\n\nExercise 3\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\nThe mean is more affected by the presence of outliers and by the skew of the distribution. Thus, the presence of a single person with very high income can increase the mean substantively, such that it’s no longer a good impression of the overall distribution. In contrast, the median (or the “middle number”) tells us the income exactly in the middle of the distribution.\n\n\nExercise 4\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram(binwidth=5000)\n\n\n\n\nI ended up choosing binwidth=5000. When binwidth=1000, there were too many small differences in income that were thus not grouped together, and it was harder to see the overall shape of the distribution.\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads |>\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl> <dbl>\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\n\nExercise 5\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nThe underlying distribution of median incomes is somewhat right-skewed, with at least 1-2 majors making a lot of money. Because of this, I’m going to use the median median income.\nIt would be probably be fine to use the mean median income as well—judging by the distribution of median incomes in each major_category, which are relatively normal-ish.\n\n\nExercise 6\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram(binwidth = 5000) +\n  facet_wrap( ~ major_category, ncol = 4)\n\n\n\n\n\n\nExercise 7\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Use the partial code below, filling it in with the appropriate statistic and function. Also note that we are looking for the highest statistic, so make sure to arrange in the correct direction.\n\ncollege_recent_grads |>\n  group_by(major_category) |>\n  summarise(median = median(median)) |>\n  arrange(desc(median))\n\n# A tibble: 16 × 2\n   major_category                      median\n   <chr>                                <dbl>\n 1 Engineering                          57000\n 2 Computers & Mathematics              45000\n 3 Business                             40000\n 4 Physical Sciences                    39500\n 5 Social Science                       38000\n 6 Biology & Life Science               36300\n 7 Law & Public Policy                  36000\n 8 Agriculture & Natural Resources      35000\n 9 Communications & Journalism          35000\n10 Health                               35000\n11 Industrial Arts & Consumer Services  35000\n12 Interdisciplinary                    35000\n13 Education                            32750\n14 Humanities & Liberal Arts            32000\n15 Arts                                 30750\n16 Psychology & Social Work             30000\n\n\nEngineering.\n\n\nExercise 8\nWhich major category is the least popular in this sample? To answer this question we use a new function called count, which first groups the data and then counts the number of observations in each category (see below). Add to the pipeline appropriately to arrange the results so that the major with the lowest observations is on top.\n\ncollege_recent_grads |>\n  count(major_category) |>\n  arrange(desc(-n))\n\n# A tibble: 16 × 2\n   major_category                          n\n   <chr>                               <int>\n 1 Interdisciplinary                       1\n 2 Communications & Journalism             4\n 3 Law & Public Policy                     5\n 4 Industrial Arts & Consumer Services     7\n 5 Arts                                    8\n 6 Psychology & Social Work                9\n 7 Social Science                          9\n 8 Agriculture & Natural Resources        10\n 9 Physical Sciences                      10\n10 Computers & Mathematics                11\n11 Health                                 12\n12 Business                               13\n13 Biology & Life Science                 14\n14 Humanities & Liberal Arts              15\n15 Education                              16\n16 Engineering                            29"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#all-stem-fields-arent-the-same",
    "href": "content/lab-ans/lab03-viz-ans.html#all-stem-fields-arent-the-same",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories <- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads <- college_recent_grads |>\n  mutate(major_type = case_when(\n    major_category %in% stem_categories ~ \"stem\",\n    TRUE ~ \"not stem\"\n  ))\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’s median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads |>\n  filter(\n    major_type == \"stem\",\n    median < median(college_recent_grads$median)\n  )\n\n# A tibble: 10 × 22\n    rank major_code major   major…¹  total sampl…²    men  women share…³ emplo…⁴\n   <dbl>      <dbl> <chr>   <chr>    <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl>\n 1    93       1301 Enviro… Biolog…  25965     225  10787  15178   0.585   20859\n 2    98       5098 Multi-… Physic…  62052     427  27015  35037   0.565   46138\n 3   102       3608 Physio… Biolog…  22060      99   8422  13638   0.618   14643\n 4   106       2001 Commun… Comput…  18035     208  11431   6604   0.366   14779\n 5   109       3611 Neuros… Biolog…  13663      53   4944   8719   0.638    9087\n 6   111       5002 Atmosp… Physic…   4043      32   2744   1299   0.321    3431\n 7   123       3699 Miscel… Biolog…  10706      63   4747   5959   0.557    7767\n 8   124       3600 Biology Biolog… 280709    1370 111762 168947   0.602  182295\n 9   133       3604 Ecology Biolog…   9154      86   3878   5276   0.576    7585\n10   169       3609 Zoology Biolog…   8409      47   3050   5359   0.637    6259\n# … with 12 more variables: employed_fulltime <dbl>, employed_parttime <dbl>,\n#   employed_fulltime_yearround <dbl>, unemployed <dbl>,\n#   unemployment_rate <dbl>, p25th <dbl>, median <dbl>, p75th <dbl>,\n#   college_jobs <dbl>, non_college_jobs <dbl>, low_wage_jobs <dbl>,\n#   major_type <chr>, and abbreviated variable names ¹​major_category,\n#   ²​sample_size, ³​sharewomen, ⁴​employed\n\n\n\nExercise 9\nWhich STEM majors have median salaries equal to or less than the median for all majors’s median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top.\n\ncollege_recent_grads |>\n  filter(\n    major_type == \"stem\",\n    median < median(college_recent_grads$median)\n  ) |>\n  select(major, median, p25th, p75th) |>\n  arrange(desc(median))\n\n# A tibble: 10 × 4\n   major                                 median p25th p75th\n   <chr>                                  <dbl> <dbl> <dbl>\n 1 Environmental Science                  35600 25000 40200\n 2 Multi-Disciplinary Or General Science  35000 24000 50000\n 3 Physiology                             35000 20000 50000\n 4 Communication Technologies             35000 25000 45000\n 5 Neuroscience                           35000 30000 44000\n 6 Atmospheric Sciences And Meteorology   35000 28000 50000\n 7 Miscellaneous Biology                  33500 23000 48000\n 8 Biology                                33400 24000 45000\n 9 Ecology                                33000 23000 42000\n10 Zoology                                26000 20000 39000"
  },
  {
    "objectID": "content/lab-ans/lab03-viz-ans.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "content/lab-ans/lab03-viz-ans.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "Lab 03 - Data Visualization (Ans)",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nExercise 10\nCreate a scatterplot of median income vs. proportion of women in that major, colored by whether the major is in a STEM field or not. Describe the association between these three variables.\n\ncollege_recent_grads |>\n  drop_na(sharewomen) |> ## This will drop rows for which sharewomen has NA value\n  ggplot(aes(x = median,\n             y = sharewomen,\n             color = major_type)) +\n  geom_point(alpha = .6) +\n  labs(x = \"Median income for major\",\n       y = \"Share of women in major\",\n       color = \"STEM major?\")\n\n\n\n\nIn general, there appears to be a negative relationship between median income and the proportion of women (sharewomen) in a major. Both variables are also correlated with major_type: stem majors tend to have a lower proportion of women (and lower median income), while not stem majors have a higher proportion (and higher median income).\n(Note that the negative relationship with median income is somewhat clearer if log(median) is used instead.)"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html",
    "href": "content/lab-ans/lab04-modelling-ans.html",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\nevals <- read_csv(\"data/evals-mod.csv\")"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-1",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-1",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 1",
    "text": "Part 1\n\nExercise 1\nFirst, we use rowwise to group evals by each row. We then use mutate to compute a new variable (bty_avg) corresponding to the average of the six beauty scores.\nFinally, we ungroup so that the dataframe isn’t still grouped by each row.\n\nevals <- evals |>\n  rowwise() |>\n  mutate(bty_avg = mean( c( bty_f1lower, bty_f1upper,\n                            bty_f2upper, bty_m1lower,\n                            bty_m1upper, bty_m2upper) )) |>\n  ungroup()"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-2",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-2",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 2",
    "text": "Part 2\n\nExercise 2\nThe distribution is slightly left-skewed, i.e., many scores are on the higher side (between 4-5) with a longer tail going to the left.\nThis is further corroborrated by the fact that the mean is lower than the median. Given that the mean is more affected by skew than the median, this is what we’d expect with negatively skewed data.\n\nevals |>\n  ggplot(aes(x = score))+\n  geom_histogram() +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nevals |>\n  summarise(mean = mean(score),\n            median = median(score),\n            sd = sd(score))\n\n# A tibble: 1 × 3\n   mean median    sd\n  <dbl>  <dbl> <dbl>\n1  4.17    4.3 0.544\n\n\n\n\nExercise 3\nIt seems like there’s a slight positive relationship between bty_avg and score, though it’s hard to tell given that many points overlap and form “bands” along particular values (e.g., many teachers have the same score).\n\nevals |>\n  ggplot(aes(x = bty_avg,\n             y = score))+\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\nExercise 4\nOne way to deal with the above is to add jitter to the plot, which reveals that certain clusters of points were either larger or smaller than initially thought. (Another apprpoach would be to change the alpha of geom_point, allowing us to detect denser clusters.)\n\nevals |>\n  ggplot(aes(x = bty_avg,\n             y = score))+\n  geom_jitter() +\n  theme_bw()"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-3",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-3",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 3",
    "text": "Part 3\n\nExercise 5\nAccording to the model, the linear equation would be as follows:\n\\(\\hat{y} = 3.88 + 0.07*X\\)\nWhere \\(X\\) is bty_avg.\nWe use broom::tidy to transform the model object into a tidy dataframe.\n\nm_bty <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg, data = evals)\n\nm_bty |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.88  \n2 bty_avg       0.0666\n\n\n\n\nExercise 6\n\nevals |>\n  ggplot(aes(x = bty_avg,\n             y = score))+\n  geom_point() +\n  geom_smooth(method = \"lm\", \n              color = \"orange\",\n              se = FALSE) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nExercise 7\nProfessors with higher average beauty ratings also tend to receive higher teaching scores.\nThe slope is approximately 0.07, which means that for every 1-unit increase in beauty rating (i.e., from 4 to 5), we can expect that a professor will receive .07 higher teaching scores (with a ~0.02 standard error).\n\nm_bty |>\n  tidy() |>\n  filter(term == \"bty_avg\") |>\n  select(term, estimate, std.error)\n\n# A tibble: 1 × 3\n  term    estimate std.error\n  <chr>      <dbl>     <dbl>\n1 bty_avg   0.0666    0.0163\n\n\n\n\nExercise 8\nThe intercept is 3.88. This is the estimated score for professors with a beauty score of 0, i.e., it is the value of \\(\\hat{y}\\) when the regression line crosses \\(x = 0\\).\nThis may or may not make sense; given that possible beauty scores ranged from 1-6, it is a little strange to imagine a scenario where the beauty score is 0 (and it would be even stranger to imagine scenarios where the beauty scores is negative).\nOn the other hand, it is not inconsistent with other features of the data: the mean score, for example, is 4.17, so it makes sense that the intercept of a linear model would be slightly lower than the mean, allowing the explanatory variable to account for increases in score throughout the data.\n\nm_bty |>\n  tidy() |>\n  filter(term == \"(Intercept)\") |>\n  select(term, estimate, std.error)\n\n# A tibble: 1 × 3\n  term        estimate std.error\n  <chr>          <dbl>     <dbl>\n1 (Intercept)     3.88    0.0761\n\n\n\n\nExercise 9\nHere, we identify the \\(R^2\\) of the model using glance, which identifies model-level characteristics.\nThe value is 0.0350, which means that our explanatory variable accounts for roughly 3.5% of the variance in our response variable.\n\nm_bty |>\n  glance() |>\n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1    0.0350"
  },
  {
    "objectID": "content/lab-ans/lab04-modelling-ans.html#part-4",
    "href": "content/lab-ans/lab04-modelling-ans.html#part-4",
    "title": "Lab 04 - Modelling (Ans)",
    "section": "Part 4",
    "text": "Part 4\n\nExercise 10\nThe linear equation would be as follows:\n\\(\\hat{y} = 4.09 + 0.142*X\\)\nWhere \\(X = 1\\) for male professors and \\(X = 0\\) for female professors. In other words, the intercept corresponds to the mean score for female professors, while the slope tells us the difference in mean between the female average and the male average.\n\nm_gen <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit( score ~ gender, data = evals)\n\nm_gen |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    4.09     0.0387    106.   0      \n2 gendermale     0.142    0.0508      2.78 0.00558\n\n\n\n\nExercise 11\nAs specified above, the slope coefficient for the \\(X\\) term signifies the difference in means between male and female professors.\n\n\nExercise 12\nHere, the model equation would look as follows:\n\\(\\hat{y} = 4.28 + -0.13*X_1 + -0.145*X_2\\)\nWhere \\(X_1 = 1\\) for tenure track professors and 0 for all else, and \\(X_2 = 1\\) for tenured professors and 0 for all else.\nIn other words: the intercept tells us the mean for teaching professors, and the two slope coefficients tell us the difference between that mean and the two other levels of rank.\n\ntable(evals$rank)\n\n\n    teaching tenure track      tenured \n         102          108          253 \n\nm_rank <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ rank, data = evals)\n\nm_rank |>\n  tidy()\n\n# A tibble: 3 × 5\n  term             estimate std.error statistic   p.value\n  <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)         4.28     0.0537     79.9  1.02e-271\n2 ranktenure track   -0.130    0.0748     -1.73 8.37e-  2\n3 ranktenured        -0.145    0.0636     -2.28 2.28e-  2\n\n\n\n\nExercise 13\nHere, we use the factor command (within mutate) to reorder the levels of rank such that tenure track is the first level.\n\nevals <- evals |>\n  mutate(rank_relevel = fct_relevel(factor(rank), c(\"tenure track\",\n                                                    \"teaching\",\n                                                    \"tenured\")))\n\n\n\nExercise 14\nThis model has the same structure as the previous model using rank, but it has reordered the levels of rank (for rank_relevel) such that tenure track is now first.\nThis means that the intercept should now be interpreted as the mean score for tenure track professors, and the coefficients represent the difference in means for teaching and tenured professors, respectively.\n\\(\\hat{y} = 4.15 + 0.13*X_1 + -0.0155*X_2\\)\nWhere \\(X_1 = 1\\) for teaching professors and 0 for all else; and \\(X_2 = 1\\) for tenured professors and 0 for all else.\nWe can compare tehse to the coefficients for the previous model, where teaching was the baseline: in both cases, we see that the difference between teaching and tenure track is 0.13 (just in opposite directions, depending on the baseline).\nThe \\(R^2\\) of the model is 0.0116 in both cases. In other words, the model explains approximately 1.16% of the variance in score.\n\nm_rank_relevel <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ rank_relevel, data = evals)\n\n\nm_rank_relevel |>\n  tidy()\n\n# A tibble: 3 × 5\n  term                 estimate std.error statistic   p.value\n  <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)            4.15      0.0521    79.7   2.58e-271\n2 rank_relevelteaching   0.130     0.0748     1.73  8.37e-  2\n3 rank_releveltenured   -0.0155    0.0623    -0.249 8.04e-  1\n\nm_rank_relevel |>\n  glance() |>\n  select(r.squared)\n\n# A tibble: 1 × 1\n  r.squared\n      <dbl>\n1    0.0116\n\n\n\n\nExercise 15\n\nevals <- evals |>\n  mutate(tenure_eligible = case_when(\n    rank == \"teaching\" ~ \"no\",\n    TRUE ~ \"yes\"\n  ))\n\n## Double check\ntable(evals$rank, evals$tenure_eligible)\n\n              \n                no yes\n  teaching     102   0\n  tenure track   0 108\n  tenured        0 253\n\n\n\n\nExercise 16\nThe linear equation is as follows:\n\\(\\hat{y} = 4.28 + -0.141*X_1\\)\nWhere \\(X_1 = 1\\) for tenure eligible professors and \\(X_1 = 0\\) for non-tenure-eligible professors.\nThus, this means that the mean score for non-tenure-eligible professors is 4.28, and professors that are tenure eligible can expect a score that’s -0.141 lower, on average.\n\nm_tenure_eligible <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ tenure_eligible, data = evals)\n\nm_tenure_eligible |>\n  tidy()\n\n# A tibble: 2 × 5\n  term               estimate std.error statistic   p.value\n  <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)           4.28     0.0536     79.9  2.72e-272\n2 tenure_eligibleyes   -0.141    0.0607     -2.32 2.10e-  2\n\n\nWe can also calculate the \\(R^2\\), and we see that it is 0.0115. In other words, the model explains approximately 1.15% of the variance in score.\n\nm_tenure_eligible |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0115 0.00935 0.541    5.36  0.0210     1  -372.  750.  762.    135.     461\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual"
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html",
    "href": "content/lab-ans/lab05-mlr-ans.html",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\n\n\n\n\n\n\n\nFirst, we read in the data from the appropriate directory.\nWe also recreate the bty_avg column as we did in Lab 4, which is just the mean of beauty ratings from each of the relevant groups.\n\nevals = read_csv(\"data/evals-mod.csv\")\nevals <- evals |>\n  rowwise() |>\n  mutate(bty_avg = mean( c( bty_f1lower, bty_f1upper,\n                            bty_f2upper, bty_m1lower,\n                            bty_m1upper, bty_m2upper) )) |>\n  ungroup()"
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html#part-1",
    "href": "content/lab-ans/lab05-mlr-ans.html#part-1",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "Part 1",
    "text": "Part 1\n\nExercise 2\nAs in Lab 4, we fit a model m_bty predicting score from bty_avg.\nThe linear equation would be written as follows:\n\\(\\hat{y} = 3.88 + 0.07*X\\)\nAdditionally, this model has an \\(R^2\\) of 0.035 and an adjusted \\(R^2\\) of 0.0329.\n\nm_bty <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg, data = evals)\n\nm_bty |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.88  \n2 bty_avg       0.0666\n\nm_bty |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0350  0.0329 0.535    16.7 5.08e-5     1  -366.  738.  751.    132.     461\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual"
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html#part-2",
    "href": "content/lab-ans/lab05-mlr-ans.html#part-2",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "Part 2",
    "text": "Part 2\n\nExercise 3\n\\(\\hat{y} = 3.75 + 0.07X_1 + 0.17X_2\\)\nWhere \\(X_1\\) is bty_avg and \\(X_2\\) corresponds to a binary variable that takes on the value of 0 for female professors and 1 for male professors.\nThis model has an \\(R^2\\) of 0.059 and an adjusted \\(R^2\\) of 0.055.\n\nm_bty_gen <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + gender, data = evals)\n\nm_bty_gen |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.75  \n2 bty_avg       0.0742\n3 gendermale    0.172 \n\nm_bty_gen |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1  0.0591  0.0550 0.529    14.5 8.18e-7     2  -360.  729.  745.    129.     460\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\n\nExercise 4\nThe intercept means: female teachers with an average beauty score of 0 are predicted to have a score of 3.75.\nThe slope for bty_avg means that for every 1-unit increase in bty_avg, teachers are predicted to have an increase in their score (relative to the intercept) of approximately 0.07. This is true for both male and female teachers, but of course, the predictions for male teachers are also affected by the slope of gender, which means: holding bty_avg constant, male teachers are predicted to have a higher score (relative to the intercept) by approximately 0.172.\n\nm_bty_gen |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.75  \n2 bty_avg       0.0742\n3 gendermale    0.172 \n\n\n\n\nExercise 5\nThis model has an \\(R^2\\) of 0.059, which means that the model explains about 5.9% of the varaince in score.\n\n\nExercise 6\nFor male professors only, the intercept term can be added to the slope coefficient for \\(X_2\\) (gender), giving us a simplified expression:\n\\(\\hat{y} = 3.92 + 0.07X_1\\)\n\n\nExercise 7\nHolding bty_avg constant, male professors are predicted to have a score that’s about 0.172 higher than female professors. Thus, according to the model, male professors with equivalent beauty ratings have higher scores.\n\n\nExercise 8\nThe adjusted \\(R^2\\) for the larger model (m_bty_gen) is 0.055, whereas it is 0.0329 for the smaller model (m_bty). The difference between these values is 0.0221.\nThis can be interpreted as follows: adding gender to a model explains an additional 2.21% of variance in score beyond the variance already explained by bty_avg.\n\n\nExercise 9\nThe slope for bty_avg in m_bty is 0.067. The slope for bty_avg in m_bty_gen is 0.074.\nThe short answer is yes: adding gender has changed the slope for bty_avg. Specifically, it has increased the steepness of this slope: the predicted increase in score for each 1-unit increase in bty_avg is higher in a model that’s also equipped with gender than in a model without gender.\n\nm_bty |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   3.88      0.0761     51.0  1.56e-191\n2 bty_avg       0.0666    0.0163      4.09 5.08e-  5\n\nm_bty_gen |>\n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   3.75      0.0847     44.3  6.23e-168\n2 bty_avg       0.0742    0.0163      4.56 6.48e-  6\n3 gendermale    0.172     0.0502      3.43 6.52e-  4\n\n\n\n\nExercise 10\n\nm_bty_rank <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + rank, data = evals)\n\nm_bty_rank |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 4 × 2\n  term             estimate\n  <chr>               <dbl>\n1 (Intercept)        3.98  \n2 bty_avg            0.0678\n3 ranktenure track  -0.161 \n4 ranktenured       -0.126 \n\n\n\\(\\hat{y} = 3.98 + 0.068X_1 - 0.16X_2 - 0.13X_3\\)\nWhere \\(X_1\\) is bty_avg, \\(X_2\\) = 1 for tenure track professors (and 0 for all else), and \\(X_3\\) = 1 for tenured professors (and 0 for all else).\nThe intercept can be interpreted as follows: teaching professors with a bty_avg of 0 are predicted to have a score of 3.98.\nFor each 1-unit increase in bty_avg, all professors should have an increase in score by 0.068. The critical difference is simply what else is added to this increase. For teaching professors, this increase is considered relative only to the intercept. However, tenure track professors are subject to a decrease of 0.16 in their predicted score, and tenured professors are also subject to a decrease of 0.13 in their predicted score."
  },
  {
    "objectID": "content/lab-ans/lab05-mlr-ans.html#part-3",
    "href": "content/lab-ans/lab05-mlr-ans.html#part-3",
    "title": "Lab05: Modelling course evaluations, Pt 2 - Answers",
    "section": "Part 3",
    "text": "Part 3\n\nExercise 11\nPersonally, I would expect either cls_did_eval or cls_perc_eval to be the worst predictors. All the other variables seem more likely to correlate with score, either because of potential societal biases (like age or gender), or because of how they might correlate with other factors that could make a class a better or worse experience for students, on average (e.g., cls_students).\nOf course, it’s possible that the proportion of students who fill out the survey (cls_perc_eval) is correlated with score: maybe people are more motivated to fill out the evaluations for really great professors. This suggests that cls_did_eval might be the worst predictor, since it won’t accurately reflect the proportion of students who chose to fill out, and it’s confounded with cls_students.\n\n\nExercise 12\nIt turns out I was wrong. The worst \\(R^2\\) values (and adjusted \\(R^2\\) values) are obtained from models with cls_profs (the number of professors teaching the class) or cls_students (the number of students).\nThe absolute worst model is cls_profs (with an \\(R^2\\) of 0.0006). In comparison, the model with cls_did_eval is about 6x better (with an \\(R^2\\) of 0.004). Neither is very good at all, but my prediction/intuition was incorrect.\nThe coefficient for cls_profs:single (i.e., classes with a single professor) is negative, but the standard error is almost 2x as larger as the coefficient, suggesting a lot of uncertainty about that estimate.\n\n## Fit with cls_students\nm_bty_cls_students <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_students, \n      data = evals)\n\nm_bty_cls_students |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma stati…¹ p.value    df logLik   AIC   BIC devia…²\n      <dbl>         <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1  0.000674      -0.00149 0.544   0.311   0.577     1  -374.  755.  767.    137.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​statistic, ²​deviance\n\n## Fit with cls_profs\nm_bty_cls_profs <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_profs, \n      data = evals)\n\nm_bty_cls_profs |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma stati…¹ p.value    df logLik   AIC   BIC devia…²\n      <dbl>         <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1  0.000649      -0.00152 0.544   0.299   0.585     1  -374.  755.  767.    137.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​statistic, ²​deviance\n\nm_bty_cls_profs |>\n  tidy()\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)       4.18      0.0311   134.      0    \n2 cls_profssingle  -0.0292    0.0534    -0.547   0.585\n\n## Fit model with the variable I predicted would be worst\nm_bty_cls_did_eval <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_did_eval, \n      data = evals)\n\nm_bty_cls_did_eval |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squ…¹ adj.r…² sigma stati…³ p.value    df logLik   AIC   BIC devia…⁴ df.re…⁵\n    <dbl>   <dbl> <dbl>   <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1 0.00395 0.00179 0.543    1.83   0.177     1  -374.  753.  766.    136.     461\n# … with 1 more variable: nobs <int>, and abbreviated variable names\n#   ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual\n\n\n\n\nExercise 13\nIf we’re already going to include cls_students (the number of students in a class) and cls_perc_eval (the percentage who filled out the evaluations), we wouldn’t need cls_did_eval, because that is simply the product of cls_students and cls_perc_eval.\nIncluding this parameter shouldn’t improve the fit of the model, and it’ll only negatively impact metrics like AIC or adjusted R-squared that take into account the number of parameters in the model.\n\n\nExercise 14\nHere, we fit a full model with every parameter but cls_did_eval.\nThe adjusted \\(R^2\\) is 0.141.\n\nm_bty_full <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + rank +\n        ethnicity + gender + language + age +\n        cls_perc_eval + cls_students + cls_level +\n        cls_profs + cls_credits + bty_avg, \n      data = evals)\n\nm_bty_full |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.164        0.141 0.504    7.33 2.40e-12    12  -333.  694.  752.    114.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n\n\n\nExercise 15\nNow, we fit a series of models, taking out one variable at a time.\nFor each model of size \\(k\\), we remove one of the original parameters and ask which parameter’s removal results in the highest adjusted \\(R^2\\).\nIn order of removal from the full model in Exercise 15:\n\ncls_profs (yields adjusted R^2 of 0.143).\n\ncls_level (yields adjusted R^2 of 0.1448).\n\nrank (yields adjusted R^2 of 0.145).\n\nAfter these three, there are no other parameters for which removing them improves model fit.\n\n## First reduced model, no cls_profs\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n\n[1] 0.1430708\n\n## Reduced model 2, no cls_level or cls_profs\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n\n[1] 0.1447586\n\n## Reduced model 3, no cls_level or cls_profs or rank\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        # rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n\n[1] 0.1453683\n\n\nThe final model is below, with the corresponding linear equation:\n\\(\\hat{y} = 3.39 + 0.062X_1 + 0.2X_2 + 0.18X_3 - 0.15X_4 - 0.005X_5 + 0.006X_6 + 0.0004X_7 + 0.52X_8\\)\nWhere:\n\n\\(X_1\\) is bty_avg\n\n\\(X_2\\) is ethnicity (corresponding to 1 for not minority professors)\n\n\\(X_3\\) is gender (corresponding to 1 for male professors)\n\n\\(X_4\\) is language (corresponding to 1 for non-english professors)\n\\(X_5\\) is age\n\n\\(X_6\\) is cls_perc_eval\n\n\\(X_7\\) is cls_students\n\n\\(X_8\\) is cls_credits (corresponding to 1 for one credit professors)\n\n\n## Final model.\nm_final <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        # rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals)\n\nm_final |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.160        0.145 0.503    10.8 5.46e-14     8  -334.  688.  730.    115.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\nm_final |>\n  tidy() |>\n  select(term, estimate)\n\n# A tibble: 9 × 2\n  term                   estimate\n  <chr>                     <dbl>\n1 (Intercept)            3.39    \n2 bty_avg                0.0619  \n3 ethnicitynot minority  0.204   \n4 gendermale             0.177   \n5 languagenon-english   -0.151   \n6 age                   -0.00487 \n7 cls_perc_eval          0.00575 \n8 cls_students           0.000407\n9 cls_creditsone credit  0.523   \n\n\nA shorter non-tidy approach…\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\n# fit model\nmod <- lm(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        cls_level +\n        cls_profs +\n        cls_credits,\n      data = evals)\n\n# determine which predictors to eliminate\nols_step_backward_p(mod)\n\n\n                           Elimination Summary                            \n-------------------------------------------------------------------------\n        Variable                   Adj.                                      \nStep     Removed     R-Square    R-Square     C(p)       AIC        RMSE     \n-------------------------------------------------------------------------\n   1    cls_profs      0.1635      0.1431    9.0279    692.3066    0.5035    \n   2    cls_level      0.1633      0.1448    7.1373    690.4192    0.5030    \n   3    rank           0.1602      0.1454    6.8068    688.1332    0.5028    \n-------------------------------------------------------------------------\n\n\n\n\nExercise 16\nThe variable cls_credits encodes whether a class is worth multiple credits or simply one credit. We see there is a positive coefficient, 0.523, corresponding to the predicted difference for one credit classes relative to multi credit. That is, all else held equal, professors of one credit classes are predicted to have a score 0.523 higher.\nThe variable cls_perc_eval encodes what proportion of students filled out the evaluation. The coefficient is 0.00575, meaning that for every percent increase in how many students filled out the evaluatino, the professor is predicted to have 0.00575 increase in their score.\n\n\nExercise 17\nA high score would be associated with a professor that is:\n\nmale\nnot a minority\nperceived as more attractive\nyounger\n\nreceived education at English-speaking school\n\nAdditionally, scores should be higher for classes for which:\n\na higher proportion of students filled out the evaluation\n\nthere are more students in the class\n\nis one credit (as opposed to multi credit)\n\n\n\nExercise 18\nThe original sample of evaluations was taken from classes/professors at University of Texas, Austin.\nAs always, there are constraints on generalizability. It is possible that the effects observed here are specific to either (or both) the professors or students who happen to attend UT Austin. For example, maybe at other universities, teacher evaluations are less impacted by perceived attractiveness of the professor—or maybe they are more impacted. In both cases, this parameter estimate might be a poor estimate (either over or underestimating the impact of perceived attractiveness on evaluations).\nOn the other hand, I don’t necessarily have a strong reason a priori to think that UT Austin students or professors are somehow outliers in the USA higher-education system. I would feel more comfortable generalizing if I had similar results from another US university, but as it stands, I would tentatively conclude that there’s evidence for the effects we observed, and suggest that the effect should be replicated in other universities."
  },
  {
    "objectID": "content/lab-ans/lab02-wrangling-ans.html",
    "href": "content/lab-ans/lab02-wrangling-ans.html",
    "title": "Lab 02 - Wrangling (Ans)",
    "section": "",
    "text": "Load packages\n\nlibrary(tidyverse) \n\n\n\nExercise 1\nHow many unique hurricanes are included in this dataset?\n(Note the specific value may differ based on the version of the dataset you’re using, but the code would not change.)\n\nn_unique <- storms |> \n  filter(status == \"hurricane\") |>\n  distinct(name, year, .keep_all = TRUE) |>\n  count() |>\n  pull(n)\n\n# OR\n\nstorms |> \n  filter(status == \"hurricane\") |>\n  group_by(year, name) |> \n  count() |>\n  nrow()\n\n[1] 310\n\n# ChatGPT answer\n# idea is correct; code is not\n# n_distinct(storms, name)\n\nThere are 310 unique hurricanes.\n(Note that we need to group by name and year, as certain storms have the same name…in different years.)\n\n\nExercise 2\nNote: If you used storms on datahub, the ts_diameter column has missing information and were likely unable to complete this question. Otherwise…this would have been the approach…\nWhich tropical storm affected the largest area experiencing tropical storm strength winds? And, what was the maximum sustained wind speed for that storm?\n\nstorms |> \n  filter(status == \"tropical storm\", \n         !is.na(ts_diameter)) |> \n  slice_max(ts_diameter)\n\nOR\n\nstorms |>\n  filter(status == \"tropical storm\",\n         !is.na(ts_diameter)) |> \n  filter(ts_diameter == max(ts_diameter, na.rm=TRUE))\n\n# ChatGPT answer is way off\n\nSandy (2012) had the largest area affected.\n\n\nExercise 3\nAmong all storms in this dataset, in which month are storms most common? Does this depend on the status of the storm? (In other words, are hurricanes more common in certain months than tropical depressions? or tropical storms?)\n\n# most common month\nstorms |> \n  distinct(name, year, .keep_all=TRUE) |>\n  group_by(month) |>\n  summarise(n = n()) |> # could alternatively use count() here\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   month     n\n   <dbl> <int>\n 1     9   208\n 2     8   173\n 3    10    99\n 4     7    67\n 5     6    41\n 6    11    29\n 7     5    13\n 8    12     5\n 9     1     2\n10     4     2\n\n# ChatGPT is close but missing uniqueness of the storm (would still get the correct answer)\n\nSeptember is the most common month.\n\n# depend on status?\nstorms |> \n  group_by(status, month) |>\n  summarise(n = n()) |> # could alternatively use count() here\n  slice_max(n)\n\n`summarise()` has grouped output by 'status'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 9 × 3\n# Groups:   status [9]\n  status                 month     n\n  <fct>                  <dbl> <int>\n1 disturbance                7    45\n2 extratropical              9   732\n3 hurricane                  9  2380\n4 other low                  9   446\n5 subtropical depression     8    36\n6 subtropical storm          9    72\n7 tropical depression        9  1315\n8 tropical storm             9  2448\n9 tropical wave              8    55\n\n\nIt does not depend on status. September is the most common for all three storm types.\n\n\nExercise 4\nYour boss asks for the name, year, and status of all category 5 storms that have happened in the 2000s. Carry out the operations that would deliver what they’re looking for.\n\nstorms |>\n  filter(category == 5,\n         between(year, 2000, 2009)) |>\n  select(name, year, status) |>\n  distinct(name, year, .keep_all=TRUE)\n\n# A tibble: 8 × 3\n  name     year status   \n  <chr>   <dbl> <fct>    \n1 Isabel   2003 hurricane\n2 Ivan     2004 hurricane\n3 Emily    2005 hurricane\n4 Katrina  2005 hurricane\n5 Rita     2005 hurricane\n6 Wilma    2005 hurricane\n7 Dean     2007 hurricane\n8 Felix    2007 hurricane\n\n# ChatGPT answer\n# incorrect b/c it mixes up status and category\n# also missing the equal to aspect for an inclusive decade\n# storms %>%\n#   filter(year >= 2000, year < 2010, status == \"5\") %>%\n#   select(name, year, status)\n\n\n\nExercise 5\nFilter these data to only include storms that occurred during your lifetime (your code and results may differ from your classmates!). Among storms that have occurred during your lifetime, what’s the mean and median air pressure across all measurements taken?\n\nmy_storms <- storms |>\n  filter(between(year, 1988, 2023)) # alternatively filter(year >= 1988)\n\nmy_storms |>\n  summarise(median_pressure = median(pressure),\n            mean_pressure = mean(pressure))\n\n# A tibble: 1 × 2\n  median_pressure mean_pressure\n            <dbl>         <dbl>\n1            1000          993.\n\n# ChatGPT did well here\n\n\nMedian: 999 millibars\nMean: 991 millibars\n\n\n\nExercise 6\nWhich decade (of the storms included in the dataset) had the largest number of unique reported storms?\n\nstorms |> \n  distinct(name, year) |>\n  mutate(decade = year - year %% 10) |> # there are MANY different ways to approach this!\n  group_by(decade) |>\n  count()\n\n# A tibble: 6 × 2\n# Groups:   decade [6]\n  decade     n\n   <dbl> <int>\n1   1970    40\n2   1980    90\n3   1990   127\n4   2000   169\n5   2010   163\n6   2020    50\n\n# ChatGPT answer\n# incorrect b/c of the distinct name/near piece missing\n# would get correct decade but not the correct answer\n# storms %>%\n#   mutate(decade = floor(year/10)*10) %>%\n#   group_by(decade) %>%\n#   summarise(unique_storms = n_distinct(name)) %>%\n#   arrange(desc(unique_storms)) %>%\n#   top_n(1)\n\nThe 2000s.\n(Note: we want to be sure to only count each storm once. Could also arrange by desc(n) to have 2000 at top.)\n\n\nExercise 7\nAmong the subset of storms occurring in your lifetime, which storm lasted the longest? Include your code and explain your answer.\n\nmy_storms |>  \n  group_by(name, year) |> \n  count() |> \n  arrange(desc(n))\n\n# A tibble: 532 × 3\n# Groups:   name, year [532]\n   name      year     n\n   <chr>    <dbl> <int>\n 1 Nadine    2012    96\n 2 Ivan      2004    94\n 3 Kyle      2002    90\n 4 Leslie    2018    89\n 5 Paulette  2020    88\n 6 Alberto   2000    87\n 7 Jose      2017    85\n 8 Nicholas  2003    80\n 9 Florence  2018    79\n10 Marilyn   1995    79\n# … with 522 more rows\n\n\nNadine lasted the longest (unless you were born after 2012).\n(Note: The logic here is that storms are reported every six hours, per the description of the dataset, so the storm that has the most rows/entries would have lasted the longest)"
  },
  {
    "objectID": "content/exams/midterm_wi23.html",
    "href": "content/exams/midterm_wi23.html",
    "title": "COGS 137 - Winter 2023 - Midterm",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-01.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Feb 13th to complete this exam and turn it in via your personal Github repo - late work will not be accepted. Technical difficulties are not an excuse for late work - do not wait until the last minute to knit / commit / push.\nThere will be no Campuswire posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please DM or email Prof Ellis as soon as possible.\nEach question requires a (brief) narrative as well as a (brief) description of your approach. You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4"
  },
  {
    "objectID": "content/exams/midterm_wi23.html#academic-integrity",
    "href": "content/exams/midterm_wi23.html#academic-integrity",
    "title": "COGS 137 - Winter 2023 - Midterm",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nBe sure to complete the AI statement in the exam you submit itself.\nA note on sharing / reusing code: I am well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden."
  },
  {
    "objectID": "content/exams/midterm_wi23.html#grading-and-feedback",
    "href": "content/exams/midterm_wi23.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Midterm",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThis exam is worth 14% of your grade. You will be graded on the correctness of your code, correctness of your answers, the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization but the template gets you started on a well-organized exam. We should be able to easily navigate your midterm to find what we’re looking for.) Organization + Clarity in written communication - 1pt"
  },
  {
    "objectID": "content/exams/midterm_wi23.html#logistics",
    "href": "content/exams/midterm_wi23.html#logistics",
    "title": "COGS 137 - Winter 2023 - Midterm",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called midterm-01.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/midterm_wi23.html#packages",
    "href": "content/exams/midterm_wi23.html#packages",
    "title": "COGS 137 - Winter 2023 - Midterm",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these package has been installed, but you will need to load them. You are allowed, but not required, to use additional packages."
  },
  {
    "objectID": "content/exams/midterm_wi23.html#the-data",
    "href": "content/exams/midterm_wi23.html#the-data",
    "title": "COGS 137 - Winter 2023 - Midterm",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Axios and Harris Poll and have been provided by the TidyTuesday team.\nThe data are stored in two different files in the data/ folder: poll.csv and reputation.csv. You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, these two files include data about the 100 “most visible” brands in America. Specifically, reputation.csv includes information from the 2022 poll about these 100 stores across different reputation categories. poll.csv includes information about the same 100 stores but includes information about their rankings across multiple years."
  },
  {
    "objectID": "content/exams/midterm_wi23.html#questions",
    "href": "content/exams/midterm_wi23.html#questions",
    "title": "COGS 137 - Winter 2023 - Midterm",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nHow many different industries (industry variable) are represented in these data?\n\n\nQuestion 2 (0.5 points)\n\nWhich company had the lowest overall ranking in 2022?\nAnd for which category (from the name variable) did this organization score lowest?\n\n\n\nQuestion 3 (1 point)\nWhich company in the reputation.csv dataset has the “best” average (mean) rank across all seven categories?\n\n\nQuestion 4 (1 point)\nWhich company had the biggest increase in rank from 2021 to 2022?\n\n\nQuestion 5 (1.5 points)\nFor the industry with only a single “most visible” company in the dataset, has their RQ score been increasing or decreasing overall since 2017?\n\n\nQuestion 6 (2 points)\nHow many companies from each industry category are represented in the 2022 ‘100 Most Visible’ companies in America data? Generate a visualization to display the answer to this question. Be sure to follow best visualization practices discussed in class.\n\n\nQuestion 7 (2 points)\nOf industries that have at least 5 companies in the dataset, which industry has the highest median 2022 rank? Generate a visualization that allows you to answer this question. Be sure to follow best practices.\n\n\nQuestion 8 (2 points)\nYour boss is curious about how much rankings change from one year to the next. To answer this question, they ask you to determine how well 2021 rankings explain the following year’s 2022 rankings. Generate a linear model to answer this question. Be sure to include your interpretation of the model (in other words your answer to the question “how well do 2021 rankings explain 2022’s rankings?”)\n\n\nQuestion 9 (2.5 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be."
  },
  {
    "objectID": "content/exams/practice-exam.html",
    "href": "content/exams/practice-exam.html",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal.\n\n\n\n\n\n\nNote\n\n\n\nThis is the midterm from the last time this course was offered. Linear regression was not covered prior to the midterm the last time this course was offered. There will be a question or two on linear regression and the interpretation of linear models on this year’s midterm."
  },
  {
    "objectID": "content/exams/practice-exam.html#academic-integrity-statement",
    "href": "content/exams/practice-exam.html#academic-integrity-statement",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nI, ____________, hereby state that I have not communicated with or gained information in any way from my classmates or anyone during this exam, and that all work is my own.\nA note on sharing / reusing code: We are well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden."
  },
  {
    "objectID": "content/exams/practice-exam.html#getting-help",
    "href": "content/exams/practice-exam.html#getting-help",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "Getting help",
    "text": "Getting help\nBecause we cannot be available to all students across the entire length of the (real) exam, there will be no questions of instructional staff about the exam. If you find wording confusing or are unsure, note that in your answer and explain how you interpreted it. This will be taken into consideration during grading. If you are having technical difficulties or think there is an error on the exam, DM or email Prof Ellis immediately and she’ll work with you as soon as possible."
  },
  {
    "objectID": "content/exams/practice-exam.html#grading-and-feedback",
    "href": "content/exams/practice-exam.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThe (real) exam is worth 16% of your grade. You will be graded on the correctness of your code, correctness of your answers (often there are multiple “correct” answers, by design), the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization, but we should be able to easily navigate your midterm to find what we’re looking for.)"
  },
  {
    "objectID": "content/exams/practice-exam.html#logistics",
    "href": "content/exams/practice-exam.html#logistics",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/practice-exam.html#packages",
    "href": "content/exams/practice-exam.html#packages",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse package for this (practice) midterm. (For the real deal, you’ll need tidyverse and tidymodels.) If working on datahub, this package has been installed, but you will need to load it. No other packages are required, but if for some reason you want to load in another package, you are permitted to do so."
  },
  {
    "objectID": "content/exams/practice-exam.html#the-data",
    "href": "content/exams/practice-exam.html#the-data",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "The data",
    "text": "The data\nThe dataset you’ll be working with on this practice midterm is all about beach volleyball. The full dataset is explained in detail here and includes match-level data from 76,756 volleyball matches. You should click on that link to see what information is stored in each column in this dataset and what information is included in each column.\nBriefly, what you’ll use for this midterm is a subset of the full dataset, including only the 11,699 observations (rows) from 2018 and 2019 but all of the original columns. Each row summarizes the results from a single, distinct match played in a volleyball tournament.\nTo briefly describe beach volleyball, it is a sport played 2 on 2, so each match involves only 4 players. These data include matches from two different volleyball circuits, the international FIVB and the US-centric AVP. You will not need to know much at all about this sport to complete this midterm, and anything you need to know will be explained.\nThe data are stored in data/vb_matches.csv. You’ll need to read the dataset in prior to answering any questions on the midterm."
  },
  {
    "objectID": "content/exams/practice-exam.html#questions",
    "href": "content/exams/practice-exam.html#questions",
    "title": "COGS 137 - Winter 2023 - Practice Midterm",
    "section": "Questions",
    "text": "Questions\nQuestion 1 (0.75 points) - How many FIVB and AVP matches are included in this dataset?\nQuestion 2 (0.75 points) - Find the match with the longest duration.\na.  Where was this tournament played (City & Country)?\nb.  How long did the match last?\nc.  Who were the two winners? </br>\nQuestion 3 (1.5 points) - Across all tournaments included in this dataset, which teams have won the most tournaments? Your response should include both the winning players, their gender, and the number of tournaments they’ve won in descending order. Who has the most wins? How many men’s and how many women’s teams are in the top 10? Note: “winning a tournament” is indicated by winning either a “Gold Medal” (FIVB) or “Finals” (AVP) match, specified in the bracket column.\nQuestion 4 (1.5 points) - Of only the AVP tournaments included in this dataset, how many different cities hosted tournaments in 2018 and 2019? And, which cities (if any) hosted a tournament in both 2018 and 2019? Note that tournaments are named for the city hosting the tournament.\nQuestion 5 (2.5 points) - Prof Ellis plays a lot of women’s beach volleyball and is only 5’5” (65 inches). Despite not having the sheer talent or raw athletic ability to make it as a professional volleyball player, she wonders if she ever had a chance at her height. (Reminder: there are 4 players in each match whose height should be considered.) To help her out, answer each of the following:\na.  Who was the shortest women's player to compete in a tournament in 2018/2019?\nb.  How tall are they?\nc.  Did they *win* a tournament in 2018 or 2019? </br>\nQuestion 6 (3 points) - Which country has hosted the most FIVB tournaments? Did this differ by year? Generate a visualization that shows how many FIVB tournaments each country hosted. Allow viewer to visualize this by year. And, be sure each tournament is only counted once (regardless of how many games were played).\nQuestion 7 (3 points) - Recreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be. (Hint: The visualization uses the variable avg_team_height, which is not included in the provided data frame. You will have to create avg_team_height yourself, by determining the average (mean) team height for each winning team.)\n\n\n\n\n\n\n\nNote\n\n\n\nQ7 had a typo when this course was offered previously leading to students spending wayyyyy longer than intended on this exam. That typo has been fixed for this practice midterm.\n\n\nQuestion 8 (1 pts) - If you were in charge of designing the plot you just recreated in the plot above, what changes would you make to improve its effectiveness as a visualization? (You do not have to write any code for this question, just explain the different design/viz choices you would make.)"
  },
  {
    "objectID": "content/exams/midterm_wi23-ans.html",
    "href": "content/exams/midterm_wi23-ans.html",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "",
    "text": "Your solutions must be written up in the R Markdown (Rmd) file called midterm-01.Rmd. This file must include your code and write up (written explanation) for each task.\nBe sure to knit your file to HTML prior to submission and include both the .Rmd and .html files on GitHub. Your “submission” will be whatever is in your exam repository at the deadline.\nIf you cannot figure out the code for a question and this is causing you to not be able to knit your file, set the code chunk to eval = FALSE (but leave your code there - chance for partial credit!) and then knit.\nThis exam is open book, open internet, closed other people. You may use any online or book-based resource you would like, but you must include citations for any code that you use. You may not consult with anyone else about this exam, including any other humans on the internet or one another.\nYou have until 11:59pm on Monday, Feb 13th to complete this exam and turn it in via your personal Github repo - late work will not be accepted. Technical difficulties are not an excuse for late work - do not wait until the last minute to knit / commit / push.\nThere will be no Campuswire posts about questions on the exam. If you are unsure of something, include a note in your exam. We’ll consider this in grading. However, if you think there is a mistake in the exam or are having technical issues, please DM or email Prof Ellis as soon as possible.\nEach question requires a (brief) narrative as well as a (brief) description of your approach. You can use comments in your code, but do not extensively count on these. I should be able to suppress all the code in your document and still be able to read and make sense of your answers to the questions.\nEven if the answer seems obvious from the R output, make sure to state it in your narrative as well. For example, if the question is asking what is 2 + 2, and you have the following in your document, you should additionally have a sentence that states “2 + 2 is 4.” You just want us to be clear that you know the answer to the question.\n\n2 + 2\n# 4"
  },
  {
    "objectID": "content/exams/midterm_wi23-ans.html#academic-integrity-statement",
    "href": "content/exams/midterm_wi23-ans.html#academic-integrity-statement",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nI, ____________, hereby state that I have not communicated with or gained information in any way from my classmates or anyone during this exam, and that all work is my own.\nA note on sharing / reusing code: I am well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden."
  },
  {
    "objectID": "content/exams/midterm_wi23-ans.html#grading-and-feedback",
    "href": "content/exams/midterm_wi23-ans.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThis exam is worth 14% of your grade. You will be graded on the correctness of your code, correctness of your answers, the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization but the template gets you started on a well-organized exam. We should be able to easily navigate your midterm to find what we’re looking for.) Organization + Clarity in written communication - 1pt"
  },
  {
    "objectID": "content/exams/midterm_wi23-ans.html#logistics",
    "href": "content/exams/midterm_wi23-ans.html#logistics",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called midterm-01.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/midterm_wi23-ans.html#packages",
    "href": "content/exams/midterm_wi23-ans.html#packages",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse and tidymodels packages for this midterm. If working on datahub, these package has been installed, but you will need to load them. You are allowed, but not required, to use additional packages.\n\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/exams/midterm_wi23-ans.html#the-data",
    "href": "content/exams/midterm_wi23-ans.html#the-data",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "The data",
    "text": "The data\nThe data we’ll be using come from The Axios and Harris Poll and have been provided by the TidyTuesday team.\nThe data are stored in two different files in the data/ folder: poll.csv and reputation.csv. You’ll want to read each table in and understand what each variable represents prior to completing the exam.\nEach variable and the data overall are described in detail here. You should click on that link to see what information is stored in each column in the datasets. But briefly, these two files include data about the 100 “most visible” brands in America. Specifically, reputation.csv includes information from the 2022 poll about these 100 stores across different reputation categories. poll.csv includes information about the same 100 stores but includes information about their rankings across multiple years.\n\npoll <- read_csv(\"data/poll.csv\")\nreputation <- read_csv(\"data/reputation.csv\")"
  },
  {
    "objectID": "content/exams/midterm_wi23-ans.html#questions",
    "href": "content/exams/midterm_wi23-ans.html#questions",
    "title": "COGS 137 - Winter 2023 - Midterm (Answer Key)",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1 (0.5 points)\nHow many different industries (industry variable) are represented in these data?\n\n# either df can be used here\npoll |> \n  distinct(industry) # |>\n\n# A tibble: 19 × 1\n   industry          \n   <chr>             \n 1 Retail            \n 2 Food & Beverage   \n 3 Groceries         \n 4 Tech              \n 5 Ecommerce         \n 6 Automotive        \n 7 Healthcare        \n 8 Other             \n 9 Logistics         \n10 Financial Services\n11 Industrial        \n12 Consumer Goods    \n13 Pharma            \n14 Telecom           \n15 Insurance         \n16 Media             \n17 Energy            \n18 Airline           \n19 Food Delivery     \n\n  # count() # can optionally include count()\n\n# OR \nn_distinct(poll$industry)\n\n[1] 19\n\n\nThere are 19 different industries represented.\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here.\n\n\n\n\nQuestion 2 (0.5 points)\n\nWhich company had the lowest overall ranking in 2022?\nAnd for which category (from the name variable) did this organization score lowest?\n\n\nreputation |>\n  filter(rank == max(rank)) |> # could also hard code rank == 100 as you know 100 companies included in dataset\n  arrange(score)\n\n# A tibble: 7 × 5\n  company                industry name        score  rank\n  <chr>                  <chr>    <chr>       <dbl> <dbl>\n1 The Trump Organization Other    ETHICS       51.2   100\n2 The Trump Organization Other    TRUST        52.9   100\n3 The Trump Organization Other    CULTURE      53.0   100\n4 The Trump Organization Other    CITIZENSHIP  53.6   100\n5 The Trump Organization Other    GROWTH       55.1   100\n6 The Trump Organization Other    P&S          55.7   100\n7 The Trump Organization Other    VISION       59.4   100\n\n# ChatGPT used a really circuitous approach using baseR indexing/filtering that would not work for this dataset\n\n\nThe Trump Organization\nEthics\n\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here. Most common error was miscalculating the score within category, due to misunderstanding the structure of the data.\n\n\n\n\nQuestion 3 (1 point)\nWhich company in the reputation.csv dataset has the “best” average (mean) rank across all seven categories?\n\nreputation |> \n  group_by(company) |> \n  summarize(avg_rank = mean(rank)) |> \n  arrange(avg_rank)\n\n# A tibble: 100 × 2\n   company                  avg_rank\n   <chr>                       <dbl>\n 1 Trader Joe's                 4.86\n 2 The Hershey Company          5   \n 3 Patagonia                    6.71\n 4 HEB Grocery                  7   \n 5 Samsung                      8.86\n 6 Wegmans                     10.4 \n 7 Amazon.com                  12.1 \n 8 Toyota Motor Corporation    12.4 \n 9 Honda Motor Company         13.1 \n10 Microsoft                   13.1 \n# … with 90 more rows\n\n\nTrader Joe’s has the highest average ranking.\n\n\n\n\n\n\nGrader Note\n\n\n\nStudents largely did well here. Most common error was misreading the question.\n\n\n\n\nQuestion 4 (1 point)\nWhich company had the biggest increase in rank from 2021 to 2022?\n\npoll |> \n  filter(year==2021) |> \n  arrange(desc(change))\n\n# A tibble: 100 × 8\n   company               industry       2022_…¹ 2022_…² change  year  rank    rq\n   <chr>                 <chr>            <dbl>   <dbl>  <dbl> <dbl> <dbl> <dbl>\n 1 The Home Depot        Retail              16    78.9     29  2021    45  75.4\n 2 Google                Tech                31    77.8     29  2021    60  73.3\n 3 IBM                   Tech                11    79.5     28  2021    39  76.3\n 4 Samsung               Tech                 6    80.5     25  2021    31  77.5\n 5 Sony                  Tech                10    79.6     24  2021    34  77.3\n 6 Starbucks Corporation Food & Bevera…      43    76.6     22  2021    65  72.3\n 7 Microsoft             Tech                15    79       21  2021    36  76.8\n 8 Adidas                Retail              29    77.9     20  2021    49  75.1\n 9 General Motors        Automotive          51    75.4     17  2021    68  72  \n10 Yum! Brands           Food & Bevera…      53    75.3     17  2021    70  71.5\n# … with 90 more rows, and abbreviated variable names ¹​`2022_rank`, ²​`2022_rq`\n\n# if \"change\" column not noticed \npoll |> \n  filter(year==2021) |> \n  mutate(rank_diff = rank - `2022_rank`) |> \n  arrange(desc(rank_diff))\n\n# A tibble: 100 × 9\n   company              indus…¹ 2022_…² 2022_…³ change  year  rank    rq rank_…⁴\n   <chr>                <chr>     <dbl>   <dbl>  <dbl> <dbl> <dbl> <dbl>   <dbl>\n 1 The Home Depot       Retail       16    78.9     29  2021    45  75.4      29\n 2 Google               Tech         31    77.8     29  2021    60  73.3      29\n 3 IBM                  Tech         11    79.5     28  2021    39  76.3      28\n 4 Samsung              Tech          6    80.5     25  2021    31  77.5      25\n 5 Sony                 Tech         10    79.6     24  2021    34  77.3      24\n 6 Starbucks Corporati… Food &…      43    76.6     22  2021    65  72.3      22\n 7 Microsoft            Tech         15    79       21  2021    36  76.8      21\n 8 Adidas               Retail       29    77.9     20  2021    49  75.1      20\n 9 General Motors       Automo…      51    75.4     17  2021    68  72        17\n10 Yum! Brands          Food &…      53    75.3     17  2021    70  71.5      17\n# … with 90 more rows, and abbreviated variable names ¹​industry, ²​`2022_rank`,\n#   ³​`2022_rq`, ⁴​rank_diff\n\n# Chat GPT assumes you need a join/doesn't understand the structure of the data\n\nThe Home Depot and Google saw the biggest jump, each increasing by 29 places.\n\n\n\n\n\n\nGrader Note\n\n\n\nBecause of the wording of the question, suggesting that there was only one company, credit was granted if students said either Home Depot or Google (or both).\n\n\n\n\nQuestion 5 (1.5 points)\nFor the industry with only a single “most visible” company in the dataset, has their RQ score been increasing or decreasing overall since 2017?\n\n# find the industry programmatically (or do a group by and figure out that it's insurance)\nindus <- poll |> \n  group_by(industry) |> \n  count() |> \n  arrange(n) |> \n  ungroup() |> \n  slice(1) |> \n  pull(industry)\n\n# or\n\nindus <- poll |>\n  distinct(company, industry) |>\n  group_by(industry) |>\n  summarise(count = n()) |>\n  filter(count == \"1\") |>\n  pull(industry)\n\n# look at this output\npoll |> \n  filter(industry == indus)\n\n# A tibble: 5 × 8\n  company                 industry  `2022_rank` 2022_…¹ change  year  rank    rq\n  <chr>                   <chr>           <dbl>   <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 Progressive Corporation Insurance          57    74.4     NA  2017    NA  72.7\n2 Progressive Corporation Insurance          57    74.4     NA  2018    NA  73.2\n3 Progressive Corporation Insurance          57    74.4     NA  2019    NA  71.5\n4 Progressive Corporation Insurance          57    74.4     NA  2020    NA  74  \n5 Progressive Corporation Insurance          57    74.4     NA  2021    NA  NA  \n# … with abbreviated variable name ¹​`2022_rq`\n\n# could also plot to determine but would take more work b/c data are across multiple columns - not tidy!\n\nWhile initially there was some variability, Progressive’s RQ has been increasing overall (74.4 in 2022, lower than that years prior).\n\n\n\n\n\n\nGrader Note\n\n\n\nAvoid hard-coding whenever possible. Porgrammatically determining the industry takes more work here, but avoids possible typos. Credit was granted if hard-coded. Most common error here was likely misreading the question.\n\n\n\n\nQuestion 6 (2 points)\nHow many companies from each industry category are represented in the 2022 ‘100 Most Visible’ companies in America data? Generate a visualization to display the answer to this question. Be sure to follow best visualization practices discussed in class.\n\n# note - should only display 100 companies total\npoll |> distinct(company, .keep_all=TRUE) |>\nggplot(aes(y=fct_rev(fct_infreq(industry)))) +\n  geom_bar() + \n  labs(title = \"Retail Companies Most Common Among 100 'Most Visible' in 2022\",\n       subtitle = \"Industries for the 100 Companies Included in the Axios and Harris Poll\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  theme_classic() +\n  theme(plot.title.position = \"plot\",\n        axis.title = element_blank())\n\n\n\n# ChatGPT does data manipulation first and then plots from there; does include some \"best practices\" for data viz...but their operations are not correct\n\n\n\n\n\n\n\nGrader Note\n\n\n\nPlot should only display 100 companies. Barplot great here. Remember that categorical variables should be ordered meaningfully. Credit was lost if they were not, unless you directly labeled the values. Titles and axis labels are required. Because feedback for hw02 had not yet been returned, points were not deducted if your title was not as informative as it could have been. Most common errors: 1) not considering how to best display the plot so that labels were most readable (for most, this involved putting them on the y-axis); 2) not ordering axis with categorical data; 3) including more than 100 companies.\n\n\n\n\nQuestion 7 (2 points)\nOf industries that have at least 5 companies in the dataset, which industry has the highest median 2022 rank? Generate a visualization that allows you to answer this question. Be sure to follow best practices.\n\nindustries <- poll |> \n  distinct(company, .keep_all=TRUE) |>\n  group_by(industry) |>\n  count() |> \n  filter(n>5) |>\n  pull(industry)\n\npoll |> \n  filter(industry %in% industries) |>\n  ggplot(aes(x=fct_reorder(industry, `2022_rank`), y=`2022_rank`)) +\n  geom_boxplot() +\n  labs(title = \"Automotive Industry Has the Highest Median Rank in 2022\",\n       subtitle = \"...among industries with at least 5 companies on the 'most visible' list\",\n       x = \"Industries\") +\n  theme_classic(base_size = 12) +\n  theme(plot.title.position = \"plot\",\n        axis.title.y = element_blank())\n\n\n\n# a bit more convoluted answer on Chat GPT\n\n\n\n\n\n\n\nGrader Note\n\n\n\nFull credit was granted whether a boxplot or a barplot was used; however, a boxplot displays more information overall. Remember that categorical variables should be ordered meaningfully. Credit was lost if they were not, unless you directly labeled the values. Titles and axis labels are required. Because feedback for hw02 had not yet been returned, points were not deducted if your title was not as informative as it could have been. Most common errors: 1) not filtering w/ correct logic and 2) not ordering axis with categorical information\n\n\n\n\nQuestion 8 (2 points)\nYour boss is curious about how much rankings change from one year to the next. To answer this question, they ask you to determine how well 2021 rankings explain the following year’s 2022 rankings. Generate a linear model to answer this question. Be sure to include your interpretation of the model (in other words your answer to the question “how well do 2021 rankings explain 2022’s rankings?”)\n\ndf <- poll %>% filter(year == 2021)\n\nmod <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(`2022_rank` ~ rank, data = df)\n\nmod |> tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    6.63     3.37        1.97 5.27e- 2\n2 rank           0.864    0.0573     15.1  6.08e-25\n\n\n\nmod |>\n  glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³\n      <dbl>        <dbl> <dbl>   <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1     0.742        0.739  14.8    227. 6.08e-25     1  -332.  671.  678.  17411.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\n# Chat GPT gets the estimate incorrect (0.8 instead of 0.86) and Rsquared states 60% rather than 74%\n\nWe can see here that the 2021 data explain 74% of the variance in the 2022 data, making it a pretty good model for explaining 2022 rankings. Further, by looking at the coefficient, a company who was ranked in both 2021 and 2022 could expect their rank to increase, on average, by 0.86 in 2022.\n\n\n\n\n\n\nGrader Note\n\n\n\nModel had to be interpreted accurately for full credit and answer the question posed. Most common errors were: 1) flipping the predictor and outcome; 2) not answering the question posed\n\n\n\n\nQuestion 9 (2.5 points)\nRecreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be.\n\n\nggplot(df, aes(x=rank, y=`2022_rank`)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", col=\"orange\") + \n  theme_classic(base_size = 14) + \n  labs(title = \"2021 Rank Explains 74% of the variance in 2022 rank\",\n       subtitle = \"On average, companies saw their rank increase almost one spot higher in 2022\",\n       x = \"2021 rank\",\n       y = \"2022 rank\") +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\nGrader Note\n\n\n\nOn the answer key I have fixed the % variance explained; this original 78% was not meant to trick. It was a mistake by prof. As such, I have graded this question leniently. Most common point deductions here were for: 1) not matching theme; 2) not changing line color; 3) not left-aligning title. No points were lost for not increasing text size as that’s hard to visually see."
  },
  {
    "objectID": "content/exams/practice-exam-ans.html",
    "href": "content/exams/practice-exam-ans.html",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "",
    "text": "There will be rules spelled out on the real midterm. Be sure to read them before taking the real exam. There will also be an academic integrity statement for you to complete. Replace the ____________ with your name below on the real deal."
  },
  {
    "objectID": "content/exams/practice-exam-ans.html#academic-integrity-statement",
    "href": "content/exams/practice-exam-ans.html#academic-integrity-statement",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Academic Integrity Statement",
    "text": "Academic Integrity Statement\nI, ____________, hereby state that I have not communicated with or gained information in any way from my classmates or anyone during this exam, and that all work is my own.\nA note on sharing / reusing code: We are well aware that a huge volume of code is available on the web to solve any number of problems. For this exam you are allowed to make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). You are also not allowed to ask a question on an external forum, you can only use answers to questions that have already been answered. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. All communication with classmates is explicitly forbidden."
  },
  {
    "objectID": "content/exams/practice-exam-ans.html#getting-help",
    "href": "content/exams/practice-exam-ans.html#getting-help",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Getting help",
    "text": "Getting help\nBecause we cannot be available to all students across the entire length of the (real) exam, there will be no questions of instructional staff about the exam. If you find wording confusing or are unsure, note that in your answer and explain how you interpreted it. This will be taken into consideration during grading. If you are having technical difficulties or think there is an error on the exam, DM or email Prof Ellis immediately and she’ll work with you as soon as possible."
  },
  {
    "objectID": "content/exams/practice-exam-ans.html#grading-and-feedback",
    "href": "content/exams/practice-exam-ans.html#grading-and-feedback",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Grading and feedback",
    "text": "Grading and feedback\nThe (real) exam is worth 16% of your grade. You will be graded on the correctness of your code, correctness of your answers (often there are multiple “correct” answers, by design), the clarity of your explanations, and the overall organization of your document. (There’s no one “right” organization, but we should be able to easily navigate your midterm to find what we’re looking for.)"
  },
  {
    "objectID": "content/exams/practice-exam-ans.html#logistics",
    "href": "content/exams/practice-exam-ans.html#logistics",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Logistics",
    "text": "Logistics\nAnswer the questions in the document called practice-exam.Rmd. Add your code and narrative in the spaces below each question. Add code chunks as needed. Use as many lines as you need, but keep your narrative concise. Be sure to knit your file to HTML and view the file prior to turning it in."
  },
  {
    "objectID": "content/exams/practice-exam-ans.html#packages",
    "href": "content/exams/practice-exam-ans.html#packages",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Packages",
    "text": "Packages\nYou will need the tidyverse package for this (practice) midterm. (For the real deal, you’ll need tidyverse and tidymodels.) If working on datahub, this package has been installed, but you will need to load it. No other packages are required, but if for some reason you want to load in another package, you are permitted to do so."
  },
  {
    "objectID": "content/exams/practice-exam-ans.html#the-data",
    "href": "content/exams/practice-exam-ans.html#the-data",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "The data",
    "text": "The data\nThe dataset you’ll be working with on this practice midterm is all about beach volleyball. The full dataset is explained in detail here and includes match-level data from 76,756 volleyball matches. You should click on that link to see what information is stored in each column in this dataset and what information is included in each column.\nBriefly, what you’ll use for this midterm is a subset of the full dataset, including only the 11,699 observations (rows) from 2018 and 2019 but all of the original columns. Each row summarizes the results from a single, distinct match played in a volleyball tournament.\nTo briefly describe beach volleyball, it is a sport played 2 on 2, so each match involves only 4 players. These data include matches from two different volleyball circuits, the international FIVB and the US-centric AVP. You will not need to know much at all about this sport to complete this midterm, and anything you need to know will be explained.\nThe data are stored in data/vb_matches.csv. You’ll need to read the dataset in prior to answering any questions on the midterm.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.0\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.2.1     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ndf <- read_csv('data/vb_matches.csv')\n\nRows: 11699 Columns: 65\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (17): circuit, tournament, country, gender, w_player1, w_p1_country, w_...\ndbl  (42): year, match_num, w_p1_age, w_p1_hgt, w_p2_age, w_p2_hgt, l_p1_age...\ndate  (5): date, w_p1_birthdate, w_p2_birthdate, l_p1_birthdate, l_p2_birthdate\ntime  (1): duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "content/exams/practice-exam-ans.html#questions",
    "href": "content/exams/practice-exam-ans.html#questions",
    "title": "COGS 137 - Winter 2023 - Practice Midterm (Answers)",
    "section": "Questions",
    "text": "Questions\nQuestion 1 (0.75 points) - How many FIVB and AVP matches are included in this dataset?\n\ndf |> \n  group_by(circuit) |>\n  count()\n\n# A tibble: 2 × 2\n# Groups:   circuit [2]\n  circuit     n\n  <chr>   <int>\n1 AVP      2656\n2 FIVB     9043\n\n\n\n2656 AVP\n9043 FIVB\n\nQuestion 2 (0.75 points) - Find the match with the longest duration. a. Where was this tournament played (City & Country)? b. How long did the match last? c. Who were the two winners?\n\ndf |> \n  slice(which.max(duration))\n\n# A tibble: 1 × 65\n  circuit tournament country   year date       gender match…¹ w_pla…² w_p1_bir…³\n  <chr>   <chr>      <chr>    <dbl> <date>     <chr>    <dbl> <chr>   <date>    \n1 AVP     Austin     United …  2018 2018-05-17 M           37 Nathan… 1991-06-15\n# … with 56 more variables: w_p1_age <dbl>, w_p1_hgt <dbl>, w_p1_country <chr>,\n#   w_player2 <chr>, w_p2_birthdate <date>, w_p2_age <dbl>, w_p2_hgt <dbl>,\n#   w_p2_country <chr>, w_rank <chr>, l_player1 <chr>, l_p1_birthdate <date>,\n#   l_p1_age <dbl>, l_p1_hgt <dbl>, l_p1_country <chr>, l_player2 <chr>,\n#   l_p2_birthdate <date>, l_p2_age <dbl>, l_p2_hgt <dbl>, l_p2_country <chr>,\n#   l_rank <chr>, score <chr>, duration <time>, bracket <chr>, round <chr>,\n#   w_p1_tot_attacks <dbl>, w_p1_tot_kills <dbl>, w_p1_tot_errors <dbl>, …\n\n\n\nAustin, USA\n1h 42 min\nNathan Yang & Steven Irvin\n\n\nQuestion 3 (1.5 points) - Across all tournaments included in this dataset, which teams have won the most tournaments? Your response should include both the winning players, their gender, and the number of tournaments they’ve won in descending order. Who has the most wins? How many men’s and how many women’s teams are in the top 10?\nNote: “winning a tournament” is indicated by winning either a “Gold Medal” (FIVB) or “Finals” (AVP) match, specified in the bracket column.\n\ndf |> \n  filter(bracket %in% c(\"Gold Medal\", \"Finals\")) |> \n  group_by(w_player1, w_player2, gender) |>\n  count() |> \n  arrange(desc(n)) |>\n  head(10)\n\n# A tibble: 10 × 4\n# Groups:   w_player1, w_player2, gender [10]\n   w_player1              w_player2                 gender     n\n   <chr>                  <chr>                     <chr>  <int>\n 1 Alix Klineman          \"April Ross\"              W         11\n 2 Anders Mol             \"Christian Sorum\"         M         10\n 3 Melissa Humana-Paredes \"Sarah Pavan\"             W          6\n 4 Nick Lucena            \"Phil Dalhausser\"         M          6\n 5 Jake Gibb              \"Taylor Crabb\"            M          5\n 6 Jingzhe Wang           \"Shuhui Wen\"              W          5\n 7 Agatha Bednarczuk      \"Eduarda \\\"Duda\\\" Lisboa\" W          4\n 8 Aleksandrs Samoilovs   \"Janis Smedins\"           M          4\n 9 Betsi Flint            \"Emily Day\"               W          4\n10 Alexander Brouwer      \"Robert Meeuwsen\"         M          3\n\n\n\nAlix Klineman and April Ross have won the most tournaments\nThere are 5 men’s teams and 5 women’s teams\n\nQuestion 4 (1.5 points) - Of only the AVP tournaments included in this dataset, how many different cities hosted tournaments in 2018 and 2019? And, which cities (if any) hosted a tournament in both 2018 and 2019?\nNote that tournaments are named for the city hosting the tournament.\n\n# distinct locations\ndf |> \n  filter(circuit == 'AVP') |>\n  distinct(tournament)\n\n# A tibble: 9 × 1\n  tournament      \n  <chr>           \n1 Austin          \n2 New York        \n3 Seattle         \n4 San Francisco   \n5 Hermosa Beach   \n6 Manhattan Beach \n7 Chicago         \n8 Waikiki         \n9 Huntington Beach\n\n\n\n9 distinct locations\n\n\nlocations <- df |> \n  filter(circuit == 'AVP') |>\n  group_by(year, date) |> \n    distinct(tournament) \n\ndf |>\n  filter(circuit == \"AVP\") |>\n  group_by(year, date) |>\n  distinct(tournament) |>\n  group_by(tournament) |>\n  count() |>\n  filter(n > 1)\n\n# A tibble: 6 × 2\n# Groups:   tournament [6]\n  tournament          n\n  <chr>           <int>\n1 Austin              2\n2 Chicago             2\n3 Hermosa Beach       2\n4 Manhattan Beach     2\n5 New York            2\n6 Seattle             2\n\n\n\n6 locations were duplicates between 2018 and 2019, including Chicago, Manhattan Beach, Hermosa Beach, Seattle, New York, and Austin\n\nQuestion 5 (2.5 points) - Prof Ellis plays a lot of women’s beach volleyball and is only 5’5” (65 inches). Despite not having the sheer talent or raw athletic ability to make it as a professional volleyball player, she wonders if she ever had a chance at her height. To help her out, answer each of the following: a. Who was the shortest women’s player to compete in a tournament in 2018/2019? b. How tall are they? c. Did they win a tournament in 2018 or 2019?\nReminder: there are 4 players in each match whose height should be considered.\n\n# find shortest in each column\ndf |>\n  filter(gender == \"W\") |>\n  summarize(min_p1 = min(w_p1_hgt, na.rm=TRUE), \n            min_p2  = min(w_p2_hgt, na.rm=TRUE), \n            min_l_p1 = min(l_p1_hgt, na.rm=TRUE),\n            min_l_p2 = min(l_p2_hgt, na.rm=TRUE))\n\n# A tibble: 1 × 4\n  min_p1 min_p2 min_l_p1 min_l_p2\n   <dbl>  <dbl>    <dbl>    <dbl>\n1     63     64       62       61\n\n# determine the shortest player\ndf |> filter(l_p2_hgt == 61)\n\n# A tibble: 2 × 65\n  circuit tournament  country  year date       gender match…¹ w_pla…² w_p1_bir…³\n  <chr>   <chr>       <chr>   <dbl> <date>     <chr>    <dbl> <chr>   <date>    \n1 FIVB    Visakhapat… India    2019 2019-02-28 W            7 Meliss… 1995-01-27\n2 FIVB    Visakhapat… India    2019 2019-02-28 W           16 Ekater… 1990-11-06\n# … with 56 more variables: w_p1_age <dbl>, w_p1_hgt <dbl>, w_p1_country <chr>,\n#   w_player2 <chr>, w_p2_birthdate <date>, w_p2_age <dbl>, w_p2_hgt <dbl>,\n#   w_p2_country <chr>, w_rank <chr>, l_player1 <chr>, l_p1_birthdate <date>,\n#   l_p1_age <dbl>, l_p1_hgt <dbl>, l_p1_country <chr>, l_player2 <chr>,\n#   l_p2_birthdate <date>, l_p2_age <dbl>, l_p2_hgt <dbl>, l_p2_country <chr>,\n#   l_rank <chr>, score <chr>, duration <time>, bracket <chr>, round <chr>,\n#   w_p1_tot_attacks <dbl>, w_p1_tot_kills <dbl>, w_p1_tot_errors <dbl>, …\n\n\nPerumal Yogeshwari was the shortest to compete (at 61 inches). By deduction, since this is the l_p2 column, we know that she did not win any tournaments.\nQuestion 6 (3 pts) - Which country has hosted the most FIVB tournaments? Did this differ by year? Generate a visualization that shows how many FIVB tournaments each country hosted. Allow viewer to visualize this by year. And, be sure each tournament is only counted once (regardless of how many games were played).\n\ndf |>\n  filter(circuit == 'FIVB') |>\n  distinct(tournament, year, .keep_all = TRUE) |>\n  ggplot(aes(y=fct_infreq(country))) + \n  geom_bar() + \n  facet_wrap(~year) +\n  labs(y=NULL,\n       x=\"Count\",\n       title = \"Number of FIVB tournaments hosted by each country\") + \n  theme_bw() +\n  theme(plot.title.position = \"plot\")\n\n\n\n\nQuestion 7 (3 pts) - Recreate the plot included below using the data you’ve been working with. Once you have created the visualization, in no more than one paragraph, describe what you think the point of this visualization might be.\nHint: The visualization uses the variable avg_team_height, which is not included in the provided data frame. You will have to create avg_team_height yourself, be determining the average (mean) team height for each winning team.\n\n\ndf |>\n  filter(circuit == 'AVP') |>\n  mutate(avg_team_height = (w_p1_hgt + w_p2_hgt)/2) |>\n  ggplot(aes(x = fct_infreq(w_p1_country), y = avg_team_height, fill=w_p1_country)) +\n  geom_boxplot() + \n  facet_wrap(~gender, \n             scales=\"free_y\", \n             nrow=2) +\n  labs(y = 'Average Team Height (in)',\n       x = 'Country',\n       title = 'Average team heights for AVP match winners in 2018 and 2019') +\n  theme(plot.title.position = \"plot\") +\n  guides(fill=\"none\") \n\nWarning: Removed 1288 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nVisualizes which countries have the tallest winning teams on average. For men, we see that players from the Netherlands are quite tall. For Women, it’s the US and Canada.\n\nQuestion 8 (1 pts) - If you were in charge of designing the plot you just recreated in the plot above, what changes would you make to improve its effectiveness as a visualization? (You do not have to write any code for this question, just explain the different design/viz choices you would make.)\n\n\nswap axes\naccurate/more informative labelling labeling\nmeaningful ordering of countries (geography?)\nimproved color choices (flag colors?)\nincrease text size\nremove background\netc."
  },
  {
    "objectID": "content/final/final.html",
    "href": "content/final/final.html",
    "title": "Final Project",
    "section": "",
    "text": "For the final project, you and your group mates (groups of 3-4 people) get to choose one of the following two options: 1) Technical Presentation or 2) Data Analysis.\nEach group will be provided with a private repo that all members as well as course instructional staff will have access to. Final projects will be “submitted” by pushing the project requirements to the group repo by the deadline.\nPresentations will be submitted on Canvas if pre-recorded or presented in-person Thursday of finals week. Students will be graded the same regardless of which presentation route they choose. The in-person option is provided for those students who want more in-person presentation experience.\nWritten, visual, and presented content will be graded on their technical merits as well as the effectiveness of their communication."
  },
  {
    "objectID": "content/final/final.html#option-1-technical-presentation",
    "href": "content/final/final.html#option-1-technical-presentation",
    "title": "Final Project",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\nGroups who choose the technical presentation route will make slides for a presentation that effectively communicates/teaches an advanced statistical topic1 and/or an R package2.\n\nDeliverable: R Markdown + Slides\nSlides will be required for the presentation and they must be generated from either an RMarkdown document or a quarto document. Chapters 4, 7, and 8 of the R Markdown: The Definitive Guide discussion options for generating slides/presentations from R Markdown documents. (Presentations from quarto have similar documentation here.) Students should commit both the .Rmd (or .qmd) document and the rendered slides to their GitHub repo.\nThis presentation must teach the details of the R package, the statistical topic, or both at a level appropriate for students in this course. (i.e. You can assume your audience knows how to program in R, know about the tidyverse, know linear regression, etc.) And, you must demonstrate how to use the package and/or carry out the statsitical analysis in R.\n\n\nDeliverable: Presentation\nStudents must also present their slides in a presentation that is 10-15min long. This presentation can either be pre-recorded and submitted on Canvas or groups can sign up for a presentation slot to present in-person Thursday of finals week. For this option, all students must participate in the presentation. There is no grade difference for those who choose to pre-record or present in person."
  },
  {
    "objectID": "content/final/final.html#option-2-data-analysis",
    "href": "content/final/final.html#option-2-data-analysis",
    "title": "Final Project",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\nGroups who choose the data analysis route will carry out a full data science project. This will include question formation, finding the data, doing background research, wrangling the data, doing EDA, analyzing the data, and answering your question of interest.\nYou can think of this as a mini case report in the fact that the process is the same, but we would not expect the data wrangling to be quite as extensive as what was done in the case studies. That said, we want to see demonstration of the skills you’ve learned in the class, so we will be looking for some data wrangling in your case study. If you have a single dataset that requires no wrangling, consider if additional datasets could be incorporated to answer your question(s) of interest more deeply.\nYou are strongly encouraged to think of your topic/question before looking for datasets. More interesting case studies start with the topic/question. Boring case studies look for the dataset first.\n\nDeliverable: Report (.Rmd + HTML)\nYour analysis will be submitted as an .Rmd document and rendered to HTML (both of which should be pushed to GitHub).\nThis will likely not be quite as long as a case study in this course, but will likely have the same sections.\n\n\nDeliverable: Presenatation\nStudents must present their case study in a presentation that is 3-5min long. What you use to visually support this presentation (slides, or something else) is up to you but should follow the effective communication aspects discussed in class. This presentation can either be pre-recorded and submitted on Canvas or groups can sign up for a presentation slot to present in-person Thursday of finals week. For this option, at least one group member must present the project (in other words, not everyone has to “speak” but everyone in the group is responsible for the contents). There is no grade difference for those who choose to pre-record or present in person."
  },
  {
    "objectID": "content/final/final.html#group-feedback",
    "href": "content/final/final.html#group-feedback",
    "title": "Final Project",
    "section": "Group Feedback",
    "text": "Group Feedback\nThere will be a form to submit upon submission of the final project to provide feedback about working with your group mates. As with the case studies, this is meant to motivate not scare. Most groups work out really really well and everyone contributes to the best of their ability. However, if and when that doesn’t happen, I want to be sure I’m aware of the circumstances and follow up as necessary."
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#qa",
    "href": "content/lectures/14-logistic-regression-slides.html#qa",
    "title": "14-logistic-regression",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Understanding the meaning behind the heat maps. How is it collinear regarding race and not age? What would collinearity look like if it were correlated with age?\nA: Great question - we’re going to continue with this today!\n\n\nQ: Are we allowed to use the analysis we use in class, for our analysis for the case study? As long as we understand it, can we just copy and paste?\nA: Kind of. What we discuss in class will help answer Q2 about collinearity…and will get you well on your way to answering Q1 about the relationship between RTC laws and violent crime. So, yes, you can copy+paste anything from class that you want, but you’ll need to additionally make some decisions to fully answer the questions.\n\n\nQ: I am confused about if our group accidentally works on the same lines/parts of the file for cs01, will our work be overwritten by those who push later? I’m not familiar to GitHub so I may need to explain more.\nA: If you work on the exact same lines, yes, this will cause a “merge conflict” …so it will not silently overwrite what someone else did…but it will force you to decide whose version you want to keep. Best solution is to not work on the same parts of the file at the same time."
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#adgross-self-promotion-last-lecture-wed-5pm",
    "href": "content/lectures/14-logistic-regression-slides.html#adgross-self-promotion-last-lecture-wed-5pm",
    "title": "14-logistic-regression",
    "section": "[ad/gross self-promotion?] Last Lecture Wed @ 5pm",
    "text": "[ad/gross self-promotion?] Last Lecture Wed @ 5pm\n Last Lecture: Life Lessons That Have Nothing to Do with Data or Science A UCSD Data Science Education will teach you a lot. There will be programming, data, dataviz, statistics, machine learning, linear algebra, ethics, capstone projects, and domain knowledge galore. But, these courses will not teach you the very specific lessons that Prof Ellis has learned along her journey. Come hear the advice that took Prof Ellis decades to receive, their surrounding stories, and the lessons she hopes you learn faster than she did in one jam-packed chat.\nThe Last Lecture Series is a huge opportunity for students to gain some insight about a Professor’s journey and the obstacles they had to overcome to get to where they are today, especially coming from professors who have reached success in the field of data science. We highly encourage you to attend!\nThis event will be happening at 5pm on Wednesday, 3/1.\nWe’ll be hosting this at the SDSC Auditorium! Registration is no longer needed."
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#course-announcements",
    "href": "content/lectures/14-logistic-regression-slides.html#course-announcements",
    "title": "14-logistic-regression",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLecture Participation survey “due” after class\nLab07 due Friday (3/3; 11:59 PM)\nCS01 due Mon (3/6; 11:59 PM)\n\n\nNotes:\n\nlab 05 and lab06 scores posted\nlab07 will be posted later today (wanted to start the material before posting lab)\ncs01 team willing to take on an additional member?"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#agenda",
    "href": "content/lectures/14-logistic-regression-slides.html#agenda",
    "title": "14-logistic-regression",
    "section": "Agenda",
    "text": "Agenda\n\nLab06 Review\nLogistic Regression\n\nSingle predictor\nMultiple predictors\n\nModel evaluation"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#lab-06-eda",
    "href": "content/lectures/14-logistic-regression-slides.html#lab-06-eda",
    "title": "14-logistic-regression",
    "section": "Lab 06: EDA",
    "text": "Lab 06: EDA\nEDA Example #1: Shenova\n\nPlotCode\n\n\n\n\n\n\nggplot(DONOHUE_DF, aes(y=Viol_crime_count, x=YEAR)) + \n  geom_line() + \n  labs(\n    title = \"Violent Crime Rate by State and Year\", \n    x = \"Year\",\n    y = \"Total Violent Crime Rate\") +\n  \n  facet_wrap(~STATE, nrow = 5)+ \n  theme(axis.text.x = element_text(angle = 90), plot.title.position = \"plot\")\n\n\n\n\nEDA Example #2\n\nPlotCode\n\n\n\n\n\n\np2 <- DONOHUE_DF |>\n  group_by(STATE) |>\n  summarise(RTC_LAW_YEAR=RTC_LAW_YEAR) |>\n  distinct() |>\n  ggplot(aes(x=RTC_LAW_YEAR)) +\n  geom_bar() +\n  scale_x_continuous(\n    breaks = seq(1980, 2015, by = 1)\n  ) +\n  labs(\n    title = \"Distribution of RTC Law Years\",\n    x = \"RTC Law Year\", y = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90), plot.title.position = \"plot\")\np2\n\n\n\n\nEDA Example #3: Sebastian\n\nPlotCode\n\n\n\n\n\n\nlibrary(maps)\n\n# load state map data\nstates_map <- map_data(\"state\")\n\n# merge state map data with DONOHUE_DF data\nDONOHUE_DF_map <- merge(states_map, DONOHUE_DF, by.x = \"region\", by.y = \"STATE\")\n\n# plot map using ggplot2\nggplot() +\n  geom_polygon(data = DONOHUE_DF_map, aes(x = long, y = lat, group = group, fill = cut(Viol_crime_rate_1k, breaks = c(0, 3, 4, 5, 6, 7, 8, 9, 10), labels = c(\"0-3\", \"3-4\", \"4-5\", \"5-6\", \"6-7\", \"7-8\", \"8-9\", \"9-10\"))), color = \"white\", size = 0.1) +\n  coord_fixed() +\n  theme_void() +\n  scale_fill_brewer(name = \"Violent Crime Rate per 1k\", palette = \"YlOrRd\", na.value = \"white\",\n                    labels = c(\"0-3\", \"3-4\", \"4-5\", \"5-6\", \"6-7\", \"7-8\", \"8-9\", \"9-10\"),\n                    breaks = c(\"0-3\", \"3-4\", \"4-5\", \"5-6\", \"6-7\", \"7-8\", \"8-9\", \"9-10\")) +\n  labs(title = \"Southern States Have the Highest Violent Crime Rates\",\n       subtitle = \"Violent Crime Rates per 1000 in Each State\",\n       caption = \"Note: White areas indicate missing data\") +\n  theme(plot.title = element_text(size = 15, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        plot.caption = element_text(size = 8, hjust = 0),\n        legend.position = \"bottom\",\n        legend.title.align = 0.5,\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8))"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#lab06-brainstorming",
    "href": "content/lectures/14-logistic-regression-slides.html#lab06-brainstorming",
    "title": "14-logistic-regression",
    "section": "Lab06: Brainstorming",
    "text": "Lab06: Brainstorming\n\n\nFocusing in on a variable in the data (i.e. poverty, # of police officers, etc.) and answering more detailed questions about it (in relation to main question)\nFocusing in on a demographic subset (i.e. specific race or specific age group) and asking the questions posed within that group specifically\nAnalyzing with a focus on geographic differences (i.e. border states, specific states - be sure to explain decision)\nFocusing on a different aspect of guns (i.e. accidental fatalities, specific types of crimes, specific type of carry, etc.)\nFocus in and go deep on a specific period of time\nStudying the relationship between gun laws and violent crime rates in other countries\nConsider a “new” but related variable (i.e. “overpolicing”)"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#spam-filters",
    "href": "content/lectures/14-logistic-regression-slides.html#spam-filters",
    "title": "14-logistic-regression",
    "section": "Spam filters",
    "text": "Spam filters\n\n\n\nData from 3921 emails and 21 variables on them\nOutcome: whether the email is spam or not\nPredictors: number of characters, whether the email had “Re:” in the subject, time at which email was sent, number of times the word “inherit” shows up in the email, etc.\n\n\n\nlibrary(openintro)\nglimpse(email)\n\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2011-12-31 22:16:41, 2011-12-31 23:03:59, 2012-01-01 08:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#section",
    "href": "content/lectures/14-logistic-regression-slides.html#section",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ Would you expect longer or shorter emails to be spam??\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 2 × 2\n  spam  mean_num_char\n  <fct>         <dbl>\n1 0             11.3 \n2 1              5.44"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#section-1",
    "href": "content/lectures/14-logistic-regression-slides.html#section-1",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ Would you expect emails that have subjects starting with “Re:”, “RE:”, “re:”, or “rE:” to be spam or not?"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#modelling-spam",
    "href": "content/lectures/14-logistic-regression-slides.html#modelling-spam",
    "title": "14-logistic-regression",
    "section": "Modelling spam",
    "text": "Modelling spam\n\n\nBoth number of characters and whether the message has “re:” in the subject might be related to whether the email is spam. How do we come up with a model that will let us explore this relationship?\nFor simplicity, we’ll focus on the number of characters (num_char) as predictor, but the model we describe can be expanded to take multiple predictors as well."
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#modelling-spam-1",
    "href": "content/lectures/14-logistic-regression-slides.html#modelling-spam-1",
    "title": "14-logistic-regression",
    "section": "Modelling spam",
    "text": "Modelling spam\nThis isn’t something we can reasonably fit a linear model to – we need something different!"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#framing-the-problem",
    "href": "content/lectures/14-logistic-regression-slides.html#framing-the-problem",
    "title": "14-logistic-regression",
    "section": "Framing the problem",
    "text": "Framing the problem\n\nWe can treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials\n\nBernoulli trial: a random experiment with exactly two possible outcomes, “success” and “failure”, in which the probability of success is the same every time the experiment is conducted\n\n\n\n\nEach Bernoulli trial can have a separate probability of success\n\n\\[ y_i ∼ Bern(p) \\]\n\n\nWe can then use the predictor variables to model that probability of success, \\(p_i\\)\nWe can’t just use a linear model for \\(p_i\\) (since \\(p_i\\) must be between 0 and 1) but we can transform the linear model to have the appropriate range"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#generalized-linear-models",
    "href": "content/lectures/14-logistic-regression-slides.html#generalized-linear-models",
    "title": "14-logistic-regression",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\n\nThis is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs)\nLogistic regression is just one example"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#three-characteristics-of-glms",
    "href": "content/lectures/14-logistic-regression-slides.html#three-characteristics-of-glms",
    "title": "14-logistic-regression",
    "section": "Three characteristics of GLMs",
    "text": "Three characteristics of GLMs\nAll GLMs have the following three characteristics:\n\n\nA probability distribution describing a generative model for the outcome variable\nA linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\]\nA link function that relates the linear model to the parameter of the outcome distribution"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#logistic-regression-2",
    "href": "content/lectures/14-logistic-regression-slides.html#logistic-regression-2",
    "title": "14-logistic-regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\nLogistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors\nTo finish specifying the Logistic model we just need to define a reasonable link function that connects \\(\\eta_i\\) to \\(p_i\\): logit function\nLogit function: For \\(0\\le p \\le 1\\)\n\n\n\n\\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#logit-function-visualised",
    "href": "content/lectures/14-logistic-regression-slides.html#logit-function-visualised",
    "title": "14-logistic-regression",
    "section": "Logit function, visualised",
    "text": "Logit function, visualised"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#properties-of-the-logit",
    "href": "content/lectures/14-logistic-regression-slides.html#properties-of-the-logit",
    "title": "14-logistic-regression",
    "section": "Properties of the logit",
    "text": "Properties of the logit\n\n\nThe logit function takes a value between 0 and 1 and maps it to a value between \\(-\\infty\\) and \\(\\infty\\)\nInverse logit (logistic) function: \\[g^{-1}(x) = \\frac{\\exp(x)}{1+\\exp(x)} = \\frac{1}{1+\\exp(-x)}\\]\nThe inverse logit function takes a value between \\(-\\infty\\) and \\(\\infty\\) and maps it to a value between 0 and 1\nThis formulation is also useful for interpreting the model, since the logit can be interpreted as the log odds of a success – more on this later"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#the-logistic-regression-model",
    "href": "content/lectures/14-logistic-regression-slides.html#the-logistic-regression-model",
    "title": "14-logistic-regression",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nBased on the three GLM criteria we have\n\n\\(y_i \\sim \\text{Bern}(p_i)\\)\n\\(\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\n\\(\\text{logit}(p_i) = \\eta_i\\)\n\n\n\n\nFrom which we get\n\n\\[p_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#modeling-spam",
    "href": "content/lectures/14-logistic-regression-slides.html#modeling-spam",
    "title": "14-logistic-regression",
    "section": "Modeling spam",
    "text": "Modeling spam\nIn R we fit a GLM in the same way as a linear model except we:\n\nspecify the model with logistic_reg()\nuse \"glm\" instead of \"lm\" as the engine\ndefine family = \"binomial\" for the link function to be used in the model\n\n\nspam_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char, data = email, family = \"binomial\")\n\ntidy(spam_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#spam-model",
    "href": "content/lectures/14-logistic-regression-slides.html#spam-model",
    "title": "14-logistic-regression",
    "section": "Spam model",
    "text": "Spam model\n\ntidy(spam_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\nModel:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times \\text{num_char}\n\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#pspam-for-an-email-with-2000-characters",
    "href": "content/lectures/14-logistic-regression-slides.html#pspam-for-an-email-with-2000-characters",
    "title": "14-logistic-regression",
    "section": "P(spam) for an email with 2000 characters",
    "text": "P(spam) for an email with 2000 characters\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times 2\\]\n\n\\[\\frac{p}{1-p} = \\exp(-1.9242) = 0.15 \\rightarrow p = 0.15 \\times (1 - p)\\]\n\n\n\\[p = 0.15 - 0.15p \\rightarrow 1.15p = 0.15\\]\n\n\n\\[p = 0.15 / 1.15 = 0.13\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#section-2",
    "href": "content/lectures/14-logistic-regression-slides.html#section-2",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ What is the probability that an email with 15000 characters is spam? What about an email with 40000 characters?\n\n\n\n\n\n\n\n\n\n\n2K chars: P(spam) = 0.13\n15K chars, P(spam) = 0.06\n40K chars, P(spam) = 0.01"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#section-3",
    "href": "content/lectures/14-logistic-regression-slides.html#section-3",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ Would you prefer an email with 2000 characters to be labelled as spam or not? How about 40,000 characters?"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#false-positive-and-negative",
    "href": "content/lectures/14-logistic-regression-slides.html#false-positive-and-negative",
    "title": "14-logistic-regression",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail labelled spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail labelled not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\nFalse negative rate = P(Labelled not spam | Email spam) = FN / (TP + FN)\nFalse positive rate = P(Labelled spam | Email not spam) = FP / (FP + TN)"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#sensitivity-and-specificity-1",
    "href": "content/lectures/14-logistic-regression-slides.html#sensitivity-and-specificity-1",
    "title": "14-logistic-regression",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail labelled spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail labelled not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\nSensitivity = P(Labelled spam | Email spam) = TP / (TP + FN)\n\nSensitivity = 1 − False negative rate\n\nSpecificity = P(Labelled not spam | Email not spam) = TN / (FP + TN)\n\nSpecificity = 1 − False positive rate\n\n\n\n\n❓ If you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#modeling-spam-multiple-predictors",
    "href": "content/lectures/14-logistic-regression-slides.html#modeling-spam-multiple-predictors",
    "title": "14-logistic-regression",
    "section": "Modeling Spam : Multiple predictors",
    "text": "Modeling Spam : Multiple predictors\n\nspam_mult <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char + to_multiple + re_subj, data = email, family = \"binomial\")\n\ntidy(spam_mult)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -1.20     0.0752     -16.0  2.21e-57\n2 num_char      -0.0686   0.00781     -8.78 1.57e-18\n3 to_multiple1  -2.14     0.299       -7.18 6.92e-13\n4 re_subj1      -3.12     0.360       -8.66 4.70e-18"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#model-multiple-predictors",
    "href": "content/lectures/14-logistic-regression-slides.html#model-multiple-predictors",
    "title": "14-logistic-regression",
    "section": "Model: Multiple predictors",
    "text": "Model: Multiple predictors\n\ntidy(spam_mult)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -1.20     0.0752     -16.0  2.21e-57\n2 num_char      -0.0686   0.00781     -8.78 1.57e-18\n3 to_multiple1  -2.14     0.299       -7.18 6.92e-13\n4 re_subj1      -3.12     0.360       -8.66 4.70e-18\n\n\n\n\\[\n\\begin{aligned}\nlog_e \\left(\\frac{p}{1 - p}\\right) &= - 1.20 - 0.07 \\times \\texttt{num_char} \\\\\n&\\quad - 2.14\\times \\texttt{to_multiple}_{\\texttt{1}} \\\\\n&\\quad - 3.12 \\times \\texttt{re_subj}_{\\texttt{1}} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#model-multiple-predictors-1",
    "href": "content/lectures/14-logistic-regression-slides.html#model-multiple-predictors-1",
    "title": "14-logistic-regression",
    "section": "Model: Multiple predictors",
    "text": "Model: Multiple predictors\nSo for an email with 4,000 characters (4), addressed to a single recipient (0), and that did start with “re:” in the subject line (1)…\n\\[\n\\begin{aligned}\nlog_e \\left(\\frac{p}{1 - p}\\right) = - 1.20 - 0.07 \\times 4 - 2.14\\times 0 - 3.12 \\times 1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#model-multiple-predictors-2",
    "href": "content/lectures/14-logistic-regression-slides.html#model-multiple-predictors-2",
    "title": "14-logistic-regression",
    "section": "Model: Multiple predictors",
    "text": "Model: Multiple predictors\n\\[\n\\begin{aligned}\nlog_e \\left(\\frac{p}{1 - p}\\right) = - 2.2\n\\end{aligned}\n\\]\n\n…solve for \\(\\widehat{p}\\)\n\\[\n\\begin{aligned}\n\\frac{e^{-2.2}}{1 + e^{-2.2}} = 0.0998 = 9.98\\%\n\\end{aligned}\n\\]\n\n\n9.98% chance that such an email would be spam"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#akaike-information-criterion-aic",
    "href": "content/lectures/14-logistic-regression-slides.html#akaike-information-criterion-aic",
    "title": "14-logistic-regression",
    "section": "Akaike information criterion (AIC)",
    "text": "Akaike information criterion (AIC)\n\npopular model selection method\nestimator of prediction error\npraised for its emphasis on model uncertainty and parsimony\nIn calculating AIC, a penalty is given for including additional variables. This penalty for added model complexity attempts to strike a balance between underfitting (too few variables in the model) and overfitting (too many variables in the model).\na lower AIC value are considered to be “better.”"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#comparing-models-aic",
    "href": "content/lectures/14-logistic-regression-slides.html#comparing-models-aic",
    "title": "14-logistic-regression",
    "section": "Comparing Models: AIC",
    "text": "Comparing Models: AIC\nSingle predictor (num_char)\n\nglance(spam_fit)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1         2437.    3920 -1173. 2350. 2363.    2346.        3919  3921\n\n\nMultiple predictors\n\nglance(spam_mult)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1         2437.    3920 -1032. 2071. 2097.    2063.        3917  3921"
  },
  {
    "objectID": "content/lectures/14-logistic-regression-slides.html#suggested-reading",
    "href": "content/lectures/14-logistic-regression-slides.html#suggested-reading",
    "title": "14-logistic-regression",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nIntroduction to Modern Statistics Chapter 9: Logistic Regression\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html",
    "href": "content/lectures/14-logistic-regression.html",
    "title": "14-logistic-regression",
    "section": "",
    "text": "Q: Understanding the meaning behind the heat maps. How is it collinear regarding race and not age? What would collinearity look like if it were correlated with age?\nA: Great question - we’re going to continue with this today!\n\n\nQ: Are we allowed to use the analysis we use in class, for our analysis for the case study? As long as we understand it, can we just copy and paste?\nA: Kind of. What we discuss in class will help answer Q2 about collinearity…and will get you well on your way to answering Q1 about the relationship between RTC laws and violent crime. So, yes, you can copy+paste anything from class that you want, but you’ll need to additionally make some decisions to fully answer the questions.\n\n\nQ: I am confused about if our group accidentally works on the same lines/parts of the file for cs01, will our work be overwritten by those who push later? I’m not familiar to GitHub so I may need to explain more.\nA: If you work on the exact same lines, yes, this will cause a “merge conflict” …so it will not silently overwrite what someone else did…but it will force you to decide whose version you want to keep. Best solution is to not work on the same parts of the file at the same time.\n\n\n\n\n Last Lecture: Life Lessons That Have Nothing to Do with Data or Science A UCSD Data Science Education will teach you a lot. There will be programming, data, dataviz, statistics, machine learning, linear algebra, ethics, capstone projects, and domain knowledge galore. But, these courses will not teach you the very specific lessons that Prof Ellis has learned along her journey. Come hear the advice that took Prof Ellis decades to receive, their surrounding stories, and the lessons she hopes you learn faster than she did in one jam-packed chat.\nThe Last Lecture Series is a huge opportunity for students to gain some insight about a Professor’s journey and the obstacles they had to overcome to get to where they are today, especially coming from professors who have reached success in the field of data science. We highly encourage you to attend!\nThis event will be happening at 5pm on Wednesday, 3/1.\nWe’ll be hosting this at the SDSC Auditorium! Registration is no longer needed.\n\n\n\nDue Dates:\n\nLecture Participation survey “due” after class\nLab07 due Friday (3/3; 11:59 PM)\nCS01 due Mon (3/6; 11:59 PM)\n\n\nNotes:\n\nlab 05 and lab06 scores posted\nlab07 will be posted later today (wanted to start the material before posting lab)\ncs01 team willing to take on an additional member?\n\n\n\n\n\n\nLab06 Review\nLogistic Regression\n\nSingle predictor\nMultiple predictors\n\nModel evaluation\n\n\n\n\nEDA Example #1: Shenova\n\nPlotCode\n\n\n\n\n\n\nggplot(DONOHUE_DF, aes(y=Viol_crime_count, x=YEAR)) + \n  geom_line() + \n  labs(\n    title = \"Violent Crime Rate by State and Year\", \n    x = \"Year\",\n    y = \"Total Violent Crime Rate\") +\n  \n  facet_wrap(~STATE, nrow = 5)+ \n  theme(axis.text.x = element_text(angle = 90), plot.title.position = \"plot\")\n\n\n\n\nEDA Example #2\n\nPlotCode\n\n\n\n\n\n\np2 <- DONOHUE_DF |>\n  group_by(STATE) |>\n  summarise(RTC_LAW_YEAR=RTC_LAW_YEAR) |>\n  distinct() |>\n  ggplot(aes(x=RTC_LAW_YEAR)) +\n  geom_bar() +\n  scale_x_continuous(\n    breaks = seq(1980, 2015, by = 1)\n  ) +\n  labs(\n    title = \"Distribution of RTC Law Years\",\n    x = \"RTC Law Year\", y = \"Count\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90), plot.title.position = \"plot\")\np2\n\n\n\n\nEDA Example #3: Sebastian\n\nPlotCode\n\n\n\n\n\n\nlibrary(maps)\n\n# load state map data\nstates_map <- map_data(\"state\")\n\n# merge state map data with DONOHUE_DF data\nDONOHUE_DF_map <- merge(states_map, DONOHUE_DF, by.x = \"region\", by.y = \"STATE\")\n\n# plot map using ggplot2\nggplot() +\n  geom_polygon(data = DONOHUE_DF_map, aes(x = long, y = lat, group = group, fill = cut(Viol_crime_rate_1k, breaks = c(0, 3, 4, 5, 6, 7, 8, 9, 10), labels = c(\"0-3\", \"3-4\", \"4-5\", \"5-6\", \"6-7\", \"7-8\", \"8-9\", \"9-10\"))), color = \"white\", size = 0.1) +\n  coord_fixed() +\n  theme_void() +\n  scale_fill_brewer(name = \"Violent Crime Rate per 1k\", palette = \"YlOrRd\", na.value = \"white\",\n                    labels = c(\"0-3\", \"3-4\", \"4-5\", \"5-6\", \"6-7\", \"7-8\", \"8-9\", \"9-10\"),\n                    breaks = c(\"0-3\", \"3-4\", \"4-5\", \"5-6\", \"6-7\", \"7-8\", \"8-9\", \"9-10\")) +\n  labs(title = \"Southern States Have the Highest Violent Crime Rates\",\n       subtitle = \"Violent Crime Rates per 1000 in Each State\",\n       caption = \"Note: White areas indicate missing data\") +\n  theme(plot.title = element_text(size = 15, face = \"bold\"),\n        plot.subtitle = element_text(size = 12),\n        plot.caption = element_text(size = 8, hjust = 0),\n        legend.position = \"bottom\",\n        legend.title.align = 0.5,\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8))\n\n\n\n\n\n\n\n\n\nFocusing in on a variable in the data (i.e. poverty, # of police officers, etc.) and answering more detailed questions about it (in relation to main question)\nFocusing in on a demographic subset (i.e. specific race or specific age group) and asking the questions posed within that group specifically\nAnalyzing with a focus on geographic differences (i.e. border states, specific states - be sure to explain decision)\nFocusing on a different aspect of guns (i.e. accidental fatalities, specific types of crimes, specific type of carry, etc.)\nFocus in and go deep on a specific period of time\nStudying the relationship between gun laws and violent crime rates in other countries\nConsider a “new” but related variable (i.e. “overpolicing”)"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#spam-filters",
    "href": "content/lectures/14-logistic-regression.html#spam-filters",
    "title": "14-logistic-regression",
    "section": "Spam filters",
    "text": "Spam filters\n\n\n\nData from 3921 emails and 21 variables on them\nOutcome: whether the email is spam or not\nPredictors: number of characters, whether the email had “Re:” in the subject, time at which email was sent, number of times the word “inherit” shows up in the email, etc.\n\n\n\nlibrary(openintro)\nglimpse(email)\n\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2011-12-31 22:16:41, 2011-12-31 23:03:59, 2012-01-01 08:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#section",
    "href": "content/lectures/14-logistic-regression.html#section",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ Would you expect longer or shorter emails to be spam??\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 2 × 2\n  spam  mean_num_char\n  <fct>         <dbl>\n1 0             11.3 \n2 1              5.44"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#section-1",
    "href": "content/lectures/14-logistic-regression.html#section-1",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ Would you expect emails that have subjects starting with “Re:”, “RE:”, “re:”, or “rE:” to be spam or not?"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#modelling-spam",
    "href": "content/lectures/14-logistic-regression.html#modelling-spam",
    "title": "14-logistic-regression",
    "section": "Modelling spam",
    "text": "Modelling spam\n\n\nBoth number of characters and whether the message has “re:” in the subject might be related to whether the email is spam. How do we come up with a model that will let us explore this relationship?\nFor simplicity, we’ll focus on the number of characters (num_char) as predictor, but the model we describe can be expanded to take multiple predictors as well."
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#modelling-spam-1",
    "href": "content/lectures/14-logistic-regression.html#modelling-spam-1",
    "title": "14-logistic-regression",
    "section": "Modelling spam",
    "text": "Modelling spam\nThis isn’t something we can reasonably fit a linear model to – we need something different!"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#framing-the-problem",
    "href": "content/lectures/14-logistic-regression.html#framing-the-problem",
    "title": "14-logistic-regression",
    "section": "Framing the problem",
    "text": "Framing the problem\n\nWe can treat each outcome (spam and not) as successes and failures arising from separate Bernoulli trials\n\nBernoulli trial: a random experiment with exactly two possible outcomes, “success” and “failure”, in which the probability of success is the same every time the experiment is conducted\n\n\n\n\nEach Bernoulli trial can have a separate probability of success\n\n\\[ y_i ∼ Bern(p) \\]\n\n\nWe can then use the predictor variables to model that probability of success, \\(p_i\\)\nWe can’t just use a linear model for \\(p_i\\) (since \\(p_i\\) must be between 0 and 1) but we can transform the linear model to have the appropriate range"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#generalized-linear-models",
    "href": "content/lectures/14-logistic-regression.html#generalized-linear-models",
    "title": "14-logistic-regression",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\n\nThis is a very general way of addressing many problems in regression and the resulting models are called generalized linear models (GLMs)\nLogistic regression is just one example"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#three-characteristics-of-glms",
    "href": "content/lectures/14-logistic-regression.html#three-characteristics-of-glms",
    "title": "14-logistic-regression",
    "section": "Three characteristics of GLMs",
    "text": "Three characteristics of GLMs\nAll GLMs have the following three characteristics:\n\n\nA probability distribution describing a generative model for the outcome variable\nA linear model: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k\\]\nA link function that relates the linear model to the parameter of the outcome distribution"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#logistic-regression-2",
    "href": "content/lectures/14-logistic-regression.html#logistic-regression-2",
    "title": "14-logistic-regression",
    "section": "Logistic regression",
    "text": "Logistic regression\n\n\nLogistic regression is a GLM used to model a binary categorical outcome using numerical and categorical predictors\nTo finish specifying the Logistic model we just need to define a reasonable link function that connects \\(\\eta_i\\) to \\(p_i\\): logit function\nLogit function: For \\(0\\le p \\le 1\\)\n\n\n\n\\[logit(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#logit-function-visualised",
    "href": "content/lectures/14-logistic-regression.html#logit-function-visualised",
    "title": "14-logistic-regression",
    "section": "Logit function, visualised",
    "text": "Logit function, visualised"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#properties-of-the-logit",
    "href": "content/lectures/14-logistic-regression.html#properties-of-the-logit",
    "title": "14-logistic-regression",
    "section": "Properties of the logit",
    "text": "Properties of the logit\n\n\nThe logit function takes a value between 0 and 1 and maps it to a value between \\(-\\infty\\) and \\(\\infty\\)\nInverse logit (logistic) function: \\[g^{-1}(x) = \\frac{\\exp(x)}{1+\\exp(x)} = \\frac{1}{1+\\exp(-x)}\\]\nThe inverse logit function takes a value between \\(-\\infty\\) and \\(\\infty\\) and maps it to a value between 0 and 1\nThis formulation is also useful for interpreting the model, since the logit can be interpreted as the log odds of a success – more on this later"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#the-logistic-regression-model",
    "href": "content/lectures/14-logistic-regression.html#the-logistic-regression-model",
    "title": "14-logistic-regression",
    "section": "The logistic regression model",
    "text": "The logistic regression model\n\nBased on the three GLM criteria we have\n\n\\(y_i \\sim \\text{Bern}(p_i)\\)\n\\(\\eta_i = \\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_n x_{n,i}\\)\n\\(\\text{logit}(p_i) = \\eta_i\\)\n\n\n\n\nFrom which we get\n\n\\[p_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}{1+\\exp(\\beta_0+\\beta_1 x_{1,i} + \\cdots + \\beta_k x_{k,i})}\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#modeling-spam",
    "href": "content/lectures/14-logistic-regression.html#modeling-spam",
    "title": "14-logistic-regression",
    "section": "Modeling spam",
    "text": "Modeling spam\nIn R we fit a GLM in the same way as a linear model except we:\n\nspecify the model with logistic_reg()\nuse \"glm\" instead of \"lm\" as the engine\ndefine family = \"binomial\" for the link function to be used in the model\n\n\nspam_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char, data = email, family = \"binomial\")\n\ntidy(spam_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#spam-model",
    "href": "content/lectures/14-logistic-regression.html#spam-model",
    "title": "14-logistic-regression",
    "section": "Spam model",
    "text": "Spam model\n\ntidy(spam_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)  -1.80     0.0716     -25.1  2.04e-139\n2 num_char     -0.0621   0.00801     -7.75 9.50e- 15\n\n\nModel:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times \\text{num_char}\n\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#pspam-for-an-email-with-2000-characters",
    "href": "content/lectures/14-logistic-regression.html#pspam-for-an-email-with-2000-characters",
    "title": "14-logistic-regression",
    "section": "P(spam) for an email with 2000 characters",
    "text": "P(spam) for an email with 2000 characters\n\\[\\log\\left(\\frac{p}{1-p}\\right) = -1.80-0.0621\\times 2\\]\n\n\\[\\frac{p}{1-p} = \\exp(-1.9242) = 0.15 \\rightarrow p = 0.15 \\times (1 - p)\\]\n\n\n\\[p = 0.15 - 0.15p \\rightarrow 1.15p = 0.15\\]\n\n\n\\[p = 0.15 / 1.15 = 0.13\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#section-2",
    "href": "content/lectures/14-logistic-regression.html#section-2",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ What is the probability that an email with 15000 characters is spam? What about an email with 40000 characters?\n\n\n\n\n\n\n\n\n\n\n2K chars: P(spam) = 0.13\n15K chars, P(spam) = 0.06\n40K chars, P(spam) = 0.01"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#section-3",
    "href": "content/lectures/14-logistic-regression.html#section-3",
    "title": "14-logistic-regression",
    "section": "",
    "text": "❓ Would you prefer an email with 2000 characters to be labelled as spam or not? How about 40,000 characters?"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#false-positive-and-negative",
    "href": "content/lectures/14-logistic-regression.html#false-positive-and-negative",
    "title": "14-logistic-regression",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail labelled spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail labelled not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\nFalse negative rate = P(Labelled not spam | Email spam) = FN / (TP + FN)\nFalse positive rate = P(Labelled spam | Email not spam) = FP / (FP + TN)"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#sensitivity-and-specificity-1",
    "href": "content/lectures/14-logistic-regression.html#sensitivity-and-specificity-1",
    "title": "14-logistic-regression",
    "section": "Sensitivity and Specificity",
    "text": "Sensitivity and Specificity\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail labelled spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail labelled not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\nSensitivity = P(Labelled spam | Email spam) = TP / (TP + FN)\n\nSensitivity = 1 − False negative rate\n\nSpecificity = P(Labelled not spam | Email not spam) = TN / (FP + TN)\n\nSpecificity = 1 − False positive rate\n\n\n\n\n❓ If you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#modeling-spam-multiple-predictors",
    "href": "content/lectures/14-logistic-regression.html#modeling-spam-multiple-predictors",
    "title": "14-logistic-regression",
    "section": "Modeling Spam : Multiple predictors",
    "text": "Modeling Spam : Multiple predictors\n\nspam_mult <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  fit(spam ~ num_char + to_multiple + re_subj, data = email, family = \"binomial\")\n\ntidy(spam_mult)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -1.20     0.0752     -16.0  2.21e-57\n2 num_char      -0.0686   0.00781     -8.78 1.57e-18\n3 to_multiple1  -2.14     0.299       -7.18 6.92e-13\n4 re_subj1      -3.12     0.360       -8.66 4.70e-18"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#model-multiple-predictors",
    "href": "content/lectures/14-logistic-regression.html#model-multiple-predictors",
    "title": "14-logistic-regression",
    "section": "Model: Multiple predictors",
    "text": "Model: Multiple predictors\n\ntidy(spam_mult)\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -1.20     0.0752     -16.0  2.21e-57\n2 num_char      -0.0686   0.00781     -8.78 1.57e-18\n3 to_multiple1  -2.14     0.299       -7.18 6.92e-13\n4 re_subj1      -3.12     0.360       -8.66 4.70e-18\n\n\n\n\\[\n\\begin{aligned}\nlog_e \\left(\\frac{p}{1 - p}\\right) &= - 1.20 - 0.07 \\times \\texttt{num_char} \\\\\n&\\quad - 2.14\\times \\texttt{to_multiple}_{\\texttt{1}} \\\\\n&\\quad - 3.12 \\times \\texttt{re_subj}_{\\texttt{1}} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#model-multiple-predictors-1",
    "href": "content/lectures/14-logistic-regression.html#model-multiple-predictors-1",
    "title": "14-logistic-regression",
    "section": "Model: Multiple predictors",
    "text": "Model: Multiple predictors\nSo for an email with 4,000 characters (4), addressed to a single recipient (0), and that did start with “re:” in the subject line (1)…\n\\[\n\\begin{aligned}\nlog_e \\left(\\frac{p}{1 - p}\\right) = - 1.20 - 0.07 \\times 4 - 2.14\\times 0 - 3.12 \\times 1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#model-multiple-predictors-2",
    "href": "content/lectures/14-logistic-regression.html#model-multiple-predictors-2",
    "title": "14-logistic-regression",
    "section": "Model: Multiple predictors",
    "text": "Model: Multiple predictors\n\\[\n\\begin{aligned}\nlog_e \\left(\\frac{p}{1 - p}\\right) = - 2.2\n\\end{aligned}\n\\]\n\n…solve for \\(\\widehat{p}\\)\n\\[\n\\begin{aligned}\n\\frac{e^{-2.2}}{1 + e^{-2.2}} = 0.0998 = 9.98\\%\n\\end{aligned}\n\\]\n\n\n9.98% chance that such an email would be spam"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#akaike-information-criterion-aic",
    "href": "content/lectures/14-logistic-regression.html#akaike-information-criterion-aic",
    "title": "14-logistic-regression",
    "section": "Akaike information criterion (AIC)",
    "text": "Akaike information criterion (AIC)\n\npopular model selection method\nestimator of prediction error\npraised for its emphasis on model uncertainty and parsimony\nIn calculating AIC, a penalty is given for including additional variables. This penalty for added model complexity attempts to strike a balance between underfitting (too few variables in the model) and overfitting (too many variables in the model).\na lower AIC value are considered to be “better.”"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#comparing-models-aic",
    "href": "content/lectures/14-logistic-regression.html#comparing-models-aic",
    "title": "14-logistic-regression",
    "section": "Comparing Models: AIC",
    "text": "Comparing Models: AIC\nSingle predictor (num_char)\n\nglance(spam_fit)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1         2437.    3920 -1173. 2350. 2363.    2346.        3919  3921\n\n\nMultiple predictors\n\nglance(spam_mult)\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1         2437.    3920 -1032. 2071. 2097.    2063.        3917  3921"
  },
  {
    "objectID": "content/lectures/14-logistic-regression.html#suggested-reading",
    "href": "content/lectures/14-logistic-regression.html#suggested-reading",
    "title": "14-logistic-regression",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nIntroduction to Modern Statistics Chapter 9: Logistic Regression"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#qa",
    "href": "content/lectures/15-cs02-data-slides.html#qa",
    "title": "15-cs02-data",
    "section": "Q&A",
    "text": "Q&A\n\nQ: So linear regression is just for numerical variables and logistic regression is just for a binary outcome? Can we only use one or the other depending on the data?\nA: The model you use, be it linear regression, logistic regression, or something else is always driven by the data-generating process, the assumptions of the model, and the question asking. Specifically, for these two models, yes, the outcome variable guides the choice here. If the outcome is binary, linear regression won’t work given the fact that extrapolation beyond the two possible outcomes (meaning, you can get values other than TRUE/FALSE) will always be possible with linear regression. For logistic regression, it models a binary outcome, given the constraints specified by the model.\n\n\nQ: I noticed the tremendous difference in the complexity/difficulty level between things introduced in lectures and the lab/HW assignments. I wonder if the expectation of the complexity level, for the case study, is similar to lab/HW assignments.\nA: So, I’d love to chat more about this from anyone who has thoughts here b/c I’m always looking for the student perspective. Partially, this is by design. The main concepts are presented in lecture, lab gives you a low-stakes environment to deepend your understanding (since it’s graded on effort and there is an answer key provided), and then hw, now the third time you’ve seen/interacted with the material is where it’s the most “difficult” because you’ve already seen this material before. If I presented the most complex stuff in lecture then people would leave confused b/c they just learned the basics. That said, while the course is designed this way, the leaps are not intended to feel insurmountable, so I’d love to hear more from students about where particularly they’re struggling. That all said, I think of the case studies similar to HW. You’ve seen the material in lecture. You’ve interacted in lab. And, now, you’re working in groups on your third interaction with the material. Keep in mind that we do expect this to be the work of multiple group members. One group members should not be doing all the work.\n\n\nQ: Will CS02 be with the same group or a different one?\nA: They will be different. I’ll be asking about feedback on this policy at the end of the course. Last time we kept the same groups for all case studies (there were three last time) and final projects. Students requested different groups, so I tried that this quarter and will get feedback from you all on this!\n\n\nQ: I am confused about some of the code provided in the boostrapping section.\nA: The shortest explanation is we wanted to run the same model a whole bunch of times…but if we ran it on the same dataset, we’d get the same answer. Instead, we want to see how stable the model is by running the model with a slightly different set of observations each time. To do this, we remove one observation for each model. If the model is stable, removing a single data point should not change the coefficients much…but if by removing a single observation we get very different coefficient estimates, that suggests something is off with our data or model. So, we run the model on all of the subsets, with each subset being slightly different (by one observation) than the next. We store the model outputs. Then, we compare all of the results."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#course-announcements",
    "href": "content/lectures/15-cs02-data-slides.html#course-announcements",
    "title": "15-cs02-data",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLecture Participation survey “due” after class\nLab07 due tomorrow (3/3; 11:59 PM)\nCS01 due Mon (3/6; 11:59 PM)\n\nRepo w/ Rmd AND HTML\nSurvey completed on group work (individually; canvas for link)\n\n\n\nNotes:\n\n\nlab07 now available\nExample case study posted\nFinal Project\n\ninstructions posted on website\nfinal project group repos will be created tomorrow\n\nCS02 Groups Discussion\nNo HW04 (full credit will be posted)"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#agenda",
    "href": "content/lectures/15-cs02-data-slides.html#agenda",
    "title": "15-cs02-data",
    "section": "Agenda",
    "text": "Agenda\n\nBackground\nData Import\nData Wrangling"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#previously-known-before-report",
    "href": "content/lectures/15-cs02-data-slides.html#previously-known-before-report",
    "title": "15-cs02-data",
    "section": "Previously known before report?",
    "text": "Previously known before report?\n\n\n\nSource: CDC"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#what-e-cigarette-vapors-contain",
    "href": "content/lectures/15-cs02-data-slides.html#what-e-cigarette-vapors-contain",
    "title": "15-cs02-data",
    "section": "What e-cigarette vapors contain…",
    "text": "What e-cigarette vapors contain…\n\n\n\nSource: CDC"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#use-associated-with-lung-injury",
    "href": "content/lectures/15-cs02-data-slides.html#use-associated-with-lung-injury",
    "title": "15-cs02-data",
    "section": "Use associated with lung injury",
    "text": "Use associated with lung injury\n\nSource: Chand et al."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#limitations",
    "href": "content/lectures/15-cs02-data-slides.html#limitations",
    "title": "15-cs02-data",
    "section": "Limitations",
    "text": "Limitations\n\nThe National Youth Tobacco Survey (NYTS) does not follow the same individual student respondents over time. A longitudinal study that does follow the same individuals over time collects data called panel data. The data in this study is called pooled cross-sectional data, and is obtained from random collection of observations across time.\nThe data include percentages of student respondents reporting use of each particular tobacco product, but the survey questions did not ask the relative amount of use of one product compared to another. For example, the survey included questions like: “What flavors of tobacco products have you used in the past 30 days?” but did not ask how often one flavor was used by the same individual over another.\nWhile gender and sex are not actually binary, the data used in this analysis only contain information for groups of individuals who answered the survey questions as male or female."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#the-data-source",
    "href": "content/lectures/15-cs02-data-slides.html#the-data-source",
    "title": "15-cs02-data",
    "section": "The Data: Source",
    "text": "The Data: Source\nData come from the National Youth Tobacco Survey (NYTS) - annual survey that asks students in high school and middle school (grades 6-12) about tobacco usage in the United States of America. - we’ll use data from 2015-2019\n\n\nData available here"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#the-data-format",
    "href": "content/lectures/15-cs02-data-slides.html#the-data-format",
    "title": "15-cs02-data",
    "section": "The Data: Format",
    "text": "The Data: Format\n\nOne excel spreadsheet for each year\nCorresponding codebook (explains what each variable stores)\n\n\nThe Data: Example"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#codebook-example-variables",
    "href": "content/lectures/15-cs02-data-slides.html#codebook-example-variables",
    "title": "15-cs02-data",
    "section": "Codebook Example: Variables",
    "text": "Codebook Example: Variables"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#codebook-example-details",
    "href": "content/lectures/15-cs02-data-slides.html#codebook-example-details",
    "title": "15-cs02-data",
    "section": "Codebook Example: Details",
    "text": "Codebook Example: Details"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-import",
    "href": "content/lectures/15-cs02-data-slides.html#data-import",
    "title": "15-cs02-data",
    "section": "Data Import",
    "text": "Data Import\n\n# only have to run this once \n# A good time for `eval=FALSE` in code chunk\nOCSdata::load_simpler_import(\"ocs-bp-vaping-case-study\", outpath = getwd())\n\n👉 Your Turn: Load the data into RStudio.\n\n\nThe data have already been cleaned to only include columns of interest\nwill store 5 CSVs in data/simpler_import"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-wrangling",
    "href": "content/lectures/15-cs02-data-slides.html#data-wrangling",
    "title": "15-cs02-data",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n# read in CSVs\nnyts_data <- list.files(\"data/simpler_import/\", \n                        pattern = \"*.csv\", \n                        full.names = TRUE) |>\n  map(~ read_csv(.))\n\n# get names\nnyts_data_names <- list.files(\"data/simpler_import/\",\n                              pattern = \"*.csv\") |>\n  str_extract(\"nyts201[5-9]\")\n\n# apply names\nnames(nyts_data) <- nyts_data_names\n\n💡 How are the data stored after this code has executed?"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-exploration",
    "href": "content/lectures/15-cs02-data-slides.html#data-exploration",
    "title": "15-cs02-data",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nglimpse(nyts_data)\n\nList of 5\n $ nyts2015: spc_tbl_ [17,711 × 29] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:17711] \"015438\" \"015438\" \"015438\" \"015438\" ...\n  ..$ finwgt    : num [1:17711] 217 325 325 397 265 ...\n  ..$ stratum   : chr [1:17711] \"BR3\" \"BR3\" \"BR3\" \"BR3\" ...\n  ..$ Qn1       : num [1:17711] 10 9 10 10 10 10 10 10 10 10 ...\n  ..$ Qn2       : num [1:17711] 2 1 1 1 2 2 1 2 1 2 ...\n  ..$ Qn3       : num [1:17711] 7 7 7 7 7 7 7 7 7 7 ...\n  ..$ ECIGT     : num [1:17711] 2 1 2 1 2 1 1 1 2 2 ...\n  ..$ ECIGAR    : num [1:17711] 1 1 2 2 2 2 1 2 2 2 ...\n  ..$ ESLT      : num [1:17711] 2 2 2 2 2 2 1 1 2 2 ...\n  ..$ EELCIGT   : num [1:17711] 2 1 2 1 2 1 1 1 2 2 ...\n  ..$ EROLLCIGTS: num [1:17711] 2 2 2 2 2 2 1 2 2 2 ...\n  ..$ EFLAVCIGTS: num [1:17711] 2 2 2 1 2 2 1 2 2 2 ...\n  ..$ EBIDIS    : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EFLAVCIGAR: num [1:17711] 2 1 2 2 2 2 1 2 2 2 ...\n  ..$ EHOOKAH   : num [1:17711] 2 2 2 2 2 2 2 1 2 2 ...\n  ..$ EPIPE     : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESNUS     : num [1:17711] 2 2 2 2 2 2 1 2 2 2 ...\n  ..$ EDISSOLV  : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGT     : num [1:17711] 2 1 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGAR    : num [1:17711] 2 1 2 2 2 2 2 2 2 2 ...\n  ..$ CSLT      : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CELCIGT   : num [1:17711] 2 2 2 1 2 2 2 2 2 2 ...\n  ..$ CROLLCIGTS: num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CFLAVCIGTS: num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CBIDIS    : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CHOOKAH   : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CPIPE     : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSNUS     : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CDISSOLV  : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Qn1 = col_double(),\n  .. ..   Qn2 = col_double(),\n  .. ..   Qn3 = col_double(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EFLAVCIGTS = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   EFLAVCIGAR = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CFLAVCIGTS = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2016: spc_tbl_ [20,675 × 34] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:20675] \"073102\" \"073102\" \"073102\" \"073102\" ...\n  ..$ finwgt    : num [1:20675] 2817 2817 2817 2817 3351 ...\n  ..$ stratum   : chr [1:20675] \"BR1\" \"BR1\" \"BR1\" \"BR1\" ...\n  ..$ Q1        : chr [1:20675] \"08\" \"08\" \"07\" \"07\" ...\n  ..$ Q2        : chr [1:20675] \"1\" \"1\" \"1\" \"1\" ...\n  ..$ Q3        : num [1:20675] 5 5 5 5 5 5 5 7 5 5 ...\n  ..$ ECIGT     : num [1:20675] 2 1 2 2 2 1 2 2 NA 1 ...\n  ..$ ECIGAR    : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ ESLT      : num [1:20675] 2 2 2 2 2 2 2 1 1 2 ...\n  ..$ EELCIGT   : num [1:20675] 2 1 2 2 2 1 2 2 NA 2 ...\n  ..$ EHOOKAH   : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ EROLLCIGTS: num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ EFLAVCIGAR: num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ EPIPE     : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ ESNUS     : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ EDISSOLV  : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ EBIDIS    : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CCIGT     : num [1:20675] 2 1 2 2 2 1 2 2 NA 1 ...\n  ..$ CCIGAR    : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ CSLT      : num [1:20675] 2 2 2 2 2 2 2 1 1 2 ...\n  ..$ CELCIGT   : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CHOOKAH   : num [1:20675] 2 2 2 2 2 2 2 2 1 2 ...\n  ..$ CROLLCIGTS: num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ CPIPE     : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CSNUS     : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CDISSOLV  : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CBIDIS    : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ Q50A      : num [1:20675] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50B      : num [1:20675] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50C      : num [1:20675] NA 1 NA NA NA NA NA NA NA NA ...\n  ..$ Q50D      : num [1:20675] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50E      : num [1:20675] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50F      : num [1:20675] NA 1 NA NA NA NA NA NA NA NA ...\n  ..$ Q50G      : num [1:20675] NA 1 NA NA NA NA NA 1 NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_character(),\n  .. ..   Q3 = col_double(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EFLAVCIGAR = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   Q50A = col_double(),\n  .. ..   Q50B = col_double(),\n  .. ..   Q50C = col_double(),\n  .. ..   Q50D = col_double(),\n  .. ..   Q50E = col_double(),\n  .. ..   Q50F = col_double(),\n  .. ..   Q50G = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2017: spc_tbl_ [17,872 × 33] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:17872] \"600815\" \"600815\" \"600815\" \"600815\" ...\n  ..$ finwgt    : num [1:17872] 1234 1234 1234 1234 1234 ...\n  ..$ stratum   : chr [1:17872] \"HR1\" \"HR1\" \"HR1\" \"HR1\" ...\n  ..$ Q1        : chr [1:17872] \"05\" \"04\" \"04\" \"04\" ...\n  ..$ Q2        : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ Q3        : num [1:17872] 2 2 2 2 2 1 1 1 1 1 ...\n  ..$ ECIGT     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ECIGAR    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESLT      : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EELCIGT   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EHOOKAH   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EROLLCIGTS: num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EPIPE     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESNUS     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EDISSOLV  : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EBIDIS    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGT     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGAR    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSLT      : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CELCIGT   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CHOOKAH   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CROLLCIGTS: num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CPIPE     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSNUS     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CDISSOLV  : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CBIDIS    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ Q50A      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50B      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50C      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50D      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50E      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50F      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50G      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_double(),\n  .. ..   Q3 = col_double(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   Q50A = col_double(),\n  .. ..   Q50B = col_double(),\n  .. ..   Q50C = col_double(),\n  .. ..   Q50D = col_double(),\n  .. ..   Q50E = col_double(),\n  .. ..   Q50F = col_double(),\n  .. ..   Q50G = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2018: spc_tbl_ [20,189 × 33] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:20189] \"015659\" \"015659\" \"015659\" \"015659\" ...\n  ..$ finwgt    : num [1:20189] 751 862 862 862 899 ...\n  ..$ stratum   : chr [1:20189] \"BR3\" \"BR3\" \"BR3\" \"BR3\" ...\n  ..$ Q1        : chr [1:20189] \"04\" \"04\" \"05\" \"04\" ...\n  ..$ Q2        : chr [1:20189] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ Q3        : chr [1:20189] \"1\" \"2\" \"2\" \"2\" ...\n  ..$ ECIGT     : num [1:20189] 2 2 2 2 2 1 2 1 2 2 ...\n  ..$ ECIGAR    : num [1:20189] 2 2 2 2 1 NA 2 2 2 2 ...\n  ..$ ESLT      : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EELCIGT   : num [1:20189] 2 2 2 2 2 1 1 2 2 2 ...\n  ..$ EHOOKAH   : num [1:20189] 2 2 2 NA 2 1 2 2 2 2 ...\n  ..$ EROLLCIGTS: num [1:20189] 2 2 2 2 2 2 2 1 2 2 ...\n  ..$ EPIPE     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESNUS     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EDISSOLV  : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EBIDIS    : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGT     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGAR    : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSLT      : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CELCIGT   : num [1:20189] 2 2 2 2 2 NA 2 2 2 2 ...\n  ..$ CHOOKAH   : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CROLLCIGTS: num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CPIPE     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSNUS     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CDISSOLV  : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CBIDIS    : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ Q50A      : num [1:20189] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50B      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50C      : num [1:20189] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50D      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50E      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50F      : num [1:20189] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50G      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_character(),\n  .. ..   Q3 = col_character(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   Q50A = col_double(),\n  .. ..   Q50B = col_double(),\n  .. ..   Q50C = col_double(),\n  .. ..   Q50D = col_double(),\n  .. ..   Q50E = col_double(),\n  .. ..   Q50F = col_double(),\n  .. ..   Q50G = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2019: spc_tbl_ [19,018 × 36] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : num [1:19018] 58123 58123 58123 58123 58123 ...\n  ..$ finwgt    : num [1:19018] 159 151 151 151 221 ...\n  ..$ stratum   : chr [1:19018] \"HR4\" \"HR4\" \"HR4\" \"HR4\" ...\n  ..$ Q1        : chr [1:19018] \"7\" \"8\" \"6\" \"6\" ...\n  ..$ Q2        : chr [1:19018] \"2\" \"1\" \"1\" \"1\" ...\n  ..$ Q3        : chr [1:19018] \"4\" \"4\" \"4\" \"4\" ...\n  ..$ ECIGT     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ ECIGAR    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ ESLT      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EELCIGT   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EHOOKAH   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EROLLCIGTS: chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EPIPE     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ ESNUS     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EDISSOLV  : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EBIDIS    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EHTP      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CCIGT     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CCIGAR    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CSLT      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CELCIGT   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CHOOKAH   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CROLLCIGTS: chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CPIPE     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CSNUS     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CDISSOLV  : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CBIDIS    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CHTP      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ Q40       : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62A      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62B      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62C      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62D      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62E      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62F      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62G      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_double(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_character(),\n  .. ..   Q3 = col_character(),\n  .. ..   ECIGT = col_character(),\n  .. ..   ECIGAR = col_character(),\n  .. ..   ESLT = col_character(),\n  .. ..   EELCIGT = col_character(),\n  .. ..   EHOOKAH = col_character(),\n  .. ..   EROLLCIGTS = col_character(),\n  .. ..   EPIPE = col_character(),\n  .. ..   ESNUS = col_character(),\n  .. ..   EDISSOLV = col_character(),\n  .. ..   EBIDIS = col_character(),\n  .. ..   EHTP = col_character(),\n  .. ..   CCIGT = col_character(),\n  .. ..   CCIGAR = col_character(),\n  .. ..   CSLT = col_character(),\n  .. ..   CELCIGT = col_character(),\n  .. ..   CHOOKAH = col_character(),\n  .. ..   CROLLCIGTS = col_character(),\n  .. ..   CPIPE = col_character(),\n  .. ..   CSNUS = col_character(),\n  .. ..   CDISSOLV = col_character(),\n  .. ..   CBIDIS = col_character(),\n  .. ..   CHTP = col_character(),\n  .. ..   Q40 = col_character(),\n  .. ..   Q62A = col_character(),\n  .. ..   Q62B = col_character(),\n  .. ..   Q62C = col_character(),\n  .. ..   Q62D = col_character(),\n  .. ..   Q62E = col_character(),\n  .. ..   Q62F = col_character(),\n  .. ..   Q62G = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-cleaning-variable-names",
    "href": "content/lectures/15-cs02-data-slides.html#data-cleaning-variable-names",
    "title": "15-cs02-data",
    "section": "Data Cleaning (Variable Names)",
    "text": "Data Cleaning (Variable Names)\n\n2015Function2016-20182019Check\n\n\n\nnyts_data[[\"nyts2015\"]] <- nyts_data[[\"nyts2015\"]] |>\n  rename(Age = Qn1,\n         Sex = Qn2,\n         Grade = Qn3)\n\n\n\n\nupdate_survey <- function(dataset) { \n  dataset |>\n    rename(Age = Q1,\n           Sex = Q2,\n           Grade = Q3,\n           menthol = Q50A,\n           clove_spice = Q50B,\n           fruit = Q50C,\n           chocolate = Q50D,\n           alcoholic_drink = Q50E,\n           candy_dessert_sweets = Q50F,\n           other = Q50G)\n}\n\n\n\n\nnyts_data <- nyts_data |> \n  map_at(c(\"nyts2016\", \"nyts2017\", \"nyts2018\"), update_survey)\n\n💡 Your Turn: Why are we only applying this function for three of the years?\n\n\nNote: some of the 2019 questions use the values “.N”, “.M”, “.S”, and “.Z” to indicate different types of missing data -> turn into NAs\n\nnyts_data[[\"nyts2019\"]] <- nyts_data[[\"nyts2019\"]] |>\n  rename(brand_ecig = Q40,\n         Age = Q1,\n         Sex = Q2,\n         Grade = Q3,\n         menthol = Q62A,\n         clove_spice = Q62B,\n         fruit = Q62C,\n         chocolate = Q62D,\n         alcoholic_drink = Q62E,\n         candy_dessert_sweets = Q62F,\n         other = Q62G) |>\n  mutate_all(~ replace(., . %in% c(\".N\", \".S\", \".Z\", \".M\", \"M\"), NA)) |>\n  mutate_at(vars(starts_with(\"E\", ignore.case = FALSE),\n                 starts_with(\"C\", ignore.case = FALSE),\n                 menthol:other), \n            list( ~ as.numeric(.)))\n\n\n\n\nmap(nyts_data, names)\n\n$nyts2015\n [1] \"psu\"        \"finwgt\"     \"stratum\"    \"Age\"        \"Sex\"       \n [6] \"Grade\"      \"ECIGT\"      \"ECIGAR\"     \"ESLT\"       \"EELCIGT\"   \n[11] \"EROLLCIGTS\" \"EFLAVCIGTS\" \"EBIDIS\"     \"EFLAVCIGAR\" \"EHOOKAH\"   \n[16] \"EPIPE\"      \"ESNUS\"      \"EDISSOLV\"   \"CCIGT\"      \"CCIGAR\"    \n[21] \"CSLT\"       \"CELCIGT\"    \"CROLLCIGTS\" \"CFLAVCIGTS\" \"CBIDIS\"    \n[26] \"CHOOKAH\"    \"CPIPE\"      \"CSNUS\"      \"CDISSOLV\"  \n\n$nyts2016\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EFLAVCIGAR\"           \"EPIPE\"                \"ESNUS\"               \n[16] \"EDISSOLV\"             \"EBIDIS\"               \"CCIGT\"               \n[19] \"CCIGAR\"               \"CSLT\"                 \"CELCIGT\"             \n[22] \"CHOOKAH\"              \"CROLLCIGTS\"           \"CPIPE\"               \n[25] \"CSNUS\"                \"CDISSOLV\"             \"CBIDIS\"              \n[28] \"menthol\"              \"clove_spice\"          \"fruit\"               \n[31] \"chocolate\"            \"alcoholic_drink\"      \"candy_dessert_sweets\"\n[34] \"other\"               \n\n$nyts2017\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EPIPE\"                \"ESNUS\"                \"EDISSOLV\"            \n[16] \"EBIDIS\"               \"CCIGT\"                \"CCIGAR\"              \n[19] \"CSLT\"                 \"CELCIGT\"              \"CHOOKAH\"             \n[22] \"CROLLCIGTS\"           \"CPIPE\"                \"CSNUS\"               \n[25] \"CDISSOLV\"             \"CBIDIS\"               \"menthol\"             \n[28] \"clove_spice\"          \"fruit\"                \"chocolate\"           \n[31] \"alcoholic_drink\"      \"candy_dessert_sweets\" \"other\"               \n\n$nyts2018\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EPIPE\"                \"ESNUS\"                \"EDISSOLV\"            \n[16] \"EBIDIS\"               \"CCIGT\"                \"CCIGAR\"              \n[19] \"CSLT\"                 \"CELCIGT\"              \"CHOOKAH\"             \n[22] \"CROLLCIGTS\"           \"CPIPE\"                \"CSNUS\"               \n[25] \"CDISSOLV\"             \"CBIDIS\"               \"menthol\"             \n[28] \"clove_spice\"          \"fruit\"                \"chocolate\"           \n[31] \"alcoholic_drink\"      \"candy_dessert_sweets\" \"other\"               \n\n$nyts2019\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EPIPE\"                \"ESNUS\"                \"EDISSOLV\"            \n[16] \"EBIDIS\"               \"EHTP\"                 \"CCIGT\"               \n[19] \"CCIGAR\"               \"CSLT\"                 \"CELCIGT\"             \n[22] \"CHOOKAH\"              \"CROLLCIGTS\"           \"CPIPE\"               \n[25] \"CSNUS\"                \"CDISSOLV\"             \"CBIDIS\"              \n[28] \"CHTP\"                 \"brand_ecig\"           \"menthol\"             \n[31] \"clove_spice\"          \"fruit\"                \"chocolate\"           \n[34] \"alcoholic_drink\"      \"candy_dessert_sweets\" \"other\""
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-cleaning-variable-values",
    "href": "content/lectures/15-cs02-data-slides.html#data-cleaning-variable-values",
    "title": "15-cs02-data",
    "section": "Data Cleaning (Variable Values)",
    "text": "Data Cleaning (Variable Values)\nValues correspond to a category:\n\nAge Value 1 == 9 years old\nGrade Value 1 == 6th grade)"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#data-cleaning-variable-values-1",
    "href": "content/lectures/15-cs02-data-slides.html#data-cleaning-variable-values-1",
    "title": "15-cs02-data",
    "section": "Data Cleaning (Variable Values)",
    "text": "Data Cleaning (Variable Values)\n\nFunctionApply2019-specificCheck\n\n\n\nupdate_values <- function(dataset){\n  dataset |>\n    mutate_all(~ replace(., . %in% c(\"*\", \"**\"), NA)) |>\n    mutate(Age = as.numeric(Age) + 8,\n           Grade = as.numeric(Grade) + 5) |>\n    mutate(Age = as.factor(Age),\n           Grade = as.factor(Grade),\n           Sex = as.factor(Sex)) |>\n    mutate(Sex = case_match(Sex,\n                            \"1\" ~ \"male\",\n                            \"2\" ~ \"female\")) |>\n    mutate_all(~ replace(., . %in% c(\"*\", \"**\"), NA)) |>\n    mutate(Age = case_match(Age, \"19\" ~ \">18\", \n                            .default = Age),\n           Grade = case_match(Grade,\n                              \"13\" ~ \"Ungraded/Other\",\n                              .default = Grade)) |>\n    mutate_at(vars(starts_with(\"E\", ignore.case = FALSE),\n                   starts_with(\"C\", ignore.case = FALSE)\n    ), list( ~ case_match(., 1 ~ TRUE,\n                             2  ~ FALSE,\n                          .default = NA)))\n}\n\n🧠 Your Turn: Explain what at least one function in here is doing?\n\n\n\nnyts_data <- map(nyts_data, update_values)\n\n# function to count how many males\ncount_sex <- function(dataset){dataset |> \n    filter(Sex=='male') |> \n    count(Sex) |> \n    pull(n)}\n\n\n\n\nnyts_data[[\"nyts2019\"]] <- nyts_data[[\"nyts2019\"]]  |>\n  mutate(psu = as.character(psu)) |>\n  mutate(brand_ecig = case_match(brand_ecig,\n                             \"1\" ~ \"Other\", # levels 1,8 combined to `Other`\n                             \"2\" ~ \"Blu\",\n                             \"3\" ~ \"JUUL\",\n                             \"4\" ~ \"Logic\",\n                             \"5\" ~ \"MarkTen\",\n                             \"6\" ~ \"NJOY\",\n                             \"7\" ~ \"Vuse\",\n                             \"8\" ~ \"Other\"))\n\n\n\nAccording to the codebook, we should have:\n\n8,958 males in 2015\n\n10,438 males in 2016\n\n8,881 males in 2017\n\n10,069 males in 2018\n\n9,803 males in 2019 ]\n\n\n# count how many males are in our dataset\nmap(nyts_data, count_sex)\n\n$nyts2015\n[1] 8958\n\n$nyts2016\n[1] 10438\n\n$nyts2017\n[1] 8881\n\n$nyts2018\n[1] 10069\n\n$nyts2019\n[1] 9803"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#flavor-data-2016-2019",
    "href": "content/lectures/15-cs02-data-slides.html#flavor-data-2016-2019",
    "title": "15-cs02-data",
    "section": "Flavor Data (2016-2019)",
    "text": "Flavor Data (2016-2019)\n\nsetting missing values to FALSE, then…\nthe TRUE values will represent those who reported using a specific flavor out of all users (rather than those that used a specific flavor compared to those who used a different flavor.)\n\n\nupdate_flavors <- function(dataset){\n  dataset |>\n    mutate_at(vars(menthol:other),\n              list(~ case_match(.,\n                            1 ~ TRUE,\n                            NA ~ FALSE))) }\n\nnyts_data  <- nyts_data  |> \n  map_at(vars(-nyts2015), update_flavors)"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#combine-the-data",
    "href": "content/lectures/15-cs02-data-slides.html#combine-the-data",
    "title": "15-cs02-data",
    "section": "Combine the data!",
    "text": "Combine the data!\n\nnyts_data <- nyts_data |>\n  map_df(bind_rows, .id = \"year\") |>\n  mutate(year = as.numeric(str_remove(year, \"nyts\")))\n\n❓ Your Turn: What does this code do?"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#the-data-1",
    "href": "content/lectures/15-cs02-data-slides.html#the-data-1",
    "title": "15-cs02-data",
    "section": "The Data",
    "text": "The Data\n\nglimpse(nyts_data)\n\nRows: 95,465\nColumns: 40\n$ year                 <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ psu                  <chr> \"015438\", \"015438\", \"015438\", \"015438\", \"015438\",…\n$ finwgt               <dbl> 216.7268, 324.9620, 324.9620, 397.1552, 264.8745,…\n$ stratum              <chr> \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", …\n$ Age                  <chr> \"18\", \"17\", \"18\", \"18\", \"18\", \"18\", \"18\", \"18\", \"…\n$ Sex                  <chr> \"female\", \"male\", \"male\", \"male\", \"female\", \"fema…\n$ Grade                <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"…\n$ ECIGT                <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ ECIGAR               <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FAL…\n$ ESLT                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, T…\n$ EELCIGT              <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ EROLLCIGTS           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n$ EFLAVCIGTS           <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FA…\n$ EBIDIS               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ EFLAVCIGAR           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FA…\n$ EHOOKAH              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ EPIPE                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ ESNUS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n$ EDISSOLV             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CCIGT                <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ CCIGAR               <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ CSLT                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CELCIGT              <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ CROLLCIGTS           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CFLAVCIGTS           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CBIDIS               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CHOOKAH              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CPIPE                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CSNUS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CDISSOLV             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ menthol              <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ clove_spice          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ fruit                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ chocolate            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ alcoholic_drink      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ candy_dessert_sweets <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ other                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EHTP                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CHTP                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ brand_ecig           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#current-vs.-ever-users",
    "href": "content/lectures/15-cs02-data-slides.html#current-vs.-ever-users",
    "title": "15-cs02-data",
    "section": "Current vs. ever users",
    "text": "Current vs. ever users\nWe define these two groups as follows:\n\ncurrent = students who used a product for >=1 day in the past 30 days\n\never = students who report having used or tried a product at any point in time\n\nAll current users are therefore ever users but not all ever users are current users. Thus, current users are a subset of ever users."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#what-this-looks-like-in-the-data..",
    "href": "content/lectures/15-cs02-data-slides.html#what-this-looks-like-in-the-data..",
    "title": "15-cs02-data",
    "section": "What this looks like in the data..",
    "text": "What this looks like in the data..\n\nEPIPE: Students who reported they have smoked tobacco from a pipe (not hookah).\n\nCPIPE: Students who reported they smoked tobacco in a pipe (not hookah) during the past 30 days.\nEROLLCIGTS: Students who reported they have tried smoking roll-your-own cigarettes.\nCROLLCIGTS: Students who reported they smoked roll-your-own cigarettes during the past 30 days."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#clean-up-columns-tobacco",
    "href": "content/lectures/15-cs02-data-slides.html#clean-up-columns-tobacco",
    "title": "15-cs02-data",
    "section": "Clean up columns: tobacco",
    "text": "Clean up columns: tobacco\n\nnyts_data <- nyts_data %>%\n  mutate(tobacco_sum_ever = rowSums(select(., starts_with(\"E\", \n                                    ignore.case = FALSE)), na.rm = TRUE),\n         tobacco_sum_current = rowSums(select(., starts_with(\"C\", \n                                    ignore.case = FALSE)), na.rm = TRUE))  |>\n  mutate(tobacco_ever = case_when(tobacco_sum_ever > 0 ~ TRUE,\n                                  tobacco_sum_ever == 0 ~ FALSE),\n         tobacco_current = case_when(tobacco_sum_current > 0 ~ TRUE,\n                                     tobacco_sum_current == 0 ~ FALSE))\n\n❓ Your Turn: What does this code do?"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#clean-up-columns-e-cigvaping-vs-others",
    "href": "content/lectures/15-cs02-data-slides.html#clean-up-columns-e-cigvaping-vs-others",
    "title": "15-cs02-data",
    "section": "Clean up columns: e-cig/vaping vs others",
    "text": "Clean up columns: e-cig/vaping vs others\n\nnyts_data <- nyts_data %>%\n  mutate(ecig_sum_ever = rowSums(select(., EELCIGT), na.rm = TRUE),\n         ecig_sum_current = rowSums(select(., CELCIGT), na.rm = TRUE),\n         non_ecig_sum_ever = rowSums(select(., starts_with(\"E\",  ignore.case = FALSE), \n                                            -EELCIGT), na.rm = TRUE),\n         non_ecig_sum_current = rowSums(select(., starts_with(\"C\", ignore.case = FALSE), \n                                               -CELCIGT), na.rm = TRUE)) |>\n  mutate(ecig_ever = case_when(ecig_sum_ever > 0 ~ TRUE,\n                               ecig_sum_ever == 0 ~ FALSE),\n         ecig_current = case_when(ecig_sum_current > 0 ~ TRUE,\n                                  ecig_sum_current == 0 ~ FALSE),\n         non_ecig_ever = case_when(non_ecig_sum_ever > 0 ~ TRUE,\n                                   non_ecig_sum_ever == 0 ~ FALSE),\n         non_ecig_current = case_when(non_ecig_sum_current > 0 ~ TRUE,\n                                      non_ecig_sum_current == 0 ~ FALSE))"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#specify-use-group",
    "href": "content/lectures/15-cs02-data-slides.html#specify-use-group",
    "title": "15-cs02-data",
    "section": "Specify use group",
    "text": "Specify use group\n\nnyts_data <- nyts_data |>\n             mutate(ecig_only_ever = case_when(ecig_ever == TRUE &\n                                           non_ecig_ever == FALSE &\n                                            ecig_current == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n              ecig_only_current = case_when(ecig_current == TRUE &\n                                           non_ecig_ever == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n            non_ecig_only_ever = case_when(non_ecig_ever == TRUE &\n                                               ecig_ever == FALSE &\n                                            ecig_current == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n      non_ecig_only_current = case_when(non_ecig_current == TRUE &\n                                               ecig_ever == FALSE &\n                                            ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n                        no_use = case_when(non_ecig_ever == FALSE &\n                                               ecig_ever == FALSE &\n                                            ecig_current == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE)) %>%\n                 mutate(Group = case_when(ecig_only_ever == TRUE |\n                                       ecig_only_current == TRUE ~ \"Only e-cigarettes\",\n                                      non_ecig_only_ever == TRUE |\n                                   non_ecig_only_current == TRUE ~ \"Only other products\",\n                                                  no_use == TRUE ~ \"Neither\",\n                                          ecig_only_ever == FALSE &\n                                       ecig_only_current == FALSE &\n                                      non_ecig_only_ever == FALSE &\n                                   non_ecig_only_current == FALSE &\n                                                  no_use == FALSE ~ \"Combination of products\"))"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#add-yearly-survey-totals",
    "href": "content/lectures/15-cs02-data-slides.html#add-yearly-survey-totals",
    "title": "15-cs02-data",
    "section": "Add yearly survey totals",
    "text": "Add yearly survey totals\n\nnyts_data <- nyts_data |> \n  add_count(year)\n\n\nThe Data\n\nglimpse(nyts_data)\n\nRows: 95,465\nColumns: 59\n$ year                  <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, …\n$ psu                   <chr> \"015438\", \"015438\", \"015438\", \"015438\", \"015438\"…\n$ finwgt                <dbl> 216.7268, 324.9620, 324.9620, 397.1552, 264.8745…\n$ stratum               <chr> \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\",…\n$ Age                   <chr> \"18\", \"17\", \"18\", \"18\", \"18\", \"18\", \"18\", \"18\", …\n$ Sex                   <chr> \"female\", \"male\", \"male\", \"male\", \"female\", \"fem…\n$ Grade                 <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", …\n$ ECIGT                 <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRU…\n$ ECIGAR                <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FA…\n$ ESLT                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, …\n$ EELCIGT               <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRU…\n$ EROLLCIGTS            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, …\n$ EFLAVCIGTS            <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, F…\n$ EBIDIS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ EFLAVCIGAR            <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n$ EHOOKAH               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ EPIPE                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ ESNUS                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, …\n$ EDISSOLV              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CCIGT                 <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CCIGAR                <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CSLT                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CELCIGT               <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, …\n$ CROLLCIGTS            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CFLAVCIGTS            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CBIDIS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CHOOKAH               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CPIPE                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CSNUS                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CDISSOLV              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ menthol               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ clove_spice           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ fruit                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ chocolate             <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ alcoholic_drink       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candy_dessert_sweets  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ other                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ EHTP                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CHTP                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ brand_ecig            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ tobacco_sum_ever      <dbl> 1, 4, 0, 3, 0, 2, 8, 4, 0, 0, 0, 1, 1, 0, 0, 4, …\n$ tobacco_sum_current   <dbl> 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ tobacco_ever          <lgl> TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ tobacco_current       <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ ecig_sum_ever         <dbl> 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, …\n$ ecig_sum_current      <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ non_ecig_sum_ever     <dbl> 1, 3, 0, 2, 0, 1, 7, 3, 0, 0, 0, 0, 1, 0, 0, 3, …\n$ non_ecig_sum_current  <dbl> 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ecig_ever             <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRU…\n$ ecig_current          <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, …\n$ non_ecig_ever         <lgl> TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ non_ecig_current      <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ ecig_only_ever        <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ ecig_only_current     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ non_ecig_only_ever    <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ non_ecig_only_current <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ no_use                <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, F…\n$ Group                 <chr> \"Only other products\", \"Combination of products\"…\n$ n                     <int> 17711, 17711, 17711, 17711, 17711, 17711, 17711,…"
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#save-the-data",
    "href": "content/lectures/15-cs02-data-slides.html#save-the-data",
    "title": "15-cs02-data",
    "section": "Save the Data",
    "text": "Save the Data\n\nsave(nyts_data, file=\"data/wrangled/wrangled_data_vaping.rda\")\n\nNote: This code assumes dplyr 1.1.0. To get most up-to-date tidyverse packages: install.packages(\"tidyverse\"). This will take 10-20 min to run."
  },
  {
    "objectID": "content/lectures/15-cs02-data-slides.html#suggested-reading",
    "href": "content/lectures/15-cs02-data-slides.html#suggested-reading",
    "title": "15-cs02-data",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nCase Study from OCS\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html",
    "href": "content/lectures/15-cs02-data.html",
    "title": "15-cs02-data",
    "section": "",
    "text": "Q: So linear regression is just for numerical variables and logistic regression is just for a binary outcome? Can we only use one or the other depending on the data?\nA: The model you use, be it linear regression, logistic regression, or something else is always driven by the data-generating process, the assumptions of the model, and the question asking. Specifically, for these two models, yes, the outcome variable guides the choice here. If the outcome is binary, linear regression won’t work given the fact that extrapolation beyond the two possible outcomes (meaning, you can get values other than TRUE/FALSE) will always be possible with linear regression. For logistic regression, it models a binary outcome, given the constraints specified by the model.\n\n\nQ: I noticed the tremendous difference in the complexity/difficulty level between things introduced in lectures and the lab/HW assignments. I wonder if the expectation of the complexity level, for the case study, is similar to lab/HW assignments.\nA: So, I’d love to chat more about this from anyone who has thoughts here b/c I’m always looking for the student perspective. Partially, this is by design. The main concepts are presented in lecture, lab gives you a low-stakes environment to deepend your understanding (since it’s graded on effort and there is an answer key provided), and then hw, now the third time you’ve seen/interacted with the material is where it’s the most “difficult” because you’ve already seen this material before. If I presented the most complex stuff in lecture then people would leave confused b/c they just learned the basics. That said, while the course is designed this way, the leaps are not intended to feel insurmountable, so I’d love to hear more from students about where particularly they’re struggling. That all said, I think of the case studies similar to HW. You’ve seen the material in lecture. You’ve interacted in lab. And, now, you’re working in groups on your third interaction with the material. Keep in mind that we do expect this to be the work of multiple group members. One group members should not be doing all the work.\n\n\nQ: Will CS02 be with the same group or a different one?\nA: They will be different. I’ll be asking about feedback on this policy at the end of the course. Last time we kept the same groups for all case studies (there were three last time) and final projects. Students requested different groups, so I tried that this quarter and will get feedback from you all on this!\n\n\nQ: I am confused about some of the code provided in the boostrapping section.\nA: The shortest explanation is we wanted to run the same model a whole bunch of times…but if we ran it on the same dataset, we’d get the same answer. Instead, we want to see how stable the model is by running the model with a slightly different set of observations each time. To do this, we remove one observation for each model. If the model is stable, removing a single data point should not change the coefficients much…but if by removing a single observation we get very different coefficient estimates, that suggests something is off with our data or model. So, we run the model on all of the subsets, with each subset being slightly different (by one observation) than the next. We store the model outputs. Then, we compare all of the results.\n\n\n\n\nDue Dates:\n\nLecture Participation survey “due” after class\nLab07 due tomorrow (3/3; 11:59 PM)\nCS01 due Mon (3/6; 11:59 PM)\n\nRepo w/ Rmd AND HTML\nSurvey completed on group work (individually; canvas for link)\n\n\n\nNotes:\n\n\nlab07 now available\nExample case study posted\nFinal Project\n\ninstructions posted on website\nfinal project group repos will be created tomorrow\n\nCS02 Groups Discussion\nNo HW04 (full credit will be posted)\n\n\n\n\n\n\n\nBackground\nData Import\nData Wrangling"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#previously-known-before-report",
    "href": "content/lectures/15-cs02-data.html#previously-known-before-report",
    "title": "15-cs02-data",
    "section": "Previously known before report?",
    "text": "Previously known before report?\n\n\n\nSource: CDC"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#what-e-cigarette-vapors-contain",
    "href": "content/lectures/15-cs02-data.html#what-e-cigarette-vapors-contain",
    "title": "15-cs02-data",
    "section": "What e-cigarette vapors contain…",
    "text": "What e-cigarette vapors contain…\n\n\n\nSource: CDC"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#use-associated-with-lung-injury",
    "href": "content/lectures/15-cs02-data.html#use-associated-with-lung-injury",
    "title": "15-cs02-data",
    "section": "Use associated with lung injury",
    "text": "Use associated with lung injury\n\nSource: Chand et al."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#limitations",
    "href": "content/lectures/15-cs02-data.html#limitations",
    "title": "15-cs02-data",
    "section": "Limitations",
    "text": "Limitations\n\nThe National Youth Tobacco Survey (NYTS) does not follow the same individual student respondents over time. A longitudinal study that does follow the same individuals over time collects data called panel data. The data in this study is called pooled cross-sectional data, and is obtained from random collection of observations across time.\nThe data include percentages of student respondents reporting use of each particular tobacco product, but the survey questions did not ask the relative amount of use of one product compared to another. For example, the survey included questions like: “What flavors of tobacco products have you used in the past 30 days?” but did not ask how often one flavor was used by the same individual over another.\nWhile gender and sex are not actually binary, the data used in this analysis only contain information for groups of individuals who answered the survey questions as male or female."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#the-data-source",
    "href": "content/lectures/15-cs02-data.html#the-data-source",
    "title": "15-cs02-data",
    "section": "The Data: Source",
    "text": "The Data: Source\nData come from the National Youth Tobacco Survey (NYTS) - annual survey that asks students in high school and middle school (grades 6-12) about tobacco usage in the United States of America. - we’ll use data from 2015-2019\n\n\nData available here"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#the-data-format",
    "href": "content/lectures/15-cs02-data.html#the-data-format",
    "title": "15-cs02-data",
    "section": "The Data: Format",
    "text": "The Data: Format\n\nOne excel spreadsheet for each year\nCorresponding codebook (explains what each variable stores)\n\n\n\nThe Data: Example"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#codebook-example-variables",
    "href": "content/lectures/15-cs02-data.html#codebook-example-variables",
    "title": "15-cs02-data",
    "section": "Codebook Example: Variables",
    "text": "Codebook Example: Variables"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#codebook-example-details",
    "href": "content/lectures/15-cs02-data.html#codebook-example-details",
    "title": "15-cs02-data",
    "section": "Codebook Example: Details",
    "text": "Codebook Example: Details"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-import",
    "href": "content/lectures/15-cs02-data.html#data-import",
    "title": "15-cs02-data",
    "section": "Data Import",
    "text": "Data Import\n\n# only have to run this once \n# A good time for `eval=FALSE` in code chunk\nOCSdata::load_simpler_import(\"ocs-bp-vaping-case-study\", outpath = getwd())\n\n👉 Your Turn: Load the data into RStudio.\n\n\nThe data have already been cleaned to only include columns of interest\nwill store 5 CSVs in data/simpler_import"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-wrangling",
    "href": "content/lectures/15-cs02-data.html#data-wrangling",
    "title": "15-cs02-data",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n# read in CSVs\nnyts_data <- list.files(\"data/simpler_import/\", \n                        pattern = \"*.csv\", \n                        full.names = TRUE) |>\n  map(~ read_csv(.))\n\n# get names\nnyts_data_names <- list.files(\"data/simpler_import/\",\n                              pattern = \"*.csv\") |>\n  str_extract(\"nyts201[5-9]\")\n\n# apply names\nnames(nyts_data) <- nyts_data_names\n\n💡 How are the data stored after this code has executed?"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-exploration",
    "href": "content/lectures/15-cs02-data.html#data-exploration",
    "title": "15-cs02-data",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nglimpse(nyts_data)\n\nList of 5\n $ nyts2015: spc_tbl_ [17,711 × 29] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:17711] \"015438\" \"015438\" \"015438\" \"015438\" ...\n  ..$ finwgt    : num [1:17711] 217 325 325 397 265 ...\n  ..$ stratum   : chr [1:17711] \"BR3\" \"BR3\" \"BR3\" \"BR3\" ...\n  ..$ Qn1       : num [1:17711] 10 9 10 10 10 10 10 10 10 10 ...\n  ..$ Qn2       : num [1:17711] 2 1 1 1 2 2 1 2 1 2 ...\n  ..$ Qn3       : num [1:17711] 7 7 7 7 7 7 7 7 7 7 ...\n  ..$ ECIGT     : num [1:17711] 2 1 2 1 2 1 1 1 2 2 ...\n  ..$ ECIGAR    : num [1:17711] 1 1 2 2 2 2 1 2 2 2 ...\n  ..$ ESLT      : num [1:17711] 2 2 2 2 2 2 1 1 2 2 ...\n  ..$ EELCIGT   : num [1:17711] 2 1 2 1 2 1 1 1 2 2 ...\n  ..$ EROLLCIGTS: num [1:17711] 2 2 2 2 2 2 1 2 2 2 ...\n  ..$ EFLAVCIGTS: num [1:17711] 2 2 2 1 2 2 1 2 2 2 ...\n  ..$ EBIDIS    : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EFLAVCIGAR: num [1:17711] 2 1 2 2 2 2 1 2 2 2 ...\n  ..$ EHOOKAH   : num [1:17711] 2 2 2 2 2 2 2 1 2 2 ...\n  ..$ EPIPE     : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESNUS     : num [1:17711] 2 2 2 2 2 2 1 2 2 2 ...\n  ..$ EDISSOLV  : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGT     : num [1:17711] 2 1 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGAR    : num [1:17711] 2 1 2 2 2 2 2 2 2 2 ...\n  ..$ CSLT      : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CELCIGT   : num [1:17711] 2 2 2 1 2 2 2 2 2 2 ...\n  ..$ CROLLCIGTS: num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CFLAVCIGTS: num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CBIDIS    : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CHOOKAH   : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CPIPE     : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSNUS     : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CDISSOLV  : num [1:17711] 2 2 2 2 2 2 2 2 2 2 ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Qn1 = col_double(),\n  .. ..   Qn2 = col_double(),\n  .. ..   Qn3 = col_double(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EFLAVCIGTS = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   EFLAVCIGAR = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CFLAVCIGTS = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2016: spc_tbl_ [20,675 × 34] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:20675] \"073102\" \"073102\" \"073102\" \"073102\" ...\n  ..$ finwgt    : num [1:20675] 2817 2817 2817 2817 3351 ...\n  ..$ stratum   : chr [1:20675] \"BR1\" \"BR1\" \"BR1\" \"BR1\" ...\n  ..$ Q1        : chr [1:20675] \"08\" \"08\" \"07\" \"07\" ...\n  ..$ Q2        : chr [1:20675] \"1\" \"1\" \"1\" \"1\" ...\n  ..$ Q3        : num [1:20675] 5 5 5 5 5 5 5 7 5 5 ...\n  ..$ ECIGT     : num [1:20675] 2 1 2 2 2 1 2 2 NA 1 ...\n  ..$ ECIGAR    : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ ESLT      : num [1:20675] 2 2 2 2 2 2 2 1 1 2 ...\n  ..$ EELCIGT   : num [1:20675] 2 1 2 2 2 1 2 2 NA 2 ...\n  ..$ EHOOKAH   : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ EROLLCIGTS: num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ EFLAVCIGAR: num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ EPIPE     : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ ESNUS     : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ EDISSOLV  : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ EBIDIS    : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CCIGT     : num [1:20675] 2 1 2 2 2 1 2 2 NA 1 ...\n  ..$ CCIGAR    : num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ CSLT      : num [1:20675] 2 2 2 2 2 2 2 1 1 2 ...\n  ..$ CELCIGT   : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CHOOKAH   : num [1:20675] 2 2 2 2 2 2 2 2 1 2 ...\n  ..$ CROLLCIGTS: num [1:20675] 2 1 2 2 2 2 2 2 NA 2 ...\n  ..$ CPIPE     : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CSNUS     : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CDISSOLV  : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ CBIDIS    : num [1:20675] 2 2 2 2 2 2 2 2 NA 2 ...\n  ..$ Q50A      : num [1:20675] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50B      : num [1:20675] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50C      : num [1:20675] NA 1 NA NA NA NA NA NA NA NA ...\n  ..$ Q50D      : num [1:20675] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50E      : num [1:20675] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50F      : num [1:20675] NA 1 NA NA NA NA NA NA NA NA ...\n  ..$ Q50G      : num [1:20675] NA 1 NA NA NA NA NA 1 NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_character(),\n  .. ..   Q3 = col_double(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EFLAVCIGAR = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   Q50A = col_double(),\n  .. ..   Q50B = col_double(),\n  .. ..   Q50C = col_double(),\n  .. ..   Q50D = col_double(),\n  .. ..   Q50E = col_double(),\n  .. ..   Q50F = col_double(),\n  .. ..   Q50G = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2017: spc_tbl_ [17,872 × 33] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:17872] \"600815\" \"600815\" \"600815\" \"600815\" ...\n  ..$ finwgt    : num [1:17872] 1234 1234 1234 1234 1234 ...\n  ..$ stratum   : chr [1:17872] \"HR1\" \"HR1\" \"HR1\" \"HR1\" ...\n  ..$ Q1        : chr [1:17872] \"05\" \"04\" \"04\" \"04\" ...\n  ..$ Q2        : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ Q3        : num [1:17872] 2 2 2 2 2 1 1 1 1 1 ...\n  ..$ ECIGT     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ECIGAR    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESLT      : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EELCIGT   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EHOOKAH   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EROLLCIGTS: num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EPIPE     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESNUS     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EDISSOLV  : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EBIDIS    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGT     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGAR    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSLT      : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CELCIGT   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CHOOKAH   : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CROLLCIGTS: num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CPIPE     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSNUS     : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CDISSOLV  : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CBIDIS    : num [1:17872] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ Q50A      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50B      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50C      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50D      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50E      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50F      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50G      : num [1:17872] NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_double(),\n  .. ..   Q3 = col_double(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   Q50A = col_double(),\n  .. ..   Q50B = col_double(),\n  .. ..   Q50C = col_double(),\n  .. ..   Q50D = col_double(),\n  .. ..   Q50E = col_double(),\n  .. ..   Q50F = col_double(),\n  .. ..   Q50G = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2018: spc_tbl_ [20,189 × 33] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : chr [1:20189] \"015659\" \"015659\" \"015659\" \"015659\" ...\n  ..$ finwgt    : num [1:20189] 751 862 862 862 899 ...\n  ..$ stratum   : chr [1:20189] \"BR3\" \"BR3\" \"BR3\" \"BR3\" ...\n  ..$ Q1        : chr [1:20189] \"04\" \"04\" \"05\" \"04\" ...\n  ..$ Q2        : chr [1:20189] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ Q3        : chr [1:20189] \"1\" \"2\" \"2\" \"2\" ...\n  ..$ ECIGT     : num [1:20189] 2 2 2 2 2 1 2 1 2 2 ...\n  ..$ ECIGAR    : num [1:20189] 2 2 2 2 1 NA 2 2 2 2 ...\n  ..$ ESLT      : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EELCIGT   : num [1:20189] 2 2 2 2 2 1 1 2 2 2 ...\n  ..$ EHOOKAH   : num [1:20189] 2 2 2 NA 2 1 2 2 2 2 ...\n  ..$ EROLLCIGTS: num [1:20189] 2 2 2 2 2 2 2 1 2 2 ...\n  ..$ EPIPE     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ ESNUS     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EDISSOLV  : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ EBIDIS    : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGT     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CCIGAR    : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSLT      : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CELCIGT   : num [1:20189] 2 2 2 2 2 NA 2 2 2 2 ...\n  ..$ CHOOKAH   : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CROLLCIGTS: num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CPIPE     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CSNUS     : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CDISSOLV  : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ CBIDIS    : num [1:20189] 2 2 2 2 2 2 2 2 2 2 ...\n  ..$ Q50A      : num [1:20189] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50B      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50C      : num [1:20189] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50D      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50E      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..$ Q50F      : num [1:20189] NA NA NA NA NA 1 NA NA NA NA ...\n  ..$ Q50G      : num [1:20189] NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_character(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_character(),\n  .. ..   Q3 = col_character(),\n  .. ..   ECIGT = col_double(),\n  .. ..   ECIGAR = col_double(),\n  .. ..   ESLT = col_double(),\n  .. ..   EELCIGT = col_double(),\n  .. ..   EHOOKAH = col_double(),\n  .. ..   EROLLCIGTS = col_double(),\n  .. ..   EPIPE = col_double(),\n  .. ..   ESNUS = col_double(),\n  .. ..   EDISSOLV = col_double(),\n  .. ..   EBIDIS = col_double(),\n  .. ..   CCIGT = col_double(),\n  .. ..   CCIGAR = col_double(),\n  .. ..   CSLT = col_double(),\n  .. ..   CELCIGT = col_double(),\n  .. ..   CHOOKAH = col_double(),\n  .. ..   CROLLCIGTS = col_double(),\n  .. ..   CPIPE = col_double(),\n  .. ..   CSNUS = col_double(),\n  .. ..   CDISSOLV = col_double(),\n  .. ..   CBIDIS = col_double(),\n  .. ..   Q50A = col_double(),\n  .. ..   Q50B = col_double(),\n  .. ..   Q50C = col_double(),\n  .. ..   Q50D = col_double(),\n  .. ..   Q50E = col_double(),\n  .. ..   Q50F = col_double(),\n  .. ..   Q50G = col_double()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr> \n $ nyts2019: spc_tbl_ [19,018 × 36] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n  ..$ psu       : num [1:19018] 58123 58123 58123 58123 58123 ...\n  ..$ finwgt    : num [1:19018] 159 151 151 151 221 ...\n  ..$ stratum   : chr [1:19018] \"HR4\" \"HR4\" \"HR4\" \"HR4\" ...\n  ..$ Q1        : chr [1:19018] \"7\" \"8\" \"6\" \"6\" ...\n  ..$ Q2        : chr [1:19018] \"2\" \"1\" \"1\" \"1\" ...\n  ..$ Q3        : chr [1:19018] \"4\" \"4\" \"4\" \"4\" ...\n  ..$ ECIGT     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ ECIGAR    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ ESLT      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EELCIGT   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EHOOKAH   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EROLLCIGTS: chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EPIPE     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ ESNUS     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EDISSOLV  : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EBIDIS    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ EHTP      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CCIGT     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CCIGAR    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CSLT      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CELCIGT   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CHOOKAH   : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CROLLCIGTS: chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CPIPE     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CSNUS     : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CDISSOLV  : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CBIDIS    : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ CHTP      : chr [1:19018] \"2\" \"2\" \"2\" \"2\" ...\n  ..$ Q40       : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62A      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62B      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62C      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62D      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62E      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62F      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..$ Q62G      : chr [1:19018] \".S\" \".S\" \".S\" \".S\" ...\n  ..- attr(*, \"spec\")=\n  .. .. cols(\n  .. ..   psu = col_double(),\n  .. ..   finwgt = col_double(),\n  .. ..   stratum = col_character(),\n  .. ..   Q1 = col_character(),\n  .. ..   Q2 = col_character(),\n  .. ..   Q3 = col_character(),\n  .. ..   ECIGT = col_character(),\n  .. ..   ECIGAR = col_character(),\n  .. ..   ESLT = col_character(),\n  .. ..   EELCIGT = col_character(),\n  .. ..   EHOOKAH = col_character(),\n  .. ..   EROLLCIGTS = col_character(),\n  .. ..   EPIPE = col_character(),\n  .. ..   ESNUS = col_character(),\n  .. ..   EDISSOLV = col_character(),\n  .. ..   EBIDIS = col_character(),\n  .. ..   EHTP = col_character(),\n  .. ..   CCIGT = col_character(),\n  .. ..   CCIGAR = col_character(),\n  .. ..   CSLT = col_character(),\n  .. ..   CELCIGT = col_character(),\n  .. ..   CHOOKAH = col_character(),\n  .. ..   CROLLCIGTS = col_character(),\n  .. ..   CPIPE = col_character(),\n  .. ..   CSNUS = col_character(),\n  .. ..   CDISSOLV = col_character(),\n  .. ..   CBIDIS = col_character(),\n  .. ..   CHTP = col_character(),\n  .. ..   Q40 = col_character(),\n  .. ..   Q62A = col_character(),\n  .. ..   Q62B = col_character(),\n  .. ..   Q62C = col_character(),\n  .. ..   Q62D = col_character(),\n  .. ..   Q62E = col_character(),\n  .. ..   Q62F = col_character(),\n  .. ..   Q62G = col_character()\n  .. .. )\n  ..- attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-cleaning-variable-names",
    "href": "content/lectures/15-cs02-data.html#data-cleaning-variable-names",
    "title": "15-cs02-data",
    "section": "Data Cleaning (Variable Names)",
    "text": "Data Cleaning (Variable Names)\n\n2015Function2016-20182019Check\n\n\n\nnyts_data[[\"nyts2015\"]] <- nyts_data[[\"nyts2015\"]] |>\n  rename(Age = Qn1,\n         Sex = Qn2,\n         Grade = Qn3)\n\n\n\n\nupdate_survey <- function(dataset) { \n  dataset |>\n    rename(Age = Q1,\n           Sex = Q2,\n           Grade = Q3,\n           menthol = Q50A,\n           clove_spice = Q50B,\n           fruit = Q50C,\n           chocolate = Q50D,\n           alcoholic_drink = Q50E,\n           candy_dessert_sweets = Q50F,\n           other = Q50G)\n}\n\n\n\n\nnyts_data <- nyts_data |> \n  map_at(c(\"nyts2016\", \"nyts2017\", \"nyts2018\"), update_survey)\n\n💡 Your Turn: Why are we only applying this function for three of the years?\n\n\nNote: some of the 2019 questions use the values “.N”, “.M”, “.S”, and “.Z” to indicate different types of missing data -> turn into NAs\n\nnyts_data[[\"nyts2019\"]] <- nyts_data[[\"nyts2019\"]] |>\n  rename(brand_ecig = Q40,\n         Age = Q1,\n         Sex = Q2,\n         Grade = Q3,\n         menthol = Q62A,\n         clove_spice = Q62B,\n         fruit = Q62C,\n         chocolate = Q62D,\n         alcoholic_drink = Q62E,\n         candy_dessert_sweets = Q62F,\n         other = Q62G) |>\n  mutate_all(~ replace(., . %in% c(\".N\", \".S\", \".Z\", \".M\", \"M\"), NA)) |>\n  mutate_at(vars(starts_with(\"E\", ignore.case = FALSE),\n                 starts_with(\"C\", ignore.case = FALSE),\n                 menthol:other), \n            list( ~ as.numeric(.)))\n\n\n\n\nmap(nyts_data, names)\n\n$nyts2015\n [1] \"psu\"        \"finwgt\"     \"stratum\"    \"Age\"        \"Sex\"       \n [6] \"Grade\"      \"ECIGT\"      \"ECIGAR\"     \"ESLT\"       \"EELCIGT\"   \n[11] \"EROLLCIGTS\" \"EFLAVCIGTS\" \"EBIDIS\"     \"EFLAVCIGAR\" \"EHOOKAH\"   \n[16] \"EPIPE\"      \"ESNUS\"      \"EDISSOLV\"   \"CCIGT\"      \"CCIGAR\"    \n[21] \"CSLT\"       \"CELCIGT\"    \"CROLLCIGTS\" \"CFLAVCIGTS\" \"CBIDIS\"    \n[26] \"CHOOKAH\"    \"CPIPE\"      \"CSNUS\"      \"CDISSOLV\"  \n\n$nyts2016\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EFLAVCIGAR\"           \"EPIPE\"                \"ESNUS\"               \n[16] \"EDISSOLV\"             \"EBIDIS\"               \"CCIGT\"               \n[19] \"CCIGAR\"               \"CSLT\"                 \"CELCIGT\"             \n[22] \"CHOOKAH\"              \"CROLLCIGTS\"           \"CPIPE\"               \n[25] \"CSNUS\"                \"CDISSOLV\"             \"CBIDIS\"              \n[28] \"menthol\"              \"clove_spice\"          \"fruit\"               \n[31] \"chocolate\"            \"alcoholic_drink\"      \"candy_dessert_sweets\"\n[34] \"other\"               \n\n$nyts2017\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EPIPE\"                \"ESNUS\"                \"EDISSOLV\"            \n[16] \"EBIDIS\"               \"CCIGT\"                \"CCIGAR\"              \n[19] \"CSLT\"                 \"CELCIGT\"              \"CHOOKAH\"             \n[22] \"CROLLCIGTS\"           \"CPIPE\"                \"CSNUS\"               \n[25] \"CDISSOLV\"             \"CBIDIS\"               \"menthol\"             \n[28] \"clove_spice\"          \"fruit\"                \"chocolate\"           \n[31] \"alcoholic_drink\"      \"candy_dessert_sweets\" \"other\"               \n\n$nyts2018\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EPIPE\"                \"ESNUS\"                \"EDISSOLV\"            \n[16] \"EBIDIS\"               \"CCIGT\"                \"CCIGAR\"              \n[19] \"CSLT\"                 \"CELCIGT\"              \"CHOOKAH\"             \n[22] \"CROLLCIGTS\"           \"CPIPE\"                \"CSNUS\"               \n[25] \"CDISSOLV\"             \"CBIDIS\"               \"menthol\"             \n[28] \"clove_spice\"          \"fruit\"                \"chocolate\"           \n[31] \"alcoholic_drink\"      \"candy_dessert_sweets\" \"other\"               \n\n$nyts2019\n [1] \"psu\"                  \"finwgt\"               \"stratum\"             \n [4] \"Age\"                  \"Sex\"                  \"Grade\"               \n [7] \"ECIGT\"                \"ECIGAR\"               \"ESLT\"                \n[10] \"EELCIGT\"              \"EHOOKAH\"              \"EROLLCIGTS\"          \n[13] \"EPIPE\"                \"ESNUS\"                \"EDISSOLV\"            \n[16] \"EBIDIS\"               \"EHTP\"                 \"CCIGT\"               \n[19] \"CCIGAR\"               \"CSLT\"                 \"CELCIGT\"             \n[22] \"CHOOKAH\"              \"CROLLCIGTS\"           \"CPIPE\"               \n[25] \"CSNUS\"                \"CDISSOLV\"             \"CBIDIS\"              \n[28] \"CHTP\"                 \"brand_ecig\"           \"menthol\"             \n[31] \"clove_spice\"          \"fruit\"                \"chocolate\"           \n[34] \"alcoholic_drink\"      \"candy_dessert_sweets\" \"other\""
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-cleaning-variable-values",
    "href": "content/lectures/15-cs02-data.html#data-cleaning-variable-values",
    "title": "15-cs02-data",
    "section": "Data Cleaning (Variable Values)",
    "text": "Data Cleaning (Variable Values)\nValues correspond to a category:\n\nAge Value 1 == 9 years old\nGrade Value 1 == 6th grade)"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#data-cleaning-variable-values-1",
    "href": "content/lectures/15-cs02-data.html#data-cleaning-variable-values-1",
    "title": "15-cs02-data",
    "section": "Data Cleaning (Variable Values)",
    "text": "Data Cleaning (Variable Values)\n\nFunctionApply2019-specificCheck\n\n\n\nupdate_values <- function(dataset){\n  dataset |>\n    mutate_all(~ replace(., . %in% c(\"*\", \"**\"), NA)) |>\n    mutate(Age = as.numeric(Age) + 8,\n           Grade = as.numeric(Grade) + 5) |>\n    mutate(Age = as.factor(Age),\n           Grade = as.factor(Grade),\n           Sex = as.factor(Sex)) |>\n    mutate(Sex = case_match(Sex,\n                            \"1\" ~ \"male\",\n                            \"2\" ~ \"female\")) |>\n    mutate_all(~ replace(., . %in% c(\"*\", \"**\"), NA)) |>\n    mutate(Age = case_match(Age, \"19\" ~ \">18\", \n                            .default = Age),\n           Grade = case_match(Grade,\n                              \"13\" ~ \"Ungraded/Other\",\n                              .default = Grade)) |>\n    mutate_at(vars(starts_with(\"E\", ignore.case = FALSE),\n                   starts_with(\"C\", ignore.case = FALSE)\n    ), list( ~ case_match(., 1 ~ TRUE,\n                             2  ~ FALSE,\n                          .default = NA)))\n}\n\n🧠 Your Turn: Explain what at least one function in here is doing?\n\n\n\nnyts_data <- map(nyts_data, update_values)\n\n# function to count how many males\ncount_sex <- function(dataset){dataset |> \n    filter(Sex=='male') |> \n    count(Sex) |> \n    pull(n)}\n\n\n\n\nnyts_data[[\"nyts2019\"]] <- nyts_data[[\"nyts2019\"]]  |>\n  mutate(psu = as.character(psu)) |>\n  mutate(brand_ecig = case_match(brand_ecig,\n                             \"1\" ~ \"Other\", # levels 1,8 combined to `Other`\n                             \"2\" ~ \"Blu\",\n                             \"3\" ~ \"JUUL\",\n                             \"4\" ~ \"Logic\",\n                             \"5\" ~ \"MarkTen\",\n                             \"6\" ~ \"NJOY\",\n                             \"7\" ~ \"Vuse\",\n                             \"8\" ~ \"Other\"))\n\n\n\nAccording to the codebook, we should have:\n\n8,958 males in 2015\n\n10,438 males in 2016\n\n8,881 males in 2017\n\n10,069 males in 2018\n\n9,803 males in 2019 ]\n\n\n# count how many males are in our dataset\nmap(nyts_data, count_sex)\n\n$nyts2015\n[1] 8958\n\n$nyts2016\n[1] 10438\n\n$nyts2017\n[1] 8881\n\n$nyts2018\n[1] 10069\n\n$nyts2019\n[1] 9803"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#flavor-data-2016-2019",
    "href": "content/lectures/15-cs02-data.html#flavor-data-2016-2019",
    "title": "15-cs02-data",
    "section": "Flavor Data (2016-2019)",
    "text": "Flavor Data (2016-2019)\n\nsetting missing values to FALSE, then…\nthe TRUE values will represent those who reported using a specific flavor out of all users (rather than those that used a specific flavor compared to those who used a different flavor.)\n\n\nupdate_flavors <- function(dataset){\n  dataset |>\n    mutate_at(vars(menthol:other),\n              list(~ case_match(.,\n                            1 ~ TRUE,\n                            NA ~ FALSE))) }\n\nnyts_data  <- nyts_data  |> \n  map_at(vars(-nyts2015), update_flavors)"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#combine-the-data",
    "href": "content/lectures/15-cs02-data.html#combine-the-data",
    "title": "15-cs02-data",
    "section": "Combine the data!",
    "text": "Combine the data!\n\nnyts_data <- nyts_data |>\n  map_df(bind_rows, .id = \"year\") |>\n  mutate(year = as.numeric(str_remove(year, \"nyts\")))\n\n❓ Your Turn: What does this code do?"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#the-data-1",
    "href": "content/lectures/15-cs02-data.html#the-data-1",
    "title": "15-cs02-data",
    "section": "The Data",
    "text": "The Data\n\nglimpse(nyts_data)\n\nRows: 95,465\nColumns: 40\n$ year                 <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ psu                  <chr> \"015438\", \"015438\", \"015438\", \"015438\", \"015438\",…\n$ finwgt               <dbl> 216.7268, 324.9620, 324.9620, 397.1552, 264.8745,…\n$ stratum              <chr> \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", …\n$ Age                  <chr> \"18\", \"17\", \"18\", \"18\", \"18\", \"18\", \"18\", \"18\", \"…\n$ Sex                  <chr> \"female\", \"male\", \"male\", \"male\", \"female\", \"fema…\n$ Grade                <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"…\n$ ECIGT                <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ ECIGAR               <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FAL…\n$ ESLT                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, T…\n$ EELCIGT              <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ EROLLCIGTS           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n$ EFLAVCIGTS           <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, FA…\n$ EBIDIS               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ EFLAVCIGAR           <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FA…\n$ EHOOKAH              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ EPIPE                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ ESNUS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n$ EDISSOLV             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CCIGT                <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ CCIGAR               <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ CSLT                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CELCIGT              <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ CROLLCIGTS           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CFLAVCIGTS           <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CBIDIS               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CHOOKAH              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CPIPE                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CSNUS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CDISSOLV             <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ menthol              <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ clove_spice          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ fruit                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ chocolate            <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ alcoholic_drink      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ candy_dessert_sweets <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ other                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EHTP                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CHTP                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ brand_ecig           <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#current-vs.-ever-users",
    "href": "content/lectures/15-cs02-data.html#current-vs.-ever-users",
    "title": "15-cs02-data",
    "section": "Current vs. ever users",
    "text": "Current vs. ever users\nWe define these two groups as follows:\n\ncurrent = students who used a product for >=1 day in the past 30 days\n\never = students who report having used or tried a product at any point in time\n\nAll current users are therefore ever users but not all ever users are current users. Thus, current users are a subset of ever users."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#what-this-looks-like-in-the-data..",
    "href": "content/lectures/15-cs02-data.html#what-this-looks-like-in-the-data..",
    "title": "15-cs02-data",
    "section": "What this looks like in the data..",
    "text": "What this looks like in the data..\n\nEPIPE: Students who reported they have smoked tobacco from a pipe (not hookah).\n\nCPIPE: Students who reported they smoked tobacco in a pipe (not hookah) during the past 30 days.\nEROLLCIGTS: Students who reported they have tried smoking roll-your-own cigarettes.\nCROLLCIGTS: Students who reported they smoked roll-your-own cigarettes during the past 30 days."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#clean-up-columns-tobacco",
    "href": "content/lectures/15-cs02-data.html#clean-up-columns-tobacco",
    "title": "15-cs02-data",
    "section": "Clean up columns: tobacco",
    "text": "Clean up columns: tobacco\n\nnyts_data <- nyts_data %>%\n  mutate(tobacco_sum_ever = rowSums(select(., starts_with(\"E\", \n                                    ignore.case = FALSE)), na.rm = TRUE),\n         tobacco_sum_current = rowSums(select(., starts_with(\"C\", \n                                    ignore.case = FALSE)), na.rm = TRUE))  |>\n  mutate(tobacco_ever = case_when(tobacco_sum_ever > 0 ~ TRUE,\n                                  tobacco_sum_ever == 0 ~ FALSE),\n         tobacco_current = case_when(tobacco_sum_current > 0 ~ TRUE,\n                                     tobacco_sum_current == 0 ~ FALSE))\n\n❓ Your Turn: What does this code do?"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#clean-up-columns-e-cigvaping-vs-others",
    "href": "content/lectures/15-cs02-data.html#clean-up-columns-e-cigvaping-vs-others",
    "title": "15-cs02-data",
    "section": "Clean up columns: e-cig/vaping vs others",
    "text": "Clean up columns: e-cig/vaping vs others\n\nnyts_data <- nyts_data %>%\n  mutate(ecig_sum_ever = rowSums(select(., EELCIGT), na.rm = TRUE),\n         ecig_sum_current = rowSums(select(., CELCIGT), na.rm = TRUE),\n         non_ecig_sum_ever = rowSums(select(., starts_with(\"E\",  ignore.case = FALSE), \n                                            -EELCIGT), na.rm = TRUE),\n         non_ecig_sum_current = rowSums(select(., starts_with(\"C\", ignore.case = FALSE), \n                                               -CELCIGT), na.rm = TRUE)) |>\n  mutate(ecig_ever = case_when(ecig_sum_ever > 0 ~ TRUE,\n                               ecig_sum_ever == 0 ~ FALSE),\n         ecig_current = case_when(ecig_sum_current > 0 ~ TRUE,\n                                  ecig_sum_current == 0 ~ FALSE),\n         non_ecig_ever = case_when(non_ecig_sum_ever > 0 ~ TRUE,\n                                   non_ecig_sum_ever == 0 ~ FALSE),\n         non_ecig_current = case_when(non_ecig_sum_current > 0 ~ TRUE,\n                                      non_ecig_sum_current == 0 ~ FALSE))"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#specify-use-group",
    "href": "content/lectures/15-cs02-data.html#specify-use-group",
    "title": "15-cs02-data",
    "section": "Specify use group",
    "text": "Specify use group\n\nnyts_data <- nyts_data |>\n             mutate(ecig_only_ever = case_when(ecig_ever == TRUE &\n                                           non_ecig_ever == FALSE &\n                                            ecig_current == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n              ecig_only_current = case_when(ecig_current == TRUE &\n                                           non_ecig_ever == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n            non_ecig_only_ever = case_when(non_ecig_ever == TRUE &\n                                               ecig_ever == FALSE &\n                                            ecig_current == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n      non_ecig_only_current = case_when(non_ecig_current == TRUE &\n                                               ecig_ever == FALSE &\n                                            ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE),\n                        no_use = case_when(non_ecig_ever == FALSE &\n                                               ecig_ever == FALSE &\n                                            ecig_current == FALSE &\n                                        non_ecig_current == FALSE ~ TRUE,\n                                                            TRUE ~ FALSE)) %>%\n                 mutate(Group = case_when(ecig_only_ever == TRUE |\n                                       ecig_only_current == TRUE ~ \"Only e-cigarettes\",\n                                      non_ecig_only_ever == TRUE |\n                                   non_ecig_only_current == TRUE ~ \"Only other products\",\n                                                  no_use == TRUE ~ \"Neither\",\n                                          ecig_only_ever == FALSE &\n                                       ecig_only_current == FALSE &\n                                      non_ecig_only_ever == FALSE &\n                                   non_ecig_only_current == FALSE &\n                                                  no_use == FALSE ~ \"Combination of products\"))"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#add-yearly-survey-totals",
    "href": "content/lectures/15-cs02-data.html#add-yearly-survey-totals",
    "title": "15-cs02-data",
    "section": "Add yearly survey totals",
    "text": "Add yearly survey totals\n\nnyts_data <- nyts_data |> \n  add_count(year)\n\n\nThe Data\n\nglimpse(nyts_data)\n\nRows: 95,465\nColumns: 59\n$ year                  <dbl> 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, …\n$ psu                   <chr> \"015438\", \"015438\", \"015438\", \"015438\", \"015438\"…\n$ finwgt                <dbl> 216.7268, 324.9620, 324.9620, 397.1552, 264.8745…\n$ stratum               <chr> \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\", \"BR3\",…\n$ Age                   <chr> \"18\", \"17\", \"18\", \"18\", \"18\", \"18\", \"18\", \"18\", …\n$ Sex                   <chr> \"female\", \"male\", \"male\", \"male\", \"female\", \"fem…\n$ Grade                 <chr> \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", \"12\", …\n$ ECIGT                 <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRU…\n$ ECIGAR                <lgl> TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FA…\n$ ESLT                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, …\n$ EELCIGT               <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRU…\n$ EROLLCIGTS            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, …\n$ EFLAVCIGTS            <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, TRUE, F…\n$ EBIDIS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ EFLAVCIGAR            <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, F…\n$ EHOOKAH               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ EPIPE                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ ESNUS                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, …\n$ EDISSOLV              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CCIGT                 <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CCIGAR                <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ CSLT                  <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CELCIGT               <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, …\n$ CROLLCIGTS            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CFLAVCIGTS            <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CBIDIS                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CHOOKAH               <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CPIPE                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CSNUS                 <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ CDISSOLV              <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ menthol               <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ clove_spice           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ fruit                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ chocolate             <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ alcoholic_drink       <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ candy_dessert_sweets  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ other                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ EHTP                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ CHTP                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ brand_ecig            <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ tobacco_sum_ever      <dbl> 1, 4, 0, 3, 0, 2, 8, 4, 0, 0, 0, 1, 1, 0, 0, 4, …\n$ tobacco_sum_current   <dbl> 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ tobacco_ever          <lgl> TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ tobacco_current       <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ ecig_sum_ever         <dbl> 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, …\n$ ecig_sum_current      <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ non_ecig_sum_ever     <dbl> 1, 3, 0, 2, 0, 1, 7, 3, 0, 0, 0, 0, 1, 0, 0, 3, …\n$ non_ecig_sum_current  <dbl> 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ecig_ever             <lgl> FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRU…\n$ ecig_current          <lgl> FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, …\n$ non_ecig_ever         <lgl> TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE…\n$ non_ecig_current      <lgl> FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ ecig_only_ever        <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ ecig_only_current     <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ non_ecig_only_ever    <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ non_ecig_only_current <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ no_use                <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, F…\n$ Group                 <chr> \"Only other products\", \"Combination of products\"…\n$ n                     <int> 17711, 17711, 17711, 17711, 17711, 17711, 17711,…"
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#save-the-data",
    "href": "content/lectures/15-cs02-data.html#save-the-data",
    "title": "15-cs02-data",
    "section": "Save the Data",
    "text": "Save the Data\n\nsave(nyts_data, file=\"data/wrangled/wrangled_data_vaping.rda\")\n\nNote: This code assumes dplyr 1.1.0. To get most up-to-date tidyverse packages: install.packages(\"tidyverse\"). This will take 10-20 min to run."
  },
  {
    "objectID": "content/lectures/15-cs02-data.html#suggested-reading",
    "href": "content/lectures/15-cs02-data.html#suggested-reading",
    "title": "15-cs02-data",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nCase Study from OCS"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#qa",
    "href": "content/lectures/10-mlr-slides.html#qa",
    "title": "10-mlr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I didn’t know we don’t have to complete all parts of the lab to get credit - how does it work if we’re graded on effort? Do we still get a full score?\nA: We grade on concerted effort. Meaning, we look at everyone’s labs, and using a rubric relative to what we expect a student could complete in ~1h of work, we grade and give full credit if it looks like at least an hour of time was spent. Now, sometimes it’s hard to judge this as students work at different speeds. If you ever receive less than full credit (2) but feel you made a concerted effort that week in lab, reach out and we’ll chat about it!\n\n\nQ: I am still confused about when are we supposed to use jitter on our scatterplot.\nA: When points on a plot are on top of one another and you can’t tell how many observations are actually plotted, jittering is something to consider. You can alternatively use transparency (alpha) or scale the points relative to how many observations are present at each point. There’s no “you must jitter now” rule, but there are times to consider it."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#course-announcements",
    "href": "content/lectures/10-mlr-slides.html#course-announcements",
    "title": "10-mlr",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 05 due Friday (2/17; 11:59 PM)\nmid-course survey (optional for EC) due Fri (2/17; 11:59 PM)\nLecture Participation survey “due” after class\n\n\n\nHW03 will be available tomorrow (Wed); due (2/27) <- that’s a change"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#agenda",
    "href": "content/lectures/10-mlr-slides.html#agenda",
    "title": "10-mlr",
    "section": "Agenda",
    "text": "Agenda\n\nMultiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#packages-data",
    "href": "content/lectures/10-mlr-slides.html#packages-data",
    "title": "10-mlr",
    "section": "Packages & Data",
    "text": "Packages & Data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nData: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |> \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#multiple-predictors",
    "href": "content/lectures/10-mlr-slides.html#multiple-predictors",
    "title": "10-mlr",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nResponse variable: log_price\nExplanatory variables: Width and height\n\n\n\nlin_mod <- linear_reg() |>\n  set_engine(\"lm\")\n\npp_fit <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data = pp)\ntidy(pp_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#linear-model-with-multiple-predictors",
    "href": "content/lectures/10-mlr-slides.html#linear-model-with-multiple-predictors",
    "title": "10-mlr",
    "section": "Linear model with multiple predictors",
    "text": "Linear model with multiple predictors\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4\n\n\n\n\\[\\widehat{log\\_price} = 4.77 + 0.0269 \\times width - 0.0133 \\times height\\]\n\n❓ How do we interpret this model?"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#price-surface-area-and-living-artist",
    "href": "content/lectures/10-mlr-slides.html#price-surface-area-and-living-artist",
    "title": "10-mlr",
    "section": "Price, surface area, and living artist",
    "text": "Price, surface area, and living artist\n\nExplore the relationship between price of paintings and surface area, conditioned on whether or not the artist is still living\nFirst visualize and explore, then model\n\n\n\nBut first, prep the data:\n\n\npp <- pp |>\n  mutate(artistliving = case_when(artistliving == 0 ~ \"Deceased\", \n                                  TRUE ~ \"Living\"))\n\npp |>\n  count(artistliving)\n\n# A tibble: 2 × 2\n  artistliving     n\n  <chr>        <int>\n1 Deceased      2937\n2 Living         456"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#typical-surface-area",
    "href": "content/lectures/10-mlr-slides.html#typical-surface-area",
    "title": "10-mlr",
    "section": "Typical surface area",
    "text": "Typical surface area\n\nPlotCode\n\n\n\n\n\n\n\nTypical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.\n\n\n\nggplot(data = pp, aes(x = Surface, fill = artistliving)) +\n  geom_histogram(binwidth = 500) + \n  facet_grid(artistliving ~ .) +\n  scale_fill_manual(values = c(\"#E48957\", \"#071381\")) +\n  guides(fill = \"none\") +\n  labs(x = \"Surface area\", y = NULL) +\n  geom_vline(xintercept = 1000) +\n  geom_vline(xintercept = 5000, linetype = \"dashed\", color = \"gray\")"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#narrowing-the-scope",
    "href": "content/lectures/10-mlr-slides.html#narrowing-the-scope",
    "title": "10-mlr",
    "section": "Narrowing the scope",
    "text": "Narrowing the scope\n\nPlotCode\n\n\nFor simplicity let’s focus on the paintings with Surface < 5000:\n\n\n\n\n\n\n\n\npp_Surf_lt_5000 <- pp |>\n  filter(Surface < 5000)\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Artist\", shape = \"Artist\") +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\"))"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#facet-to-get-a-better-look",
    "href": "content/lectures/10-mlr-slides.html#facet-to-get-a-better-look",
    "title": "10-mlr",
    "section": "Facet to get a better look",
    "text": "Facet to get a better look\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~artistliving) +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\")) +\n  labs(color = \"Artist\", shape = \"Artist\")"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#two-ways-to-model",
    "href": "content/lectures/10-mlr-slides.html#two-ways-to-model",
    "title": "10-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living.\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#interacting-explanatory-variables",
    "href": "content/lectures/10-mlr-slides.html#interacting-explanatory-variables",
    "title": "10-mlr",
    "section": "Interacting explanatory variables",
    "text": "Interacting explanatory variables\n\nIncluding an interaction effect in the model allows for different slopes, i.e.  nonparallel lines.\nThis implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\nThis can be accomplished by adding an interaction variable: the product of two explanatory variables."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#two-ways-to-model-1",
    "href": "content/lectures/10-mlr-slides.html#two-ways-to-model-1",
    "title": "10-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\n\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living\n\n\n\n\n\n\n\n\n\n\n❓ Which does your intuition/knowledge of the data suggest is more appropriate?\nPut a green sticky if you think main; pink if you think interaction."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#fit-model-with-main-effects",
    "href": "content/lectures/10-mlr-slides.html#fit-model-with-main-effects",
    "title": "10-mlr",
    "section": "Fit model with main effects",
    "text": "Fit model with main effects\n\nResponse variable: log_price\nExplanatory variables: Surface area and artistliving\n\n\npp_main_fit <- lin_mod |>\n  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)\ntidy(pp_main_fit)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        4.88     0.0424       115.   0       \n2 Surface            0.000265 0.0000415      6.39 1.85e-10\n3 artistlivingLiving 0.137    0.0970         1.41 1.57e- 1\n\n\n\n\\[\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times artistliving\\]"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#solving-the-model",
    "href": "content/lectures/10-mlr-slides.html#solving-the-model",
    "title": "10-mlr",
    "section": "Solving the model",
    "text": "Solving the model\n\nNon-living artist: Plug in 0 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 0\\)\n\\(= 4.88 + 0.000265 \\times surface\\)\n\n\nLiving artist: Plug in 1 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 1\\)\n\\(= 5.017 + 0.000265 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#visualizing-main-effects",
    "href": "content/lectures/10-mlr-slides.html#visualizing-main-effects",
    "title": "10-mlr",
    "section": "Visualizing main effects",
    "text": "Visualizing main effects\n\n\n\nSame slope: Rate of change in price as the surface area increases does not vary between paintings by living and non-living artists.\nDifferent intercept: Paintings by living artists are consistently more expensive than paintings by non-living artists."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#interpreting-main-effects",
    "href": "content/lectures/10-mlr-slides.html#interpreting-main-effects",
    "title": "10-mlr",
    "section": "Interpreting main effects",
    "text": "Interpreting main effects\n\ntidy(pp_main_fit) |> \n  mutate(exp_estimate = exp(estimate)) |>\n  select(term, estimate, exp_estimate)\n\n# A tibble: 3 × 3\n  term               estimate exp_estimate\n  <chr>                 <dbl>        <dbl>\n1 (Intercept)        4.88           132.  \n2 Surface            0.000265         1.00\n3 artistlivingLiving 0.137            1.15\n\n\n\n\nAll else held constant, for each additional square inch in painting’s surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.\nAll else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.\nPaintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#main-vs.-interaction-effects",
    "href": "content/lectures/10-mlr-slides.html#main-vs.-interaction-effects",
    "title": "10-mlr",
    "section": "Main vs. interaction effects",
    "text": "Main vs. interaction effects\n\nThe way we specified our main effects model only lets artistliving affect the intercept.\nModel implicitly assumes that paintings with living and deceased artists have the same slope and only allows for different intercepts.\n\n\n❓ What seems more appropriate in this case?\n\nSame slope and same intercept for both colors\nSame slope and different intercept for both colors\nDifferent slope and different intercept for both colors"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#interaction-surface-artistliving",
    "href": "content/lectures/10-mlr-slides.html#interaction-surface-artistliving",
    "title": "10-mlr",
    "section": "Interaction: Surface * artistliving",
    "text": "Interaction: Surface * artistliving"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#fit-model-with-interaction-effects",
    "href": "content/lectures/10-mlr-slides.html#fit-model-with-interaction-effects",
    "title": "10-mlr",
    "section": "Fit model with interaction effects",
    "text": "Fit model with interaction effects\n\nResponse variable: log_price\nExplanatory variables: Surface area, artistliving, and their interaction\n\n\npp_int_fit <- lin_mod |>\n  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)\ntidy(pp_int_fit)\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#linear-model-with-interaction-effects",
    "href": "content/lectures/10-mlr-slides.html#linear-model-with-interaction-effects",
    "title": "10-mlr",
    "section": "Linear model with interaction effects",
    "text": "Linear model with interaction effects\n\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139  \n\n\n\\[\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface - 0.126 \\times artistliving\\] \\[+ ~ 0.00048 \\times surface * artistliving\\]"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#interpretation-of-interaction-effects",
    "href": "content/lectures/10-mlr-slides.html#interpretation-of-interaction-effects",
    "title": "10-mlr",
    "section": "Interpretation of interaction effects",
    "text": "Interpretation of interaction effects\n\n\nRate of change in price as the surface area of the painting increases does vary between paintings by living and non-living artists (different slopes)\nSome paintings by living artists are more expensive than paintings by non-living artists, and some are not (different intercept).\n\n\n\n\n\n\n\nNon-living artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 0 + 0.00048 \\times surface \\times 0\\) \\(= 4.91 + 0.00021 \\times surface\\)\nLiving artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 1 + 0.00048 \\times surface \\times 1\\) \\(= 4.91 + 0.00021 \\times surface\\) \\(- 0.126 + 0.00048 \\times surface\\) \\(= 4.784 + 0.00069 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#r-squared",
    "href": "content/lectures/10-mlr-slides.html#r-squared",
    "title": "10-mlr",
    "section": "R-squared",
    "text": "R-squared\n\n\\(R^2\\) is the percentage of variability in the response variable explained by the regression model.\n\n\nglance(pp_main_fit)$r.squared\n\n[1] 0.01320884\n\nglance(pp_int_fit)$r.squared\n\n[1] 0.0176922\n\n\n\n\nClearly the model with interactions has a higher \\(R^2\\).\n\n\n\n\nHowever using \\(R^2\\) for model selection in models with multiple explanatory variables is not a good idea as \\(R^2\\) increases when any variable is added to the model."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#adjusted-r-squared",
    "href": "content/lectures/10-mlr-slides.html#adjusted-r-squared",
    "title": "10-mlr",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\nIt appears that adding the interaction actually increased adjusted \\(R^2\\), so we should indeed use the model with the interactions.\n\nglance(pp_main_fit)$adj.r.squared\n\n[1] 0.01258977\n\nglance(pp_int_fit)$adj.r.squared\n\n[1] 0.01676753"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#third-order-interactions",
    "href": "content/lectures/10-mlr-slides.html#third-order-interactions",
    "title": "10-mlr",
    "section": "Third order interactions",
    "text": "Third order interactions\n\nCan you? Yes\nShould you? Probably not if you want to interpret these interactions in context of the data."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#in-pursuit-of-occams-razor",
    "href": "content/lectures/10-mlr-slides.html#in-pursuit-of-occams-razor",
    "title": "10-mlr",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model."
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#backward-selection",
    "href": "content/lectures/10-mlr-slides.html#backward-selection",
    "title": "10-mlr",
    "section": "Backward selection",
    "text": "Backward selection\nFor this demo, we’ll ignore interaction effects…and just model main effects to start:\n\npp_full <-  lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp) \n\nglance(pp_full)$adj.r.squared\n\n[1] 0.02570141\n\n\n\n\\(R^2\\) (full): 0.0257014`\n\n\nRemove artistliving\n\npp_noartist <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface, data=pp) \n\nglance(pp_noartist)$adj.r.squared\n\n[1] 0.02579859\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\n…Improved variance explained\n\nRemove Surface\n\npp_noartist_nosurface <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data=pp) \n\nglance(pp_noartist_nosurface)$adj.r.squared\n\n[1] 0.02231559\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\\(R^2\\) (no artistliving or Surface): 0.0223\n\n\n\n…no longer gaining improvement, so we stick with: log_price ~ Width_in + Height_in + Surface"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#other-approach",
    "href": "content/lectures/10-mlr-slides.html#other-approach",
    "title": "10-mlr",
    "section": "Other approach:",
    "text": "Other approach:\n\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n\nStep 1: Fit model (w/o tidymodels)\n\n# fit the model (not using tidymodels)\nmod <- lm(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp_Surf_lt_5000)\n\n\n\nStep 2: Determine which variables to remove\n\nols_step_backward_p(mod)\n\n\n                             Elimination Summary                               \n------------------------------------------------------------------------------\n        Variable                      Adj.                                        \nStep      Removed       R-Square    R-Square     C(p)        AIC         RMSE     \n------------------------------------------------------------------------------\n   1    artistliving      0.0261      0.0251    3.8495    12603.7727    1.8315    \n------------------------------------------------------------------------------\n\n\n…specifies that artistliving should be removed\n\n\nStep 2 (alternate): Compare all possible models…\n\nols_step_all_possible(mod) |>\n  arrange(desc(adjr))\n\n   Index N                              Predictors     R-Square Adj. R-Square\n1     11 3              Width_in Height_in Surface 0.0260749939  0.0251349118\n2     15 4 Width_in Height_in Surface artistliving 0.0263412027  0.0250876993\n3      5 2                      Width_in Height_in 0.0256902566  0.0250634893\n4     12 3         Width_in Height_in artistliving 0.0259732581  0.0250330779\n5      6 2                        Width_in Surface 0.0249136264  0.0242863596\n6     13 3           Width_in Surface artistliving 0.0251787948  0.0242378477\n7      7 2                   Width_in artistliving 0.0212864021  0.0206568018\n8      1 1                                Width_in 0.0209415833  0.0206267736\n9      8 2                    Surface artistliving 0.0132088377  0.0125897717\n10     2 1                                 Surface 0.0125899681  0.0122803381\n11    14 3          Height_in Surface artistliving 0.0130836930  0.0121310711\n12     9 2                       Height_in Surface 0.0126782901  0.0120431523\n13    10 2                  Height_in artistliving 0.0062698155  0.0056305552\n14     3 1                               Height_in 0.0058797727  0.0055601199\n15     4 1                            artistliving 0.0005531617  0.0002397573\n   Mallow's Cp\n1     3.849487\n2     5.000000\n3     3.077206\n4     4.174132\n5     5.555476\n6     6.709309\n7    17.130153\n8    16.230489\n9    51.971190\n10   52.001268\n11   45.305459\n12   44.599123\n13   65.048926\n14   64.293574\n15   91.485605"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#recap",
    "href": "content/lectures/10-mlr-slides.html#recap",
    "title": "10-mlr",
    "section": "Recap",
    "text": "Recap\n\nCan you model and interpret linear models with multiple predictors?\nCan you explain the difference in a model with main effects vs. interaction effects?\nCan you compare different models and determine how to proceed?\nCan you carry out and explain backward selection?"
  },
  {
    "objectID": "content/lectures/10-mlr-slides.html#suggested-reading",
    "href": "content/lectures/10-mlr-slides.html#suggested-reading",
    "title": "10-mlr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nIntroduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/10-mlr.html",
    "href": "content/lectures/10-mlr.html",
    "title": "10-mlr",
    "section": "",
    "text": "Q: I didn’t know we don’t have to complete all parts of the lab to get credit - how does it work if we’re graded on effort? Do we still get a full score?\nA: We grade on concerted effort. Meaning, we look at everyone’s labs, and using a rubric relative to what we expect a student could complete in ~1h of work, we grade and give full credit if it looks like at least an hour of time was spent. Now, sometimes it’s hard to judge this as students work at different speeds. If you ever receive less than full credit (2) but feel you made a concerted effort that week in lab, reach out and we’ll chat about it!\n\n\nQ: I am still confused about when are we supposed to use jitter on our scatterplot.\nA: When points on a plot are on top of one another and you can’t tell how many observations are actually plotted, jittering is something to consider. You can alternatively use transparency (alpha) or scale the points relative to how many observations are present at each point. There’s no “you must jitter now” rule, but there are times to consider it.\n\n\n\n\nDue Dates:\n\nLab 05 due Friday (2/17; 11:59 PM)\nmid-course survey (optional for EC) due Fri (2/17; 11:59 PM)\nLecture Participation survey “due” after class\n\n\n\nHW03 will be available tomorrow (Wed); due (2/27) <- that’s a change\n\n\n\n\n\n\nMultiple Linear Regression\n\nMultiple predictors\nMain vs interaction effects\nModel comparison\nBackward selection\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\nData: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\")) |> \n  mutate(log_price = log(price))\n\n\nNumber of observations: 3393\nNumber of variables: 62"
  },
  {
    "objectID": "content/lectures/10-mlr.html#multiple-predictors",
    "href": "content/lectures/10-mlr.html#multiple-predictors",
    "title": "10-mlr",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nResponse variable: log_price\nExplanatory variables: Width and height\n\n\n\nlin_mod <- linear_reg() |>\n  set_engine(\"lm\")\n\npp_fit <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data = pp)\ntidy(pp_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4"
  },
  {
    "objectID": "content/lectures/10-mlr.html#linear-model-with-multiple-predictors",
    "href": "content/lectures/10-mlr.html#linear-model-with-multiple-predictors",
    "title": "10-mlr",
    "section": "Linear model with multiple predictors",
    "text": "Linear model with multiple predictors\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   4.77     0.0579      82.4  0       \n2 Width_in      0.0269   0.00373      7.22 6.58e-13\n3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4\n\n\n\n\\[\\widehat{log\\_price} = 4.77 + 0.0269 \\times width - 0.0133 \\times height\\]\n\n❓ How do we interpret this model?"
  },
  {
    "objectID": "content/lectures/10-mlr.html#price-surface-area-and-living-artist",
    "href": "content/lectures/10-mlr.html#price-surface-area-and-living-artist",
    "title": "10-mlr",
    "section": "Price, surface area, and living artist",
    "text": "Price, surface area, and living artist\n\nExplore the relationship between price of paintings and surface area, conditioned on whether or not the artist is still living\nFirst visualize and explore, then model\n\n\n\nBut first, prep the data:\n\n\npp <- pp |>\n  mutate(artistliving = case_when(artistliving == 0 ~ \"Deceased\", \n                                  TRUE ~ \"Living\"))\n\npp |>\n  count(artistliving)\n\n# A tibble: 2 × 2\n  artistliving     n\n  <chr>        <int>\n1 Deceased      2937\n2 Living         456"
  },
  {
    "objectID": "content/lectures/10-mlr.html#typical-surface-area",
    "href": "content/lectures/10-mlr.html#typical-surface-area",
    "title": "10-mlr",
    "section": "Typical surface area",
    "text": "Typical surface area\n\nPlotCode\n\n\n\n\n\n\n\nTypical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.\n\n\n\nggplot(data = pp, aes(x = Surface, fill = artistliving)) +\n  geom_histogram(binwidth = 500) + \n  facet_grid(artistliving ~ .) +\n  scale_fill_manual(values = c(\"#E48957\", \"#071381\")) +\n  guides(fill = \"none\") +\n  labs(x = \"Surface area\", y = NULL) +\n  geom_vline(xintercept = 1000) +\n  geom_vline(xintercept = 5000, linetype = \"dashed\", color = \"gray\")"
  },
  {
    "objectID": "content/lectures/10-mlr.html#narrowing-the-scope",
    "href": "content/lectures/10-mlr.html#narrowing-the-scope",
    "title": "10-mlr",
    "section": "Narrowing the scope",
    "text": "Narrowing the scope\n\nPlotCode\n\n\nFor simplicity let’s focus on the paintings with Surface < 5000:\n\n\n\n\n\n\n\n\npp_Surf_lt_5000 <- pp |>\n  filter(Surface < 5000)\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Artist\", shape = \"Artist\") +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\"))"
  },
  {
    "objectID": "content/lectures/10-mlr.html#facet-to-get-a-better-look",
    "href": "content/lectures/10-mlr.html#facet-to-get-a-better-look",
    "title": "10-mlr",
    "section": "Facet to get a better look",
    "text": "Facet to get a better look\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data = pp_Surf_lt_5000, \n       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~artistliving) +\n  scale_color_manual(values = c(\"#E48957\", \"#071381\")) +\n  labs(color = \"Artist\", shape = \"Artist\")"
  },
  {
    "objectID": "content/lectures/10-mlr.html#two-ways-to-model",
    "href": "content/lectures/10-mlr.html#two-ways-to-model",
    "title": "10-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living.\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living."
  },
  {
    "objectID": "content/lectures/10-mlr.html#interacting-explanatory-variables",
    "href": "content/lectures/10-mlr.html#interacting-explanatory-variables",
    "title": "10-mlr",
    "section": "Interacting explanatory variables",
    "text": "Interacting explanatory variables\n\nIncluding an interaction effect in the model allows for different slopes, i.e.  nonparallel lines.\nThis implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\nThis can be accomplished by adding an interaction variable: the product of two explanatory variables."
  },
  {
    "objectID": "content/lectures/10-mlr.html#two-ways-to-model-1",
    "href": "content/lectures/10-mlr.html#two-ways-to-model-1",
    "title": "10-mlr",
    "section": "Two ways to model",
    "text": "Two ways to model\n\n\n\nMain effects: Assuming relationship between surface and logged price does not vary by whether or not the artist is living\nInteraction effects: Assuming relationship between surface and logged price varies by whether or not the artist is living\n\n\n\n\n\n\n\n\n\n\n❓ Which does your intuition/knowledge of the data suggest is more appropriate?\nPut a green sticky if you think main; pink if you think interaction."
  },
  {
    "objectID": "content/lectures/10-mlr.html#fit-model-with-main-effects",
    "href": "content/lectures/10-mlr.html#fit-model-with-main-effects",
    "title": "10-mlr",
    "section": "Fit model with main effects",
    "text": "Fit model with main effects\n\nResponse variable: log_price\nExplanatory variables: Surface area and artistliving\n\n\npp_main_fit <- lin_mod |>\n  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)\ntidy(pp_main_fit)\n\n# A tibble: 3 × 5\n  term               estimate std.error statistic  p.value\n  <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        4.88     0.0424       115.   0       \n2 Surface            0.000265 0.0000415      6.39 1.85e-10\n3 artistlivingLiving 0.137    0.0970         1.41 1.57e- 1\n\n\n\n\\[\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times artistliving\\]"
  },
  {
    "objectID": "content/lectures/10-mlr.html#solving-the-model",
    "href": "content/lectures/10-mlr.html#solving-the-model",
    "title": "10-mlr",
    "section": "Solving the model",
    "text": "Solving the model\n\nNon-living artist: Plug in 0 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 0\\)\n\\(= 4.88 + 0.000265 \\times surface\\)\n\n\nLiving artist: Plug in 1 for artistliving\n\n\\(\\widehat{log\\_price} = 4.88 + 0.000265 \\times surface + 0.137 \\times 1\\)\n\\(= 5.017 + 0.000265 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/10-mlr.html#visualizing-main-effects",
    "href": "content/lectures/10-mlr.html#visualizing-main-effects",
    "title": "10-mlr",
    "section": "Visualizing main effects",
    "text": "Visualizing main effects\n\n\n\nSame slope: Rate of change in price as the surface area increases does not vary between paintings by living and non-living artists.\nDifferent intercept: Paintings by living artists are consistently more expensive than paintings by non-living artists."
  },
  {
    "objectID": "content/lectures/10-mlr.html#interpreting-main-effects",
    "href": "content/lectures/10-mlr.html#interpreting-main-effects",
    "title": "10-mlr",
    "section": "Interpreting main effects",
    "text": "Interpreting main effects\n\ntidy(pp_main_fit) |> \n  mutate(exp_estimate = exp(estimate)) |>\n  select(term, estimate, exp_estimate)\n\n# A tibble: 3 × 3\n  term               estimate exp_estimate\n  <chr>                 <dbl>        <dbl>\n1 (Intercept)        4.88           132.  \n2 Surface            0.000265         1.00\n3 artistlivingLiving 0.137            1.15\n\n\n\n\nAll else held constant, for each additional square inch in painting’s surface area, the price of the painting is predicted, on average, to be higher by a factor of 1.\nAll else held constant, paintings by a living artist are predicted, on average, to be higher by a factor of 1.15 compared to paintings by an artist who is no longer alive.\nPaintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be 132 livres."
  },
  {
    "objectID": "content/lectures/10-mlr.html#main-vs.-interaction-effects",
    "href": "content/lectures/10-mlr.html#main-vs.-interaction-effects",
    "title": "10-mlr",
    "section": "Main vs. interaction effects",
    "text": "Main vs. interaction effects\n\nThe way we specified our main effects model only lets artistliving affect the intercept.\nModel implicitly assumes that paintings with living and deceased artists have the same slope and only allows for different intercepts.\n\n\n❓ What seems more appropriate in this case?\n\nSame slope and same intercept for both colors\nSame slope and different intercept for both colors\nDifferent slope and different intercept for both colors"
  },
  {
    "objectID": "content/lectures/10-mlr.html#interaction-surface-artistliving",
    "href": "content/lectures/10-mlr.html#interaction-surface-artistliving",
    "title": "10-mlr",
    "section": "Interaction: Surface * artistliving",
    "text": "Interaction: Surface * artistliving"
  },
  {
    "objectID": "content/lectures/10-mlr.html#fit-model-with-interaction-effects",
    "href": "content/lectures/10-mlr.html#fit-model-with-interaction-effects",
    "title": "10-mlr",
    "section": "Fit model with interaction effects",
    "text": "Fit model with interaction effects\n\nResponse variable: log_price\nExplanatory variables: Surface area, artistliving, and their interaction\n\n\npp_int_fit <- lin_mod |>\n  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)\ntidy(pp_int_fit)\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139"
  },
  {
    "objectID": "content/lectures/10-mlr.html#linear-model-with-interaction-effects",
    "href": "content/lectures/10-mlr.html#linear-model-with-interaction-effects",
    "title": "10-mlr",
    "section": "Linear model with interaction effects",
    "text": "Linear model with interaction effects\n\n\n# A tibble: 4 × 5\n  term                        estimate std.error statistic    p.value\n  <chr>                          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)                 4.91     0.0432       114.   0         \n2 Surface                     0.000206 0.0000442      4.65 0.00000337\n3 artistlivingLiving         -0.126    0.119         -1.06 0.289     \n4 Surface:artistlivingLiving  0.000479 0.000126       3.81 0.000139  \n\n\n\\[\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface - 0.126 \\times artistliving\\] \\[+ ~ 0.00048 \\times surface * artistliving\\]"
  },
  {
    "objectID": "content/lectures/10-mlr.html#interpretation-of-interaction-effects",
    "href": "content/lectures/10-mlr.html#interpretation-of-interaction-effects",
    "title": "10-mlr",
    "section": "Interpretation of interaction effects",
    "text": "Interpretation of interaction effects\n\n\nRate of change in price as the surface area of the painting increases does vary between paintings by living and non-living artists (different slopes)\nSome paintings by living artists are more expensive than paintings by non-living artists, and some are not (different intercept).\n\n\n\n\n\n\n\nNon-living artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 0 + 0.00048 \\times surface \\times 0\\) \\(= 4.91 + 0.00021 \\times surface\\)\nLiving artist: \\(\\widehat{log\\_price} = 4.91 + 0.00021 \\times surface\\) \\(- 0.126 \\times 1 + 0.00048 \\times surface \\times 1\\) \\(= 4.91 + 0.00021 \\times surface\\) \\(- 0.126 + 0.00048 \\times surface\\) \\(= 4.784 + 0.00069 \\times surface\\)"
  },
  {
    "objectID": "content/lectures/10-mlr.html#r-squared",
    "href": "content/lectures/10-mlr.html#r-squared",
    "title": "10-mlr",
    "section": "R-squared",
    "text": "R-squared\n\n\\(R^2\\) is the percentage of variability in the response variable explained by the regression model.\n\n\nglance(pp_main_fit)$r.squared\n\n[1] 0.01320884\n\nglance(pp_int_fit)$r.squared\n\n[1] 0.0176922\n\n\n\n\nClearly the model with interactions has a higher \\(R^2\\).\n\n\n\n\nHowever using \\(R^2\\) for model selection in models with multiple explanatory variables is not a good idea as \\(R^2\\) increases when any variable is added to the model."
  },
  {
    "objectID": "content/lectures/10-mlr.html#adjusted-r-squared",
    "href": "content/lectures/10-mlr.html#adjusted-r-squared",
    "title": "10-mlr",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\nIt appears that adding the interaction actually increased adjusted \\(R^2\\), so we should indeed use the model with the interactions.\n\nglance(pp_main_fit)$adj.r.squared\n\n[1] 0.01258977\n\nglance(pp_int_fit)$adj.r.squared\n\n[1] 0.01676753"
  },
  {
    "objectID": "content/lectures/10-mlr.html#third-order-interactions",
    "href": "content/lectures/10-mlr.html#third-order-interactions",
    "title": "10-mlr",
    "section": "Third order interactions",
    "text": "Third order interactions\n\nCan you? Yes\nShould you? Probably not if you want to interpret these interactions in context of the data."
  },
  {
    "objectID": "content/lectures/10-mlr.html#in-pursuit-of-occams-razor",
    "href": "content/lectures/10-mlr.html#in-pursuit-of-occams-razor",
    "title": "10-mlr",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\n\nOccam’s Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\nModel selection follows this principle.\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\nIn other words, we prefer the simplest best model, i.e. parsimonious model."
  },
  {
    "objectID": "content/lectures/10-mlr.html#backward-selection",
    "href": "content/lectures/10-mlr.html#backward-selection",
    "title": "10-mlr",
    "section": "Backward selection",
    "text": "Backward selection\nFor this demo, we’ll ignore interaction effects…and just model main effects to start:\n\npp_full <-  lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp) \n\nglance(pp_full)$adj.r.squared\n\n[1] 0.02570141\n\n\n\n\\(R^2\\) (full): 0.0257014`\n\n\n\nRemove artistliving\n\npp_noartist <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in + Surface, data=pp) \n\nglance(pp_noartist)$adj.r.squared\n\n[1] 0.02579859\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\n…Improved variance explained\n\n\n\nRemove Surface\n\npp_noartist_nosurface <- lin_mod |>\n  fit(log_price ~ Width_in + Height_in, data=pp) \n\nglance(pp_noartist_nosurface)$adj.r.squared\n\n[1] 0.02231559\n\n\n\n\n\\(R^2\\) (full): 0.0257\n\\(R^2\\) (no artistliving): 0.0258\n\\(R^2\\) (no artistliving or Surface): 0.0223\n\n\n\n…no longer gaining improvement, so we stick with: log_price ~ Width_in + Height_in + Surface"
  },
  {
    "objectID": "content/lectures/10-mlr.html#other-approach",
    "href": "content/lectures/10-mlr.html#other-approach",
    "title": "10-mlr",
    "section": "Other approach:",
    "text": "Other approach:\n\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n\n\nStep 1: Fit model (w/o tidymodels)\n\n# fit the model (not using tidymodels)\nmod <- lm(log_price ~ Width_in + Height_in + Surface + artistliving, data=pp_Surf_lt_5000)\n\n\n\nStep 2: Determine which variables to remove\n\nols_step_backward_p(mod)\n\n\n                             Elimination Summary                               \n------------------------------------------------------------------------------\n        Variable                      Adj.                                        \nStep      Removed       R-Square    R-Square     C(p)        AIC         RMSE     \n------------------------------------------------------------------------------\n   1    artistliving      0.0261      0.0251    3.8495    12603.7727    1.8315    \n------------------------------------------------------------------------------\n\n\n…specifies that artistliving should be removed\n\n\nStep 2 (alternate): Compare all possible models…\n\nols_step_all_possible(mod) |>\n  arrange(desc(adjr))\n\n   Index N                              Predictors     R-Square Adj. R-Square\n1     11 3              Width_in Height_in Surface 0.0260749939  0.0251349118\n2     15 4 Width_in Height_in Surface artistliving 0.0263412027  0.0250876993\n3      5 2                      Width_in Height_in 0.0256902566  0.0250634893\n4     12 3         Width_in Height_in artistliving 0.0259732581  0.0250330779\n5      6 2                        Width_in Surface 0.0249136264  0.0242863596\n6     13 3           Width_in Surface artistliving 0.0251787948  0.0242378477\n7      7 2                   Width_in artistliving 0.0212864021  0.0206568018\n8      1 1                                Width_in 0.0209415833  0.0206267736\n9      8 2                    Surface artistliving 0.0132088377  0.0125897717\n10     2 1                                 Surface 0.0125899681  0.0122803381\n11    14 3          Height_in Surface artistliving 0.0130836930  0.0121310711\n12     9 2                       Height_in Surface 0.0126782901  0.0120431523\n13    10 2                  Height_in artistliving 0.0062698155  0.0056305552\n14     3 1                               Height_in 0.0058797727  0.0055601199\n15     4 1                            artistliving 0.0005531617  0.0002397573\n   Mallow's Cp\n1     3.849487\n2     5.000000\n3     3.077206\n4     4.174132\n5     5.555476\n6     6.709309\n7    17.130153\n8    16.230489\n9    51.971190\n10   52.001268\n11   45.305459\n12   44.599123\n13   65.048926\n14   64.293574\n15   91.485605"
  },
  {
    "objectID": "content/lectures/10-mlr.html#recap",
    "href": "content/lectures/10-mlr.html#recap",
    "title": "10-mlr",
    "section": "Recap",
    "text": "Recap\n\nCan you model and interpret linear models with multiple predictors?\nCan you explain the difference in a model with main effects vs. interaction effects?\nCan you compare different models and determine how to proceed?\nCan you carry out and explain backward selection?"
  },
  {
    "objectID": "content/lectures/10-mlr.html#suggested-reading",
    "href": "content/lectures/10-mlr.html#suggested-reading",
    "title": "10-mlr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nIntroduction to Modern Statistics Chapter 8: Linear Regression with Multiple Predictors"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#qa",
    "href": "content/lectures/03-tidyr-slides.html#qa",
    "title": "03-tidyr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: The most confusing part for the “transmute” part of the lecture. Does this remove the column from the original dataframe or does it just separate it but it remains in the dataframe\nA: The original dataframe is not changed. It just creates a new dataframe. Nothing in dplyr will change your original dataframe, unless you explicitly assign it back to the same variable name.\n\n\nQ: It’s a totally new language for me and it’s kind of hard to write the function correctly and quickly at first\nA: That’s expected. You’re not alone!\n\n\nQ: I’m not sure when it would be ideal to use Select vs Transmute, as in today’s lecture\nA: When you want to extract existing columns, select! When you want to create a new column in a new dataframe, transmute!\n\n\nQ: Pulls - when are they necessary?\nA: dplyr pulls are necessary when you have values in a dataframe, but you want them in a typical vector…or you want to pull a single value.\n\n\nQ: If we don’t finish the notes on a given day, do I have to learn them on my own?\nA: Nope! We’ll pick up there on the next class day."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#course-announcements",
    "href": "content/lectures/03-tidyr-slides.html#course-announcements",
    "title": "03-tidyr",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 02 due Friday (1/20; 11:59 PM)\nHW01 due Monday (1/23; 11:59 PM)\nLecture Participation survey “due” after class\n\nNotes (1/19):\n\nHW01 available - let’s try this again\n\nQ2 can delete expect_true(true_var) and expect_false(false_var) from testing (they should only be in Q3)\n\nStaff office hours updated (see Canvas or website)"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#student-survey",
    "href": "content/lectures/03-tidyr-slides.html#student-survey",
    "title": "03-tidyr",
    "section": "Student Survey",
    "text": "Student Survey\n\n88% know Python; 20% know R; most (but not all!) have programmed before\nReasons for taking course: learn R, add to resume, analyze data, improve data science skills\n\n\nWhat y’all do outside of school:\n\nsports: soccer, rugby, vball, basketball, surfing, skating, etc.\nindoor activities: reading, gaming, cook, crochet, art, write, youtube, tiktok\nwork\nsleep\n\n\n\nMy favorite boring facts:\n\nI love cupcakes\nI like drinking tea\nI’ve never had a nosebleed\nI can sleep for up to 16-17 hours if I forget to set an alarm\nI don’t cook"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#tidy-data",
    "href": "content/lectures/03-tidyr-slides.html#tidy-data",
    "title": "03-tidyr",
    "section": "Tidy Data",
    "text": "Tidy Data\nThe opinionated tidyverse is named as such b/c it assumes/necessitates your data be “tidy”.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. —- Hadley Wickham\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\n\nSource: https://r4ds.had.co.nz/tidy-data.html"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#tidy-or-not",
    "href": "content/lectures/03-tidyr-slides.html#tidy-or-not",
    "title": "03-tidyr",
    "section": "Tidy or not?",
    "text": "Tidy or not?\n❓ Given the rules discussed, is the cat_lovers dataset tidy?\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")\ncat_lovers |> datatable()\n\n\n\n\n\n\n\n❓ Given the rules discussed, is the bike dataset tidy?\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))\nbike |> datatable()"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#summary-tables",
    "href": "content/lectures/03-tidyr-slides.html#summary-tables",
    "title": "03-tidyr",
    "section": "Summary tables",
    "text": "Summary tables\n❓ Which is a dataset? Which is a summary table?"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#your-turn",
    "href": "content/lectures/03-tidyr-slides.html#your-turn",
    "title": "03-tidyr",
    "section": "Your Turn",
    "text": "Your Turn\nThere are four representations of the same data/information provided in the tidyr packages: table1, table2, table3, and the combination of table4a and table4b. Given what we’ve discussed, which is the best (tidiest) way to represent these data?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#common-issues",
    "href": "content/lectures/03-tidyr-slides.html#common-issues",
    "title": "03-tidyr",
    "section": "Common issues",
    "text": "Common issues\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\nSolution: pivoting!"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#pivoting",
    "href": "content/lectures/03-tidyr-slides.html#pivoting",
    "title": "03-tidyr",
    "section": "Pivoting",
    "text": "Pivoting\n\npivot_longerpivot_widerlong vs. wide\n\n\nFor when some of the column names are not names of variables, but values of a variable…\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  <chr>       <chr>  <int>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n❓ Why are there backticks around the years? (Note: we have not discussed this yet)\n\n\nFor when an observation is scattered across multiple rows…\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable2 |>\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n❓ Why aren’t there quotes around column names here…but there were in pivot_longer? (Note: we have not discussed this yet.)\n\n\n\nwide data contains values that do not repeat in the first column.\nlong format contains values that do repeat in the first column.\n\nBoth are good/helpful! We’ll return to this idea and discuss more during dataviz next week.\nBriefly:\n\nwide data: analysis\nlong data: plotting"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#separating-uniting",
    "href": "content/lectures/03-tidyr-slides.html#separating-uniting",
    "title": "03-tidyr",
    "section": "Separating & Uniting",
    "text": "Separating & Uniting\n\nseparateunite\n\n\nFor when multiple pieces of information are stored in a single column…\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n* <chr>       <int> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  <chr>       <int> <chr>  <chr>     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n…but…but…cases and population should be numeric…\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nUnite is the opposite…it combines data stored across multiple columns.\nThe general syntax is:\n\ndf |>\n  unite(new_col, first_col, second_col)"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#joins",
    "href": "content/lectures/03-tidyr-slides.html#joins",
    "title": "03-tidyr",
    "section": "Joins",
    "text": "Joins\nIf we look at table4a, it’s missing the population information. That’s stored in a separate table…table4b\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n* <chr>            <int>      <int>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n…which is also in the “wide” format\n\n…so we pivot both tables longer\n\ntidy4a <- table4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntidy4b <- table4b |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\ntidy4b\n\n# A tibble: 6 × 3\n  country     year  population\n  <chr>       <chr>      <int>\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n\n\n…but how do we get them into a single tidy dataset?\n\n\nA join!\n\nleft_join(tidy4a, tidy4b)\n\n# A tibble: 6 × 4\n  country     year   cases population\n  <chr>       <chr>  <int>      <int>\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\n\nSource: R4DS"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#the-data-nycflights13",
    "href": "content/lectures/03-tidyr-slides.html#the-data-nycflights13",
    "title": "03-tidyr",
    "section": "The Data: nycflights13",
    "text": "The Data: nycflights13\n\nlibrary(nycflights13)\n\n\nairlines : links airline to two letter code\nairports : ID’ed by FAA code\nplanes : ID’ed by tailnum\nairport : weather each hour; id’ed by two letter airport code\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html)\n\n\n\n\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time)."
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#mutating-joins",
    "href": "content/lectures/03-tidyr-slides.html#mutating-joins",
    "title": "03-tidyr",
    "section": "Mutating Joins",
    "text": "Mutating Joins\nmutating joins - add new variables to a data frame from matching observations in another\n\nFor simplicity, we’ll work with only a handful of columns…\n\nflights |> \n  select(year:day, hour, tailnum, carrier) |> \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 7\n    year month   day  hour tailnum carrier name                    \n   <int> <int> <int> <dbl> <chr>   <chr>   <chr>                   \n 1  2013     1     1     5 N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 N3ALAA  AA      American Airlines Inc.  \n# … with 336,766 more rows\n\n\nThere is now a new column name…coming from the airlines data frame.\n\n\nleft_join:\n\nkeeps all rows in first df (here: flights)\nadds all matching information from second df (here: airlines); adds NAs for any observations not in airlines\n\n\n\nOther joins:\nright_join: keeps all observations in second df full_join: keeps all observations in either df\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html\n\n\n\n\ninner_join:\n\ntakes only rows in both dfs\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html"
  },
  {
    "objectID": "content/lectures/03-tidyr-slides.html#suggested-reading",
    "href": "content/lectures/03-tidyr-slides.html#suggested-reading",
    "title": "03-tidyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 12: Tidy Data\nChapter 13: Relational Data\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/03-tidyr.html",
    "href": "content/lectures/03-tidyr.html",
    "title": "03-tidyr",
    "section": "",
    "text": "Q: The most confusing part for the “transmute” part of the lecture. Does this remove the column from the original dataframe or does it just separate it but it remains in the dataframe\nA: The original dataframe is not changed. It just creates a new dataframe. Nothing in dplyr will change your original dataframe, unless you explicitly assign it back to the same variable name.\n\n\nQ: It’s a totally new language for me and it’s kind of hard to write the function correctly and quickly at first\nA: That’s expected. You’re not alone!\n\n\nQ: I’m not sure when it would be ideal to use Select vs Transmute, as in today’s lecture\nA: When you want to extract existing columns, select! When you want to create a new column in a new dataframe, transmute!\n\n\nQ: Pulls - when are they necessary?\nA: dplyr pulls are necessary when you have values in a dataframe, but you want them in a typical vector…or you want to pull a single value.\n\n\nQ: If we don’t finish the notes on a given day, do I have to learn them on my own?\nA: Nope! We’ll pick up there on the next class day.\n\n\n\n\nDue Dates:\n\nLab 02 due Friday (1/20; 11:59 PM)\nHW01 due Monday (1/23; 11:59 PM)\nLecture Participation survey “due” after class\n\nNotes (1/19):\n\nHW01 available - let’s try this again\n\nQ2 can delete expect_true(true_var) and expect_false(false_var) from testing (they should only be in Q3)\n\nStaff office hours updated (see Canvas or website)\n\n\n\n\n\n88% know Python; 20% know R; most (but not all!) have programmed before\nReasons for taking course: learn R, add to resume, analyze data, improve data science skills\n\n\nWhat y’all do outside of school:\n\nsports: soccer, rugby, vball, basketball, surfing, skating, etc.\nindoor activities: reading, gaming, cook, crochet, art, write, youtube, tiktok\nwork\nsleep\n\n\n\nMy favorite boring facts:\n\nI love cupcakes\nI like drinking tea\nI’ve never had a nosebleed\nI can sleep for up to 16-17 hours if I forget to set an alarm\nI don’t cook\n\n\n\n\n\nThe opinionated tidyverse is named as such b/c it assumes/necessitates your data be “tidy”.\n\n\nTidy datasets are all alike, but every messy dataset is messy in its own way. —- Hadley Wickham\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n\n\nSource: https://r4ds.had.co.nz/tidy-data.html\n\n\n\n\n❓ Given the rules discussed, is the cat_lovers dataset tidy?\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")\n\nRows: 60 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): name, number_of_cats, handedness\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncat_lovers |> datatable()\n\n\n\n\n\n\n\n❓ Given the rules discussed, is the bike dataset tidy?\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 5716 Columns: 54\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (44): AmbulanceR, BikeAge_Gr, Bike_Alc_D, Bike_Dir, Bike_Injur, Bike_Po...\ndbl   (8): FID, OBJECTID, Bike_Age, Crash_Hour, Crash_Ty_1, Crash_Year, Drvr...\ndttm  (1): Crash_Time\ndate  (1): Crash_Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike |> datatable()\n\nWarning in instance$preRenderHook(instance): It seems your data is too\nbig for client-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\n\n\n\n\n\n❓ Which is a dataset? Which is a summary table?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are four representations of the same data/information provided in the tidyr packages: table1, table2, table3, and the combination of table4a and table4b. Given what we’ve discussed, which is the best (tidiest) way to represent these data?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\n\nOne variable might be spread across multiple columns.\nOne observation might be scattered across multiple rows.\n\n\nSolution: pivoting!\n\n\n\n\n\npivot_longerpivot_widerlong vs. wide\n\n\nFor when some of the column names are not names of variables, but values of a variable…\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n\n\ntable4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  <chr>       <chr>  <int>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n❓ Why are there backticks around the years? (Note: we have not discussed this yet)\n\n\nFor when an observation is scattered across multiple rows…\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\ntable2 |>\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n❓ Why aren’t there quotes around column names here…but there were in pivot_longer? (Note: we have not discussed this yet.)\n\n\n\nwide data contains values that do not repeat in the first column.\nlong format contains values that do repeat in the first column.\n\nBoth are good/helpful! We’ll return to this idea and discuss more during dataviz next week.\nBriefly:\n\nwide data: analysis\nlong data: plotting\n\n\n\n\n\n\n\n\nseparateunite\n\n\nFor when multiple pieces of information are stored in a single column…\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n* <chr>       <int> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  <chr>       <int> <chr>  <chr>     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n…but…but…cases and population should be numeric…\n\ntable3 |> \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nUnite is the opposite…it combines data stored across multiple columns.\nThe general syntax is:\n\ndf |>\n  unite(new_col, first_col, second_col)\n\n\n\n\n\n\n\nIf we look at table4a, it’s missing the population information. That’s stored in a separate table…table4b\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n* <chr>            <int>      <int>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\n…which is also in the “wide” format\n\n…so we pivot both tables longer\n\ntidy4a <- table4a |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntidy4b <- table4b |> \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"population\")\n\ntidy4b\n\n# A tibble: 6 × 3\n  country     year  population\n  <chr>       <chr>      <int>\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\n\n\n\n…but how do we get them into a single tidy dataset?\n\n\nA join!\n\nleft_join(tidy4a, tidy4b)\n\nJoining with `by = join_by(country, year)`\n\n\n# A tibble: 6 × 4\n  country     year   cases population\n  <chr>       <chr>  <int>      <int>\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\n\n\n\nSource: R4DS\n\n\n\n\n\nlibrary(nycflights13)\n\n\nairlines : links airline to two letter code\nairports : ID’ed by FAA code\nplanes : ID’ed by tailnum\nairport : weather each hour; id’ed by two letter airport code\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html)\n\n\n\n\n\nflights connects to planes via a single variable, tailnum.\nflights connects to airlines through the carrier variable.\nflights connects to airports in two ways: via the origin and dest variables.\nflights connects to weather via origin (the location), and year, month, day and hour (the time).\n\n\n\n\n\nmutating joins - add new variables to a data frame from matching observations in another\n\nFor simplicity, we’ll work with only a handful of columns…\n\nflights |> \n  select(year:day, hour, tailnum, carrier) |> \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 7\n    year month   day  hour tailnum carrier name                    \n   <int> <int> <int> <dbl> <chr>   <chr>   <chr>                   \n 1  2013     1     1     5 N14228  UA      United Air Lines Inc.   \n 2  2013     1     1     5 N24211  UA      United Air Lines Inc.   \n 3  2013     1     1     5 N619AA  AA      American Airlines Inc.  \n 4  2013     1     1     5 N804JB  B6      JetBlue Airways         \n 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.    \n 6  2013     1     1     5 N39463  UA      United Air Lines Inc.   \n 7  2013     1     1     6 N516JB  B6      JetBlue Airways         \n 8  2013     1     1     6 N829AS  EV      ExpressJet Airlines Inc.\n 9  2013     1     1     6 N593JB  B6      JetBlue Airways         \n10  2013     1     1     6 N3ALAA  AA      American Airlines Inc.  \n# … with 336,766 more rows\n\n\nThere is now a new column name…coming from the airlines data frame.\n\n\nleft_join:\n\nkeeps all rows in first df (here: flights)\nadds all matching information from second df (here: airlines); adds NAs for any observations not in airlines\n\n\n\nOther joins:\nright_join: keeps all observations in second df full_join: keeps all observations in either df\n\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html\n\n\n\n\ninner_join:\n\ntakes only rows in both dfs\n\n\n\n\nImage Source: https://r4ds.had.co.nz/relational-data.html"
  },
  {
    "objectID": "content/lectures/03-tidyr.html#suggested-reading",
    "href": "content/lectures/03-tidyr.html#suggested-reading",
    "title": "03-tidyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 12: Tidy Data\nChapter 13: Relational Data"
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#course-announcements",
    "href": "content/lectures/16-cs02-eda-slides.html#course-announcements",
    "title": "16-cs02-eda",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLecture Participation survey “due” after class\nLab08 Due Friday (CS02 EDA)\nCS02 due Monday of Finals week (3/20)\nFinal Project due Th of Finals week (3/23)\n\n\nNotes:\n\ncs01 group work form is still open - please complete (if you haven’t)\ncs02 and final project groups/repos assigned\ndon’t wait until finals week to do cs02 and final project"
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#agenda",
    "href": "content/lectures/16-cs02-eda-slides.html#agenda",
    "title": "16-cs02-eda",
    "section": "Agenda",
    "text": "Agenda"
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#data",
    "href": "content/lectures/16-cs02-eda-slides.html#data",
    "title": "16-cs02-eda",
    "section": "Data",
    "text": "Data\n…will only work if you finished last set of notes.\n\nload(\"data/wrangled/wrangled_data_vaping.rda\")"
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#question",
    "href": "content/lectures/16-cs02-eda-slides.html#question",
    "title": "16-cs02-eda",
    "section": "Question",
    "text": "Question\nWhen is pivoting your data from wide to long (or long to wide) helpful?\n\n\ndata |>\n   pivot_longer(cols = columns_to_pivot , names_to = \"new_col_for_labels\" , values_to = \"new_col_for_values\")"
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#eda",
    "href": "content/lectures/16-cs02-eda-slides.html#eda",
    "title": "16-cs02-eda",
    "section": "EDA",
    "text": "EDA\nWhat do you want to know?\n[We’re going to fill these in during class]\n\nHow would you go about doing that?"
  },
  {
    "objectID": "content/lectures/16-cs02-eda-slides.html#suggested-reading",
    "href": "content/lectures/16-cs02-eda-slides.html#suggested-reading",
    "title": "16-cs02-eda",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nCase Study from OCS\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html",
    "href": "content/lectures/16-cs02-eda.html",
    "title": "16-cs02-eda",
    "section": "",
    "text": "Due Dates:\n\nLecture Participation survey “due” after class\nLab08 Due Friday (CS02 EDA)\nCS02 due Monday of Finals week (3/20)\nFinal Project due Th of Finals week (3/23)\n\n\nNotes:\n\ncs01 group work form is still open - please complete (if you haven’t)\ncs02 and final project groups/repos assigned\ndon’t wait until finals week to do cs02 and final project"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#data",
    "href": "content/lectures/16-cs02-eda.html#data",
    "title": "16-cs02-eda",
    "section": "Data",
    "text": "Data\n…will only work if you finished last set of notes.\n\nload(\"data/wrangled/wrangled_data_vaping.rda\")"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#question",
    "href": "content/lectures/16-cs02-eda.html#question",
    "title": "16-cs02-eda",
    "section": "Question",
    "text": "Question\nWhen is pivoting your data from wide to long (or long to wide) helpful?\n\n\ndata |>\n   pivot_longer(cols = columns_to_pivot , names_to = \"new_col_for_labels\" , values_to = \"new_col_for_values\")"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#eda",
    "href": "content/lectures/16-cs02-eda.html#eda",
    "title": "16-cs02-eda",
    "section": "EDA",
    "text": "EDA\nWhat do you want to know?\n[We’re going to fill these in during class]\n\nHow would you go about doing that?"
  },
  {
    "objectID": "content/lectures/16-cs02-eda.html#suggested-reading",
    "href": "content/lectures/16-cs02-eda.html#suggested-reading",
    "title": "16-cs02-eda",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nCase Study from OCS"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#agenda",
    "href": "content/lectures/00-welcome-slides.html#agenda",
    "title": "00-welcome",
    "section": "Agenda",
    "text": "Agenda\n\nDescribe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-r",
    "href": "content/lectures/00-welcome-slides.html#what-is-r",
    "title": "00-welcome",
    "section": "What is R?",
    "text": "What is R?\n : R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-data-science",
    "href": "content/lectures/00-welcome-slides.html#what-is-data-science",
    "title": "00-welcome",
    "section": "What is data science?",
    "text": "What is data science?\n: Data science is the scientific process of using data to answer interesting questions and/or solve important problems."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#practical-data-science-in-r",
    "href": "content/lectures/00-welcome-slides.html#practical-data-science-in-r",
    "title": "00-welcome",
    "section": "Practical Data Science in R",
    "text": "Practical Data Science in R\n\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#who-am-i",
    "href": "content/lectures/00-welcome-slides.html#who-am-i",
    "title": "00-welcome",
    "section": "Who am I?",
    "text": "Who am I?\nShannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com     Peterson Hall 104     Tu/Th 2-3:20PM (Lab: Fri 1-1:50PM)"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#who-all-is-involved",
    "href": "content/lectures/00-welcome-slides.html#who-all-is-involved",
    "title": "00-welcome",
    "section": "Who all is involved?",
    "text": "Who all is involved?\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 2-3\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nShubham Kulkarni\n\nTime TBD\nLocation TBD\n\n\nIAs\nChristian Kim\n\nTime TBD\nLocation TBD"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-this-course",
    "href": "content/lectures/00-welcome-slides.html#what-is-this-course",
    "title": "00-welcome",
    "section": "What is this course?",
    "text": "What is this course?\nEverything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#soi-dont-have-to-know-how-to-program-already",
    "href": "content/lectures/00-welcome-slides.html#soi-dont-have-to-know-how-to-program-already",
    "title": "00-welcome",
    "section": "So…I don’t have to know how to program already?",
    "text": "So…I don’t have to know how to program already?\n\n\n\n\nNope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-general-plan",
    "href": "content/lectures/00-welcome-slides.html#the-general-plan",
    "title": "00-welcome",
    "section": "The General Plan",
    "text": "The General Plan\n\nWeeks 1-4: Learn to program in the tidyverse in R\nWeeks 5-10: Communication, Data Analysis, Statistics, & Case Studies\n\nCS01: Right to Carry\nCS02: Vaping Behaviors"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-nitty-gritty",
    "href": "content/lectures/00-welcome-slides.html#the-nitty-gritty",
    "title": "00-welcome",
    "section": "The Nitty Gritty",
    "text": "The Nitty Gritty\n\nLectureIn-personWaitlistLab & OHMaterials\n\n\nClass Meetings\n\nInteractive\nLectures & lots of learn-by-doing\nBring your laptop to class every day\n\n\n\nIn-person, synchronous learning\n\nI will be teaching (so long as I’m healthy and have child care) in person.\nLectures will be podcast.\nAttendance will be incentivized using a daily participation survey.\nIf you’re not feeling well, please stay home. I will do the same.\nExam will be take-home.\n\n\n\nThe (Dreaded) Waitlist\n\nCourse enrollment is supposed to be 50 for this course\nThere are 72 people currently enrolled\nI don’t control the waitlist (cogsadvising@ucsd.edu does)\nI’d anticipate our staff adding 3-5 people from the waitlist (but cannot guarantee this)\n\n\n\nLab & Office Hours\n\nMy office hours begin week 1; TA/IA OH begin week 2\nLab will start this Fri (week 1)\nI will hang out after class today for questions/concerns from students\n\n\n\nCourse Materials\n\nTextbooks are free and available online\nCourse platforms:\n\nWebsite : schedule, policies, due dates, etc.\nGitHub : retrieving assignments, labs, exams, etc.\ndatahub : completing assignments, labs, exams etc.\nGradescope : submitting assignments\nCanvas : grades, course-specific links\nCampuswire : Q&A"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#diversity-inclusion",
    "href": "content/lectures/00-welcome-slides.html#diversity-inclusion",
    "title": "00-welcome",
    "section": "Diversity & Inclusion:",
    "text": "Diversity & Inclusion:\nGoal: every student be well-served by this course\n\nPhilosophy: The diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding.\n\n\nPlan: Present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture)\n\n\nBut… if I ever fall short or if you ever have suggestions for improvement, please do share with me! There is also an anonymous Google Form if you’re more comfortable there."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#a-new-ish-course",
    "href": "content/lectures/00-welcome-slides.html#a-new-ish-course",
    "title": "00-welcome",
    "section": "A new-ish course!",
    "text": "A new-ish course!\n\nOffered once previously\nIf something doesn’t make sense, tell me!\nIf you’ve got feedback/suggestions, I’m all ears!\n\n\nDifferences since last iteration:\n\nLess…pretty much everything\nNew website/lecture platform (quarto)\nTu/Th and not early in the morning\nGroup mates differ\nIncreased focus on communication\nMultiple final project options\nSlightly larger class & slightly smaller staff\nI’m a mom & just coming back from parental leave"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-to-get-help",
    "href": "content/lectures/00-welcome-slides.html#how-to-get-help",
    "title": "00-welcome",
    "section": "How to get help",
    "text": "How to get help\n\nLab\nOffice Hours\nCampuswire\n\n\nA few guidelines:\n1. No duplicates.\n2. Public posts are best.\n3. Posts should include your question, what you've tried so far, & resources used.\n4. Helping others is encouraged.\n5. No assignment code in public posts.\n6. We're not robots."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#the-r-community",
    "href": "content/lectures/00-welcome-slides.html#the-r-community",
    "title": "00-welcome",
    "section": " The R Community",
    "text": "The R Community\n\n\n\nR Rollercoaster\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#academic-integrity",
    "href": "content/lectures/00-welcome-slides.html#academic-integrity",
    "title": "00-welcome",
    "section": "Academic integrity",
    "text": "Academic integrity\nDon’t cheat.\n\nTeamwork is allowed, but you should be able to answer “Yes” to each of the following: - Can I explain each piece of code and each analysis carried out in what I’m submitting? - Could I reproduce this code/analysis on my own?\n\n\nThe Internet is a great resource. - Cite your sources.\n\n\nTeamwork is not allowed on your midterm. - It is open-notes and open-Google - You cannot discuss the questions on the exam with anyone."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#course-components",
    "href": "content/lectures/00-welcome-slides.html#course-components",
    "title": "00-welcome",
    "section": "Course components:",
    "text": "Course components:\n\n\nLabs (8): Individual submission; graded on effort\nHomework (4): Individual submission; graded on correctness\nExam (1): Individual completion & submission, take-home midterm\nCase Studies (2): Team submission, technical analysis report\nFinal Project (1) : Team submission, due Thurs of finals week"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#grading",
    "href": "content/lectures/00-welcome-slides.html#grading",
    "title": "00-welcome",
    "section": "Grading",
    "text": "Grading\nYour final grade will be comprised of the following:\n\n\n\nAssignment (#)\n% of grade\n\n\n\n\nLabs (8)\n16%\n\n\nHomework (4)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal project* (1)\n17%\n\n\n\n* indicates group submission"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#latemissed-work-policy",
    "href": "content/lectures/00-welcome-slides.html#latemissed-work-policy",
    "title": "00-welcome",
    "section": "Late/missed work policy",
    "text": "Late/missed work policy\n\nHomework and case study projects: accepted up to 3 days (72 hours) after the assigned deadline for a 25% deduction\nNo late deadlines for labs, the exam, or the final project\n\n\n\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#datahub",
    "href": "content/lectures/00-welcome-slides.html#datahub",
    "title": "00-welcome",
    "section": "Datahub",
    "text": "Datahub\nDatahub is a platform hosted by UCSD that gives students access to computational resources.\nThis means that while you’ll be typing on your keyboard, you’ll be using UCSD’s computers in this class.\nWebsite: https://datahub.ucsd.edu/\n\nLaunch Environment\nWhen working on “stuff” for this course, select the COGS 137 environment.\n\n\n\ndatahub"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#toolkit",
    "href": "content/lectures/00-welcome-slides.html#toolkit",
    "title": "00-welcome",
    "section": "Toolkit",
    "text": "Toolkit\n\ntoolkit\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-and-rstudio",
    "href": "content/lectures/00-welcome-slides.html#r-and-rstudio",
    "title": "00-welcome",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR/RStudioTourTryR packages\n\n\nR & RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integreated development environment, IDE)\n\n\n\n\n[DEMO]\n\nConcepts introduced:\n\nConsole\nUsing R as a calculator\nEnvironment\nLoading and viewing a data frame\nAccessing a variable in a data frame\nR functions\n\n\n\nYour Turn\n\nLogin to datahub\nCarry out a mathematical operation in the console\nView the airquality dataframe\nAccess a column from the airquality dataframe\nCalculate the median for one of the numeric columns\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\nPackages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data 1\nAs of Jan 2023, there are ~18,979 R packages available on CRAN (the Comprehensive R Archive Network)2\nWe’re going to work with a small (but important) subset of these!\n\n\n\n\nWickham and Bryan, R PackagesCRAN contributed packages"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#what-is-the-tidyverse",
    "href": "content/lectures/00-welcome-slides.html#what-is-the-tidyverse",
    "title": "00-welcome",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\n\n\n\n\n\ntidyverse.org\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying philosophy and a common syntax."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#rstudio-projects3",
    "href": "content/lectures/00-welcome-slides.html#rstudio-projects3",
    "title": "00-welcome",
    "section": "RStudio Projects1",
    "text": "RStudio Projects1\n\nBuilt-in functionality to keep all files for a single project organized\n\nRStudio Projects Documentation"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-markdown",
    "href": "content/lectures/00-welcome-slides.html#r-markdown",
    "title": "00-welcome",
    "section": "R Markdown",
    "text": "R Markdown\n\nFully reproducible reports – each time you knit, the document is executed from top to bottom\nSimple markdown syntax for text\nCode goes in chunks, defined by three backticks, narrative goes outside of chunks"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#r-markdown-tips",
    "href": "content/lectures/00-welcome-slides.html#r-markdown-tips",
    "title": "00-welcome",
    "section": "R Markdown tips",
    "text": "R Markdown tips\n\nKeep the R Markdown cheat sheet and Markdown Quick Reference (Help -> Markdown Quick Reference) handy, we’ll refer to it often as the course progresses\nThe workspace of your R Markdown document is separate from the Console\n\n\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#how-will-we-use-r-markdown",
    "href": "content/lectures/00-welcome-slides.html#how-will-we-use-r-markdown",
    "title": "00-welcome",
    "section": "How will we use R Markdown?",
    "text": "How will we use R Markdown?\n\nEvery lab / project / homework / notes / etc. is an R Markdown document\nYou’ll always have a template R Markdown document to start with\nThe amount of scaffolding in the template will decrease over the quarter"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#collaboration-git-github",
    "href": "content/lectures/00-welcome-slides.html#collaboration-git-github",
    "title": "00-welcome",
    "section": "Collaboration: Git & GitHub",
    "text": "Collaboration: Git & GitHub\n\nThe statistical programming language we’ll use is R\nThe software we use to interface with R is RStudio\nBut how do I get you the course materials that you can build on for your assignments?\n\nHint: I’m not going to email you documents, that would be a mess!"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#version-control",
    "href": "content/lectures/00-welcome-slides.html#version-control",
    "title": "00-welcome",
    "section": "Version control",
    "text": "Version control\n\nWe introduced GitHub as a platform for collaboration\nBut it’s much more than that…\nIt’s actually designed for version control"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#versioning",
    "href": "content/lectures/00-welcome-slides.html#versioning",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\n\nLego versions"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#versioning-1",
    "href": "content/lectures/00-welcome-slides.html#versioning-1",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\nwith human readable messages\n\nLego versions with commit messages"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#why-do-we-need-version-control",
    "href": "content/lectures/00-welcome-slides.html#why-do-we-need-version-control",
    "title": "00-welcome",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\nPhD Comics"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#git-and-github-tips",
    "href": "content/lectures/00-welcome-slides.html#git-and-github-tips",
    "title": "00-welcome",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\nGit is a version control system – like “Track Changes” features from Microsoft Word on steroids. GitHub is the home for your Git-based projects on the internet – like DropBox but much, much better).\n\n\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\n\n\nWe will be doing Git things and interfacing with GitHub through RStudio, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\n\n\n\n\nResource: happygitwithr.com: book for working with git in R; Some content is beyond the scope of this course, but it’s a good resource"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#lets-take-a-tour-git-github",
    "href": "content/lectures/00-welcome-slides.html#lets-take-a-tour-git-github",
    "title": "00-welcome",
    "section": "Let’s take a tour – Git / GitHub",
    "text": "Let’s take a tour – Git / GitHub\nWe’ll cover this time permitting, you’ll see it again in lab this week\nConcepts introduced:\n\nConnect an R project to Github repository\nWorking with a local and remote repository\nCommitting, Pushing and Pulling\n\nThere is a bit more of GitHub that we’ll use in this class, but for today this is enough."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#recap",
    "href": "content/lectures/00-welcome-slides.html#recap",
    "title": "00-welcome",
    "section": "Recap",
    "text": "Recap\nCan you answer these questions?\n\nWhat is R vs RStudio?\nWhat are RStudio Projects?\nWhat is version control, and why do we care?\nWhat is git vs GitHub (and do I need to care)?"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#additional-git-resources",
    "href": "content/lectures/00-welcome-slides.html#additional-git-resources",
    "title": "00-welcome",
    "section": "Additional git Resources",
    "text": "Additional git Resources\nVersion Control (git and GitHub):\n\nGetting Started with git\nGitHub Guide\nGitHub Desktop App Tutorial\nGit Command Line Resource\nUsing git from the command line\n\nInstalling and using git (Part 1), by COGS 108 TA Ganesh (youtube, 22min tutorial)\nmerge conflicts and branching (Part 2), by IA Shubham Kulkarni (youtube, 8min tutorial)\n\nUsing git with GitHub Desktop, by COGS 108 TA Sidharth Suresh (youtube, 13min tutorial)\nGIT & GITHUB TUTORIAL, from edureka!\n\nwith notes from COGS 18/108 TA Holly(Yueying) Dong"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#slides-to-pdf",
    "href": "content/lectures/00-welcome-slides.html#slides-to-pdf",
    "title": "00-welcome",
    "section": "Slides to PDF",
    "text": "Slides to PDF\n\nToggle into Print View using the E key (or using the Navigation Menu)\nOpen the in-browser print dialog (CTRL/CMD+P).\nChange the Destination setting to Save as PDF.\nChange the Layout to Landscape.\nChange the Margins to None.\nEnable the Background graphics option.\nClick Save 🎉\n\n\n\nInstructions from quarto documentation"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster <- read_sheet('10_NsXld_swxoTL_01pCklRKR95OH5XoNlRzTl7L5XXs')\n\nggplot(roster, aes(x = College)) +\n  geom_bar() +\n  labs(title = \"COGS 137\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\nNote: This code will not run for you because you don’t have access to the roster for this course."
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class-1",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class-1",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  mutate(major = substr(Major, 1, 2)) |>\n  ggplot(aes(fct_infreq(major))) + \n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Major\") +\n  theme_bw(base_size = 12) + \n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#whos-in-this-class-2",
    "href": "content/lectures/00-welcome-slides.html#whos-in-this-class-2",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  ggplot(aes(fct_relevel(Level, \"JR\", \"SR\"))) +\n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Level\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/00-welcome-slides.html#id-like-to-know-more",
    "href": "content/lectures/00-welcome-slides.html#id-like-to-know-more",
    "title": "00-welcome",
    "section": "I’d like to know more!",
    "text": "I’d like to know more!\n(optional) Student Survey - complete by Monday at 11:59 PM for small amount of extra credit\n\n(optional) Daily Post-Lecture Feedback\n\nopportunity to reflect on learning\nopportunity to ask questions (I will read and answer these.)\nopportunity for extra credit on final project\n\n\n\n\n\nhttps://cogs137.github.io/website/\n\n\n\nNote: Links to both surveys are also on Canvas."
  },
  {
    "objectID": "content/lectures/00-welcome.html",
    "href": "content/lectures/00-welcome.html",
    "title": "00-welcome",
    "section": "",
    "text": "Practical Data Science in R\nPlease take one green sticky and one pink sticky as they come around. If you’re able, try and save these. We’ll use them most classes. (But, I’ll always have extra!)\n\n\n\n\n\n\n\n\nDescribe what this class is\nDescribe how the class will run\nGo over the tooling for this course: R, RStudio, GitHub\n\n\n\n\n : R is a statistical programming language.\nWhile R has most/all of the functionality of YFPL (your favorite programming language), it was designed for the specific use of analyzing data.\n\n\n\n: Data science is the scientific process of using data to answer interesting questions and/or solve important problems.\n\n\n\n\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports\n\n\n\n\n\nShannon Ellis: Associate Teaching Professor, Mom & wife, volleyball-obsessed, and baking & cooking lover\n   sellis@ucsd.edu     shanellis.com     Peterson Hall 104     Tu/Th 2-3:20PM (Lab: Fri 1-1:50PM)\n\n\n\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 2-3\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50\nCSB 243\n\n\nTA\nShubham Kulkarni\n\nTime TBD\nLocation TBD\n\n\nIAs\nChristian Kim\n\nTime TBD\nLocation TBD\n\n\n\n\n\n\nEverything you want to know about the course, and everything you will need for the course will be posted at: https://cogs137.github.io/website/\n\n\nIs this an intro CS course? No.\nWill we be doing computing? Yes.\nWhat computing language will we learn? R.\nIs this an intro stats course? No.\nWill we be doing stats? Yes.\nAre there any prerequisites? Yes, an intro statistics course!\n\n\n\n\n\n\n\n\n\nNope! The first few weeks of the course will be all about getting comfortable using the R programming language!\n After that, we’ll focus on delving into interesting statistical analyses through case studies.\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-general-plan",
    "href": "content/lectures/00-welcome.html#the-general-plan",
    "title": "00-welcome",
    "section": "The General Plan",
    "text": "The General Plan\n\nWeeks 1-4: Learn to program in the tidyverse in R\nWeeks 5-10: Communication, Data Analysis, Statistics, & Case Studies\n\nCS01: Right to Carry\nCS02: Vaping Behaviors"
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-nitty-gritty",
    "href": "content/lectures/00-welcome.html#the-nitty-gritty",
    "title": "00-welcome",
    "section": "The Nitty Gritty",
    "text": "The Nitty Gritty\n\nLectureIn-personWaitlistLab & OHMaterials\n\n\nClass Meetings\n\nInteractive\nLectures & lots of learn-by-doing\nBring your laptop to class every day\n\n\n\nIn-person, synchronous learning\n\nI will be teaching (so long as I’m healthy and have child care) in person.\nLectures will be podcast.\nAttendance will be incentivized using a daily participation survey.\nIf you’re not feeling well, please stay home. I will do the same.\nExam will be take-home.\n\n\n\nThe (Dreaded) Waitlist\n\nCourse enrollment is supposed to be 50 for this course\nThere are 72 people currently enrolled\nI don’t control the waitlist (cogsadvising@ucsd.edu does)\nI’d anticipate our staff adding 3-5 people from the waitlist (but cannot guarantee this)\n\n\n\nLab & Office Hours\n\nMy office hours begin week 1; TA/IA OH begin week 2\nLab will start this Fri (week 1)\nI will hang out after class today for questions/concerns from students\n\n\n\nCourse Materials\n\nTextbooks are free and available online\nCourse platforms:\n\nWebsite : schedule, policies, due dates, etc.\nGitHub : retrieving assignments, labs, exams, etc.\ndatahub : completing assignments, labs, exams etc.\nGradescope : submitting assignments\nCanvas : grades, course-specific links\nCampuswire : Q&A"
  },
  {
    "objectID": "content/lectures/00-welcome.html#diversity-inclusion",
    "href": "content/lectures/00-welcome.html#diversity-inclusion",
    "title": "00-welcome",
    "section": "Diversity & Inclusion:",
    "text": "Diversity & Inclusion:\nGoal: every student be well-served by this course\n\nPhilosophy: The diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding.\n\n\nPlan: Present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture)\n\n\nBut… if I ever fall short or if you ever have suggestions for improvement, please do share with me! There is also an anonymous Google Form if you’re more comfortable there."
  },
  {
    "objectID": "content/lectures/00-welcome.html#a-new-ish-course",
    "href": "content/lectures/00-welcome.html#a-new-ish-course",
    "title": "00-welcome",
    "section": "A new-ish course!",
    "text": "A new-ish course!\n\nOffered once previously\nIf something doesn’t make sense, tell me!\nIf you’ve got feedback/suggestions, I’m all ears!\n\n\nDifferences since last iteration:\n\nLess…pretty much everything\nNew website/lecture platform (quarto)\nTu/Th and not early in the morning\nGroup mates differ\nIncreased focus on communication\nMultiple final project options\nSlightly larger class & slightly smaller staff\nI’m a mom & just coming back from parental leave"
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-to-get-help",
    "href": "content/lectures/00-welcome.html#how-to-get-help",
    "title": "00-welcome",
    "section": "How to get help",
    "text": "How to get help\n\nLab\nOffice Hours\nCampuswire\n\n\nA few guidelines:\n1. No duplicates.\n2. Public posts are best.\n3. Posts should include your question, what you've tried so far, & resources used.\n4. Helping others is encouraged.\n5. No assignment code in public posts.\n6. We're not robots."
  },
  {
    "objectID": "content/lectures/00-welcome.html#the-r-community",
    "href": "content/lectures/00-welcome.html#the-r-community",
    "title": "00-welcome",
    "section": " The R Community",
    "text": "The R Community\n\n\n\nR Rollercoaster\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/00-welcome.html#academic-integrity",
    "href": "content/lectures/00-welcome.html#academic-integrity",
    "title": "00-welcome",
    "section": "Academic integrity",
    "text": "Academic integrity\nDon’t cheat.\n\nTeamwork is allowed, but you should be able to answer “Yes” to each of the following: - Can I explain each piece of code and each analysis carried out in what I’m submitting? - Could I reproduce this code/analysis on my own?\n\n\nThe Internet is a great resource. - Cite your sources.\n\n\nTeamwork is not allowed on your midterm. - It is open-notes and open-Google - You cannot discuss the questions on the exam with anyone."
  },
  {
    "objectID": "content/lectures/00-welcome.html#course-components",
    "href": "content/lectures/00-welcome.html#course-components",
    "title": "00-welcome",
    "section": "Course components:",
    "text": "Course components:\n\n\nLabs (8): Individual submission; graded on effort\nHomework (4): Individual submission; graded on correctness\nExam (1): Individual completion & submission, take-home midterm\nCase Studies (2): Team submission, technical analysis report\nFinal Project (1) : Team submission, due Thurs of finals week"
  },
  {
    "objectID": "content/lectures/00-welcome.html#grading",
    "href": "content/lectures/00-welcome.html#grading",
    "title": "00-welcome",
    "section": "Grading",
    "text": "Grading\nYour final grade will be comprised of the following:\n\n\n\nAssignment (#)\n% of grade\n\n\n\n\nLabs (8)\n16%\n\n\nHomework (4)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal project* (1)\n17%\n\n\n\n* indicates group submission"
  },
  {
    "objectID": "content/lectures/00-welcome.html#latemissed-work-policy",
    "href": "content/lectures/00-welcome.html#latemissed-work-policy",
    "title": "00-welcome",
    "section": "Late/missed work policy",
    "text": "Late/missed work policy\n\nHomework and case study projects: accepted up to 3 days (72 hours) after the assigned deadline for a 25% deduction\nNo late deadlines for labs, the exam, or the final project\n\n\n\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter."
  },
  {
    "objectID": "content/lectures/00-welcome.html#datahub",
    "href": "content/lectures/00-welcome.html#datahub",
    "title": "00-welcome",
    "section": "Datahub",
    "text": "Datahub\nDatahub is a platform hosted by UCSD that gives students access to computational resources.\nThis means that while you’ll be typing on your keyboard, you’ll be using UCSD’s computers in this class.\nWebsite: https://datahub.ucsd.edu/\n\nLaunch Environment\nWhen working on “stuff” for this course, select the COGS 137 environment.\n\n\n\ndatahub"
  },
  {
    "objectID": "content/lectures/00-welcome.html#toolkit",
    "href": "content/lectures/00-welcome.html#toolkit",
    "title": "00-welcome",
    "section": "Toolkit",
    "text": "Toolkit\n\n\n\ntoolkit\n\n\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-and-rstudio",
    "href": "content/lectures/00-welcome.html#r-and-rstudio",
    "title": "00-welcome",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR/RStudioTourTryR packages\n\n\nR & RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integreated development environment, IDE)\n\n\n\n\n[DEMO]\n\nConcepts introduced:\n\nConsole\nUsing R as a calculator\nEnvironment\nLoading and viewing a data frame\nAccessing a variable in a data frame\nR functions\n\n\n\nYour Turn\n\nLogin to datahub\nCarry out a mathematical operation in the console\nView the airquality dataframe\nAccess a column from the airquality dataframe\nCalculate the median for one of the numeric columns\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question.\n\n\n\nPackages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data 1\nAs of Jan 2023, there are ~18,979 R packages available on CRAN (the Comprehensive R Archive Network)2\nWe’re going to work with a small (but important) subset of these!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "href": "content/lectures/00-welcome.html#what-is-the-tidyverse",
    "title": "00-welcome",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\n\n\n\n\n\ntidyverse.org\n\n\nThe tidyverse is an opinionated collection of R packages designed for data science.\nAll packages share an underlying philosophy and a common syntax."
  },
  {
    "objectID": "content/lectures/00-welcome.html#rstudio-projects3",
    "href": "content/lectures/00-welcome.html#rstudio-projects3",
    "title": "00-welcome",
    "section": "RStudio Projects3",
    "text": "RStudio Projects3\n\nBuilt-in functionality to keep all files for a single project organized"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown",
    "href": "content/lectures/00-welcome.html#r-markdown",
    "title": "00-welcome",
    "section": "R Markdown",
    "text": "R Markdown\n\nFully reproducible reports – each time you knit, the document is executed from top to bottom\nSimple markdown syntax for text\nCode goes in chunks, defined by three backticks, narrative goes outside of chunks"
  },
  {
    "objectID": "content/lectures/00-welcome.html#r-markdown-tips",
    "href": "content/lectures/00-welcome.html#r-markdown-tips",
    "title": "00-welcome",
    "section": "R Markdown tips",
    "text": "R Markdown tips\n\nKeep the R Markdown cheat sheet and Markdown Quick Reference (Help -> Markdown Quick Reference) handy, we’ll refer to it often as the course progresses\nThe workspace of your R Markdown document is separate from the Console\n\n\n\n[DEMO]"
  },
  {
    "objectID": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "href": "content/lectures/00-welcome.html#how-will-we-use-r-markdown",
    "title": "00-welcome",
    "section": "How will we use R Markdown?",
    "text": "How will we use R Markdown?\n\nEvery lab / project / homework / notes / etc. is an R Markdown document\nYou’ll always have a template R Markdown document to start with\nThe amount of scaffolding in the template will decrease over the quarter"
  },
  {
    "objectID": "content/lectures/00-welcome.html#collaboration-git-github",
    "href": "content/lectures/00-welcome.html#collaboration-git-github",
    "title": "00-welcome",
    "section": "Collaboration: Git & GitHub",
    "text": "Collaboration: Git & GitHub\n\nThe statistical programming language we’ll use is R\nThe software we use to interface with R is RStudio\nBut how do I get you the course materials that you can build on for your assignments?\n\nHint: I’m not going to email you documents, that would be a mess!"
  },
  {
    "objectID": "content/lectures/00-welcome.html#version-control",
    "href": "content/lectures/00-welcome.html#version-control",
    "title": "00-welcome",
    "section": "Version control",
    "text": "Version control\n\nWe introduced GitHub as a platform for collaboration\nBut it’s much more than that…\nIt’s actually designed for version control"
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning",
    "href": "content/lectures/00-welcome.html#versioning",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\n\n\n\nLego versions"
  },
  {
    "objectID": "content/lectures/00-welcome.html#versioning-1",
    "href": "content/lectures/00-welcome.html#versioning-1",
    "title": "00-welcome",
    "section": "Versioning",
    "text": "Versioning\nwith human readable messages\n\n\n\nLego versions with commit messages"
  },
  {
    "objectID": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "href": "content/lectures/00-welcome.html#why-do-we-need-version-control",
    "title": "00-welcome",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\nPhD Comics"
  },
  {
    "objectID": "content/lectures/00-welcome.html#git-and-github-tips",
    "href": "content/lectures/00-welcome.html#git-and-github-tips",
    "title": "00-welcome",
    "section": "Git and GitHub tips",
    "text": "Git and GitHub tips\n\nGit is a version control system – like “Track Changes” features from Microsoft Word on steroids. GitHub is the home for your Git-based projects on the internet – like DropBox but much, much better).\n\n\n\nThere are millions of git commands – ok, that’s an exaggeration, but there are a lot of them – and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\n\n\n\n\nWe will be doing Git things and interfacing with GitHub through RStudio, but if you google for help you might come across methods for doing these things in the command line – skip that and move on to the next resource unless you feel comfortable trying it out.\n\n\n\nResource: happygitwithr.com: book for working with git in R; Some content is beyond the scope of this course, but it’s a good resource"
  },
  {
    "objectID": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "href": "content/lectures/00-welcome.html#lets-take-a-tour-git-github",
    "title": "00-welcome",
    "section": "Let’s take a tour – Git / GitHub",
    "text": "Let’s take a tour – Git / GitHub\nWe’ll cover this time permitting, you’ll see it again in lab this week\nConcepts introduced:\n\nConnect an R project to Github repository\nWorking with a local and remote repository\nCommitting, Pushing and Pulling\n\nThere is a bit more of GitHub that we’ll use in this class, but for today this is enough."
  },
  {
    "objectID": "content/lectures/00-welcome.html#recap",
    "href": "content/lectures/00-welcome.html#recap",
    "title": "00-welcome",
    "section": "Recap",
    "text": "Recap\nCan you answer these questions?\n\nWhat is R vs RStudio?\nWhat are RStudio Projects?\nWhat is version control, and why do we care?\nWhat is git vs GitHub (and do I need to care)?"
  },
  {
    "objectID": "content/lectures/00-welcome.html#additional-git-resources",
    "href": "content/lectures/00-welcome.html#additional-git-resources",
    "title": "00-welcome",
    "section": "Additional git Resources",
    "text": "Additional git Resources\n\nVersion Control (git and GitHub):\n\nGetting Started with git\nGitHub Guide\nGitHub Desktop App Tutorial\nGit Command Line Resource\nUsing git from the command line\n\nInstalling and using git (Part 1), by COGS 108 TA Ganesh (youtube, 22min tutorial)\nmerge conflicts and branching (Part 2), by IA Shubham Kulkarni (youtube, 8min tutorial)\n\nUsing git with GitHub Desktop, by COGS 108 TA Sidharth Suresh (youtube, 13min tutorial)\nGIT & GITHUB TUTORIAL, from edureka!\n\nwith notes from COGS 18/108 TA Holly(Yueying) Dong"
  },
  {
    "objectID": "content/lectures/00-welcome.html#slides-to-pdf",
    "href": "content/lectures/00-welcome.html#slides-to-pdf",
    "title": "00-welcome",
    "section": "Slides to PDF",
    "text": "Slides to PDF\n\nToggle into Print View using the E key (or using the Navigation Menu)\nOpen the in-browser print dialog (CTRL/CMD+P).\nChange the Destination setting to Save as PDF.\nChange the Layout to Landscape.\nChange the Margins to None.\nEnable the Background graphics option.\nClick Save 🎉\n\n\n\nInstructions from quarto documentation"
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class",
    "href": "content/lectures/00-welcome.html#whos-in-this-class",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster <- read_sheet('10_NsXld_swxoTL_01pCklRKR95OH5XoNlRzTl7L5XXs')\n\nggplot(roster, aes(x = College)) +\n  geom_bar() +\n  labs(title = \"COGS 137\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\nNote: This code will not run for you because you don’t have access to the roster for this course."
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-1",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  mutate(major = substr(Major, 1, 2)) |>\n  ggplot(aes(fct_infreq(major))) + \n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Major\") +\n  theme_bw(base_size = 12) + \n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "href": "content/lectures/00-welcome.html#whos-in-this-class-2",
    "title": "00-welcome",
    "section": "Who’s in this class?",
    "text": "Who’s in this class?\n\nroster |>\n  ggplot(aes(fct_relevel(Level, \"JR\", \"SR\"))) +\n  geom_bar() +\n  labs(title = \"COGS 137\",\n       x = \"Level\") +\n  theme_bw(base_size = 14) + \n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/00-welcome.html#id-like-to-know-more",
    "href": "content/lectures/00-welcome.html#id-like-to-know-more",
    "title": "00-welcome",
    "section": "I’d like to know more!",
    "text": "I’d like to know more!\n(optional) Student Survey - complete by Monday at 11:59 PM for small amount of extra credit\n\n(optional) Daily Post-Lecture Feedback\n\nopportunity to reflect on learning\nopportunity to ask questions (I will read and answer these.)\nopportunity for extra credit on final project\n\n\n\n\n\nNote: Links to both surveys are also on Canvas."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#qa",
    "href": "content/lectures/07-linear-models-slides.html#qa",
    "title": "07-linear-models",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Do you recommend review some statistics before attending lectures?\nA: This is very much up to you. It certainly wouldn’t hurt! There will be readings connected to each lecture; these are a great place to start. It can be helpful to work through these readings and the exercises either before or after lecture!\n\n\nQ: I am still not quite sure how to do HW part 2 and 3, do we download data into the folder, read the file, and plot it?\nA: Yup! That’s one approach. The other approach (if the data are not available for download) would be to create the dataset on your own, estimating the values as best you can from the plot and creating a dataframe/tibble containing that information directly in your code (similar to what you did in HW01)….and then plot from there."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#course-announcements",
    "href": "content/lectures/07-linear-models-slides.html#course-announcements",
    "title": "07-linear-models",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 04 due tomorrow (2/3; 11:59 PM)\nLecture Participation survey “due” after class\nHW02 due Monday (2/6; 11:59 PM)\n\n\n\nMidterm Exam\n\npractice exam(s) will be released tomorrow; answers posted next week\nwill cover material through “Effective Communication”\nwill be released/posted next Friday after lab\nwill be due Monday Feb 13th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be commpleted individually (open Notes; open Google)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#agenda",
    "href": "content/lectures/07-linear-models-slides.html#agenda",
    "title": "07-linear-models",
    "section": "Agenda",
    "text": "Agenda\n\nLinear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & >2 levels)\nresiduals\ndata transformations"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#data-paris-paintings",
    "href": "content/lectures/07-linear-models-slides.html#data-paris-paintings",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nNumber of observations: 3393\nNumber of variables: 61"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#goal-predict-height-from-width",
    "href": "content/lectures/07-linear-models-slides.html#goal-predict-height-from-width",
    "title": "07-linear-models",
    "section": "Goal: Predict height from width",
    "text": "Goal: Predict height from width\n\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#tidymodels-1",
    "href": "content/lectures/07-linear-models-slides.html#tidymodels-1",
    "title": "07-linear-models",
    "section": "tidymodels",
    "text": "tidymodels\n\nNOT a core tidyverse package\nfollows the structure of a tidyverse package\n\n\n\n# should already be installed for you on datahub\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-1-specify-model",
    "href": "content/lectures/07-linear-models-slides.html#step-1-specify-model",
    "title": "07-linear-models",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-2-set-model-fitting-engine",
    "href": "content/lectures/07-linear-models-slides.html#step-2-set-model-fitting-engine",
    "title": "07-linear-models",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |>\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#step-3-fit-model-estimate-parameters",
    "href": "content/lectures/07-linear-models-slides.html#step-3-fit-model-estimate-parameters",
    "title": "07-linear-models",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\n… using formula syntax\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#a-closer-look-at-model-output",
    "href": "content/lectures/07-linear-models-slides.html#a-closer-look-at-model-output",
    "title": "07-linear-models",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n\n\n\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#a-tidy-look-at-model-output",
    "href": "content/lectures/07-linear-models-slides.html#a-tidy-look-at-model-output",
    "title": "07-linear-models",
    "section": "A tidy look at model output",
    "text": "A tidy look at model output\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n\n\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#slope-and-intercept",
    "href": "content/lectures/07-linear-models-slides.html#slope-and-intercept",
    "title": "07-linear-models",
    "section": "Slope and intercept",
    "text": "Slope and intercept\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]\n\n\nSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n\n\n\nIntercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#correlation-does-not-imply-causation",
    "href": "content/lectures/07-linear-models-slides.html#correlation-does-not-imply-causation",
    "title": "07-linear-models",
    "section": "Correlation does not imply causation",
    "text": "Correlation does not imply causation\nRemember this when interpreting model coefficients\n\n\n\n\n\n\n\nSource: XKCD, Cell phones"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#linear-model-with-a-single-predictor",
    "href": "content/lectures/07-linear-models-slides.html#linear-model-with-a-single-predictor",
    "title": "07-linear-models",
    "section": "Linear model with a single predictor",
    "text": "Linear model with a single predictor\n\nWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\n\n\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n\n\nTough luck, you can’t have them…\n\n\n\n\nSo we use sample statistics to estimate them:\n\n\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#least-squares-regression",
    "href": "content/lectures/07-linear-models-slides.html#least-squares-regression",
    "title": "07-linear-models",
    "section": "Least squares regression",
    "text": "Least squares regression\n\nThe regression line minimizes the sum of squared residuals.\n\n\n\nIf \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\)."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals",
    "title": "07-linear-models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.-1",
    "href": "content/lectures/07-linear-models-slides.html#visualizing-residuals-cont.-1",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#properties-of-least-squares-regression",
    "href": "content/lectures/07-linear-models-slides.html#properties-of-least-squares-regression",
    "title": "07-linear-models",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\n\n\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\nThe slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\n\n\n\n\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\n\n\n\n\nThe residuals and \\(x\\) values are uncorrelated"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 2 levels",
    "text": "Categorical predictor with 2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   <chr>         <dbl>    <dbl>\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# … with 3,373 more rows\n\n\n\n\nlandsALL = 0: No landscape features\nlandsALL = 1: Some landscape features"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#height-landscape-features",
    "href": "content/lectures/07-linear-models-slides.html#height-landscape-features",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\nm_ht_lands <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |> tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#height-landscape-features-1",
    "href": "content/lectures/07-linear-models-slides.html#height-landscape-features-1",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\nSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n\nCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\n\nIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels-1",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-2-levels-1",
    "title": "07-linear-models",
    "section": "Categorical predictor with >2 levels",
    "text": "Categorical predictor with >2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# … with 3,373 more rows\n\n\n\n\nschool from which painting came (details in a few slides)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#relationship-between-height-and-school",
    "href": "content/lectures/07-linear-models-slides.html#relationship-between-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship between height and school",
    "text": "Relationship between height and school\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ school_pntg, data = pp) |>\n  tidy()\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#dummy-variables",
    "href": "content/lectures/07-linear-models-slides.html#dummy-variables",
    "title": "07-linear-models",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nEach coefficient describes the expected difference between heights in that particular school compared to the baseline level"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-3-levels",
    "href": "content/lectures/07-linear-models-slides.html#categorical-predictor-with-3-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 3+ levels",
    "text": "Categorical predictor with 3+ levels\n\n\n\n\n\n\n\nschool_pntg\nD_FL\nF\nG\nI\nS\nX\n\n\n\n\nA\n0\n0\n0\n0\n0\n0\n\n\nD/FL\n1\n0\n0\n0\n0\n0\n\n\nF\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n1\n0\n0\n0\n\n\nI\n0\n0\n0\n1\n0\n0\n\n\nS\n0\n0\n0\n0\n1\n0\n\n\nX\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# … with 3,373 more rows"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#the-linear-model-with-multiple-predictors",
    "href": "content/lectures/07-linear-models-slides.html#the-linear-model-with-multiple-predictors",
    "title": "07-linear-models",
    "section": "The linear model with multiple predictors",
    "text": "The linear model with multiple predictors\n\nPopulation model:\n\n\\[ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k \\]\n\n\nSample model that we use to estimate the population model:\n\n\\[ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k \\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#relationship-bw-height-and-school",
    "href": "content/lectures/07-linear-models-slides.html#relationship-bw-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship b/w height and school",
    "text": "Relationship b/w height and school\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nAustrian school (A) paintings are expected, on average, to be 14 inches tall.\nDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\nFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\nGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\nItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\nSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\nPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings. ]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#predict-height-from-width",
    "href": "content/lectures/07-linear-models-slides.html#predict-height-from-width",
    "title": "07-linear-models",
    "section": "Predict height from width",
    "text": "Predict height from width\n❓ On average, how tall are paintings that are 60 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]\n\n\n3.62 + 0.78 * 60\n\n[1] 50.42\n\n\n“On average, we expect paintings that are 60 inches wide to be 50.42 inches high.”\nWarning: We “expect” this to happen, but there will be some variability. (We’ll learn about measuring the variability around the prediction later.)"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#prediction-vs.-extrapolation",
    "href": "content/lectures/07-linear-models-slides.html#prediction-vs.-extrapolation",
    "title": "07-linear-models",
    "section": "Prediction vs. extrapolation",
    "text": "Prediction vs. extrapolation\n❓ On average, how tall are paintings that are 400 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#watch-out-for-extrapolation",
    "href": "content/lectures/07-linear-models-slides.html#watch-out-for-extrapolation",
    "title": "07-linear-models",
    "section": "Watch out for extrapolation!",
    "text": "Watch out for extrapolation!\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”1  Stephen Colbert, April 6th, 2010\n\nIntroduction to Modern Statistics. “Extrapolation is treacherous.”"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#measuring-the-strength-of-the-fit",
    "href": "content/lectures/07-linear-models-slides.html#measuring-the-strength-of-the-fit",
    "title": "07-linear-models",
    "section": "Measuring the strength of the fit",
    "text": "Measuring the strength of the fit\n\nThe strength of the fit of a linear model is most commonly evaluated using \\(R^2\\).\nIt tells us what percent of variability in the response variable is explained by the model.\nThe remainder of the variability is explained by variables not included in the model.\n\\(R^2\\) is sometimes called the coefficient of determination."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#obtaining-r2-in-r",
    "href": "content/lectures/07-linear-models-slides.html#obtaining-r2-in-r",
    "title": "07-linear-models",
    "section": "Obtaining \\(R^2\\) in R",
    "text": "Obtaining \\(R^2\\) in R\n\nHeight vs. width\n\n\nglance(m_ht_wt)\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³\n      <dbl>      <dbl> <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n1     0.683      0.683  8.30   6749.       0     1 -11083. 22173. 22191. 216055.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\nglance(m_ht_wt)$r.squared # extract R-squared\n\n[1] 0.6829468\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n\n\nHeight vs. landscape features\n\n\nglance(m_ht_lands)$r.squared\n\n[1] 0.03456724"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#data-paris-paintings-1",
    "href": "content/lectures/07-linear-models-slides.html#data-paris-paintings-1",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Describe the relationship between price and width of paintings whose width is less than 100in."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width-1",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width-1",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Which plot shows a more linear relationship?"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#price-vs.-width-residuals",
    "href": "content/lectures/07-linear-models-slides.html#price-vs.-width-residuals",
    "title": "07-linear-models",
    "section": "Price vs. width, residuals",
    "text": "Price vs. width, residuals\n❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❓What’s the unit of residuals?"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#transforming-the-data",
    "href": "content/lectures/07-linear-models-slides.html#transforming-the-data",
    "title": "07-linear-models",
    "section": "Transforming the data",
    "text": "Transforming the data\n\nWe saw that price has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n\n\nIn these situations a transformation applied to the response variable may be useful.\n\n\n\n\nIn order to decide which transformation to use, we should examine the distribution of the response variable.\n\n\n\n\nThe extremely right skewed distribution suggests that a log transformation may be useful.\n\nlog = natural log, \\(ln\\)\nDefault base of the log function in R is the natural log:  log(x, base = exp(1))"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#logged-price-vs.-width",
    "href": "content/lectures/07-linear-models-slides.html#logged-price-vs.-width",
    "title": "07-linear-models",
    "section": "Logged price vs. width",
    "text": "Logged price vs. width\n❓ How do we interpret the slope of this model?"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation",
    "href": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\n\nm_lprice_wt <- lm(log(price) ~ Width_in, data = pp_wt_lt_100)\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#linear-model-with-log-transformation",
    "href": "content/lectures/07-linear-models-slides.html#linear-model-with-log-transformation",
    "title": "07-linear-models",
    "section": "Linear model with log transformation",
    "text": "Linear model with log transformation\n\\[ \\widehat{log(price)} = 4.67 + 0.02 Width \\]\n\n\nFor each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n\n\n\nwhich is not a very useful statement…"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#working-with-logs",
    "href": "content/lectures/07-linear-models-slides.html#working-with-logs",
    "title": "07-linear-models",
    "section": "Working with logs",
    "text": "Working with logs\n\nSubtraction and logs: \\(log(a) − log(b) = log(a / b)\\)\n\n\n\nNatural logarithm: \\(e^{log(x)} = x\\)\n\n\n\n\nWe can use these identities to “undo” the log transformation"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation-1",
    "href": "content/lectures/07-linear-models-slides.html#interpreting-models-with-log-transformation-1",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n\n\\[ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 \\]\n\n\n\\[ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 \\]\n\n\n\\[ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} \\]\n\n\n\\[ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 \\]\n\n\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#shortcuts-in-r",
    "href": "content/lectures/07-linear-models-slides.html#shortcuts-in-r",
    "title": "07-linear-models",
    "section": "Shortcuts in R",
    "text": "Shortcuts in R\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019\n\n\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(exp(estimate), 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   107.  \n2 Width_in        1.02"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#recap-log-transformations",
    "href": "content/lectures/07-linear-models-slides.html#recap-log-transformations",
    "title": "07-linear-models",
    "section": "Recap: Log Transformations",
    "text": "Recap: Log Transformations\n\nNon-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n\n\nThe most common transformation when the response variable is right skewed is the log transform: \\(log(y)\\), especially useful when the response variable is (extremely) right skewed.\n\n\n\n\nThis transformation is also useful for variance stabilization.\n\n\n\n\nWhen using a log transformation on the response variable the interpretation of the slope changes: “For each unit increase in x, y is expected on average to be higher/lower  by a factor of \\(e^{b_1}\\).”\n\n\n\n\nAnother useful transformation is the square root: \\(\\sqrt{y}\\), especially useful when the response variable is counts."
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#aside-when-y-0",
    "href": "content/lectures/07-linear-models-slides.html#aside-when-y-0",
    "title": "07-linear-models",
    "section": "Aside: when \\(y = 0\\)",
    "text": "Aside: when \\(y = 0\\)\nIn some cases the value of the response variable might be 0, and\n\nlog(0)\n\n[1] -Inf\n\n\n\nThe trick is to add a very small number to the value of the response variable for these cases so that the log function can still be applied:\n\nlog(0 + 0.00001)\n\n[1] -11.51293"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#recap",
    "href": "content/lectures/07-linear-models-slides.html#recap",
    "title": "07-linear-models",
    "section": "Recap",
    "text": "Recap\n\nCan I carry out linear regression using the tidymodels approach?\nCan I interpret and explain the results from a linear model with a single predictor?\nDo I understand the limitations of modelling data w/ linear regression?\nCan I describe and implement the use of a dummy variable in linear regression?\nCan I determine when logistic transformation may be appropriate? Can I interpret these results?"
  },
  {
    "objectID": "content/lectures/07-linear-models-slides.html#suggested-reading",
    "href": "content/lectures/07-linear-models-slides.html#suggested-reading",
    "title": "07-linear-models",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/07-linear-models.html",
    "href": "content/lectures/07-linear-models.html",
    "title": "07-linear-models",
    "section": "",
    "text": "Q: Do you recommend review some statistics before attending lectures?\nA: This is very much up to you. It certainly wouldn’t hurt! There will be readings connected to each lecture; these are a great place to start. It can be helpful to work through these readings and the exercises either before or after lecture!\n\n\nQ: I am still not quite sure how to do HW part 2 and 3, do we download data into the folder, read the file, and plot it?\nA: Yup! That’s one approach. The other approach (if the data are not available for download) would be to create the dataset on your own, estimating the values as best you can from the plot and creating a dataframe/tibble containing that information directly in your code (similar to what you did in HW01)….and then plot from there.\n\n\n\n\nDue Dates:\n\nLab 04 due tomorrow (2/3; 11:59 PM)\nLecture Participation survey “due” after class\nHW02 due Monday (2/6; 11:59 PM)\n\n\n\nMidterm Exam\n\npractice exam(s) will be released tomorrow; answers posted next week\nwill cover material through “Effective Communication”\nwill be released/posted next Friday after lab\nwill be due Monday Feb 13th at 11:59 PM\nwill be an Rmd document and submitted via GitHub (like everything so far)\nwill be commpleted individually (open Notes; open Google)\n\n\n\n\n\n\n\nLinear Models\n\nQuantitative Predictor\nCategorical Predictor (2 & >2 levels)\nresiduals\ndata transformations"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nNumber of observations: 3393\nNumber of variables: 61"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#goal-predict-height-from-width",
    "title": "07-linear-models",
    "section": "Goal: Predict height from width",
    "text": "Goal: Predict height from width\n\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#tidymodels-1",
    "href": "content/lectures/07-linear-models.html#tidymodels-1",
    "title": "07-linear-models",
    "section": "tidymodels",
    "text": "tidymodels\n\nNOT a core tidyverse package\nfollows the structure of a tidyverse package\n\n\n\n\n\n\n\n# should already be installed for you on datahub\nlibrary(tidymodels)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-1-specify-model",
    "href": "content/lectures/07-linear-models.html#step-1-specify-model",
    "title": "07-linear-models",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "href": "content/lectures/07-linear-models.html#step-2-set-model-fitting-engine",
    "title": "07-linear-models",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |>\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "href": "content/lectures/07-linear-models.html#step-3-fit-model-estimate-parameters",
    "title": "07-linear-models",
    "section": "Step 3: Fit model & estimate parameters",
    "text": "Step 3: Fit model & estimate parameters\n… using formula syntax\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-closer-look-at-model-output",
    "title": "07-linear-models",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n\n\n\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "href": "content/lectures/07-linear-models.html#a-tidy-look-at-model-output",
    "title": "07-linear-models",
    "section": "A tidy look at model output",
    "text": "A tidy look at model output\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp) |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n\n\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#slope-and-intercept",
    "href": "content/lectures/07-linear-models.html#slope-and-intercept",
    "title": "07-linear-models",
    "section": "Slope and intercept",
    "text": "Slope and intercept\n\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]\n\n\nSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n\n\n\nIntercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "href": "content/lectures/07-linear-models.html#correlation-does-not-imply-causation",
    "title": "07-linear-models",
    "section": "Correlation does not imply causation",
    "text": "Correlation does not imply causation\nRemember this when interpreting model coefficients\n\n\n\n\n\n\n\nSource: XKCD, Cell phones"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "href": "content/lectures/07-linear-models.html#linear-model-with-a-single-predictor",
    "title": "07-linear-models",
    "section": "Linear model with a single predictor",
    "text": "Linear model with a single predictor\n\nWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\n\n\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n\n\nTough luck, you can’t have them…\n\n\n\n\nSo we use sample statistics to estimate them:\n\n\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#least-squares-regression",
    "href": "content/lectures/07-linear-models.html#least-squares-regression",
    "title": "07-linear-models",
    "section": "Least squares regression",
    "text": "Least squares regression\n\nThe regression line minimizes the sum of squared residuals.\n\n\n\nIf \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes \\(\\sum_{i = 1}^n e_i^2\\)."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals",
    "title": "07-linear-models",
    "section": "Visualizing residuals",
    "text": "Visualizing residuals"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "href": "content/lectures/07-linear-models.html#visualizing-residuals-cont.-1",
    "title": "07-linear-models",
    "section": "Visualizing residuals (cont.)",
    "text": "Visualizing residuals (cont.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "href": "content/lectures/07-linear-models.html#properties-of-least-squares-regression",
    "title": "07-linear-models",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\n\n\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\n\nThe slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\n\n\n\n\nThe sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\n\n\n\n\nThe residuals and \\(x\\) values are uncorrelated"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 2 levels",
    "text": "Categorical predictor with 2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   <chr>         <dbl>    <dbl>\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# … with 3,373 more rows\n\n\n\n\nlandsALL = 0: No landscape features\nlandsALL = 1: Some landscape features"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features",
    "href": "content/lectures/07-linear-models.html#height-landscape-features",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\nm_ht_lands <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |> tidy()\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "href": "content/lectures/07-linear-models.html#height-landscape-features-1",
    "title": "07-linear-models",
    "section": "Height & landscape features",
    "text": "Height & landscape features\n\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\nSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n\nCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\n\nIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-2-levels-1",
    "title": "07-linear-models",
    "section": "Categorical predictor with >2 levels",
    "text": "Categorical predictor with >2 levels\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# … with 3,373 more rows\n\n\n\n\nschool from which painting came (details in a few slides)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-between-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship between height and school",
    "text": "Relationship between height and school\n\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ school_pntg, data = pp) |>\n  tidy()\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#dummy-variables",
    "href": "content/lectures/07-linear-models.html#dummy-variables",
    "title": "07-linear-models",
    "section": "Dummy variables",
    "text": "Dummy variables\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\nEach coefficient describes the expected difference between heights in that particular school compared to the baseline level"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "href": "content/lectures/07-linear-models.html#categorical-predictor-with-3-levels",
    "title": "07-linear-models",
    "section": "Categorical predictor with 3+ levels",
    "text": "Categorical predictor with 3+ levels\n\n\n\n\n\n\n\nschool_pntg\nD_FL\nF\nG\nI\nS\nX\n\n\n\n\nA\n0\n0\n0\n0\n0\n0\n\n\nD/FL\n1\n0\n0\n0\n0\n0\n\n\nF\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n1\n0\n0\n0\n\n\nI\n0\n0\n0\n1\n0\n0\n\n\nS\n0\n0\n0\n0\n1\n0\n\n\nX\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# … with 3,373 more rows"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "href": "content/lectures/07-linear-models.html#the-linear-model-with-multiple-predictors",
    "title": "07-linear-models",
    "section": "The linear model with multiple predictors",
    "text": "The linear model with multiple predictors\n\nPopulation model:\n\n\\[ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k \\]\n\n\nSample model that we use to estimate the population model:\n\n\\[ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k \\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "href": "content/lectures/07-linear-models.html#relationship-bw-height-and-school",
    "title": "07-linear-models",
    "section": "Relationship b/w height and school",
    "text": "Relationship b/w height and school\n\n\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n\n\n\nAustrian school (A) paintings are expected, on average, to be 14 inches tall.\nDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\nFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\nGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\nItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\nSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\nPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings. ]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#predict-height-from-width",
    "href": "content/lectures/07-linear-models.html#predict-height-from-width",
    "title": "07-linear-models",
    "section": "Predict height from width",
    "text": "Predict height from width\n❓ On average, how tall are paintings that are 60 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]\n\n\n3.62 + 0.78 * 60\n\n[1] 50.42\n\n\n“On average, we expect paintings that are 60 inches wide to be 50.42 inches high.”\nWarning: We “expect” this to happen, but there will be some variability. (We’ll learn about measuring the variability around the prediction later.)"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "href": "content/lectures/07-linear-models.html#prediction-vs.-extrapolation",
    "title": "07-linear-models",
    "section": "Prediction vs. extrapolation",
    "text": "Prediction vs. extrapolation\n❓ On average, how tall are paintings that are 400 inches wide? \\[\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}\\]"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "href": "content/lectures/07-linear-models.html#watch-out-for-extrapolation",
    "title": "07-linear-models",
    "section": "Watch out for extrapolation!",
    "text": "Watch out for extrapolation!\n\n“When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.”1  Stephen Colbert, April 6th, 2010"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "href": "content/lectures/07-linear-models.html#measuring-the-strength-of-the-fit",
    "title": "07-linear-models",
    "section": "Measuring the strength of the fit",
    "text": "Measuring the strength of the fit\n\nThe strength of the fit of a linear model is most commonly evaluated using \\(R^2\\).\nIt tells us what percent of variability in the response variable is explained by the model.\nThe remainder of the variability is explained by variables not included in the model.\n\\(R^2\\) is sometimes called the coefficient of determination."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "href": "content/lectures/07-linear-models.html#obtaining-r2-in-r",
    "title": "07-linear-models",
    "section": "Obtaining \\(R^2\\) in R",
    "text": "Obtaining \\(R^2\\) in R\n\nHeight vs. width\n\n\nglance(m_ht_wt)\n\n# A tibble: 1 × 12\n  r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³\n      <dbl>      <dbl> <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n1     0.683      0.683  8.30   6749.       0     1 -11083. 22173. 22191. 216055.\n# … with 2 more variables: df.residual <int>, nobs <int>, and abbreviated\n#   variable names ¹​adj.r.squared, ²​statistic, ³​deviance\n\nglance(m_ht_wt)$r.squared # extract R-squared\n\n[1] 0.6829468\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n\n\nHeight vs. landscape features\n\n\nglance(m_ht_lands)$r.squared\n\n[1] 0.03456724"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "href": "content/lectures/07-linear-models.html#data-paris-paintings-1",
    "title": "07-linear-models",
    "section": "Data: Paris Paintings",
    "text": "Data: Paris Paintings"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width",
    "href": "content/lectures/07-linear-models.html#price-vs.-width",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Describe the relationship between price and width of paintings whose width is less than 100in."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-1",
    "title": "07-linear-models",
    "section": "Price vs. width",
    "text": "Price vs. width\n❓ Which plot shows a more linear relationship?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "href": "content/lectures/07-linear-models.html#price-vs.-width-residuals",
    "title": "07-linear-models",
    "section": "Price vs. width, residuals",
    "text": "Price vs. width, residuals\n❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❓What’s the unit of residuals?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#transforming-the-data",
    "href": "content/lectures/07-linear-models.html#transforming-the-data",
    "title": "07-linear-models",
    "section": "Transforming the data",
    "text": "Transforming the data\n\nWe saw that price has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n\n\nIn these situations a transformation applied to the response variable may be useful.\n\n\n\n\nIn order to decide which transformation to use, we should examine the distribution of the response variable.\n\n\n\n\nThe extremely right skewed distribution suggests that a log transformation may be useful.\n\nlog = natural log, \\(ln\\)\nDefault base of the log function in R is the natural log:  log(x, base = exp(1))"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "href": "content/lectures/07-linear-models.html#logged-price-vs.-width",
    "title": "07-linear-models",
    "section": "Logged price vs. width",
    "text": "Logged price vs. width\n❓ How do we interpret the slope of this model?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\n\nm_lprice_wt <- lm(log(price) ~ Width_in, data = pp_wt_lt_100)\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "href": "content/lectures/07-linear-models.html#linear-model-with-log-transformation",
    "title": "07-linear-models",
    "section": "Linear model with log transformation",
    "text": "Linear model with log transformation\n\\[ \\widehat{log(price)} = 4.67 + 0.02 Width \\]\n\n\nFor each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n\n\n\nwhich is not a very useful statement…"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#working-with-logs",
    "href": "content/lectures/07-linear-models.html#working-with-logs",
    "title": "07-linear-models",
    "section": "Working with logs",
    "text": "Working with logs\n\nSubtraction and logs: \\(log(a) − log(b) = log(a / b)\\)\n\n\n\nNatural logarithm: \\(e^{log(x)} = x\\)\n\n\n\n\nWe can use these identities to “undo” the log transformation"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "href": "content/lectures/07-linear-models.html#interpreting-models-with-log-transformation-1",
    "title": "07-linear-models",
    "section": "Interpreting models with log transformation",
    "text": "Interpreting models with log transformation\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n\n\\[ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 \\]\n\n\n\\[ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 \\]\n\n\n\\[ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} \\]\n\n\n\\[ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 \\]\n\n\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "href": "content/lectures/07-linear-models.html#shortcuts-in-r",
    "title": "07-linear-models",
    "section": "Shortcuts in R",
    "text": "Shortcuts in R\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019\n\n\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(exp(estimate), 3))\n\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   107.  \n2 Width_in        1.02"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap-log-transformations",
    "href": "content/lectures/07-linear-models.html#recap-log-transformations",
    "title": "07-linear-models",
    "section": "Recap: Log Transformations",
    "text": "Recap: Log Transformations\n\nNon-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n\n\nThe most common transformation when the response variable is right skewed is the log transform: \\(log(y)\\), especially useful when the response variable is (extremely) right skewed.\n\n\n\n\nThis transformation is also useful for variance stabilization.\n\n\n\n\nWhen using a log transformation on the response variable the interpretation of the slope changes: “For each unit increase in x, y is expected on average to be higher/lower  by a factor of \\(e^{b_1}\\).”\n\n\n\n\nAnother useful transformation is the square root: \\(\\sqrt{y}\\), especially useful when the response variable is counts."
  },
  {
    "objectID": "content/lectures/07-linear-models.html#aside-when-y-0",
    "href": "content/lectures/07-linear-models.html#aside-when-y-0",
    "title": "07-linear-models",
    "section": "Aside: when \\(y = 0\\)",
    "text": "Aside: when \\(y = 0\\)\nIn some cases the value of the response variable might be 0, and\n\nlog(0)\n\n[1] -Inf\n\n\n\nThe trick is to add a very small number to the value of the response variable for these cases so that the log function can still be applied:\n\nlog(0 + 0.00001)\n\n[1] -11.51293"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#recap",
    "href": "content/lectures/07-linear-models.html#recap",
    "title": "07-linear-models",
    "section": "Recap",
    "text": "Recap\n\nCan I carry out linear regression using the tidymodels approach?\nCan I interpret and explain the results from a linear model with a single predictor?\nDo I understand the limitations of modelling data w/ linear regression?\nCan I describe and implement the use of a dummy variable in linear regression?\nCan I determine when logistic transformation may be appropriate? Can I interpret these results?"
  },
  {
    "objectID": "content/lectures/07-linear-models.html#suggested-reading",
    "href": "content/lectures/07-linear-models.html#suggested-reading",
    "title": "07-linear-models",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 24: Model Building\nIntroduction to Modern Statistics Chapter 7: Linear Regression with a Single Predictor"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#qa",
    "href": "content/lectures/13-cs01-analysis-slides.html#qa",
    "title": "13-cs01-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I’m curious about how different groups will approach this dataset differently, and hope that we can have some kind of showcase after this case study is completed!\nA: I am too - sometimes students add another (related) question. Some find a new related dataset. Some go really deep on visualization. I like the idea of a showcase! Was not in my plan, but I think it should be!\n\n\nQ: I’m confused about…how to choose the best EDA.\nA: The best EDA is the EDA that best helps you understand the datasets being used for analysis. So, while there’s not “one plot to rule them all,” a great EDA uses visualizations and data summaries that let the analyst really understand the data. Two EDAs can make different decisions and both be “best”."
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#course-announcements",
    "href": "content/lectures/13-cs01-analysis-slides.html#course-announcements",
    "title": "13-cs01-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLecture Participation survey “due” after class\nLab06 due tomorrow (2/24; 11:59 PM)\nHW03 due Mon (2/27; 11:59 PM)\n\n\nNotes:\n\nFinal Project Groups survey (link also on canvas; “due” Friday)\nAccept the GH repo for cs01; talk with group mates\nMidterm grades posted (Canvas) and feedback available (GitHub Issue)\n\nAnswer key on website\nRegrades open until Sunday at 5PM (GitHub issue or Campuswire post)\n\nCS01 Data Wrangling: Campuswire Post"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#agenda",
    "href": "content/lectures/13-cs01-analysis-slides.html#agenda",
    "title": "13-cs01-analysis",
    "section": "Agenda",
    "text": "Agenda\n\npanel data & analysis\nmodelling the LOTT and DONOGHUE data\nMulticollinearity\nVIF"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#packages-data",
    "href": "content/lectures/13-cs01-analysis-slides.html#packages-data",
    "title": "13-cs01-analysis",
    "section": "Packages & Data",
    "text": "Packages & Data\n\n# i've asked ITS to install those on datahub for you\n# if that's not yet complete, you can install using install.packages()\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(plm) \nlibrary(car) \nlibrary(rsample) \nlibrary(GGally) \nlibrary(ggcorrplot) \n\n\nThis will only work if you finished the wrangling…\n\nload(\"data/wrangled/wrangled_data_rtc.rda\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#panel-data-1",
    "href": "content/lectures/13-cs01-analysis-slides.html#panel-data-1",
    "title": "13-cs01-analysis",
    "section": "Panel Data",
    "text": "Panel Data\n\nrepeated measures for multiple panel members or individuals over time.\n\nmultiple units (violent crime and other variables for each state)\nmultiple time points (multiple years)\n\n\n\nLingo:\n\n\\(N\\) individual panel members\n\\(T\\) time points\n\n\n\n\nBalanced Panels: At each time point ( \\(T\\) ), there are data points for each individual( \\(N\\) ). ( \\(n = N∗T\\) )\nUnbalanced Panels: May be data points missing for some individuals ( \\(N\\) ) at some time points ( \\(T\\) ) ( \\(n\\) observations \\(\\lt N∗T\\))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#our-panel-data",
    "href": "content/lectures/13-cs01-analysis-slides.html#our-panel-data",
    "title": "13-cs01-analysis",
    "section": "Our Panel Data",
    "text": "Our Panel Data\nIn our case we have:\n\n\\(N\\) = 44 states (in the data wrangling process we removed those who had adopted an RTC law before 1980)\n\\(T\\) = 31 years (1980 - 2010)\n\nPanel is balanced: \\(n=44∗31\\), thus \\(n = 1364\\)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#panel-linear-regression",
    "href": "content/lectures/13-cs01-analysis-slides.html#panel-linear-regression",
    "title": "13-cs01-analysis",
    "section": "Panel Linear Regression",
    "text": "Panel Linear Regression\n\\[Y_{it}=β_{0}+β_{1}X_{1it}+...+β_{K}X_{Kit}+e_{it}\\]\n\n\\(i\\) is the individual dimension (in our case individual states)\n\\(t\\) is the time dimension.\n\n\nNotes:\n\nSome explanatory variables \\(X_{it}\\) will vary across individuals and time\nothers will be fixed across the time of the study (or don’t change over time)\nothers still will be fixed across individuals but vary across time periods\n\n\n\n❓ Which are examples of variables in our analysis that likely fall into each of these categories?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#fixed-effects-panel-regression-analysis",
    "href": "content/lectures/13-cs01-analysis-slides.html#fixed-effects-panel-regression-analysis",
    "title": "13-cs01-analysis",
    "section": "Fixed Effects Panel Regression Analysis",
    "text": "Fixed Effects Panel Regression Analysis\n\nassumes that there are unknown or unobserved unique aspects about the individuals or heterogeneity among individuals (states) \\(a_i\\) that are not explained by the independent variables but influence the outcome variable of interest (crime).\nthey do not vary with time or in other words are fixed over time but may be correlated with independent variables \\(X_{it}\\)\n\n\n\nThe intercept can be different for each individual \\(\\beta_{0i}\\) (each state)\nOther coefficients are assumed to be the same across all the individuals.\n\n\n\nIn our data…some of the unobserved qualities about the different states may be correlated with some of our independent variables (i.e.level of economic opportunity might be an unobserved feature about the states that influences violent crime rate (outcome) and would be possibly correlated with poverty rate and unemployment (predictors))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#fixed-effects-panel-regression-analysis-model",
    "href": "content/lectures/13-cs01-analysis-slides.html#fixed-effects-panel-regression-analysis-model",
    "title": "13-cs01-analysis",
    "section": "Fixed Effects Panel Regression Analysis (Model)",
    "text": "Fixed Effects Panel Regression Analysis (Model)\nThese individual \\(a_i\\) effects can be correlated with the independent variables \\(X\\):\n\\[Y_{it}=\\beta_{0}+\\beta_{1}X_{1it}+...\\beta_{K}X_{Kit}+ a_i +e_{it}\\] …or alternatively the individual effects can be absorbed into an individual-specific intercept term \\(\\beta_{0i}\\):\n\\[Y_{it}=\\beta_{0i}+\\beta_{1}X_{1it}+...\\beta_{k}X_{kit} +e_{it}\\]"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#panel-linear-model-plm",
    "href": "content/lectures/13-cs01-analysis-slides.html#panel-linear-model-plm",
    "title": "13-cs01-analysis",
    "section": "Panel Linear Model (plm)",
    "text": "Panel Linear Model (plm)\n\ncarry out panel linear models\nNew object: pdata.frame (panel data frame); can indicate:\n\nwhich variable should be used to identify the individuals in our panel (STATE)\nwhat variable should be used to identify the time periods in our panel (YEAR)\n\nWill be specified in index parameter: index = c(\"Individual_Variable_NAME\", \"Time_Period_Variable_NAME\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#pdata.frame",
    "href": "content/lectures/13-cs01-analysis-slides.html#pdata.frame",
    "title": "13-cs01-analysis",
    "section": "pdata.frame",
    "text": "pdata.frame\n\nd_panel_DONOHUE <- pdata.frame(DONOHUE_DF, index = c(\"STATE\", \"YEAR\"))\nclass(d_panel_DONOHUE)\n\n[1] \"pdata.frame\" \"data.frame\" \n\nhead(d_panel_DONOHUE, n = 3)\n\n            YEAR  STATE Black_Male_15_to_19_years Black_Male_20_to_39_years\nAlaska-1980 1980 Alaska                 0.1670456                 0.9933775\nAlaska-1981 1981 Alaska                 0.1732299                 1.0028219\nAlaska-1982 1982 Alaska                 0.1737069                 1.0204445\n            Other_Male_15_to_19_years Other_Male_20_to_39_years\nAlaska-1980                  1.129782                  2.963329\nAlaska-1981                  1.124441                  2.974775\nAlaska-1982                  1.069821                  3.015071\n            White_Male_15_to_19_years White_Male_20_to_39_years\nAlaska-1980                  3.627805                  18.28852\nAlaska-1981                  3.558261                  18.12821\nAlaska-1982                  3.391844                  18.10666\n            Unemployment_rate Poverty_rate Viol_crime_count Population\nAlaska-1980               9.6          9.6             1919     404680\nAlaska-1981               9.4          9.0             2537     418519\nAlaska-1982               9.9         10.6             2732     449608\n            police_per_100k_lag RTC_LAW_YEAR RTC_LAW TIME_0 TIME_INF\nAlaska-1980            194.7218         1995   FALSE   1980     2010\nAlaska-1981            200.2299         1995   FALSE   1980     2010\nAlaska-1982            191.0553         1995   FALSE   1980     2010\n            Viol_crime_rate_1k Viol_crime_rate_1k_log Population_log\nAlaska-1980           4.742018               1.556463       12.91085\nAlaska-1981           6.061851               1.802015       12.94448\nAlaska-1982           6.076404               1.804413       13.01613"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#model-considerations-effect",
    "href": "content/lectures/13-cs01-analysis-slides.html#model-considerations-effect",
    "title": "13-cs01-analysis",
    "section": "Model Considerations: effect",
    "text": "Model Considerations: effect\nThere are three main options for the effect argument:\n\n\nindividual - model for the effect of individual identity\ntime - model for the effect of time\ntwoways - meaning modeling for the effect of both individual identity and time\n\n\n\n\n\nspeculate that there is an effect of individual STATE identity and time on violent crime rate (effect = \"twoways\")\n\naka expect some states to have high rates of crime, and others to have low rates of crime\nexpect crime rates to change over time"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#model-considerations-model",
    "href": "content/lectures/13-cs01-analysis-slides.html#model-considerations-model",
    "title": "13-cs01-analysis",
    "section": "Model Considerations: model",
    "text": "Model Considerations: model\n\n\npooling - standard pooled ordinary least squares regression model\n\nwithin - fixed effects model (variation between individuals is ignored, model compares individuals to themselves at different periods of time)\n\nbetween - fixed effects model (variation within individuals from one time point to another is ignored, model compares different individuals at each point of time)\n\nrandom - random effects (each state has a different intercept but force it to follow a normal distribution - requires more assumptions)\n\n\n\ninterested in how violence in each state varied over time: within STATE variation (model = within)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#model-plm-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#model-plm-donohue",
    "title": "13-cs01-analysis",
    "section": "Model: plm (DONOHUE)",
    "text": "Model: plm (DONOHUE)\n\nDONOHUE_OUTPUT <- plm(Viol_crime_rate_1k_log ~\n                      RTC_LAW +\n                      White_Male_15_to_19_years +\n                      White_Male_20_to_39_years +\n                      Black_Male_15_to_19_years +\n                      Black_Male_20_to_39_years +\n                      Other_Male_15_to_19_years +\n                      Other_Male_20_to_39_years +\n                      Unemployment_rate +\n                      Poverty_rate +\n                      Population_log +\n                      police_per_100k_lag,\n                      effect = \"twoways\",\n                      model = \"within\",\n                      data = d_panel_DONOHUE)\n\n\n❓ What is our outcome variable here? What is our primary predictor of interest? Why are all the other variables included?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#model-output-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#model-output-donohue",
    "title": "13-cs01-analysis",
    "section": "Model Output: DONOHUE",
    "text": "Model Output: DONOHUE\n\nDONOHUE_OUTPUT_TIDY <- tidy(DONOHUE_OUTPUT, conf.int = 0.95)\nDONOHUE_OUTPUT_TIDY\n\n# A tibble: 11 × 7\n   term                      estimate std.e…¹ stati…²  p.value conf.low conf.h…³\n   <chr>                        <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>\n 1 RTC_LAWTRUE                2.40e-2 1.70e-2   1.42  1.56e- 1 -9.19e-3  5.73e-2\n 2 White_Male_15_to_19_years  1.04e-2 2.82e-2   0.367 7.13e- 1 -4.49e-2  6.56e-2\n 3 White_Male_20_to_39_years  2.93e-2 1.00e-2   2.93  3.50e- 3  9.68e-3  4.90e-2\n 4 Black_Male_15_to_19_years -5.97e-2 5.78e-2  -1.03  3.02e- 1 -1.73e-1  5.36e-2\n 5 Black_Male_20_to_39_years  1.23e-1 1.95e-2   6.34  3.17e-10  8.53e-2  1.62e-1\n 6 Other_Male_15_to_19_years  6.74e-1 1.14e-1   5.92  4.15e- 9  4.51e-1  8.97e-1\n 7 Other_Male_20_to_39_years -3.04e-1 3.83e-2  -7.95  4.21e-15 -3.79e-1 -2.29e-1\n 8 Unemployment_rate         -1.71e-2 4.98e-3  -3.42  6.36e- 4 -2.68e-2 -7.29e-3\n 9 Poverty_rate              -7.60e-3 2.99e-3  -2.54  1.12e- 2 -1.35e-2 -1.74e-3\n10 Population_log            -2.11e-1 6.17e-2  -3.42  6.55e- 4 -3.32e-1 -8.99e-2\n11 police_per_100k_lag        5.59e-4 1.40e-4   4.00  6.72e- 5  2.85e-4  8.33e-4\n# … with abbreviated variable names ¹​std.error, ²​statistic, ³​conf.high\n\nDONOHUE_OUTPUT_TIDY$Analysis <- \"Donohue\""
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#formula-as.formula-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#formula-as.formula-lott",
    "title": "13-cs01-analysis",
    "section": "Formula: as.formula() (LOTT)",
    "text": "Formula: as.formula() (LOTT)\n\nLOTT_variables <- LOTT_DF |>\n  select(RTC_LAW,\n         contains(c(\"White\", \"Black\", \"Other\")),\n         Unemployment_rate,\n         Poverty_rate,\n         Population_log,\n         police_per_100k_lag) |>\n  colnames()\n\nLOTT_fmla <- as.formula(paste(\"Viol_crime_rate_1k_log ~\", paste(LOTT_variables, collapse = \" + \")))\nLOTT_fmla\n\nViol_crime_rate_1k_log ~ RTC_LAW + White_Female_10_to_19_years + \n    White_Female_20_to_29_years + White_Female_30_to_39_years + \n    White_Female_40_to_49_years + White_Female_50_to_64_years + \n    White_Female_65_years_and_over + White_Male_10_to_19_years + \n    White_Male_20_to_29_years + White_Male_30_to_39_years + White_Male_40_to_49_years + \n    White_Male_50_to_64_years + White_Male_65_years_and_over + \n    Black_Female_10_to_19_years + Black_Female_20_to_29_years + \n    Black_Female_30_to_39_years + Black_Female_40_to_49_years + \n    Black_Female_50_to_64_years + Black_Female_65_years_and_over + \n    Black_Male_10_to_19_years + Black_Male_20_to_29_years + Black_Male_30_to_39_years + \n    Black_Male_40_to_49_years + Black_Male_50_to_64_years + Black_Male_65_years_and_over + \n    Other_Female_10_to_19_years + Other_Female_20_to_29_years + \n    Other_Female_30_to_39_years + Other_Female_40_to_49_years + \n    Other_Female_50_to_64_years + Other_Female_65_years_and_over + \n    Other_Male_10_to_19_years + Other_Male_20_to_29_years + Other_Male_30_to_39_years + \n    Other_Male_40_to_49_years + Other_Male_50_to_64_years + Other_Male_65_years_and_over + \n    Unemployment_rate + Poverty_rate + Population_log + police_per_100k_lag"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#model-plm-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#model-plm-lott",
    "title": "13-cs01-analysis",
    "section": "Model: plm (LOTT)",
    "text": "Model: plm (LOTT)\n\nd_panel_LOTT <- pdata.frame(LOTT_DF, index = c(\"STATE\", \"YEAR\"))\n\nLOTT_OUTPUT <- plm(LOTT_fmla,\n                   model = \"within\",\n                   effect = \"twoways\",\n                   data = d_panel_LOTT)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#model-output-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#model-output-lott",
    "title": "13-cs01-analysis",
    "section": "Model Output: LOTT",
    "text": "Model Output: LOTT\n\nLOTT_OUTPUT_TIDY <- tidy(LOTT_OUTPUT, conf.int = 0.95)\nLOTT_OUTPUT_TIDY\n\n# A tibble: 41 × 7\n   term                        estimate std.e…¹ stati…²  p.value conf.…³ conf.…⁴\n   <chr>                          <dbl>   <dbl>   <dbl>    <dbl>   <dbl>   <dbl>\n 1 RTC_LAWTRUE                 -0.0518   0.0162  -3.20  1.39e- 3 -0.0835 -0.0201\n 2 White_Female_10_to_19_years  0.636    0.149    4.26  2.24e- 5  0.343   0.929 \n 3 White_Female_20_to_29_years  0.00698  0.0670   0.104 9.17e- 1 -0.124   0.138 \n 4 White_Female_30_to_39_years  0.261    0.0813   3.21  1.38e- 3  0.101   0.420 \n 5 White_Female_40_to_49_years  0.0168   0.0814   0.206 8.37e- 1 -0.143   0.176 \n 6 White_Female_50_to_64_years -0.459    0.0625  -7.35  3.60e-13 -0.582  -0.337 \n 7 White_Female_65_years_and_…  0.156    0.0469   3.33  9.04e- 4  0.0641  0.248 \n 8 White_Male_10_to_19_years   -0.583    0.143   -4.07  4.92e- 5 -0.863  -0.302 \n 9 White_Male_20_to_29_years    0.0639   0.0623   1.03  3.05e- 1 -0.0582  0.186 \n10 White_Male_30_to_39_years   -0.200    0.0859  -2.33  2.01e- 2 -0.368  -0.0315\n# … with 31 more rows, and abbreviated variable names ¹​std.error, ²​statistic,\n#   ³​conf.low, ⁴​conf.high\n\nLOTT_OUTPUT_TIDY$Analysis <- \"Lott\""
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#rtc-coefficient-comparison",
    "href": "content/lectures/13-cs01-analysis-slides.html#rtc-coefficient-comparison",
    "title": "13-cs01-analysis",
    "section": "RTC coefficient comparison",
    "text": "RTC coefficient comparison\n\ncomparing_analyses <- DONOHUE_OUTPUT_TIDY |>\n  bind_rows(LOTT_OUTPUT_TIDY) |>\n  filter(term == \"RTC_LAWTRUE\")\n\ncomparing_analyses\n\n# A tibble: 2 × 8\n  term        estimate std.error statistic p.value conf.low conf.high Analysis\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl> <chr>   \n1 RTC_LAWTRUE   0.0240    0.0170      1.42 0.156   -0.00919    0.0573 Donohue \n2 RTC_LAWTRUE  -0.0518    0.0162     -3.20 0.00139 -0.0835    -0.0201 Lott    \n\n\n\n🧠 With each analysis, what would your conclusion be to the question “What is the relationship between right to carry laws and violence rates in the US”?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#rtc",
    "href": "content/lectures/13-cs01-analysis-slides.html#rtc",
    "title": "13-cs01-analysis",
    "section": "RTC",
    "text": "RTC\n\nCodePlot\n\n\n\nggplot(comparing_analyses) +\n  geom_point(aes(x = Analysis, y = estimate)) +\n  geom_errorbar(aes(x = Analysis, ymin = conf.low, ymax = conf.high), width = 0.25) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  scale_y_continuous(\n    breaks = seq(-0.2, 0.2, by = 0.05),\n    labels = seq(-0.2, 0.2, by = 0.05),\n    limits = c(-0.2, 0.2)\n  ) +\n  geom_segment(aes(x = 1, y = 0.125, xend = 1, yend = 0.175),\n    arrow = arrow(angle = 45, ends = \"last\", type = \"open\"),\n    size = 2, color = \"green\", lineend = \"butt\", linejoin = \"mitre\"\n  ) +\n  geom_segment(aes(x = 2, y = -0.125, xend = 2, yend = -0.175),\n    arrow = arrow(angle = 45, ends = \"last\", type = \"open\"),\n    size = 2, color = \"red\", lineend = \"butt\", linejoin = \"mitre\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text = element_text(size = 8, color = \"black\")\n  ) +\n  labs(\n    title = \"Effect estimate on ln(violent crimes per 100,000 people)\",\n    y = \"  Effect estimate (95% CI)\"\n  )"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#similar-data---different-results",
    "href": "content/lectures/13-cs01-analysis-slides.html#similar-data---different-results",
    "title": "13-cs01-analysis",
    "section": "Similar Data - different results",
    "text": "Similar Data - different results\n\nThe only difference between the two data frames rests in how the demographic variables were parameterized.\n\nDonohue: Males only; ages 15 to 39\nLott: Males/Females; ages 10-65+\n\n\n\n…so how did this occur?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#multicollinearity-1",
    "href": "content/lectures/13-cs01-analysis-slides.html#multicollinearity-1",
    "title": "13-cs01-analysis",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nmulticollinearity occurs when independent variables (predictors) are highly related to one another\n\n\n\nWhen seemingly independent variables are highly related to one another, the relationships estimated in an analysis may be distorted.\nsince linear regression aims to determine how a one unit change in a regressor influences a one unit change in the dependent variable, if the regressors are collinear…effect of each regressor cannot be accurately estimated"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#diagnosing-multicollinearity-pairplots",
    "href": "content/lectures/13-cs01-analysis-slides.html#diagnosing-multicollinearity-pairplots",
    "title": "13-cs01-analysis",
    "section": "Diagnosing Multicollinearity: Pairplots",
    "text": "Diagnosing Multicollinearity: Pairplots\n\nCodePlot\n\n\n\nDONOHUE_DF |>\n  select(RTC_LAW,\n         Viol_crime_rate_1k_log,\n         Unemployment_rate,\n         Poverty_rate,\n         Population_log) |>\n  ggpairs(columns = c(2:5),\n          lower = list(continuous = wrap(\"smooth_loess\",\n                                         color = \"red\",\n                                         alpha = 0.5,\n                                         size = 0.1)))\n\n\n\n\n\n\n\n\n…not much correlation for non-demographic variables - unemployment and poverty rate show some (as expeted)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#diagnosing-multicollinearity-heatmaps",
    "href": "content/lectures/13-cs01-analysis-slides.html#diagnosing-multicollinearity-heatmaps",
    "title": "13-cs01-analysis",
    "section": "Diagnosing Multicollinearity: Heatmaps",
    "text": "Diagnosing Multicollinearity: Heatmaps\n\nCodePlotNotes\n\n\n\ncor_DONOHUE_dem <- cor(DONOHUE_DF |> select(contains(\"_years\")))\n\nggcorrplot(cor_DONOHUE_dem,\n  tl.cex = 6,\n  hc.order = TRUE,\n  colors = c(\n    \"red\",\n    \"white\",\n    \"red\"\n  ),\n  outline.color = \"transparent\",\n  title = \"Correlation Matrix: Donohue\",\n  legend.title = expression(rho)\n)\n\n\n\n\n\n\n\n\n\n\n\nstrong correlation within race\nrace shows much stronger correlation than age\nsuggests collinearity"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#heatmap-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#heatmap-lott",
    "title": "13-cs01-analysis",
    "section": "Heatmap: Lott",
    "text": "Heatmap: Lott\n\nCodePlot\n\n\n\ncor_LOTT_dem <- cor(LOTT_DF |> select(contains(\"_years\")))\n\ncorr_mat_LOTT <- ggcorrplot(cor_LOTT_dem,\n  tl.cex = 6,\n  hc.order = TRUE,\n  colors = c(\n    \"red\",\n    \"white\",\n    \"red\"\n  ),\n  outline.color = \"transparent\",\n  title = \"Correlation Matrix: Lott\",\n  legend.title = expression(rho)\n)\n\ncorr_mat_LOTT"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#suggested-multicollinearity",
    "href": "content/lectures/13-cs01-analysis-slides.html#suggested-multicollinearity",
    "title": "13-cs01-analysis",
    "section": "Suggested multicollinearity",
    "text": "Suggested multicollinearity\n…so how do you find out for sure?\n\nlook at the stability of the coefficient estimates under perturbations of the data\nwe’ll focus on RTC_LAW\nuse resampling (here: remove one observation, see if estimates change == LOOCV - leave one out cross-validation): rsample::loo_cv"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#loocv-splits-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#loocv-splits-donohue",
    "title": "13-cs01-analysis",
    "section": "LOOCV: splits (Donohue)",
    "text": "LOOCV: splits (Donohue)\n\n## split data\nset.seed(124)\nDONOHUE_splits <- d_panel_DONOHUE |> loo_cv()\nDONOHUE_splits\n\n# Leave-one-out cross-validation \n# A tibble: 1,364 × 2\n   splits           id        \n   <list>           <chr>     \n 1 <split [1363/1]> Resample1 \n 2 <split [1363/1]> Resample2 \n 3 <split [1363/1]> Resample3 \n 4 <split [1363/1]> Resample4 \n 5 <split [1363/1]> Resample5 \n 6 <split [1363/1]> Resample6 \n 7 <split [1363/1]> Resample7 \n 8 <split [1363/1]> Resample8 \n 9 <split [1363/1]> Resample9 \n10 <split [1363/1]> Resample10\n# … with 1,354 more rows"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#loocv-get-data-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#loocv-get-data-donohue",
    "title": "13-cs01-analysis",
    "section": "LOOCV: get data (Donohue)",
    "text": "LOOCV: get data (Donohue)\n\nDONOHUE_subsets <- map(pull(DONOHUE_splits, splits), training)\nglimpse(DONOHUE_subsets[[1]])\n\nRows: 1,363\nColumns: 20\n$ YEAR                      <fct> 1980, 1981, 1982, 1983, 1984, 1985, 1986, 19…\n$ STATE                     <fct> Alaska, Alaska, Alaska, Alaska, Alaska, Alas…\n$ Black_Male_15_to_19_years <pseries> 0.1670456, 0.1732299, 0.1737069, 0.17095…\n$ Black_Male_20_to_39_years <pseries> 0.9933775, 1.0028219, 1.0204445, 1.03127…\n$ Other_Male_15_to_19_years <pseries> 1.1297816, 1.1244412, 1.0698208, 0.98828…\n$ Other_Male_20_to_39_years <pseries> 2.963329, 2.974775, 3.015071, 3.008048, …\n$ White_Male_15_to_19_years <pseries> 3.627805, 3.558261, 3.391844, 3.222002, …\n$ White_Male_20_to_39_years <pseries> 18.28852, 18.12821, 18.10666, 17.90600, …\n$ Unemployment_rate         <pseries> 9.6, 9.4, 9.9, 9.9, 9.8, 9.7, 10.9, 10.3…\n$ Poverty_rate              <pseries> 9.6, 9.0, 10.6, 12.6, 9.6, 8.7, 11.4, 12…\n$ Viol_crime_count          <pseries> 1919, 2537, 2732, 2940, 3108, 3031, 3046…\n$ Population                <pseries> 404680, 418519, 449608, 488423, 513697, …\n$ police_per_100k_lag       <pseries> 194.7218, 200.2299, 191.0553, 364.2335, …\n$ RTC_LAW_YEAR              <pseries> 1995, 1995, 1995, 1995, 1995, 1995, 1995…\n$ RTC_LAW                   <pseries> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ TIME_0                    <pseries> 1980, 1980, 1980, 1980, 1980, 1980, 1980…\n$ TIME_INF                  <pseries> 2010, 2010, 2010, 2010, 2010, 2010, 2010…\n$ Viol_crime_rate_1k        <pseries> 4.742018, 6.061851, 6.076404, 6.019373, …\n$ Viol_crime_rate_1k_log    <pseries> 1.556463, 1.802015, 1.804413, 1.794983, …\n$ Population_log            <pseries> 12.91085, 12.94448, 13.01613, 13.09894, …"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#function-bootstrapping-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#function-bootstrapping-donohue",
    "title": "13-cs01-analysis",
    "section": "Function: bootstrapping (Donohue)",
    "text": "Function: bootstrapping (Donohue)\n\nfit_nls_on_bootstrap_DONOHUE <- function(subset) {\n  plm(Viol_crime_rate_1k_log ~ RTC_LAW +\n        White_Male_15_to_19_years +\n        White_Male_20_to_39_years +\n        Black_Male_15_to_19_years +\n        Black_Male_20_to_39_years +\n        Other_Male_15_to_19_years +\n        Other_Male_20_to_39_years +\n        Unemployment_rate +\n        Poverty_rate +\n        Population_log +\n        police_per_100k_lag,\n      data = data.frame(subset),\n      index = c(\"STATE\", \"YEAR\"),\n      model = \"within\",\n      effect = \"twoways\")\n}"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#use-the-function-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#use-the-function-donohue",
    "title": "13-cs01-analysis",
    "section": "Use the Function (Donohue)",
    "text": "Use the Function (Donohue)\n\nsubsets_models_DONOHUE <- map(DONOHUE_subsets, fit_nls_on_bootstrap_DONOHUE)\n\nsubsets_models_DONOHUE <- subsets_models_DONOHUE |>\n  map(tidy)\n\nsubsets_models_DONOHUE[[1]]\n\n# A tibble: 11 × 5\n   term                       estimate std.error statistic  p.value\n   <chr>                         <dbl>     <dbl>     <dbl>    <dbl>\n 1 RTC_LAWTRUE                0.0237    0.0170       1.40  1.62e- 1\n 2 White_Male_15_to_19_years  0.0108    0.0282       0.382 7.02e- 1\n 3 White_Male_20_to_39_years  0.0294    0.0100       2.93  3.42e- 3\n 4 Black_Male_15_to_19_years -0.0596    0.0578      -1.03  3.02e- 1\n 5 Black_Male_20_to_39_years  0.123     0.0195       6.34  3.12e-10\n 6 Other_Male_15_to_19_years  0.676     0.114        5.94  3.68e- 9\n 7 Other_Male_20_to_39_years -0.304     0.0383      -7.94  4.43e-15\n 8 Unemployment_rate         -0.0171    0.00498     -3.43  6.14e- 4\n 9 Poverty_rate              -0.00759   0.00299     -2.53  1.14e- 2\n10 Population_log            -0.211     0.0617      -3.42  6.43e- 4\n11 police_per_100k_lag        0.000556  0.000140     3.98  7.32e- 5\n\n\nNote: This code takes a while to run. Suggestion to cache this code chunk!"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#save-bootstrap-output-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#save-bootstrap-output-donohue",
    "title": "13-cs01-analysis",
    "section": "Save bootstrap output (Donohue)",
    "text": "Save bootstrap output (Donohue)\n\nsave(subsets_models_DONOHUE,\n  file = \"data/wrangled/DONOHUE_simulations.rda\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#loocv-splits-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#loocv-splits-lott",
    "title": "13-cs01-analysis",
    "section": "LOOCV: splits (Lott)",
    "text": "LOOCV: splits (Lott)\n\nset.seed(124)\nLOTT_splits <- d_panel_LOTT |> loo_cv()\nLOTT_subsets <- map(pull(LOTT_splits, splits), training)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#function-bootstrapping-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#function-bootstrapping-lott",
    "title": "13-cs01-analysis",
    "section": "Function: bootstrapping (Lott)",
    "text": "Function: bootstrapping (Lott)\n\nfit_nls_on_bootstrap_LOTT <- function(split) {\n  plm(LOTT_fmla,\n      data = data.frame(split),\n      index = c(\"STATE\", \"YEAR\"),\n      model = \"within\",\n      effect = \"twoways\"\n  )\n}"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#use-the-function-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#use-the-function-lott",
    "title": "13-cs01-analysis",
    "section": "Use the Function (Lott)",
    "text": "Use the Function (Lott)\n\nsubsets_models_LOTT <- map(LOTT_subsets, fit_nls_on_bootstrap_LOTT)\nsubsets_models_LOTT <- subsets_models_LOTT |>\n  map(tidy)\n\nNote: This code takes a while to run. Suggestion to cache this code chunk!"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#save-bootstrap-output-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#save-bootstrap-output-lott",
    "title": "13-cs01-analysis",
    "section": "Save bootstrap output (Lott)",
    "text": "Save bootstrap output (Lott)\n\nsave(subsets_models_LOTT,\n  file = \"data/wrangled/LOTT_simulations.rda\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#visualize-the-results",
    "href": "content/lectures/13-cs01-analysis-slides.html#visualize-the-results",
    "title": "13-cs01-analysis",
    "section": "Visualize the results",
    "text": "Visualize the results\n\nnames(subsets_models_DONOHUE) <- paste0(\"DONOHUE_\", seq_len(length(subsets_models_DONOHUE)))\n\nnames(subsets_models_LOTT) <-\n  paste0(\"LOTT_\", 1:length(subsets_models_LOTT))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#combine-simulation-data",
    "href": "content/lectures/13-cs01-analysis-slides.html#combine-simulation-data",
    "title": "13-cs01-analysis",
    "section": "Combine simulation data",
    "text": "Combine simulation data\n\nCodeData\n\n\n\nsimulations_DONOHUE <- subsets_models_DONOHUE |>\n  bind_rows(.id = \"ID\") |>\n  mutate(Analysis = \"Donohue\")\n\nsimulations_LOTT <- subsets_models_LOTT |>\n  bind_rows(.id = \"ID\") |>\n  mutate(Analysis = \"Lott\")\n\nsimulations <- bind_rows(simulations_DONOHUE, simulations_LOTT)\n\n# order for easier comparison\nsimulations <- simulations |>\n  mutate(term = factor(term,\n    levels = c(\n      str_subset(unique(pull(simulations, term)), \"years\", negate = TRUE),\n      sort(str_subset(unique(pull(simulations, term)), \"years\")))))\n\n\n\n\nhead(simulations)\n\n# A tibble: 6 × 7\n  ID        term                      estimate std.er…¹ stati…²  p.value Analy…³\n  <chr>     <fct>                        <dbl>    <dbl>   <dbl>    <dbl> <chr>  \n1 DONOHUE_1 RTC_LAWTRUE                 0.0237   0.0170   1.40  1.62e- 1 Donohue\n2 DONOHUE_1 White_Male_15_to_19_years   0.0108   0.0282   0.382 7.02e- 1 Donohue\n3 DONOHUE_1 White_Male_20_to_39_years   0.0294   0.0100   2.93  3.42e- 3 Donohue\n4 DONOHUE_1 Black_Male_15_to_19_years  -0.0596   0.0578  -1.03  3.02e- 1 Donohue\n5 DONOHUE_1 Black_Male_20_to_39_years   0.123    0.0195   6.34  3.12e-10 Donohue\n6 DONOHUE_1 Other_Male_15_to_19_years   0.676    0.114    5.94  3.68e- 9 Donohue\n# … with abbreviated variable names ¹​std.error, ²​statistic, ³​Analysis"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#simulation-plot-coefficients",
    "href": "content/lectures/13-cs01-analysis-slides.html#simulation-plot-coefficients",
    "title": "13-cs01-analysis",
    "section": "Simulation: Plot (coefficients)",
    "text": "Simulation: Plot (coefficients)\n\nCodePlot\n\n\n\nsimulations |>\n  ggplot(aes(x = term, y = estimate)) +\n  geom_boxplot() +\n  facet_grid(. ~ Analysis, scale = \"free_x\", space = \"free\", drop = TRUE) +\n  labs(title = \"Coefficient estimates\",\n       subtitle = \"Estimates across leave-one-out analyses\",\n       x = \"Term\",\n       y = \"Coefficient\",\n       caption = \"Results from simulations\") +\n  theme_linedraw() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_text(angle = 70, hjust = 1),\n        strip.text.x = element_text(size = 14, face = \"bold\"),\n        plot.title.position=\"plot\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#simulation-plot-sd",
    "href": "content/lectures/13-cs01-analysis-slides.html#simulation-plot-sd",
    "title": "13-cs01-analysis",
    "section": "Simulation: Plot (sd)",
    "text": "Simulation: Plot (sd)\n\nCodePlot\n\n\n\ncoeff_sd <- simulations |>\n  group_by(Analysis, term) |>\n  summarize(\"SD\" = sd(estimate))\n\ncoeff_sd |>\n  ggplot(aes(x = Analysis, y = SD)) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  labs(title = \"Coefficient variability\",\n       subtitle = \"SDs of coefficient estimates from leave-one-out analysis\",\n       x = \"Term\",\n       y = \"Coefficient Estimate \\n Standard Deviations\",\n       caption = \"Results from simulations\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_text(size = 8, color = \"black\"),\n        axis.text.y = element_text(color = \"black\"),\n        plot.title.position=\"plot\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#vif-evaluating-presence-and-severity-of-multicollinearity",
    "href": "content/lectures/13-cs01-analysis-slides.html#vif-evaluating-presence-and-severity-of-multicollinearity",
    "title": "13-cs01-analysis",
    "section": "VIF: Evaluating presence and severity of multicollinearity",
    "text": "VIF: Evaluating presence and severity of multicollinearity\nVariance Inflation Factor (VIF): index that measures how much the variance (the square of the estimate’s standard deviation) of an estimated regression coefficient is increased because of collinearity."
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculating-vif",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculating-vif",
    "title": "13-cs01-analysis",
    "section": "Calculating VIF:",
    "text": "Calculating VIF:\nIf our model is : \\(Y = β_0 + β_1X_1 + β_2X_2 + β_3X_3 + e\\), then:\nTo calculate the VIF value for \\(X_1\\), perform another OLS model, where \\(X_1\\) is now the dependent variable explained by the other explanatory variables:\n\\[X_1 = β_0 +  β_2X_2 + β_3X_3 + e\\]\n\nFrom this model : \\[VIF = \\frac{1}{1-R^{2}}\\]\n…where \\(R^2\\) value is the proportion of variance in \\(X_1\\) explained by the other variables ( \\(X_2\\) and \\(X_3\\))\n\n\nRepeat for each explanatory variable in the model.\n. . . - the larger the VIF, the more multicollinearity - VIF of >=10 typically indicates problematic multicollinearity"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculating-vif-donohue",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculating-vif-donohue",
    "title": "13-cs01-analysis",
    "section": "Calculating VIF (Donohue)",
    "text": "Calculating VIF (Donohue)\n\nCodeTable\n\n\n\n# create model matrix\nlm_DONOHUE_data <- as.data.frame(model.matrix(DONOHUE_OUTPUT))\n\n# define model\nlm_DONOHUE_data <- lm_DONOHUE_data |> \n  mutate(Viol_crime_rate_1k_log = plm::Within(pull(\n    d_panel_DONOHUE, Viol_crime_rate_1k_log\n  )), effect = \"twoways\")\n\n# specify model\nlm_DONOHUE <- lm(Viol_crime_rate_1k_log ~\nRTC_LAWTRUE +\n  White_Male_15_to_19_years +\n  White_Male_20_to_39_years +\n  Black_Male_15_to_19_years +\n  Black_Male_20_to_39_years +\n  Other_Male_15_to_19_years +\n  Other_Male_20_to_39_years +\n  Unemployment_rate +\n  Poverty_rate +\n  Population_log +\n  police_per_100k_lag,\ndata = lm_DONOHUE_data\n)\n\n# calculate VIF\nvif_DONOHUE <- vif(lm_DONOHUE)\n\n\n\n\n# combine into nice table\nvif_DONOHUE <- vif_DONOHUE |>\n  as_tibble() |>\n  cbind(names(vif_DONOHUE)) |>\n  as_tibble()\ncolnames(vif_DONOHUE) <- c(\"VIF\", \"Variable\")\n\nvif_DONOHUE |>\n  arrange(desc(VIF))\n\n# A tibble: 11 × 2\n     VIF Variable                 \n   <dbl> <chr>                    \n 1  1.72 White_Male_20_to_39_years\n 2  1.66 Black_Male_20_to_39_years\n 3  1.58 Other_Male_15_to_19_years\n 4  1.52 Other_Male_20_to_39_years\n 5  1.34 Black_Male_15_to_19_years\n 6  1.27 Poverty_rate             \n 7  1.23 Unemployment_rate        \n 8  1.21 police_per_100k_lag      \n 9  1.17 Population_log           \n10  1.15 White_Male_15_to_19_years\n11  1.11 RTC_LAWTRUE"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#calculating-vif-lott",
    "href": "content/lectures/13-cs01-analysis-slides.html#calculating-vif-lott",
    "title": "13-cs01-analysis",
    "section": "Calculating VIF (Lott)",
    "text": "Calculating VIF (Lott)\n\nCodeTable\n\n\n\nlm_LOTT_data <- as.data.frame(model.matrix(LOTT_OUTPUT))\nlm_LOTT_data <- lm_LOTT_data |>\n  mutate(Viol_crime_rate_1k_log = plm::Within(pull(d_panel_LOTT, Viol_crime_rate_1k_log), effect = \"twoways\")) |>\n  rename(RTC_LAW = RTC_LAWTRUE)\nlm_LOTT <- lm(LOTT_fmla, data = lm_LOTT_data)\n\nvif_LOTT <- vif(lm_LOTT)\nvif_LOTT <- vif_LOTT |>\n  as_tibble() |>\n  cbind(names(vif_LOTT)) |>\n  as_tibble()\ncolnames(vif_LOTT) <- c(\"VIF\", \"Variable\")\n\n\n\n\n# clean up names\nvif_LOTT |> \n  mutate(Variable = str_replace(string = Variable,\n                                pattern = \"RTC_LAW\",\n                                replacement = \"RTC_LAWTRUE\")) |>\n  arrange(desc(VIF))\n\n# A tibble: 41 × 2\n     VIF Variable                   \n   <dbl> <chr>                      \n 1  342. Black_Female_10_to_19_years\n 2  327. Black_Male_10_to_19_years  \n 3  251. Other_Male_40_to_49_years  \n 4  227. Other_Female_40_to_49_years\n 5  177. Other_Male_50_to_64_years  \n 6  157. Other_Male_10_to_19_years  \n 7  148. Other_Female_10_to_19_years\n 8  134. Other_Female_50_to_64_years\n 9  121. White_Female_10_to_19_years\n10  120. White_Male_10_to_19_years  \n# … with 31 more rows"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#vif-across-analyses",
    "href": "content/lectures/13-cs01-analysis-slides.html#vif-across-analyses",
    "title": "13-cs01-analysis",
    "section": "VIF Across Analyses",
    "text": "VIF Across Analyses\n\nCodeTable\n\n\n\nvif_DONOHUE <- vif_DONOHUE |>\n  mutate(Analysis = \"Donohue\")\nvif_LOTT <- vif_LOTT |>\n  mutate(Analysis = \"Lott\")\nvif_df <- bind_rows(vif_DONOHUE, vif_LOTT)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#vif-plot",
    "href": "content/lectures/13-cs01-analysis-slides.html#vif-plot",
    "title": "13-cs01-analysis",
    "section": "VIF Plot",
    "text": "VIF Plot\n\nCodePlot\n\n\n\nvif_df |>\n  ggplot(aes(x = Analysis, y = VIF)) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = 10, color = \"red\") +\n  geom_text(aes(.75, 13, label = \"typical cutoff of 10\")) +\n  coord_trans(y = \"log10\") +\n  labs(title = \"Variance inflation factors\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_text(color = \"black\"),\n        axis.text.y = element_text(color = \"black\"))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#what-to-do",
    "href": "content/lectures/13-cs01-analysis-slides.html#what-to-do",
    "title": "13-cs01-analysis",
    "section": "What to do?",
    "text": "What to do?\n\ndifferent model?\ndifferent predictors in this model?\ndifferent parameterization of predictors?\narticle for a detailed discussion about what to consider when your model has variables with high VIF values"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#where-to-go-from-here",
    "href": "content/lectures/13-cs01-analysis-slides.html#where-to-go-from-here",
    "title": "13-cs01-analysis",
    "section": "Where to Go From Here",
    "text": "Where to Go From Here\n\ncomplete the lab!\nstart to work with your group through this analysis\nconsider what model you’ll want to use to answer the questions\nthink about/consider extension for CS01"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis-slides.html#suggested-reading",
    "href": "content/lectures/13-cs01-analysis-slides.html#suggested-reading",
    "title": "13-cs01-analysis",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nCase Study from OCS\nWrangling for this case study\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html",
    "href": "content/lectures/13-cs01-analysis.html",
    "title": "13-cs01-analysis",
    "section": "",
    "text": "Q: I’m curious about how different groups will approach this dataset differently, and hope that we can have some kind of showcase after this case study is completed!\nA: I am too - sometimes students add another (related) question. Some find a new related dataset. Some go really deep on visualization. I like the idea of a showcase! Was not in my plan, but I think it should be!\n\n\nQ: I’m confused about…how to choose the best EDA.\nA: The best EDA is the EDA that best helps you understand the datasets being used for analysis. So, while there’s not “one plot to rule them all,” a great EDA uses visualizations and data summaries that let the analyst really understand the data. Two EDAs can make different decisions and both be “best”.\n\n\n\n\nDue Dates:\n\nLecture Participation survey “due” after class\nLab06 due tomorrow (2/24; 11:59 PM)\nHW03 due Mon (2/27; 11:59 PM)\n\n\nNotes:\n\nFinal Project Groups survey (link also on canvas; “due” Friday)\nAccept the GH repo for cs01; talk with group mates\nMidterm grades posted (Canvas) and feedback available (GitHub Issue)\n\nAnswer key on website\nRegrades open until Sunday at 5PM (GitHub issue or Campuswire post)\n\nCS01 Data Wrangling: Campuswire Post\n\n\n\n\n\n\npanel data & analysis\nmodelling the LOTT and DONOGHUE data\nMulticollinearity\nVIF"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#packages-data",
    "href": "content/lectures/13-cs01-analysis.html#packages-data",
    "title": "13-cs01-analysis",
    "section": "Packages & Data",
    "text": "Packages & Data\n\n# i've asked ITS to install those on datahub for you\n# if that's not yet complete, you can install using install.packages()\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(plm) \nlibrary(car) \nlibrary(rsample) \nlibrary(GGally) \nlibrary(ggcorrplot) \n\n\nThis will only work if you finished the wrangling…\n\nload(\"data/wrangled/wrangled_data_rtc.rda\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#panel-data-1",
    "href": "content/lectures/13-cs01-analysis.html#panel-data-1",
    "title": "13-cs01-analysis",
    "section": "Panel Data",
    "text": "Panel Data\n\nrepeated measures for multiple panel members or individuals over time.\n\nmultiple units (violent crime and other variables for each state)\nmultiple time points (multiple years)\n\n\n\nLingo:\n\n\\(N\\) individual panel members\n\\(T\\) time points\n\n\n\n\nBalanced Panels: At each time point ( \\(T\\) ), there are data points for each individual( \\(N\\) ). ( \\(n = N∗T\\) )\nUnbalanced Panels: May be data points missing for some individuals ( \\(N\\) ) at some time points ( \\(T\\) ) ( \\(n\\) observations \\(\\lt N∗T\\))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#our-panel-data",
    "href": "content/lectures/13-cs01-analysis.html#our-panel-data",
    "title": "13-cs01-analysis",
    "section": "Our Panel Data",
    "text": "Our Panel Data\nIn our case we have:\n\n\\(N\\) = 44 states (in the data wrangling process we removed those who had adopted an RTC law before 1980)\n\\(T\\) = 31 years (1980 - 2010)\n\nPanel is balanced: \\(n=44∗31\\), thus \\(n = 1364\\)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#panel-linear-regression",
    "href": "content/lectures/13-cs01-analysis.html#panel-linear-regression",
    "title": "13-cs01-analysis",
    "section": "Panel Linear Regression",
    "text": "Panel Linear Regression\n\\[Y_{it}=β_{0}+β_{1}X_{1it}+...+β_{K}X_{Kit}+e_{it}\\]\n\n\\(i\\) is the individual dimension (in our case individual states)\n\\(t\\) is the time dimension.\n\n\nNotes:\n\nSome explanatory variables \\(X_{it}\\) will vary across individuals and time\nothers will be fixed across the time of the study (or don’t change over time)\nothers still will be fixed across individuals but vary across time periods\n\n\n\n❓ Which are examples of variables in our analysis that likely fall into each of these categories?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#fixed-effects-panel-regression-analysis",
    "href": "content/lectures/13-cs01-analysis.html#fixed-effects-panel-regression-analysis",
    "title": "13-cs01-analysis",
    "section": "Fixed Effects Panel Regression Analysis",
    "text": "Fixed Effects Panel Regression Analysis\n\nassumes that there are unknown or unobserved unique aspects about the individuals or heterogeneity among individuals (states) \\(a_i\\) that are not explained by the independent variables but influence the outcome variable of interest (crime).\nthey do not vary with time or in other words are fixed over time but may be correlated with independent variables \\(X_{it}\\)\n\n\n\nThe intercept can be different for each individual \\(\\beta_{0i}\\) (each state)\nOther coefficients are assumed to be the same across all the individuals.\n\n\n\nIn our data…some of the unobserved qualities about the different states may be correlated with some of our independent variables (i.e.level of economic opportunity might be an unobserved feature about the states that influences violent crime rate (outcome) and would be possibly correlated with poverty rate and unemployment (predictors))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#fixed-effects-panel-regression-analysis-model",
    "href": "content/lectures/13-cs01-analysis.html#fixed-effects-panel-regression-analysis-model",
    "title": "13-cs01-analysis",
    "section": "Fixed Effects Panel Regression Analysis (Model)",
    "text": "Fixed Effects Panel Regression Analysis (Model)\nThese individual \\(a_i\\) effects can be correlated with the independent variables \\(X\\):\n\\[Y_{it}=\\beta_{0}+\\beta_{1}X_{1it}+...\\beta_{K}X_{Kit}+ a_i +e_{it}\\] …or alternatively the individual effects can be absorbed into an individual-specific intercept term \\(\\beta_{0i}\\):\n\\[Y_{it}=\\beta_{0i}+\\beta_{1}X_{1it}+...\\beta_{k}X_{kit} +e_{it}\\]"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#panel-linear-model-plm",
    "href": "content/lectures/13-cs01-analysis.html#panel-linear-model-plm",
    "title": "13-cs01-analysis",
    "section": "Panel Linear Model (plm)",
    "text": "Panel Linear Model (plm)\n\ncarry out panel linear models\nNew object: pdata.frame (panel data frame); can indicate:\n\nwhich variable should be used to identify the individuals in our panel (STATE)\nwhat variable should be used to identify the time periods in our panel (YEAR)\n\nWill be specified in index parameter: index = c(\"Individual_Variable_NAME\", \"Time_Period_Variable_NAME\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#pdata.frame",
    "href": "content/lectures/13-cs01-analysis.html#pdata.frame",
    "title": "13-cs01-analysis",
    "section": "pdata.frame",
    "text": "pdata.frame\n\nd_panel_DONOHUE <- pdata.frame(DONOHUE_DF, index = c(\"STATE\", \"YEAR\"))\nclass(d_panel_DONOHUE)\n\n[1] \"pdata.frame\" \"data.frame\" \n\nhead(d_panel_DONOHUE, n = 3)\n\n            YEAR  STATE Black_Male_15_to_19_years Black_Male_20_to_39_years\nAlaska-1980 1980 Alaska                 0.1670456                 0.9933775\nAlaska-1981 1981 Alaska                 0.1732299                 1.0028219\nAlaska-1982 1982 Alaska                 0.1737069                 1.0204445\n            Other_Male_15_to_19_years Other_Male_20_to_39_years\nAlaska-1980                  1.129782                  2.963329\nAlaska-1981                  1.124441                  2.974775\nAlaska-1982                  1.069821                  3.015071\n            White_Male_15_to_19_years White_Male_20_to_39_years\nAlaska-1980                  3.627805                  18.28852\nAlaska-1981                  3.558261                  18.12821\nAlaska-1982                  3.391844                  18.10666\n            Unemployment_rate Poverty_rate Viol_crime_count Population\nAlaska-1980               9.6          9.6             1919     404680\nAlaska-1981               9.4          9.0             2537     418519\nAlaska-1982               9.9         10.6             2732     449608\n            police_per_100k_lag RTC_LAW_YEAR RTC_LAW TIME_0 TIME_INF\nAlaska-1980            194.7218         1995   FALSE   1980     2010\nAlaska-1981            200.2299         1995   FALSE   1980     2010\nAlaska-1982            191.0553         1995   FALSE   1980     2010\n            Viol_crime_rate_1k Viol_crime_rate_1k_log Population_log\nAlaska-1980           4.742018               1.556463       12.91085\nAlaska-1981           6.061851               1.802015       12.94448\nAlaska-1982           6.076404               1.804413       13.01613"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#model-considerations-effect",
    "href": "content/lectures/13-cs01-analysis.html#model-considerations-effect",
    "title": "13-cs01-analysis",
    "section": "Model Considerations: effect",
    "text": "Model Considerations: effect\nThere are three main options for the effect argument:\n\n\nindividual - model for the effect of individual identity\ntime - model for the effect of time\ntwoways - meaning modeling for the effect of both individual identity and time\n\n\n\n\n\nspeculate that there is an effect of individual STATE identity and time on violent crime rate (effect = \"twoways\")\n\naka expect some states to have high rates of crime, and others to have low rates of crime\nexpect crime rates to change over time"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#model-considerations-model",
    "href": "content/lectures/13-cs01-analysis.html#model-considerations-model",
    "title": "13-cs01-analysis",
    "section": "Model Considerations: model",
    "text": "Model Considerations: model\n\n\npooling - standard pooled ordinary least squares regression model\n\nwithin - fixed effects model (variation between individuals is ignored, model compares individuals to themselves at different periods of time)\n\nbetween - fixed effects model (variation within individuals from one time point to another is ignored, model compares different individuals at each point of time)\n\nrandom - random effects (each state has a different intercept but force it to follow a normal distribution - requires more assumptions)\n\n\n\ninterested in how violence in each state varied over time: within STATE variation (model = within)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#model-plm-donohue",
    "href": "content/lectures/13-cs01-analysis.html#model-plm-donohue",
    "title": "13-cs01-analysis",
    "section": "Model: plm (DONOHUE)",
    "text": "Model: plm (DONOHUE)\n\nDONOHUE_OUTPUT <- plm(Viol_crime_rate_1k_log ~\n                      RTC_LAW +\n                      White_Male_15_to_19_years +\n                      White_Male_20_to_39_years +\n                      Black_Male_15_to_19_years +\n                      Black_Male_20_to_39_years +\n                      Other_Male_15_to_19_years +\n                      Other_Male_20_to_39_years +\n                      Unemployment_rate +\n                      Poverty_rate +\n                      Population_log +\n                      police_per_100k_lag,\n                      effect = \"twoways\",\n                      model = \"within\",\n                      data = d_panel_DONOHUE)\n\n\n❓ What is our outcome variable here? What is our primary predictor of interest? Why are all the other variables included?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#model-output-donohue",
    "href": "content/lectures/13-cs01-analysis.html#model-output-donohue",
    "title": "13-cs01-analysis",
    "section": "Model Output: DONOHUE",
    "text": "Model Output: DONOHUE\n\nDONOHUE_OUTPUT_TIDY <- tidy(DONOHUE_OUTPUT, conf.int = 0.95)\nDONOHUE_OUTPUT_TIDY\n\n# A tibble: 11 × 7\n   term                      estimate std.e…¹ stati…²  p.value conf.low conf.h…³\n   <chr>                        <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>\n 1 RTC_LAWTRUE                2.40e-2 1.70e-2   1.42  1.56e- 1 -9.19e-3  5.73e-2\n 2 White_Male_15_to_19_years  1.04e-2 2.82e-2   0.367 7.13e- 1 -4.49e-2  6.56e-2\n 3 White_Male_20_to_39_years  2.93e-2 1.00e-2   2.93  3.50e- 3  9.68e-3  4.90e-2\n 4 Black_Male_15_to_19_years -5.97e-2 5.78e-2  -1.03  3.02e- 1 -1.73e-1  5.36e-2\n 5 Black_Male_20_to_39_years  1.23e-1 1.95e-2   6.34  3.17e-10  8.53e-2  1.62e-1\n 6 Other_Male_15_to_19_years  6.74e-1 1.14e-1   5.92  4.15e- 9  4.51e-1  8.97e-1\n 7 Other_Male_20_to_39_years -3.04e-1 3.83e-2  -7.95  4.21e-15 -3.79e-1 -2.29e-1\n 8 Unemployment_rate         -1.71e-2 4.98e-3  -3.42  6.36e- 4 -2.68e-2 -7.29e-3\n 9 Poverty_rate              -7.60e-3 2.99e-3  -2.54  1.12e- 2 -1.35e-2 -1.74e-3\n10 Population_log            -2.11e-1 6.17e-2  -3.42  6.55e- 4 -3.32e-1 -8.99e-2\n11 police_per_100k_lag        5.59e-4 1.40e-4   4.00  6.72e- 5  2.85e-4  8.33e-4\n# … with abbreviated variable names ¹​std.error, ²​statistic, ³​conf.high\n\nDONOHUE_OUTPUT_TIDY$Analysis <- \"Donohue\""
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#formula-as.formula-lott",
    "href": "content/lectures/13-cs01-analysis.html#formula-as.formula-lott",
    "title": "13-cs01-analysis",
    "section": "Formula: as.formula() (LOTT)",
    "text": "Formula: as.formula() (LOTT)\n\nLOTT_variables <- LOTT_DF |>\n  select(RTC_LAW,\n         contains(c(\"White\", \"Black\", \"Other\")),\n         Unemployment_rate,\n         Poverty_rate,\n         Population_log,\n         police_per_100k_lag) |>\n  colnames()\n\nLOTT_fmla <- as.formula(paste(\"Viol_crime_rate_1k_log ~\", paste(LOTT_variables, collapse = \" + \")))\nLOTT_fmla\n\nViol_crime_rate_1k_log ~ RTC_LAW + White_Female_10_to_19_years + \n    White_Female_20_to_29_years + White_Female_30_to_39_years + \n    White_Female_40_to_49_years + White_Female_50_to_64_years + \n    White_Female_65_years_and_over + White_Male_10_to_19_years + \n    White_Male_20_to_29_years + White_Male_30_to_39_years + White_Male_40_to_49_years + \n    White_Male_50_to_64_years + White_Male_65_years_and_over + \n    Black_Female_10_to_19_years + Black_Female_20_to_29_years + \n    Black_Female_30_to_39_years + Black_Female_40_to_49_years + \n    Black_Female_50_to_64_years + Black_Female_65_years_and_over + \n    Black_Male_10_to_19_years + Black_Male_20_to_29_years + Black_Male_30_to_39_years + \n    Black_Male_40_to_49_years + Black_Male_50_to_64_years + Black_Male_65_years_and_over + \n    Other_Female_10_to_19_years + Other_Female_20_to_29_years + \n    Other_Female_30_to_39_years + Other_Female_40_to_49_years + \n    Other_Female_50_to_64_years + Other_Female_65_years_and_over + \n    Other_Male_10_to_19_years + Other_Male_20_to_29_years + Other_Male_30_to_39_years + \n    Other_Male_40_to_49_years + Other_Male_50_to_64_years + Other_Male_65_years_and_over + \n    Unemployment_rate + Poverty_rate + Population_log + police_per_100k_lag"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#model-plm-lott",
    "href": "content/lectures/13-cs01-analysis.html#model-plm-lott",
    "title": "13-cs01-analysis",
    "section": "Model: plm (LOTT)",
    "text": "Model: plm (LOTT)\n\nd_panel_LOTT <- pdata.frame(LOTT_DF, index = c(\"STATE\", \"YEAR\"))\n\nLOTT_OUTPUT <- plm(LOTT_fmla,\n                   model = \"within\",\n                   effect = \"twoways\",\n                   data = d_panel_LOTT)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#model-output-lott",
    "href": "content/lectures/13-cs01-analysis.html#model-output-lott",
    "title": "13-cs01-analysis",
    "section": "Model Output: LOTT",
    "text": "Model Output: LOTT\n\nLOTT_OUTPUT_TIDY <- tidy(LOTT_OUTPUT, conf.int = 0.95)\nLOTT_OUTPUT_TIDY\n\n# A tibble: 41 × 7\n   term                        estimate std.e…¹ stati…²  p.value conf.…³ conf.…⁴\n   <chr>                          <dbl>   <dbl>   <dbl>    <dbl>   <dbl>   <dbl>\n 1 RTC_LAWTRUE                 -0.0518   0.0162  -3.20  1.39e- 3 -0.0835 -0.0201\n 2 White_Female_10_to_19_years  0.636    0.149    4.26  2.24e- 5  0.343   0.929 \n 3 White_Female_20_to_29_years  0.00698  0.0670   0.104 9.17e- 1 -0.124   0.138 \n 4 White_Female_30_to_39_years  0.261    0.0813   3.21  1.38e- 3  0.101   0.420 \n 5 White_Female_40_to_49_years  0.0168   0.0814   0.206 8.37e- 1 -0.143   0.176 \n 6 White_Female_50_to_64_years -0.459    0.0625  -7.35  3.60e-13 -0.582  -0.337 \n 7 White_Female_65_years_and_…  0.156    0.0469   3.33  9.04e- 4  0.0641  0.248 \n 8 White_Male_10_to_19_years   -0.583    0.143   -4.07  4.92e- 5 -0.863  -0.302 \n 9 White_Male_20_to_29_years    0.0639   0.0623   1.03  3.05e- 1 -0.0582  0.186 \n10 White_Male_30_to_39_years   -0.200    0.0859  -2.33  2.01e- 2 -0.368  -0.0315\n# … with 31 more rows, and abbreviated variable names ¹​std.error, ²​statistic,\n#   ³​conf.low, ⁴​conf.high\n\nLOTT_OUTPUT_TIDY$Analysis <- \"Lott\""
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#rtc-coefficient-comparison",
    "href": "content/lectures/13-cs01-analysis.html#rtc-coefficient-comparison",
    "title": "13-cs01-analysis",
    "section": "RTC coefficient comparison",
    "text": "RTC coefficient comparison\n\ncomparing_analyses <- DONOHUE_OUTPUT_TIDY |>\n  bind_rows(LOTT_OUTPUT_TIDY) |>\n  filter(term == \"RTC_LAWTRUE\")\n\ncomparing_analyses\n\n# A tibble: 2 × 8\n  term        estimate std.error statistic p.value conf.low conf.high Analysis\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl> <chr>   \n1 RTC_LAWTRUE   0.0240    0.0170      1.42 0.156   -0.00919    0.0573 Donohue \n2 RTC_LAWTRUE  -0.0518    0.0162     -3.20 0.00139 -0.0835    -0.0201 Lott    \n\n\n\n🧠 With each analysis, what would your conclusion be to the question “What is the relationship between right to carry laws and violence rates in the US”?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#rtc",
    "href": "content/lectures/13-cs01-analysis.html#rtc",
    "title": "13-cs01-analysis",
    "section": "RTC",
    "text": "RTC\n\nCodePlot\n\n\n\nggplot(comparing_analyses) +\n  geom_point(aes(x = Analysis, y = estimate)) +\n  geom_errorbar(aes(x = Analysis, ymin = conf.low, ymax = conf.high), width = 0.25) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  scale_y_continuous(\n    breaks = seq(-0.2, 0.2, by = 0.05),\n    labels = seq(-0.2, 0.2, by = 0.05),\n    limits = c(-0.2, 0.2)\n  ) +\n  geom_segment(aes(x = 1, y = 0.125, xend = 1, yend = 0.175),\n    arrow = arrow(angle = 45, ends = \"last\", type = \"open\"),\n    size = 2, color = \"green\", lineend = \"butt\", linejoin = \"mitre\"\n  ) +\n  geom_segment(aes(x = 2, y = -0.125, xend = 2, yend = -0.175),\n    arrow = arrow(angle = 45, ends = \"last\", type = \"open\"),\n    size = 2, color = \"red\", lineend = \"butt\", linejoin = \"mitre\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text = element_text(size = 8, color = \"black\")\n  ) +\n  labs(\n    title = \"Effect estimate on ln(violent crimes per 100,000 people)\",\n    y = \"  Effect estimate (95% CI)\"\n  )"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#similar-data---different-results",
    "href": "content/lectures/13-cs01-analysis.html#similar-data---different-results",
    "title": "13-cs01-analysis",
    "section": "Similar Data - different results",
    "text": "Similar Data - different results\n\nThe only difference between the two data frames rests in how the demographic variables were parameterized.\n\nDonohue: Males only; ages 15 to 39\nLott: Males/Females; ages 10-65+\n\n\n\n…so how did this occur?"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#multicollinearity-1",
    "href": "content/lectures/13-cs01-analysis.html#multicollinearity-1",
    "title": "13-cs01-analysis",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nmulticollinearity occurs when independent variables (predictors) are highly related to one another\n\n\n\nWhen seemingly independent variables are highly related to one another, the relationships estimated in an analysis may be distorted.\nsince linear regression aims to determine how a one unit change in a regressor influences a one unit change in the dependent variable, if the regressors are collinear…effect of each regressor cannot be accurately estimated"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#diagnosing-multicollinearity-pairplots",
    "href": "content/lectures/13-cs01-analysis.html#diagnosing-multicollinearity-pairplots",
    "title": "13-cs01-analysis",
    "section": "Diagnosing Multicollinearity: Pairplots",
    "text": "Diagnosing Multicollinearity: Pairplots\n\nCodePlot\n\n\n\nDONOHUE_DF |>\n  select(RTC_LAW,\n         Viol_crime_rate_1k_log,\n         Unemployment_rate,\n         Poverty_rate,\n         Population_log) |>\n  ggpairs(columns = c(2:5),\n          lower = list(continuous = wrap(\"smooth_loess\",\n                                         color = \"red\",\n                                         alpha = 0.5,\n                                         size = 0.1)))\n\n\n\n\n\n\n\n\n…not much correlation for non-demographic variables - unemployment and poverty rate show some (as expeted)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#diagnosing-multicollinearity-heatmaps",
    "href": "content/lectures/13-cs01-analysis.html#diagnosing-multicollinearity-heatmaps",
    "title": "13-cs01-analysis",
    "section": "Diagnosing Multicollinearity: Heatmaps",
    "text": "Diagnosing Multicollinearity: Heatmaps\n\nCodePlotNotes\n\n\n\ncor_DONOHUE_dem <- cor(DONOHUE_DF |> select(contains(\"_years\")))\n\nggcorrplot(cor_DONOHUE_dem,\n  tl.cex = 6,\n  hc.order = TRUE,\n  colors = c(\n    \"red\",\n    \"white\",\n    \"red\"\n  ),\n  outline.color = \"transparent\",\n  title = \"Correlation Matrix: Donohue\",\n  legend.title = expression(rho)\n)\n\n\n\n\n\n\n\n\n\n\n\nstrong correlation within race\nrace shows much stronger correlation than age\nsuggests collinearity"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#heatmap-lott",
    "href": "content/lectures/13-cs01-analysis.html#heatmap-lott",
    "title": "13-cs01-analysis",
    "section": "Heatmap: Lott",
    "text": "Heatmap: Lott\n\nCodePlot\n\n\n\ncor_LOTT_dem <- cor(LOTT_DF |> select(contains(\"_years\")))\n\ncorr_mat_LOTT <- ggcorrplot(cor_LOTT_dem,\n  tl.cex = 6,\n  hc.order = TRUE,\n  colors = c(\n    \"red\",\n    \"white\",\n    \"red\"\n  ),\n  outline.color = \"transparent\",\n  title = \"Correlation Matrix: Lott\",\n  legend.title = expression(rho)\n)\n\ncorr_mat_LOTT"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#suggested-multicollinearity",
    "href": "content/lectures/13-cs01-analysis.html#suggested-multicollinearity",
    "title": "13-cs01-analysis",
    "section": "Suggested multicollinearity",
    "text": "Suggested multicollinearity\n…so how do you find out for sure?\n\nlook at the stability of the coefficient estimates under perturbations of the data\nwe’ll focus on RTC_LAW\nuse resampling (here: remove one observation, see if estimates change == LOOCV - leave one out cross-validation): rsample::loo_cv"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#loocv-splits-donohue",
    "href": "content/lectures/13-cs01-analysis.html#loocv-splits-donohue",
    "title": "13-cs01-analysis",
    "section": "LOOCV: splits (Donohue)",
    "text": "LOOCV: splits (Donohue)\n\n## split data\nset.seed(124)\nDONOHUE_splits <- d_panel_DONOHUE |> loo_cv()\nDONOHUE_splits\n\n# Leave-one-out cross-validation \n# A tibble: 1,364 × 2\n   splits           id        \n   <list>           <chr>     \n 1 <split [1363/1]> Resample1 \n 2 <split [1363/1]> Resample2 \n 3 <split [1363/1]> Resample3 \n 4 <split [1363/1]> Resample4 \n 5 <split [1363/1]> Resample5 \n 6 <split [1363/1]> Resample6 \n 7 <split [1363/1]> Resample7 \n 8 <split [1363/1]> Resample8 \n 9 <split [1363/1]> Resample9 \n10 <split [1363/1]> Resample10\n# … with 1,354 more rows"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#loocv-get-data-donohue",
    "href": "content/lectures/13-cs01-analysis.html#loocv-get-data-donohue",
    "title": "13-cs01-analysis",
    "section": "LOOCV: get data (Donohue)",
    "text": "LOOCV: get data (Donohue)\n\nDONOHUE_subsets <- map(pull(DONOHUE_splits, splits), training)\nglimpse(DONOHUE_subsets[[1]])\n\nRows: 1,363\nColumns: 20\n$ YEAR                      <fct> 1980, 1981, 1982, 1983, 1984, 1985, 1986, 19…\n$ STATE                     <fct> Alaska, Alaska, Alaska, Alaska, Alaska, Alas…\n$ Black_Male_15_to_19_years <pseries> 0.1670456, 0.1732299, 0.1737069, 0.17095…\n$ Black_Male_20_to_39_years <pseries> 0.9933775, 1.0028219, 1.0204445, 1.03127…\n$ Other_Male_15_to_19_years <pseries> 1.1297816, 1.1244412, 1.0698208, 0.98828…\n$ Other_Male_20_to_39_years <pseries> 2.963329, 2.974775, 3.015071, 3.008048, …\n$ White_Male_15_to_19_years <pseries> 3.627805, 3.558261, 3.391844, 3.222002, …\n$ White_Male_20_to_39_years <pseries> 18.28852, 18.12821, 18.10666, 17.90600, …\n$ Unemployment_rate         <pseries> 9.6, 9.4, 9.9, 9.9, 9.8, 9.7, 10.9, 10.3…\n$ Poverty_rate              <pseries> 9.6, 9.0, 10.6, 12.6, 9.6, 8.7, 11.4, 12…\n$ Viol_crime_count          <pseries> 1919, 2537, 2732, 2940, 3108, 3031, 3046…\n$ Population                <pseries> 404680, 418519, 449608, 488423, 513697, …\n$ police_per_100k_lag       <pseries> 194.7218, 200.2299, 191.0553, 364.2335, …\n$ RTC_LAW_YEAR              <pseries> 1995, 1995, 1995, 1995, 1995, 1995, 1995…\n$ RTC_LAW                   <pseries> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ TIME_0                    <pseries> 1980, 1980, 1980, 1980, 1980, 1980, 1980…\n$ TIME_INF                  <pseries> 2010, 2010, 2010, 2010, 2010, 2010, 2010…\n$ Viol_crime_rate_1k        <pseries> 4.742018, 6.061851, 6.076404, 6.019373, …\n$ Viol_crime_rate_1k_log    <pseries> 1.556463, 1.802015, 1.804413, 1.794983, …\n$ Population_log            <pseries> 12.91085, 12.94448, 13.01613, 13.09894, …"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#function-bootstrapping-donohue",
    "href": "content/lectures/13-cs01-analysis.html#function-bootstrapping-donohue",
    "title": "13-cs01-analysis",
    "section": "Function: bootstrapping (Donohue)",
    "text": "Function: bootstrapping (Donohue)\n\nfit_nls_on_bootstrap_DONOHUE <- function(subset) {\n  plm(Viol_crime_rate_1k_log ~ RTC_LAW +\n        White_Male_15_to_19_years +\n        White_Male_20_to_39_years +\n        Black_Male_15_to_19_years +\n        Black_Male_20_to_39_years +\n        Other_Male_15_to_19_years +\n        Other_Male_20_to_39_years +\n        Unemployment_rate +\n        Poverty_rate +\n        Population_log +\n        police_per_100k_lag,\n      data = data.frame(subset),\n      index = c(\"STATE\", \"YEAR\"),\n      model = \"within\",\n      effect = \"twoways\")\n}"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#use-the-function-donohue",
    "href": "content/lectures/13-cs01-analysis.html#use-the-function-donohue",
    "title": "13-cs01-analysis",
    "section": "Use the Function (Donohue)",
    "text": "Use the Function (Donohue)\n\nsubsets_models_DONOHUE <- map(DONOHUE_subsets, fit_nls_on_bootstrap_DONOHUE)\n\nsubsets_models_DONOHUE <- subsets_models_DONOHUE |>\n  map(tidy)\n\nsubsets_models_DONOHUE[[1]]\n\n# A tibble: 11 × 5\n   term                       estimate std.error statistic  p.value\n   <chr>                         <dbl>     <dbl>     <dbl>    <dbl>\n 1 RTC_LAWTRUE                0.0237    0.0170       1.40  1.62e- 1\n 2 White_Male_15_to_19_years  0.0108    0.0282       0.382 7.02e- 1\n 3 White_Male_20_to_39_years  0.0294    0.0100       2.93  3.42e- 3\n 4 Black_Male_15_to_19_years -0.0596    0.0578      -1.03  3.02e- 1\n 5 Black_Male_20_to_39_years  0.123     0.0195       6.34  3.12e-10\n 6 Other_Male_15_to_19_years  0.676     0.114        5.94  3.68e- 9\n 7 Other_Male_20_to_39_years -0.304     0.0383      -7.94  4.43e-15\n 8 Unemployment_rate         -0.0171    0.00498     -3.43  6.14e- 4\n 9 Poverty_rate              -0.00759   0.00299     -2.53  1.14e- 2\n10 Population_log            -0.211     0.0617      -3.42  6.43e- 4\n11 police_per_100k_lag        0.000556  0.000140     3.98  7.32e- 5\n\n\nNote: This code takes a while to run. Suggestion to cache this code chunk!"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#save-bootstrap-output-donohue",
    "href": "content/lectures/13-cs01-analysis.html#save-bootstrap-output-donohue",
    "title": "13-cs01-analysis",
    "section": "Save bootstrap output (Donohue)",
    "text": "Save bootstrap output (Donohue)\n\nsave(subsets_models_DONOHUE,\n  file = \"data/wrangled/DONOHUE_simulations.rda\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#loocv-splits-lott",
    "href": "content/lectures/13-cs01-analysis.html#loocv-splits-lott",
    "title": "13-cs01-analysis",
    "section": "LOOCV: splits (Lott)",
    "text": "LOOCV: splits (Lott)\n\nset.seed(124)\nLOTT_splits <- d_panel_LOTT |> loo_cv()\nLOTT_subsets <- map(pull(LOTT_splits, splits), training)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#function-bootstrapping-lott",
    "href": "content/lectures/13-cs01-analysis.html#function-bootstrapping-lott",
    "title": "13-cs01-analysis",
    "section": "Function: bootstrapping (Lott)",
    "text": "Function: bootstrapping (Lott)\n\nfit_nls_on_bootstrap_LOTT <- function(split) {\n  plm(LOTT_fmla,\n      data = data.frame(split),\n      index = c(\"STATE\", \"YEAR\"),\n      model = \"within\",\n      effect = \"twoways\"\n  )\n}"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#use-the-function-lott",
    "href": "content/lectures/13-cs01-analysis.html#use-the-function-lott",
    "title": "13-cs01-analysis",
    "section": "Use the Function (Lott)",
    "text": "Use the Function (Lott)\n\nsubsets_models_LOTT <- map(LOTT_subsets, fit_nls_on_bootstrap_LOTT)\nsubsets_models_LOTT <- subsets_models_LOTT |>\n  map(tidy)\n\nNote: This code takes a while to run. Suggestion to cache this code chunk!"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#save-bootstrap-output-lott",
    "href": "content/lectures/13-cs01-analysis.html#save-bootstrap-output-lott",
    "title": "13-cs01-analysis",
    "section": "Save bootstrap output (Lott)",
    "text": "Save bootstrap output (Lott)\n\nsave(subsets_models_LOTT,\n  file = \"data/wrangled/LOTT_simulations.rda\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#visualize-the-results",
    "href": "content/lectures/13-cs01-analysis.html#visualize-the-results",
    "title": "13-cs01-analysis",
    "section": "Visualize the results",
    "text": "Visualize the results\n\nnames(subsets_models_DONOHUE) <- paste0(\"DONOHUE_\", seq_len(length(subsets_models_DONOHUE)))\n\nnames(subsets_models_LOTT) <-\n  paste0(\"LOTT_\", 1:length(subsets_models_LOTT))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#combine-simulation-data",
    "href": "content/lectures/13-cs01-analysis.html#combine-simulation-data",
    "title": "13-cs01-analysis",
    "section": "Combine simulation data",
    "text": "Combine simulation data\n\nCodeData\n\n\n\nsimulations_DONOHUE <- subsets_models_DONOHUE |>\n  bind_rows(.id = \"ID\") |>\n  mutate(Analysis = \"Donohue\")\n\nsimulations_LOTT <- subsets_models_LOTT |>\n  bind_rows(.id = \"ID\") |>\n  mutate(Analysis = \"Lott\")\n\nsimulations <- bind_rows(simulations_DONOHUE, simulations_LOTT)\n\n# order for easier comparison\nsimulations <- simulations |>\n  mutate(term = factor(term,\n    levels = c(\n      str_subset(unique(pull(simulations, term)), \"years\", negate = TRUE),\n      sort(str_subset(unique(pull(simulations, term)), \"years\")))))\n\n\n\n\nhead(simulations)\n\n# A tibble: 6 × 7\n  ID        term                      estimate std.er…¹ stati…²  p.value Analy…³\n  <chr>     <fct>                        <dbl>    <dbl>   <dbl>    <dbl> <chr>  \n1 DONOHUE_1 RTC_LAWTRUE                 0.0237   0.0170   1.40  1.62e- 1 Donohue\n2 DONOHUE_1 White_Male_15_to_19_years   0.0108   0.0282   0.382 7.02e- 1 Donohue\n3 DONOHUE_1 White_Male_20_to_39_years   0.0294   0.0100   2.93  3.42e- 3 Donohue\n4 DONOHUE_1 Black_Male_15_to_19_years  -0.0596   0.0578  -1.03  3.02e- 1 Donohue\n5 DONOHUE_1 Black_Male_20_to_39_years   0.123    0.0195   6.34  3.12e-10 Donohue\n6 DONOHUE_1 Other_Male_15_to_19_years   0.676    0.114    5.94  3.68e- 9 Donohue\n# … with abbreviated variable names ¹​std.error, ²​statistic, ³​Analysis"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#simulation-plot-coefficients",
    "href": "content/lectures/13-cs01-analysis.html#simulation-plot-coefficients",
    "title": "13-cs01-analysis",
    "section": "Simulation: Plot (coefficients)",
    "text": "Simulation: Plot (coefficients)\n\nCodePlot\n\n\n\nsimulations |>\n  ggplot(aes(x = term, y = estimate)) +\n  geom_boxplot() +\n  facet_grid(. ~ Analysis, scale = \"free_x\", space = \"free\", drop = TRUE) +\n  labs(title = \"Coefficient estimates\",\n       subtitle = \"Estimates across leave-one-out analyses\",\n       x = \"Term\",\n       y = \"Coefficient\",\n       caption = \"Results from simulations\") +\n  theme_linedraw() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_text(angle = 70, hjust = 1),\n        strip.text.x = element_text(size = 14, face = \"bold\"),\n        plot.title.position=\"plot\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#simulation-plot-sd",
    "href": "content/lectures/13-cs01-analysis.html#simulation-plot-sd",
    "title": "13-cs01-analysis",
    "section": "Simulation: Plot (sd)",
    "text": "Simulation: Plot (sd)\n\nCodePlot\n\n\n\ncoeff_sd <- simulations |>\n  group_by(Analysis, term) |>\n  summarize(\"SD\" = sd(estimate))\n\ncoeff_sd |>\n  ggplot(aes(x = Analysis, y = SD)) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  labs(title = \"Coefficient variability\",\n       subtitle = \"SDs of coefficient estimates from leave-one-out analysis\",\n       x = \"Term\",\n       y = \"Coefficient Estimate \\n Standard Deviations\",\n       caption = \"Results from simulations\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_text(size = 8, color = \"black\"),\n        axis.text.y = element_text(color = \"black\"),\n        plot.title.position=\"plot\")"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#vif-evaluating-presence-and-severity-of-multicollinearity",
    "href": "content/lectures/13-cs01-analysis.html#vif-evaluating-presence-and-severity-of-multicollinearity",
    "title": "13-cs01-analysis",
    "section": "VIF: Evaluating presence and severity of multicollinearity",
    "text": "VIF: Evaluating presence and severity of multicollinearity\nVariance Inflation Factor (VIF): index that measures how much the variance (the square of the estimate’s standard deviation) of an estimated regression coefficient is increased because of collinearity."
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculating-vif",
    "href": "content/lectures/13-cs01-analysis.html#calculating-vif",
    "title": "13-cs01-analysis",
    "section": "Calculating VIF:",
    "text": "Calculating VIF:\nIf our model is : \\(Y = β_0 + β_1X_1 + β_2X_2 + β_3X_3 + e\\), then:\nTo calculate the VIF value for \\(X_1\\), perform another OLS model, where \\(X_1\\) is now the dependent variable explained by the other explanatory variables:\n\\[X_1 = β_0 +  β_2X_2 + β_3X_3 + e\\]\n\nFrom this model : \\[VIF = \\frac{1}{1-R^{2}}\\]\n…where \\(R^2\\) value is the proportion of variance in \\(X_1\\) explained by the other variables ( \\(X_2\\) and \\(X_3\\))\n\n\nRepeat for each explanatory variable in the model.\n. . . - the larger the VIF, the more multicollinearity - VIF of >=10 typically indicates problematic multicollinearity"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculating-vif-donohue",
    "href": "content/lectures/13-cs01-analysis.html#calculating-vif-donohue",
    "title": "13-cs01-analysis",
    "section": "Calculating VIF (Donohue)",
    "text": "Calculating VIF (Donohue)\n\nCodeTable\n\n\n\n# create model matrix\nlm_DONOHUE_data <- as.data.frame(model.matrix(DONOHUE_OUTPUT))\n\n# define model\nlm_DONOHUE_data <- lm_DONOHUE_data |> \n  mutate(Viol_crime_rate_1k_log = plm::Within(pull(\n    d_panel_DONOHUE, Viol_crime_rate_1k_log\n  )), effect = \"twoways\")\n\n# specify model\nlm_DONOHUE <- lm(Viol_crime_rate_1k_log ~\nRTC_LAWTRUE +\n  White_Male_15_to_19_years +\n  White_Male_20_to_39_years +\n  Black_Male_15_to_19_years +\n  Black_Male_20_to_39_years +\n  Other_Male_15_to_19_years +\n  Other_Male_20_to_39_years +\n  Unemployment_rate +\n  Poverty_rate +\n  Population_log +\n  police_per_100k_lag,\ndata = lm_DONOHUE_data\n)\n\n# calculate VIF\nvif_DONOHUE <- vif(lm_DONOHUE)\n\n\n\n\n# combine into nice table\nvif_DONOHUE <- vif_DONOHUE |>\n  as_tibble() |>\n  cbind(names(vif_DONOHUE)) |>\n  as_tibble()\ncolnames(vif_DONOHUE) <- c(\"VIF\", \"Variable\")\n\nvif_DONOHUE |>\n  arrange(desc(VIF))\n\n# A tibble: 11 × 2\n     VIF Variable                 \n   <dbl> <chr>                    \n 1  1.72 White_Male_20_to_39_years\n 2  1.66 Black_Male_20_to_39_years\n 3  1.58 Other_Male_15_to_19_years\n 4  1.52 Other_Male_20_to_39_years\n 5  1.34 Black_Male_15_to_19_years\n 6  1.27 Poverty_rate             \n 7  1.23 Unemployment_rate        \n 8  1.21 police_per_100k_lag      \n 9  1.17 Population_log           \n10  1.15 White_Male_15_to_19_years\n11  1.11 RTC_LAWTRUE"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#calculating-vif-lott",
    "href": "content/lectures/13-cs01-analysis.html#calculating-vif-lott",
    "title": "13-cs01-analysis",
    "section": "Calculating VIF (Lott)",
    "text": "Calculating VIF (Lott)\n\nCodeTable\n\n\n\nlm_LOTT_data <- as.data.frame(model.matrix(LOTT_OUTPUT))\nlm_LOTT_data <- lm_LOTT_data |>\n  mutate(Viol_crime_rate_1k_log = plm::Within(pull(d_panel_LOTT, Viol_crime_rate_1k_log), effect = \"twoways\")) |>\n  rename(RTC_LAW = RTC_LAWTRUE)\nlm_LOTT <- lm(LOTT_fmla, data = lm_LOTT_data)\n\nvif_LOTT <- vif(lm_LOTT)\nvif_LOTT <- vif_LOTT |>\n  as_tibble() |>\n  cbind(names(vif_LOTT)) |>\n  as_tibble()\ncolnames(vif_LOTT) <- c(\"VIF\", \"Variable\")\n\n\n\n\n# clean up names\nvif_LOTT |> \n  mutate(Variable = str_replace(string = Variable,\n                                pattern = \"RTC_LAW\",\n                                replacement = \"RTC_LAWTRUE\")) |>\n  arrange(desc(VIF))\n\n# A tibble: 41 × 2\n     VIF Variable                   \n   <dbl> <chr>                      \n 1  342. Black_Female_10_to_19_years\n 2  327. Black_Male_10_to_19_years  \n 3  251. Other_Male_40_to_49_years  \n 4  227. Other_Female_40_to_49_years\n 5  177. Other_Male_50_to_64_years  \n 6  157. Other_Male_10_to_19_years  \n 7  148. Other_Female_10_to_19_years\n 8  134. Other_Female_50_to_64_years\n 9  121. White_Female_10_to_19_years\n10  120. White_Male_10_to_19_years  \n# … with 31 more rows"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#vif-across-analyses",
    "href": "content/lectures/13-cs01-analysis.html#vif-across-analyses",
    "title": "13-cs01-analysis",
    "section": "VIF Across Analyses",
    "text": "VIF Across Analyses\n\nCodeTable\n\n\n\nvif_DONOHUE <- vif_DONOHUE |>\n  mutate(Analysis = \"Donohue\")\nvif_LOTT <- vif_LOTT |>\n  mutate(Analysis = \"Lott\")\nvif_df <- bind_rows(vif_DONOHUE, vif_LOTT)"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#vif-plot",
    "href": "content/lectures/13-cs01-analysis.html#vif-plot",
    "title": "13-cs01-analysis",
    "section": "VIF Plot",
    "text": "VIF Plot\n\nCodePlot\n\n\n\nvif_df |>\n  ggplot(aes(x = Analysis, y = VIF)) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = 10, color = \"red\") +\n  geom_text(aes(.75, 13, label = \"typical cutoff of 10\")) +\n  coord_trans(y = \"log10\") +\n  labs(title = \"Variance inflation factors\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_text(color = \"black\"),\n        axis.text.y = element_text(color = \"black\"))"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#what-to-do",
    "href": "content/lectures/13-cs01-analysis.html#what-to-do",
    "title": "13-cs01-analysis",
    "section": "What to do?",
    "text": "What to do?\n\ndifferent model?\ndifferent predictors in this model?\ndifferent parameterization of predictors?\narticle for a detailed discussion about what to consider when your model has variables with high VIF values"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#where-to-go-from-here",
    "href": "content/lectures/13-cs01-analysis.html#where-to-go-from-here",
    "title": "13-cs01-analysis",
    "section": "Where to Go From Here",
    "text": "Where to Go From Here\n\ncomplete the lab!\nstart to work with your group through this analysis\nconsider what model you’ll want to use to answer the questions\nthink about/consider extension for CS01"
  },
  {
    "objectID": "content/lectures/13-cs01-analysis.html#suggested-reading",
    "href": "content/lectures/13-cs01-analysis.html#suggested-reading",
    "title": "13-cs01-analysis",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nCase Study from OCS\nWrangling for this case study"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#qa",
    "href": "content/lectures/11-cs01-data-slides.html#qa",
    "title": "11-cs01-data",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Is there ever a time when we should use \\(R^2\\) instead of adjusted R^2 when analyzing a model?\nA: When talking about variance explained of a single model, \\(R^2\\) is great, but when comparing across models, you’ll always want to use adjusted \\(R^2\\)\n\n\nQ: How do we use the data from OCS for our case study? Should we merge the data files?\nA: Excellent question! That’s what today’s lecture is all about. There’s a whole lot of wrangling to do before we can use these data!\n\n\nQ: Do we need to look at the p-value when we do analysis? (Midterm01)\nA: When interpreting a model, no. When doing hypothesis testing (we’ll get there), it is one piece you can look at. A few people did interpret p-values on the midterm, and that’s ok! (But it was not required.)\n\n\nQ: Can we have a system where we can find other students to group with? Like a google form?\nA: Great question! I’ll start a pinned thread on Campuswire so you all can find one another.\n\n\nQ: I think the data seems pretty confusing.\nA: That’s b/c it is! We’ve got a lot of work to do to get it into a usable/understandable format.\n\n\nQ: In what context of data we should use interaction model or main effect model?\nA: Interaction terms should be included when the relationship between one predictor and the outcome varies by another predictor."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#course-announcements",
    "href": "content/lectures/11-cs01-data-slides.html#course-announcements",
    "title": "11-cs01-data",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 05 due tomorrow (2/17; 11:59 PM)\nmid-course survey (optional for EC) due tomorrow (2/17; 11:59 PM)\nLecture Participation survey “due” after class\n\n\nNotes:\n\nCS01\n\ninstructions posted on website\ninvited to GH repo (accept invitation, please!)\nhave an email with other group mates\ngoal: meet more people in the class & work together\n\nHW03 posted\nReminder to think about final project group mates; thread on campuswire"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#agenda",
    "href": "content/lectures/11-cs01-data-slides.html#agenda",
    "title": "11-cs01-data",
    "section": "Agenda",
    "text": "Agenda\n\nBackground\nData Intro\nWrangle\nCombine!"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#right-to-carry-laws1",
    "href": "content/lectures/11-cs01-data-slides.html#right-to-carry-laws1",
    "title": "11-cs01-data",
    "section": "Right To Carry Laws1",
    "text": "Right To Carry Laws1\nRight to Carry (RTC) Laws - “a law that specifies if and how citizens are allowed to have a firearm on their person or nearby (for example, in a citizen’s car) in public.”2\nCase Study Reference: Wright, Carrie and Ontiveros, Michael and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com//opencasestudies/ocs-bp-RTC-analysis. Influence of Multicollinearity on Measured Impact of Right-to-Carry Gun Laws (Version v1.0.0).In this discussion, we will use the National Rifle Association (NRA) terminology. Please keep in mind that there are other terms that people use."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#rtc-laws-contd",
    "href": "content/lectures/11-cs01-data-slides.html#rtc-laws-contd",
    "title": "11-cs01-data",
    "section": "RTC Laws (cont’d)",
    "text": "RTC Laws (cont’d)\n\n\nThe Second Amendment to the United States Constitution guarantees the right to “keep and bear arms”. The amendment was ratified in 1791 as part of the Bill of Rights.\nThere are no federal laws about carrying firearms in public.\nThese laws are created and enforced at the US state level. States vary greatly in their laws about the right to carry firearms.\nSome require extensive effort to obtain a permit to legally carry a firearm, while other states require very minimal effort to do so. An increasing number of states do not require permits at all."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#rtc-laws-across-the-us",
    "href": "content/lectures/11-cs01-data-slides.html#rtc-laws-across-the-us",
    "title": "11-cs01-data",
    "section": "RTC Laws Across the US",
    "text": "RTC Laws Across the US"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#rtc-laws-across-the-us-1",
    "href": "content/lectures/11-cs01-data-slides.html#rtc-laws-across-the-us-1",
    "title": "11-cs01-data",
    "section": "RTC Laws Across the US",
    "text": "RTC Laws Across the US"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#the-data-source",
    "href": "content/lectures/11-cs01-data-slides.html#the-data-source",
    "title": "11-cs01-data",
    "section": "The Data: Source",
    "text": "The Data: Source\nTwo contradictory analyses:\n\nJohn J. Donohue et al., Right‐to‐Carry Laws and Violent Crime: A Comprehensive Assessment Using Panel Data and a State‐Level Synthetic Control Analysis. Journal of Empirical Legal Studies, 16,2 (2019).\nDavid B. Mustard & John Lott. Crime, Deterrence, and Right-to-Carry Concealed Handguns. Coase-Sandor Institute for Law & Economics Working Paper No. 41, (1996)."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#the-data-1",
    "href": "content/lectures/11-cs01-data-slides.html#the-data-1",
    "title": "11-cs01-data",
    "section": "The Data",
    "text": "The Data"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#two-analyses",
    "href": "content/lectures/11-cs01-data-slides.html#two-analyses",
    "title": "11-cs01-data",
    "section": "Two Analyses",
    "text": "Two Analyses"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#limitations",
    "href": "content/lectures/11-cs01-data-slides.html#limitations",
    "title": "11-cs01-data",
    "section": "Limitations",
    "text": "Limitations\n\n\nThe analyses differed in variables used; we will not be recreating either analysis in full\nWe’ll account for either the adoption or lack of adoption of a permissive right-to-carry law in each state; we will not account for differences in the level of permissiveness of the laws.\nRace is included here (as it was in initial analysis); however, any association between demographic variables (indicating the proportion of the population from specific race and age groups) and violent crime does not necessarily indicate that the two are linked causally."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#packages",
    "href": "content/lectures/11-cs01-data-slides.html#packages",
    "title": "11-cs01-data",
    "section": "Packages",
    "text": "Packages\n\nlibrary(OCSdata) # will need to be installed\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(readxl)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#raw-data",
    "href": "content/lectures/11-cs01-data-slides.html#raw-data",
    "title": "11-cs01-data",
    "section": "Raw Data",
    "text": "Raw Data\nThere are a whole bunch of different data files we’ll be using…\n\n# only get the data once\nOCSdata::load_raw_data(\"ocs-bp-RTC-wrangling\", outpath = '.')\n\n\ncreates a “data” sub-directory in your current working directory (if it does not already exist)\ncreates a “raw” sub-directory within “data”; contains the directories with the data\n\n👉 Your Turn: Load the data into RStudio. It will take a while…so just let it get started."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#the-goal",
    "href": "content/lectures/11-cs01-data-slides.html#the-goal",
    "title": "11-cs01-data",
    "section": "The Goal",
    "text": "The Goal\nGet two datasets (Lott, Donohue) that contain demographic, population, police staffing, unemployment, violent crime, RTC, and poverty information at the state level across time.\n\n❓ What would be the tidy way to store these data?"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#your-turn",
    "href": "content/lectures/11-cs01-data-slides.html#your-turn",
    "title": "11-cs01-data",
    "section": "Your Turn",
    "text": "Your Turn\n🧠 Take a look in one of the data folders, open at least one of the data files to view it, and try to get a sense of the type of information contained within it.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#demographic-population-data",
    "href": "content/lectures/11-cs01-data-slides.html#demographic-population-data",
    "title": "11-cs01-data",
    "section": "Demographic & Population Data",
    "text": "Demographic & Population Data\n\nCodeData\n\n\n\ndem_77_79 <- read_csv(\"data/raw/Demographics/Decade_1970/pe-19.csv\", skip = 5)\n\ndem_80_89 <- list.files(recursive = TRUE,\n                  path = \"data/raw/Demographics/Decade_1980\",\n                  pattern = \"*.csv\",\n                  full.names = TRUE) |> \n  purrr::map(~read_csv(., skip=5))\n\ndem_90_99 <- list.files(recursive = TRUE,\n                  path = \"data/raw/Demographics/Decade_1990\",\n                  pattern = \"*.txt\",\n                  full.names = TRUE) |> \n  map(~read_table2(., skip = 14))\n\ndem_00_10 <- list.files(recursive = TRUE,\n                  path = \"data/raw/Demographics/Decade_2000\",\n                  pattern = \"*.csv\",\n                   full.names = TRUE) |> \n   map(~read_csv(.))\n\nSource: US Census Bureau Data\n\n\n\nglimpse(dem_00_10[[1]])\n\nRows: 62,244\nColumns: 21\n$ REGION            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ DIVISION          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ STATE             <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ NAME              <chr> \"United States\", \"United States\", \"United States\", \"…\n$ SEX               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ORIGIN            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ RACE              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ AGEGRP            <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ ESTIMATESBASE2000 <dbl> 281424600, 19176154, 20549855, 20528425, 20218782, 1…\n$ POPESTIMATE2000   <dbl> 282162411, 19178293, 20463852, 20637696, 20294955, 1…\n$ POPESTIMATE2001   <dbl> 284968955, 19298217, 20173362, 20978678, 20456284, 1…\n$ POPESTIMATE2002   <dbl> 287625193, 19429192, 19872417, 21261421, 20610370, 2…\n$ POPESTIMATE2003   <dbl> 290107933, 19592446, 19620851, 21415353, 20797166, 2…\n$ POPESTIMATE2004   <dbl> 292805298, 19785885, 19454237, 21411680, 21102552, 2…\n$ POPESTIMATE2005   <dbl> 295516599, 19917400, 19389067, 21212579, 21486214, 2…\n$ POPESTIMATE2006   <dbl> 298379912, 19938883, 19544688, 21033138, 21807709, 2…\n$ POPESTIMATE2007   <dbl> 301231207, 20125962, 19714611, 20841042, 22067816, 2…\n$ POPESTIMATE2008   <dbl> 304093966, 20271127, 19929602, 20706655, 22210880, 2…\n$ POPESTIMATE2009   <dbl> 306771529, 20244518, 20182499, 20660564, 22192810, 2…\n$ CENSUS2010POP     <dbl> 308745538, 20201362, 20348657, 20677194, 22040343, 2…\n$ POPESTIMATE2010   <dbl> 309349689, 20200529, 20382409, 20694011, 21959087, 2…"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#state-fips-codes",
    "href": "content/lectures/11-cs01-data-slides.html#state-fips-codes",
    "title": "11-cs01-data",
    "section": "State FIPS Codes",
    "text": "State FIPS Codes\n\nImageCodeDataWrangling\n\n\n\n\n\n\nSTATE_FIPS <- readxl::read_xls(\"data/raw/State_FIPS_codes/state-geocodes-v2014.xls\", skip = 5)\n\n\n\n\nglimpse(STATE_FIPS)\n\nRows: 64\nColumns: 4\n$ Region          <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",…\n$ Division        <chr> \"0\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\",…\n$ `State\\n(FIPS)` <chr> \"00\", \"00\", \"09\", \"23\", \"25\", \"33\", \"44\", \"50\", \"00\", …\n$ Name            <chr> \"Northeast Region\", \"New England Division\", \"Connectic…\n\n\n\n\n\nSTATE_FIPS <- STATE_FIPS |>\n  rename(STATEFP = `State\\n(FIPS)`,\n         STATE = Name) |>\n  select(STATEFP, STATE) |>\n  filter(STATEFP != \"00\")\n\nSTATE_FIPS\n\n# A tibble: 51 × 2\n   STATEFP STATE        \n   <chr>   <chr>        \n 1 09      Connecticut  \n 2 23      Maine        \n 3 25      Massachusetts\n 4 33      New Hampshire\n 5 44      Rhode Island \n 6 50      Vermont      \n 7 34      New Jersey   \n 8 36      New York     \n 9 42      Pennsylvania \n10 17      Illinois     \n# … with 41 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#police-staffing-data",
    "href": "content/lectures/11-cs01-data-slides.html#police-staffing-data",
    "title": "11-cs01-data",
    "section": "Police Staffing Data",
    "text": "Police Staffing Data\n\nCodeData\n\n\nThere’s an issue currently with the ps_data file from OCS, so we’ll use this file instead:\n\nps_data <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/pe_1960_2018.csv\")\n\n\n\n\nglimpse(ps_data)\n\nRows: 2,242\nColumns: 3\n$ data_year           <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 19…\n$ state_abbr          <chr> \"AK\", \"AL\", \"AR\", \"AS\", \"AZ\", \"CA\", \"CO\", \"CT\", \"C…\n$ officer_state_total <dbl> 544, 7380, 3344, 0, 6414, 65596, 7337, 6051, 0, 47…"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#unemployment-data",
    "href": "content/lectures/11-cs01-data-slides.html#unemployment-data",
    "title": "11-cs01-data",
    "section": "Unemployment Data",
    "text": "Unemployment Data\n\nImageCodeDataWrangle\n\n\n\n\n\nunemployment\n\n\n\n\n\nue_rate_data <- list.files(recursive = TRUE,\n                          path = \"data/raw/Unemployment\",\n                          pattern = \"*.xlsx\",\n                          full.names = TRUE) |> \nmap(~read_xlsx(., skip = 10))\n\nue_rate_names <- list.files(recursive = TRUE,\n                          path = \"data/raw/Unemployment\",\n                          pattern = \"*.xlsx\",\n                          full.names = TRUE) |>\nmap(~read_xlsx(., range = \"B4:B6\")) |>\n  map(c(1,2)) |>\nunlist()\n\nnames(ue_rate_data) <- ue_rate_names\n\n\n\n\nhead(ue_rate_data)[1]\n\n$Alabama\n# A tibble: 44 × 14\n    Year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  1977   7.5   9     7.7   7.2   6.8   8.6   8     7.8   6.7   6.3   6.3   6  \n 2  1978   7.1   6.9   6.2   5.4   5.1   6.9   6.7   6.7   6.5   6.3   6.3   6.5\n 3  1979   6.7   7.5   6.9   6.6   6.4   8.4   7.7   7.8   7.1   7.2   6.9   6.7\n 4  1980   7.7   7.8   7.4   7.4   8.4   9.7  10.4  10.3   9.3   9.6   9.4   9  \n 5  1981  10    10.3   9.5   9.1   9.4  11.1  10.4  10.9  10.8  11.7  11.5  11.8\n 6  1982  13.2  13.2  12.9  12.6  12.8  14.5  14.7  14.8  14.7  15.1  15.4  15.3\n 7  1983  16    16    14.5  13.7  13.3  14.6  13.9  13.8  13.2  12.8  12.1  11.8\n 8  1984  12.5  12.4  11.4  10.8  10.1  11.3  11.5  11.3  10.8  10.2   9.7  10.1\n 9  1985  10.7  10.5   9.8   8.7   8.4   9.6   9.2   8.8   8.6   8.6   8.4   8.7\n10  1986   9.3  10.4  10.1   9.4   9.4  10.5   9.7   9.6   9.7   9.7   9.6   9  \n# … with 34 more rows, and 1 more variable: Annual <dbl>\n\n\n\n\n\nue_rate_data <- ue_rate_data |>\n  map_df(bind_rows, .id = \"STATE\") |>\n  select(STATE, Year, Annual) |>\n  rename(\"YEAR\" = Year,\n         \"VALUE\" = Annual) |>\n  mutate(VARIABLE = \"Unemployment_rate\")\n\nue_rate_data\n\n# A tibble: 2,244 × 4\n   STATE    YEAR VALUE VARIABLE         \n   <chr>   <dbl> <dbl> <chr>            \n 1 Alabama  1977   7.3 Unemployment_rate\n 2 Alabama  1978   6.4 Unemployment_rate\n 3 Alabama  1979   7.2 Unemployment_rate\n 4 Alabama  1980   8.9 Unemployment_rate\n 5 Alabama  1981  10.6 Unemployment_rate\n 6 Alabama  1982  14.1 Unemployment_rate\n 7 Alabama  1983  13.8 Unemployment_rate\n 8 Alabama  1984  11   Unemployment_rate\n 9 Alabama  1985   9.2 Unemployment_rate\n10 Alabama  1986   9.7 Unemployment_rate\n# … with 2,234 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#poverty-data",
    "href": "content/lectures/11-cs01-data-slides.html#poverty-data",
    "title": "11-cs01-data",
    "section": "Poverty Data",
    "text": "Poverty Data\n\nCodeData\n\n\n\npoverty_rate_data <- read_xls(\"data/raw/Poverty/hstpov21.xls\", skip=2)\n\n\n\n\nhead(poverty_rate_data)\n\n# A tibble: 6 × 6\n  `NOTE: Number in thousands.` ...2  ...3   ...4              ...5         ...6 \n  <chr>                        <chr> <chr>  <chr>             <chr>        <chr>\n1 2018                         <NA>  <NA>    <NA>             <NA>          <NA>\n2 STATE                        Total Number \"Standard\\nerror\" Percent      \"Sta…\n3 Alabama                      4877  779    \"65\"              16           \"1.3\"\n4 Alaska                       720   94     \"9\"               13.1         \"1.2\"\n5 Arizona                      7241  929    \"80\"              12.80000000… \"1.1…\n6 Arkansas                     2912  462    \"38\"              15.9         \"1.3\"\n\n\n\n\n\nSource: US Census Bureau Data"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#violent-crime-data",
    "href": "content/lectures/11-cs01-data-slides.html#violent-crime-data",
    "title": "11-cs01-data",
    "section": "Violent Crime Data",
    "text": "Violent Crime Data\n\nCodeData\n\n\n\ncrime_data <- read_lines(\"data/raw/Crime/CrimeStatebyState.csv\",\n                         skip = 2, \n                         skip_empty_rows = TRUE)\n\nDue to spaces and / in the column names, read_lines() from the readr package works better than read_csv()\n\n\n\nhead(crime_data)\n\n[1] \"Estimated crime in Alabama\"                                                                                                         \n[2] \",,National or state crime,,,,,,,\"                                                                                                   \n[3] \",,Violent crime,,,,,,,\"                                                                                                             \n[4] \"Year,Population,Violent crime total,Murder and nonnegligent Manslaughter,Legacy rape /1,Revised rape /2,Robbery,Aggravated assault,\"\n[5] \"1977,   3690000,      15293,         524,         929,,       3572,      10268 \"                                                    \n[6] \"1978,   3742000,      15682,         499,         954,,       3708,      10521 \""
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#right-to-carry-data",
    "href": "content/lectures/11-cs01-data-slides.html#right-to-carry-data",
    "title": "11-cs01-data",
    "section": "Right-To-Carry Data",
    "text": "Right-To-Carry Data\n\nCodeData\n\n\n\nDAWpaper <- pdf_text(\"data/raw/w23510.pdf\")\n\n\n\n\nhead(DAWpaper[1])\n\n[1] \"                              NBER WORKING PAPER SERIES\\n\\n\\n\\n                  RIGHT-TO-CARRY LAWS AND VIOLENT CRIME:\\n             A COMPREHENSIVE ASSESSMENT USING PANEL DATA AND\\n                 A STATE-LEVEL SYNTHETIC CONTROL ANALYSIS\\n\\n                                        John J. Donohue\\n                                          Abhay Aneja\\n                                         Kyle D. Weber\\n\\n                                      Working Paper 23510\\n                              http://www.nber.org/papers/w23510\\n\\n\\n                     NATIONAL BUREAU OF ECONOMIC RESEARCH\\n                              1050 Massachusetts Avenue\\n                                 Cambridge, MA 02138\\n                           June 2017, Revised November 2018\\n\\n\\n\\nPreviously circulated as \\\"Right-to-Carry Laws and Violent Crime: A Comprehensive Assessment\\nUsing Panel Data and a State-Level Synthetic Controls Analysis.\\\" We thank Dan Ho, Stefano\\nDellaVigna, Rob Tibshirani, Trevor Hastie, StefanWager, Jeff Strnad, and participants at the\\n2011 Conference of Empirical Legal Studies (CELS), 2012 American Law and Economics\\nAssociation (ALEA) Annual Meeting, 2013 Canadian Law and Economics Association (CLEA)\\nAnnual Meeting, 2015 NBER Summer Institute (Crime), and the Stanford Law School faculty\\nworkshop for their comments and helpful suggestions. Financial support was provided by\\nStanford Law School. We are indebted to Alberto Abadie, Alexis Diamond, and Jens\\nHainmueller for their work developing the synthetic control algorithm and programming the Stata\\nmodule used in this paper and for their helpful comments. The authors would also like to thank\\nAlex Albright, Andrew Baker, Jacob Dorn, Bhargav Gopal, Crystal Huang, Mira Korb, Haksoo\\nLee, Isaac Rabbani, Akshay Rao, Vikram Rao, Henrik Sachs and Sidharth Sah who provided\\nexcellent research assistance, as well as Addis O’Connor and Alex Chekholko at the Research\\nComputing division of Stanford’s Information Technology Services for their technical support.\\nThe views expressed herein are those of the author and do not necessarily reflect the views of the\\nNational Bureau of Economic Research.\\n\\nNBER working papers are circulated for discussion and comment purposes. They have not been\\npeer-reviewed or been subject to the review by the NBER Board of Directors that accompanies\\nofficial NBER publications.\\n\\n© 2017 by John J. Donohue, Abhay Aneja, and Kyle D. Weber. All rights reserved. Short\\nsections of text, not to exceed two paragraphs, may be quoted without explicit permission\\nprovided that full credit, including © notice, is given to the source.\\n\""
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#save-imported-data",
    "href": "content/lectures/11-cs01-data-slides.html#save-imported-data",
    "title": "11-cs01-data",
    "section": "Save (Imported) Data",
    "text": "Save (Imported) Data\n\nsave(dem_77_79, dem_80_89, dem_90_99, dem_00_10, \n     STATE_FIPS, \n     ps_data, \n     ue_rate_data, \n     poverty_rate_data,\n     crime_data,\n     DAWpaper, file = \"data/imported_data_rtc.rda\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangle-demo-data",
    "href": "content/lectures/11-cs01-data-slides.html#wrangle-demo-data",
    "title": "11-cs01-data",
    "section": "Wrangle: Demo Data",
    "text": "Wrangle: Demo Data\n\n77-7980s90s00s\n\n\n\ndem_77_79 <- dem_77_79 |>\n  rename(\"race_sex\" =`Race/Sex Indicator`) |>\n  mutate(SEX = str_extract(race_sex, \"male|female\"),\n        RACE = str_extract(race_sex, \"Black|White|Other\"))|>\n  select(-`FIPS State Code`, -`race_sex`) |>\n  rename(\"YEAR\" = `Year of Estimate`,\n        \"STATE\" = `State Name`) |>\n  filter(YEAR %in% 1977:1979)\n\ndem_77_79 <- dem_77_79 |>\n  pivot_longer(cols=contains(\"years\"),\n               names_to = \"AGE_GROUP\",\n               values_to = \"SUB_POP\")\n\nglimpse(dem_77_79)\n\nRows: 16,524\nColumns: 6\n$ YEAR      <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, …\n$ STATE     <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alab…\n$ SEX       <chr> \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"mal…\n$ RACE      <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"White…\n$ AGE_GROUP <chr> \"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to 19…\n$ SUB_POP   <dbl> 98814, 113365, 123107, 135343, 126053, 111547, 100674, 81038…\n\n\n\n\n\ndem_80_89 <- dem_80_89 |>\n  map_df(bind_rows)\n\ndem_80_89 <- dem_80_89 |>\n  rename(\"race_sex\" =`Race/Sex Indicator`) |>\n  mutate(SEX = str_extract(race_sex, \"male|female\"),\n        RACE = str_extract(race_sex, \"Black|White|Other\"))|>\n  select( -`race_sex`) |>\n  rename(\"YEAR\" = `Year of Estimate`) |> \n  rename(\"STATEFP_temp\" = \"FIPS State and County Codes\") |>\n  mutate(STATEFP = str_sub(STATEFP_temp, start = 1, end = 2)) |>\n    left_join(STATE_FIPS, by = \"STATEFP\") |>\n  select(-STATEFP)\n\ndem_80_89 <- dem_80_89 |>\n  pivot_longer(cols=contains(\"years\"),\n               names_to = \"AGE_GROUP\",\n               values_to = \"SUB_POP_temp\") |>\n  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) |>\n  summarize(SUB_POP = sum(SUB_POP_temp), .groups=\"drop\")\n\ndem_80_89\n\n# A tibble: 55,080 × 6\n    YEAR STATE   AGE_GROUP      SEX    RACE  SUB_POP\n   <dbl> <chr>   <chr>          <chr>  <chr>   <dbl>\n 1  1980 Alabama 10 to 14 years female Black   50108\n 2  1980 Alabama 10 to 14 years female Other     805\n 3  1980 Alabama 10 to 14 years female White  109066\n 4  1980 Alabama 10 to 14 years male   Black   50768\n 5  1980 Alabama 10 to 14 years male   Other     826\n 6  1980 Alabama 10 to 14 years male   White  115988\n 7  1980 Alabama 15 to 19 years female Black   58428\n 8  1980 Alabama 15 to 19 years female Other     743\n 9  1980 Alabama 15 to 19 years female White  126783\n10  1980 Alabama 15 to 19 years male   Black   56808\n# … with 55,070 more rows\n\n\n\n\n\ndem_90_99 <- dem_90_99 |>\n  map_df(bind_rows)\n\ncolnames(dem_90_99) <- c(\"YEAR\", \"STATEFP\", \"Age\", \"NH_W_M\", \"NH_W_F\", \"NH_B_M\",\n                         \"NH_B_F\", \"NH_AIAN_M\", \"NH_AIAN_F\", \"NH_API_M\", \"NH_API_F\",\n                         \"H_W_M\", \"H_W_F\", \"H_B_M\", \"H_B_F\", \"H_AIAN_M\", \"H_AIAN_F\",\n                         \"H_API_M\", \"H_API_F\")\n\ndem_90_99 <- dem_90_99 |>\n  drop_na() |>\n  mutate(W_M = NH_W_M + H_W_M, W_F = NH_W_F + H_W_F,\n         B_M = NH_B_M + H_B_M, B_F = NH_B_F + H_B_F,\n         AIAN_M = NH_AIAN_M + H_AIAN_M, AIAN_F = NH_AIAN_F + H_AIAN_F,\n         API_M = NH_API_M + H_API_M, API_F = NH_API_F + H_API_F) |>\n  select(-starts_with(\"NH_\"), -starts_with(\"H_\"))\n\ndem_90_99 <- dem_90_99 |>\n  mutate(AGE_GROUP = cut(Age,\n                         breaks = seq(0,90, by=5),\n                         right = FALSE, labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP))) |>\n  select(-Age) |>\n  pivot_longer(cols = c(starts_with(\"W_\"),\n                        starts_with(\"B_\"),\n                        starts_with(\"AIAN_\"),\n                        starts_with(\"API_\")),\n               names_to = \"RACE\",\n               values_to = \"SUB_POP_temp\") |>\n  mutate(SEX = case_when(str_detect(RACE, \"_M\") ~ \"Male\",\n                         TRUE ~ \"Female\"),\n         RACE = case_when(str_detect(RACE, \"W_\") ~ \"White\",\n                          str_detect(RACE, \"B_\") ~ \"Black\",\n                          TRUE ~ \"Other\"))\n\ndem_90_99 <- dem_90_99 |>\n  left_join(STATE_FIPS, by = \"STATEFP\") |>\n  select(-STATEFP) |>\n  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) |>\n  summarize(SUB_POP = sum(SUB_POP_temp), .groups=\"drop\")\n\nglimpse(dem_90_99)\n\nRows: 55,080\nColumns: 6\n$ YEAR      <dbl> 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, …\n$ STATE     <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alab…\n$ AGE_GROUP <fct> Under 5 years, Under 5 years, Under 5 years, Under 5 years, …\n$ SEX       <chr> \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Femal…\n$ RACE      <chr> \"Black\", \"Other\", \"White\", \"Black\", \"Other\", \"White\", \"Black…\n$ SUB_POP   <dbl> 45377, 1406, 92380, 46635, 1360, 98524, 46067, 1698, 92530, …\n\n\n\n\n\ndem_00_10 <- dem_00_10 |>\n  map_df(bind_rows)\n\ndem_00_10 <- dem_00_10 |>\n  select(-ESTIMATESBASE2000,-CENSUS2010POP) |>\n  filter(NAME != \"United States\",\n         SEX != 0,\n         RACE != 0,\n         AGEGRP != 0, \n         ORIGIN == 0) |>\n  select(-REGION, -DIVISION, -ORIGIN, -STATE) |>\n  rename(\"STATE\" = NAME,\n         \"AGE_GROUP\" = AGEGRP)\n\ndem_00_10 <- dem_00_10 |>\n  mutate(SEX = factor(SEX, levels = 1:2, labels = c(\"Male\", \"Female\")),\n         RACE = factor(RACE, levels = 1:6, labels = c(\"White\", \"Black\", rep(\"Other\",4))),\n         AGE_GROUP = factor(AGE_GROUP, levels = 1:18,\n                            labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP)))\n\ndem_00_10 <- dem_00_10 |>\n  pivot_longer(cols=contains(\"ESTIMATE\"), names_to = \"YEAR\", values_to = \"SUB_POP_temp\") |>\n   mutate(YEAR = str_sub(YEAR, start=-4),\n          YEAR = as.numeric(YEAR)) |> \n  group_by(YEAR, AGE_GROUP, STATE, SEX, RACE) |>\n  summarize(SUB_POP = sum(SUB_POP_temp), .groups = \"drop\")\n                            \nglimpse(dem_00_10)\n\nRows: 60,588\nColumns: 6\n$ YEAR      <dbl> 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, …\n$ AGE_GROUP <fct> Under 5 years, Under 5 years, Under 5 years, Under 5 years, …\n$ STATE     <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alab…\n$ SEX       <fct> Male, Male, Male, Female, Female, Female, Male, Male, Male, …\n$ RACE      <fct> White, Black, Other, White, Black, Other, White, Black, Othe…\n$ SUB_POP   <dbl> 99527, 46595, 4487, 94473, 45672, 4431, 14765, 1039, 8572, 1…"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangle-population-data",
    "href": "content/lectures/11-cs01-data-slides.html#wrangle-population-data",
    "title": "11-cs01-data",
    "section": "Wrangle: Population Data",
    "text": "Wrangle: Population Data\n\n77-7980s90s00s\n\n\n\npop_77_79 <- dem_77_79 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\") \n\npop_77_79 \n\n# A tibble: 153 × 3\n    YEAR STATE                 TOT_POP\n   <dbl> <chr>                   <dbl>\n 1  1977 Alabama               3782571\n 2  1977 Alaska                 397220\n 3  1977 Arizona               2427296\n 4  1977 Arkansas              2207195\n 5  1977 California           22350332\n 6  1977 Colorado              2696179\n 7  1977 Connecticut           3088745\n 8  1977 Delaware               594815\n 9  1977 District of Columbia   681766\n10  1977 Florida               8888806\n# … with 143 more rows\n\n\n\n\n\npop_80_89 <- dem_80_89 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\") \n\n\n\n\npop_90_99 <- dem_90_99 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\")\n\n\n\n\npop_00_10 <- dem_00_10 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#combine-demo-population",
    "href": "content/lectures/11-cs01-data-slides.html#combine-demo-population",
    "title": "11-cs01-data",
    "section": "Combine: Demo + Population",
    "text": "Combine: Demo + Population\n\n77-7980s90s00sIdea\n\n\n\ndem_77_79 <- dem_77_79 |>\n  left_join(pop_77_79, by = c(\"YEAR\", \"STATE\")) |> \n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP) |>\n  mutate(SEX = str_to_title(SEX))\n\n\ndem_77_79\n\n# A tibble: 16,524 × 6\n    YEAR STATE   SEX   RACE  AGE_GROUP      PERC_SUB_POP\n   <dbl> <chr>   <chr> <chr> <chr>                 <dbl>\n 1  1977 Alabama Male  White Under 5 years          2.61\n 2  1977 Alabama Male  White 5 to 9 years           3.00\n 3  1977 Alabama Male  White 10 to 14 years         3.25\n 4  1977 Alabama Male  White 15 to 19 years         3.58\n 5  1977 Alabama Male  White 20 to 24 years         3.33\n 6  1977 Alabama Male  White 25 to 29 years         2.95\n 7  1977 Alabama Male  White 30 to 34 years         2.66\n 8  1977 Alabama Male  White 35 to 39 years         2.14\n 9  1977 Alabama Male  White 40 to 44 years         1.98\n10  1977 Alabama Male  White 45 to 49 years         2.02\n# … with 16,514 more rows\n\n\n\n\n\ndem_80_89 <- dem_80_89 |>\n  left_join(pop_80_89, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP) |>\n  mutate(SEX = str_to_title(SEX))\n\n\n\n\ndem_90_99 <- dem_90_99 |>\n  left_join(pop_90_99, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP)\n\ndem_90_99\n\n# A tibble: 55,080 × 6\n    YEAR STATE   AGE_GROUP     SEX    RACE  PERC_SUB_POP\n   <dbl> <chr>   <fct>         <chr>  <chr>        <dbl>\n 1  1990 Alabama Under 5 years Female Black       1.12  \n 2  1990 Alabama Under 5 years Female Other       0.0347\n 3  1990 Alabama Under 5 years Female White       2.28  \n 4  1990 Alabama Under 5 years Male   Black       1.15  \n 5  1990 Alabama Under 5 years Male   Other       0.0336\n 6  1990 Alabama Under 5 years Male   White       2.43  \n 7  1990 Alabama 5 to 9 years  Female Black       1.14  \n 8  1990 Alabama 5 to 9 years  Female Other       0.0419\n 9  1990 Alabama 5 to 9 years  Female White       2.29  \n10  1990 Alabama 5 to 9 years  Male   Black       1.16  \n# … with 55,070 more rows\n\n\n\n\n\ndem_00_10 <- dem_00_10 |>\n  left_join(pop_00_10, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP)\n\ndem_00_10\n\n# A tibble: 60,588 × 6\n    YEAR AGE_GROUP     STATE   SEX    RACE  PERC_SUB_POP\n   <dbl> <fct>         <chr>   <fct>  <fct>        <dbl>\n 1  2000 Under 5 years Alabama Male   White       2.24  \n 2  2000 Under 5 years Alabama Male   Black       1.05  \n 3  2000 Under 5 years Alabama Male   Other       0.101 \n 4  2000 Under 5 years Alabama Female White       2.12  \n 5  2000 Under 5 years Alabama Female Black       1.03  \n 6  2000 Under 5 years Alabama Female Other       0.0995\n 7  2000 Under 5 years Alaska  Male   White       2.35  \n 8  2000 Under 5 years Alaska  Male   Black       0.165 \n 9  2000 Under 5 years Alaska  Male   Other       1.37  \n10  2000 Under 5 years Alaska  Female White       2.26  \n# … with 60,578 more rows\n\n\n\n\n❗ This would be a good part of the code to write a user-defined function…"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#aside-udf",
    "href": "content/lectures/11-cs01-data-slides.html#aside-udf",
    "title": "11-cs01-data",
    "section": "Aside: UDF",
    "text": "Aside: UDF\n\nThe WhatThe HowExampleUsing the function\n\n\n\nUser-defined functions (UDFs) are functions you write to make your code cleaner\nAny time you copy+paste very similar code, think to yourself…I should make this a function!\n\n\n\nThe general syntax for a function in R is:\n\nfunction_name <- function(parameters){\n  # code to carry out\n  # using the parameters\n}\n\nNote: by default the last object created within the function is returned from the function\n\n\n\ncombine_demo_pop <- function(df_dem, df_pop){\n  df_dem <- df_dem |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\")\n  \n  df_dem |>\n  left_join(df_pop, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP) |>\n  mutate(SEX = str_to_title(SEX))\n}\n\n\n\n\ncombined_df <- combine_demo_pop(dem_00_10, pop_00_10)\n\n\nNote: if you’ve already combined the data into dem_00_10, this would not work.\nCleaning up/improving code in your case studies is encouraged!"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#combine-demopop-across-decades",
    "href": "content/lectures/11-cs01-data-slides.html#combine-demopop-across-decades",
    "title": "11-cs01-data",
    "section": "Combine: Demo/Pop Across Decades",
    "text": "Combine: Demo/Pop Across Decades\n\ndem <- bind_rows(dem_77_79,\n                 dem_80_89,\n                 dem_90_99,\n                 dem_00_10)\n\nglimpse(dem)\n\nRows: 187,272\nColumns: 6\n$ YEAR         <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 197…\n$ STATE        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"A…\n$ SEX          <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n$ RACE         <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"Wh…\n$ AGE_GROUP    <chr> \"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to…\n$ PERC_SUB_POP <dbl> 2.6123502, 2.9970356, 3.2545853, 3.5780690, 3.3324688, 2.…"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#demographic-data-donohue",
    "href": "content/lectures/11-cs01-data-slides.html#demographic-data-donohue",
    "title": "11-cs01-data",
    "section": "Demographic Data (Donohue)",
    "text": "Demographic Data (Donohue)\n\nDONOHUE_AGE_GROUPS <- c(\"15 to 19 years\",\n                        \"20 to 24 years\",\n                        \"25 to 29 years\",\n                        \"30 to 34 years\",\n                        \"35 to 39 years\")\n\ndem_DONOHUE <- dem |>\n  filter(AGE_GROUP %in% DONOHUE_AGE_GROUPS,\n               SEX == \"Male\") |>\n  mutate(AGE_GROUP = fct_collapse(AGE_GROUP, \"20 to 39 years\"=c(\"20 to 24 years\",\n                                                                \"25 to 29 years\",\n                                                                \"30 to 34 years\",\n                                                                \"35 to 39 years\")),\n         AGE_GROUP = str_replace_all(string = AGE_GROUP, \n                                     pattern = \" \", \n                                     replacement = \"_\")) |>\n  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) |>\n  summarize(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = \"drop\") |>\n  unite(col = \"VARIABLE\", RACE, SEX, AGE_GROUP, sep = \"_\") |>\n  rename(\"VALUE\" = PERC_SUB_POP)\n\ndem_DONOHUE\n\n# A tibble: 10,404 × 4\n    YEAR STATE   VARIABLE                    VALUE\n   <dbl> <chr>   <chr>                       <dbl>\n 1  1977 Alabama Black_Male_15_to_19_years  1.55  \n 2  1977 Alabama Black_Male_20_to_39_years  3.04  \n 3  1977 Alabama Other_Male_15_to_19_years  0.0178\n 4  1977 Alabama Other_Male_20_to_39_years  0.0642\n 5  1977 Alabama White_Male_15_to_19_years  3.58  \n 6  1977 Alabama White_Male_20_to_39_years 11.1   \n 7  1977 Alaska  Black_Male_15_to_19_years  0.163 \n 8  1977 Alaska  Black_Male_20_to_39_years  0.968 \n 9  1977 Alaska  Other_Male_15_to_19_years  1.12  \n10  1977 Alaska  Other_Male_20_to_39_years  2.73  \n# … with 10,394 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#demographic-data-lott",
    "href": "content/lectures/11-cs01-data-slides.html#demographic-data-lott",
    "title": "11-cs01-data",
    "section": "Demographic Data (Lott)",
    "text": "Demographic Data (Lott)\n\nLOTT_AGE_GROUPS_NULL <- c(\"Under 5 years\",\n                          \"5 to 9 years\")\n\ndem_LOTT <- dem |>\n  filter(!(AGE_GROUP %in% LOTT_AGE_GROUPS_NULL) )|>\n  mutate(AGE_GROUP = fct_collapse(AGE_GROUP,\n                                  \"10 to 19 years\"=c(\"10 to 14 years\", \"15 to 19 years\"),\n                                  \"20 to 29 years\"=c(\"20 to 24 years\", \"25 to 29 years\"),\n                                  \"30 to 39 years\"=c(\"30 to 34 years\", \"35 to 39 years\"),\n                                  \"40 to 49 years\"=c(\"40 to 44 years\", \"45 to 49 years\"),\n                                  \"50 to 64 years\"=c(\"50 to 54 years\", \"55 to 59 years\",\n                                                     \"60 to 64 years\"),\n                                  \"65 years and over\"=c(\"65 to 69 years\", \"70 to 74 years\", \n                                                        \"75 to 79 years\", \"80 to 84 years\",\n                                                        \"85 years and over\")),\n         AGE_GROUP = str_replace_all(AGE_GROUP, \" \", \"_\")) |>\n  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) |>\n  summarize(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = \"drop\") |>\n  unite(col = \"VARIABLE\", RACE, SEX, AGE_GROUP, sep = \"_\") |>\n  rename(\"VALUE\" = PERC_SUB_POP)\n\nglimpse(dem_LOTT)\n\nRows: 62,424\nColumns: 4\n$ YEAR     <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1…\n$ STATE    <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alaba…\n$ VARIABLE <chr> \"Black_Female_10_to_19_years\", \"Black_Female_20_to_29_years\",…\n$ VALUE    <dbl> 3.01067713, 2.32860137, 1.29295656, 1.18231753, 1.73263106, 1…"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#combine-population-data",
    "href": "content/lectures/11-cs01-data-slides.html#combine-population-data",
    "title": "11-cs01-data",
    "section": "Combine: Population Data",
    "text": "Combine: Population Data\n\npopulation_data <- bind_rows(pop_77_79,\n                             pop_80_89,\n                             pop_90_99,\n                             pop_00_10)\n\npopulation_data <- population_data |>\n  mutate(VARIABLE = \"Population\") |>\n  rename(\"VALUE\" = TOT_POP)"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangling-police-staffing",
    "href": "content/lectures/11-cs01-data-slides.html#wrangling-police-staffing",
    "title": "11-cs01-data",
    "section": "Wrangling: Police staffing",
    "text": "Wrangling: Police staffing\n\n# the provided dataset has already had a bit of wrangling done for you\nps_data\n\n# A tibble: 2,242 × 3\n   data_year state_abbr officer_state_total\n       <dbl> <chr>                    <dbl>\n 1      1977 AK                         544\n 2      1977 AL                        7380\n 3      1977 AR                        3344\n 4      1977 AS                           0\n 5      1977 AZ                        6414\n 6      1977 CA                       65596\n 7      1977 CO                        7337\n 8      1977 CT                        6051\n 9      1977 CZ                           0\n10      1977 DC                        4751\n# … with 2,232 more rows\n\n\n\nremove territories\n\nstate_of_interest_NULL <- c(\"AS\", \"GM\", \"CZ\", \"FS\", \"MP\", \"OT\", \"PR\", \"VI\")\n\nps_data <- ps_data |>\n  filter(!(state_abbr %in% state_of_interest_NULL)) \n\nUse state abbreviations\n\nstate_abb_data <- tibble(\"state_abbr\" = state.abb, \"STATE\" = state.name)\nstate_abb_data <- state_abb_data |>\n  mutate(state_abbr = str_replace(string = state_abbr, \n                                  pattern = \"NE\", \n                                  replacement = \"NB\")) |>\n  add_row(state_abbr = \"DC\", STATE = \"District of Columbia\")\n\nps_data <- ps_data |> \n  left_join(state_abb_data, by = \"state_abbr\") |>\n  select(-state_abbr) |> \n  rename(YEAR = \"data_year\",\n         VALUE = \"officer_state_total\") |>\n  mutate(VARIABLE = \"officer_state_total\")\n\nps_data\n\n# A tibble: 1,938 × 4\n    YEAR VALUE STATE                VARIABLE           \n   <dbl> <dbl> <chr>                <chr>              \n 1  1977   544 Alaska               officer_state_total\n 2  1977  7380 Alabama              officer_state_total\n 3  1977  3344 Arkansas             officer_state_total\n 4  1977  6414 Arizona              officer_state_total\n 5  1977 65596 California           officer_state_total\n 6  1977  7337 Colorado             officer_state_total\n 7  1977  6051 Connecticut          officer_state_total\n 8  1977  4751 District of Columbia officer_state_total\n 9  1977  1018 Delaware             officer_state_total\n10  1977 24588 Florida              officer_state_total\n# … with 1,928 more rows\n\n\nScaling\n\ndenominator_temp <- population_data |> \n  select(-VARIABLE) |>\n  rename(\"Population_temp\"=VALUE) \n\nps_data <- ps_data |> \n  left_join(denominator_temp, by=c(\"STATE\",\"YEAR\")) |>\n  mutate(VALUE = (VALUE * 100000) / Population_temp) |>\n  mutate(VARIABLE = \"police_per_100k_lag\") |>\n  select(-Population_temp)\n\nhead(ps_data)\n\n# A tibble: 6 × 4\n   YEAR VALUE STATE      VARIABLE           \n  <dbl> <dbl> <chr>      <chr>              \n1  1977  137. Alaska     police_per_100k_lag\n2  1977  195. Alabama    police_per_100k_lag\n3  1977  152. Arkansas   police_per_100k_lag\n4  1977  264. Arizona    police_per_100k_lag\n5  1977  293. California police_per_100k_lag\n6  1977  272. Colorado   police_per_100k_lag"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangling-poverty-rate",
    "href": "content/lectures/11-cs01-data-slides.html#wrangling-poverty-rate",
    "title": "11-cs01-data",
    "section": "Wrangling: Poverty Rate",
    "text": "Wrangling: Poverty Rate\n\n\n# A tibble: 5 × 6\n  `NOTE: Number in thousands.` ...2  ...3   ...4              ...5         ...6 \n  <chr>                        <chr> <chr>  <chr>             <chr>        <chr>\n1 2018                         <NA>  <NA>    <NA>             <NA>          <NA>\n2 STATE                        Total Number \"Standard\\nerror\" Percent      \"Sta…\n3 Alabama                      4877  779    \"65\"              16           \"1.3\"\n4 Alaska                       720   94     \"9\"               13.1         \"1.2\"\n5 Arizona                      7241  929    \"80\"              12.80000000… \"1.1…\n\n\n\n\ncolnames(poverty_rate_data) <- c(\"STATE\", \"Total\", \"Number\", \"Number_se\",\n                                 \"Percent\", \"Percent_se\")\n\npoverty_rate_data <- poverty_rate_data |>\n  filter(STATE != \"STATE\") |> \n  mutate(length_state = map_dbl(STATE, str_length)) |> # determine how long string in \"STATE\" column is\n  filter(length_state < 100) |> # filter to only include possible state lengths\n  mutate(STATE = str_replace(STATE, pattern = \"D.C.\", \n                              replacement = \"District of Columbia\" )) \n\nyear_values <- poverty_rate_data |>\n  filter(str_detect(STATE, \"[:digit:]\")) |>\n  distinct(STATE)\nyear_values <- rep(pull(year_values, STATE), each = 52) # repeat values from STATE column 52 times each\n\npoverty_rate_data <- poverty_rate_data |>\n  mutate(year_value = year_values) |>\n  select(-length_state) |>\n  filter(str_detect(STATE, \"[:alpha:]\"))\n\npoverty_rate_data <- poverty_rate_data |>\n  filter(year_value != \"2017\") |> \n  filter(year_value != \"2013 (18)\") |>\n  mutate(YEAR = str_sub(year_value, start = 1, end = 4)) |>\n  select(-c(Number, Number_se, Percent_se, Total, year_value)) |>\n  rename(\"VALUE\" = Percent) |>\n  mutate(VARIABLE = \"Poverty_rate\",\n         YEAR = as.numeric(YEAR),\n         VALUE = as.numeric(VALUE))\n\npoverty_rate_data\n\n# A tibble: 1,989 × 4\n   STATE                VALUE  YEAR VARIABLE    \n   <chr>                <dbl> <dbl> <chr>       \n 1 Alabama               16    2018 Poverty_rate\n 2 Alaska                13.1  2018 Poverty_rate\n 3 Arizona               12.8  2018 Poverty_rate\n 4 Arkansas              15.9  2018 Poverty_rate\n 5 California            11.9  2018 Poverty_rate\n 6 Colorado               9.1  2018 Poverty_rate\n 7 Connecticut           10.2  2018 Poverty_rate\n 8 Delaware               7.4  2018 Poverty_rate\n 9 District of Columbia  14.7  2018 Poverty_rate\n10 Florida               13.7  2018 Poverty_rate\n# … with 1,979 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangling-crime-data",
    "href": "content/lectures/11-cs01-data-slides.html#wrangling-crime-data",
    "title": "11-cs01-data",
    "section": "Wrangling: Crime Data",
    "text": "Wrangling: Crime Data\n\ncrime_data <- crime_data[-((str_which(crime_data, \"The figures shown in this column for the offense of rape were estimated using the legacy UCR definition of rape\")-1): length(crime_data)+1)]\n\nn_rows <- 2014-1977+1 # determine how many rows there are for each state\nrep_cycle <- 4 + n_rows\nrep_cycle_cut <- 2 + n_rows\ncolnames_crime <- (crime_data[4])\n\n# specify which rows are to be deleted based on the file format\ndelete_rows <- c(seq(from = 2, \n                       to = length(crime_data),  \n                       by = rep_cycle),\n                 seq(from = 3, \n                       to = length(crime_data),\n                       by = rep_cycle), \n                 seq(from = 4,\n                       to = length(crime_data),\n                       by = rep_cycle))\nsort(delete_rows) # which rows are to be deleted\n\n  [1]    2    3    4   44   45   46   86   87   88  128  129  130  170  171  172\n [16]  212  213  214  254  255  256  296  297  298  338  339  340  380  381  382\n [31]  422  423  424  464  465  466  506  507  508  548  549  550  590  591  592\n [46]  632  633  634  674  675  676  716  717  718  758  759  760  800  801  802\n [61]  842  843  844  884  885  886  926  927  928  968  969  970 1010 1011 1012\n [76] 1052 1053 1054 1094 1095 1096 1136 1137 1138 1178 1179 1180 1220 1221 1222\n [91] 1262 1263 1264 1304 1305 1306 1346 1347 1348 1388 1389 1390 1430 1431 1432\n[106] 1472 1473 1474 1514 1515 1516 1556 1557 1558 1598 1599 1600 1640 1641 1642\n[121] 1682 1683 1684 1724 1725 1726 1766 1767 1768 1808 1809 1810 1850 1851 1852\n[136] 1892 1893 1894 1934 1935 1936 1976 1977 1978 2018 2019 2020 2060 2061 2062\n[151] 2102 2103 2104\n\n# convince yourself you did it right\n# should these rows be deleted?\ncrime_data[44:46]\n\n[1] \",,National or state crime,,,,,,,\"                                                                                                   \n[2] \",,Violent crime,,,,,,,\"                                                                                                             \n[3] \"Year,Population,Violent crime total,Murder and nonnegligent Manslaughter,Legacy rape /1,Revised rape /2,Robbery,Aggravated assault,\"\n\ncrime_data <- crime_data[-delete_rows]\n\n# extract state labels from data\nstate_labels <- crime_data[str_which(crime_data, \"Estimated crime in \")]\nstate_labels <- str_remove(state_labels, pattern = \"Estimated crime in \")\nstate_label_order <- rep(state_labels, each = n_rows) # repeat n_rows times\n\ncrime_data <- crime_data[-str_which(crime_data, \"Estimated crime\")]\ncrime_data_sep <- read_csv(I(crime_data), col_names = FALSE) |> \n  select(-X6) # remove random extra-comma column\n\n# get column names for later\ncolnames(crime_data_sep) <- c(\"Year\", \n                              \"Population\", \n                              \"Violent_crime_total\",\n                              \"Murder_and_nonnegligent_Manslaughter\",\n                              \"Legacy_rape\",\n                              \"Revised_rape\", \n                              \"Robbery\",\n                              \"Aggravated_assault\")\n# add column names in\ncrime_data_sep <- bind_cols(STATE = state_label_order, crime_data_sep)\n\ncrime_data <- crime_data_sep |>\n  mutate(VARIABLE = \"Viol_crime_count\") |>\n  rename(\"VALUE\" = Violent_crime_total) |>\n  rename(\"YEAR\" = Year) |>\n  select(YEAR,STATE, VARIABLE, VALUE)\n\ncrime_data\n\n# A tibble: 1,938 × 4\n    YEAR STATE   VARIABLE         VALUE\n   <dbl> <chr>   <chr>            <dbl>\n 1  1977 Alabama Viol_crime_count 15293\n 2  1978 Alabama Viol_crime_count 15682\n 3  1979 Alabama Viol_crime_count 15578\n 4  1980 Alabama Viol_crime_count 17320\n 5  1981 Alabama Viol_crime_count 18423\n 6  1982 Alabama Viol_crime_count 17653\n 7  1983 Alabama Viol_crime_count 16471\n 8  1984 Alabama Viol_crime_count 17204\n 9  1985 Alabama Viol_crime_count 18398\n10  1986 Alabama Viol_crime_count 22616\n# … with 1,928 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangling-rtc-laws",
    "href": "content/lectures/11-cs01-data-slides.html#wrangling-rtc-laws",
    "title": "11-cs01-data",
    "section": "Wrangling: RTC Laws",
    "text": "Wrangling: RTC Laws\n\nDAWpaper_p_62 <- DAWpaper[[62]]\nstr(DAWpaper_p_62, nchar.max = 1000) # see data\n\n chr \"                                          Table A1: RTC Adoption Dates\\n\\n         State         Effective Date of RTC Law   Fraction of Year In Effect Year of Passage   RTC Date (Synthetic Controls Analysis)\\n      Alabama                      1975                                                                        1975\\n       Alaska                   10/1/1994                            0.252                                     1995\\n       Arizona                  7/17/1994                            0.460                                     1995\\n      Arkansas                  7/27/1995                            0.433                                     1996\\n     California                    N/A                                                                            0\\n      Colorado                  5/17/2003                            0.627                                     2003\\n    Connecticut                    1970                                \"| __truncated__\n\np_62 <- DAWpaper_p_62 |>\n  str_split(\"\\n\") |>\n  unlist() |>\n  as_tibble() |>\n  slice(-(1:2)) |> \n  rename(RTC = value) |>\n  slice(-c(53:54)) |>  # physical page 60 marking; empty line removal\n  mutate(RTC = str_replace_all(RTC, \"\\\\s{40,}\", \"|N/A|\"),\n         RTC = str_trim(RTC, side = \"left\"),\n         RTC = str_replace_all(RTC, \"\\\\s{2,15}\", \"|\"))\n\nhead(p_62)\n\n# A tibble: 6 × 1\n  RTC                                                                           \n  <chr>                                                                         \n1 State|Effective Date of RTC Law|Fraction of Year In Effect Year of Passage|RT…\n2 Alabama||1975|N/A|1975                                                        \n3 Alaska||10/1/1994||0.252|||1995                                               \n4 Arizona||7/17/1994||0.460|||1995                                              \n5 Arkansas||7/27/1995||0.433|||1996                                             \n6 California||N/A|N/A|0                                                         \n\np_62 <- pull(p_62, RTC) |>\n  str_split( \"\\\\|{1,}\")  # split data on \"|\" symbol\n\n# get the tibble!\np_62 <- as_tibble(do.call(rbind, p_62)) # rbind and not bind_cols here b/c we have no column names yet\n\ncolnames(p_62) <- c(\"STATE\",\n                    \"E_Date_RTC\",\n                    \"Frac_Yr_Eff_Yr_Pass\",\n                    \"RTC_Date_SA\")\n\np_62 <- p_62 |>\n  slice(-c(1, 53:nrow(p_62))) # remove unnecessary rows\n\nRTC <- p_62 |> \n  select(STATE, RTC_Date_SA) |>\n  rename(RTC_LAW_YEAR = RTC_Date_SA) |>\n  mutate(RTC_LAW_YEAR = as.numeric(RTC_LAW_YEAR)) |>\n  mutate(RTC_LAW_YEAR = case_when(RTC_LAW_YEAR == 0 ~ Inf,\n                                  TRUE ~ RTC_LAW_YEAR))"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangling-combining-donohue",
    "href": "content/lectures/11-cs01-data-slides.html#wrangling-combining-donohue",
    "title": "11-cs01-data",
    "section": "Wrangling: Combining! (Donohue)",
    "text": "Wrangling: Combining! (Donohue)\n\n# combine after all that wrangling!\nDONOHUE_DF <- bind_rows(dem_DONOHUE,\n                        ue_rate_data,\n                        poverty_rate_data,\n                        crime_data,\n                        population_data,\n                        ps_data)\nDONOHUE_DF\n\n# A tibble: 20,247 × 4\n    YEAR STATE   VARIABLE                    VALUE\n   <dbl> <chr>   <chr>                       <dbl>\n 1  1977 Alabama Black_Male_15_to_19_years  1.55  \n 2  1977 Alabama Black_Male_20_to_39_years  3.04  \n 3  1977 Alabama Other_Male_15_to_19_years  0.0178\n 4  1977 Alabama Other_Male_20_to_39_years  0.0642\n 5  1977 Alabama White_Male_15_to_19_years  3.58  \n 6  1977 Alabama White_Male_20_to_39_years 11.1   \n 7  1977 Alaska  Black_Male_15_to_19_years  0.163 \n 8  1977 Alaska  Black_Male_20_to_39_years  0.968 \n 9  1977 Alaska  Other_Male_15_to_19_years  1.12  \n10  1977 Alaska  Other_Male_20_to_39_years  2.73  \n# … with 20,237 more rows\n\n# to wide format!\nDONOHUE_DF <- DONOHUE_DF |>\n  pivot_wider(names_from = \"VARIABLE\",\n              values_from = \"VALUE\")\n\n# add in RTC data!\nDONOHUE_DF <- DONOHUE_DF |>\n  left_join(RTC , by = c(\"STATE\")) |>\n  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,\n                              TRUE ~ FALSE)) |>\n drop_na() # drop rows with missing information\n\n# filter to only data where RTC laws were adopted between 1980-2010\n# have crime data pre- and post-adoption this way\nbaseline_year <- min(DONOHUE_DF$YEAR)\ncensoring_year <- max(DONOHUE_DF$YEAR)\n\nDONOHUE_DF <- DONOHUE_DF |>\n  mutate(TIME_0 = baseline_year,\n         TIME_INF = censoring_year) |>\n  filter(RTC_LAW_YEAR > TIME_0)\n\n# calculate violent crime rate; put population/crime on log scale\nDONOHUE_DF <- DONOHUE_DF |>\n  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,\n         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),\n         Population_log = log(Population))\n\nDONOHUE_DF |>\n  slice_sample(n = 10) |>\n  glimpse()\n\nRows: 10\nColumns: 20\n$ YEAR                      <dbl> 2007, 1982, 1986, 2008, 2001, 1994, 1982, 19…\n$ STATE                     <chr> \"South Carolina\", \"Wisconsin\", \"Kentucky\", \"…\n$ Black_Male_15_to_19_years <dbl> 1.3360605, 0.2259944, 0.3777841, 0.2314658, …\n$ Black_Male_20_to_39_years <dbl> 3.7130944, 0.6522053, 1.1693897, 0.7158293, …\n$ Other_Male_15_to_19_years <dbl> 0.12933973, 0.06599873, 0.02546219, 0.214153…\n$ Other_Male_20_to_39_years <dbl> 0.4677877, 0.2201790, 0.1076788, 0.7418817, …\n$ White_Male_15_to_19_years <dbl> 2.329578, 4.280084, 3.786301, 3.309214, 2.85…\n$ White_Male_20_to_39_years <dbl> 9.151731, 15.368907, 15.100435, 12.062884, 1…\n$ Unemployment_rate         <dbl> 5.7, 10.5, 9.4, 3.3, 3.7, 5.1, 15.4, 8.8, 5.…\n$ Poverty_rate              <dbl> 14.1, 9.5, 17.7, 10.6, 8.9, 15.6, 16.2, 18.6…\n$ Viol_crime_count          <dbl> 34630, 9077, 12467, 5570, 30585, 39240, 5980…\n$ Population                <dbl> 4444110, 4728879, 3687821, 1796378, 6397634,…\n$ police_per_100k_lag       <dbl> 373.3256, 251.3704, 189.7326, 279.9522, 326.…\n$ RTC_LAW_YEAR              <dbl> 1997, 2012, 1997, 2007, Inf, 2004, 2001, 199…\n$ RTC_LAW                   <lgl> TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALS…\n$ TIME_0                    <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 1980, 19…\n$ TIME_INF                  <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ Viol_crime_rate_1k        <dbl> 7.792336, 1.919482, 3.380587, 3.100684, 4.78…\n$ Viol_crime_rate_1k_log    <dbl> 2.0531407, 0.6520556, 1.2180494, 1.1316226, …\n$ Population_log            <dbl> 15.30709, 15.36920, 15.12055, 14.40128, 15.6…"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#wrangling-combining-lott",
    "href": "content/lectures/11-cs01-data-slides.html#wrangling-combining-lott",
    "title": "11-cs01-data",
    "section": "Wrangling: Combining! (Lott)",
    "text": "Wrangling: Combining! (Lott)\n\nLOTT_DF <- bind_rows(dem_LOTT,\n                     ue_rate_data,\n                     poverty_rate_data,\n                     crime_data,\n                     population_data,\n                     ps_data) |>\n  pivot_wider(names_from = \"VARIABLE\",\n              values_from = \"VALUE\") |>\n  left_join(RTC , by = c(\"STATE\")) |>\n  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,\n                              TRUE ~ FALSE)) |>\n   drop_na()\n\nbaseline_year <- min(LOTT_DF$YEAR)\ncensoring_year <- max(LOTT_DF$YEAR)\n\nLOTT_DF <- LOTT_DF |>\n  mutate(TIME_0 = baseline_year,\n         TIME_INF = censoring_year) |>\n  filter(RTC_LAW_YEAR > TIME_0)\n\nLOTT_DF <- LOTT_DF |>\n  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,\n         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),\n         Population_log = log(Population))\n\nLOTT_DF\n\n# A tibble: 1,364 × 50\n    YEAR STATE   Black…¹ Black…² Black…³ Black…⁴ Black…⁵ Black…⁶ Black…⁷ Black…⁸\n   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1980 Alaska   0.264    0.443  0.201   0.116   0.0924 0.0264    0.297   0.695\n 2  1980 Arizona  0.287    0.278  0.165   0.119   0.136  0.103     0.311   0.338\n 3  1980 Arkans…  1.82     1.50   0.842   0.634   1.02   1.16      1.81    1.26 \n 4  1980 Califo…  0.780    0.815  0.581   0.394   0.456  0.292     0.808   0.815\n 5  1980 Colora…  0.352    0.388  0.245   0.172   0.164  0.103     0.377   0.467\n 6  1980 Delawa…  1.87     1.68   1.14    0.783   0.952  0.670     1.81    1.36 \n 7  1980 Distri…  6.53     7.54   5.18    3.89    6.10   4.15      6.32    6.40 \n 8  1980 Florida  1.50     1.37   0.912   0.679   0.812  0.604     1.49    1.20 \n 9  1980 Georgia  2.90     2.78   1.85    1.22    1.56   1.35      2.92    2.45 \n10  1980 Hawaii   0.0930   0.215  0.0776  0.0253  0.0197 0.00738   0.180   0.656\n# … with 1,354 more rows, 40 more variables: Black_Male_30_to_39_years <dbl>,\n#   Black_Male_40_to_49_years <dbl>, Black_Male_50_to_64_years <dbl>,\n#   Black_Male_65_years_and_over <dbl>, Other_Female_10_to_19_years <dbl>,\n#   Other_Female_20_to_29_years <dbl>, Other_Female_30_to_39_years <dbl>,\n#   Other_Female_40_to_49_years <dbl>, Other_Female_50_to_64_years <dbl>,\n#   Other_Female_65_years_and_over <dbl>, Other_Male_10_to_19_years <dbl>,\n#   Other_Male_20_to_29_years <dbl>, Other_Male_30_to_39_years <dbl>, …"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#why",
    "href": "content/lectures/11-cs01-data-slides.html#why",
    "title": "11-cs01-data",
    "section": "Why?",
    "text": "Why?\n❓ Why are there different dimensions for LOTT vs DONOHUE??\n\ndim(LOTT_DF)\n\n[1] 1364   50\n\n\n\ndim(DONOHUE_DF)\n\n[1] 1364   20"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#things-to-consider",
    "href": "content/lectures/11-cs01-data-slides.html#things-to-consider",
    "title": "11-cs01-data",
    "section": "Things to Consider",
    "text": "Things to Consider\n\n\nHow RMarkdown documents work\nHow to control what is executed each time you render\nWhat to do with all this after this lecture?"
  },
  {
    "objectID": "content/lectures/11-cs01-data-slides.html#save",
    "href": "content/lectures/11-cs01-data-slides.html#save",
    "title": "11-cs01-data",
    "section": "Save",
    "text": "Save\n\nsave(LOTT_DF, DONOHUE_DF, file = \"data/wrangled/wrangled_data_rtc.rda\")\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html",
    "href": "content/lectures/11-cs01-data.html",
    "title": "11-cs01-data",
    "section": "",
    "text": "Q: Is there ever a time when we should use \\(R^2\\) instead of adjusted R^2 when analyzing a model?\nA: When talking about variance explained of a single model, \\(R^2\\) is great, but when comparing across models, you’ll always want to use adjusted \\(R^2\\)\n\n\nQ: How do we use the data from OCS for our case study? Should we merge the data files?\nA: Excellent question! That’s what today’s lecture is all about. There’s a whole lot of wrangling to do before we can use these data!\n\n\nQ: Do we need to look at the p-value when we do analysis? (Midterm01)\nA: When interpreting a model, no. When doing hypothesis testing (we’ll get there), it is one piece you can look at. A few people did interpret p-values on the midterm, and that’s ok! (But it was not required.)\n\n\nQ: Can we have a system where we can find other students to group with? Like a google form?\nA: Great question! I’ll start a pinned thread on Campuswire so you all can find one another.\n\n\nQ: I think the data seems pretty confusing.\nA: That’s b/c it is! We’ve got a lot of work to do to get it into a usable/understandable format.\n\n\nQ: In what context of data we should use interaction model or main effect model?\nA: Interaction terms should be included when the relationship between one predictor and the outcome varies by another predictor.\n\n\n\n\nDue Dates:\n\nLab 05 due tomorrow (2/17; 11:59 PM)\nmid-course survey (optional for EC) due tomorrow (2/17; 11:59 PM)\nLecture Participation survey “due” after class\n\n\nNotes:\n\nCS01\n\ninstructions posted on website\ninvited to GH repo (accept invitation, please!)\nhave an email with other group mates\ngoal: meet more people in the class & work together\n\nHW03 posted\nReminder to think about final project group mates; thread on campuswire\n\n\n\n\n\n\nBackground\nData Intro\nWrangle\nCombine!"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#right-to-carry-laws1",
    "href": "content/lectures/11-cs01-data.html#right-to-carry-laws1",
    "title": "11-cs01-data",
    "section": "Right To Carry Laws1",
    "text": "Right To Carry Laws1\nRight to Carry (RTC) Laws - “a law that specifies if and how citizens are allowed to have a firearm on their person or nearby (for example, in a citizen’s car) in public.”2"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#rtc-laws-contd",
    "href": "content/lectures/11-cs01-data.html#rtc-laws-contd",
    "title": "11-cs01-data",
    "section": "RTC Laws (cont’d)",
    "text": "RTC Laws (cont’d)\n\n\nThe Second Amendment to the United States Constitution guarantees the right to “keep and bear arms”. The amendment was ratified in 1791 as part of the Bill of Rights.\nThere are no federal laws about carrying firearms in public.\nThese laws are created and enforced at the US state level. States vary greatly in their laws about the right to carry firearms.\nSome require extensive effort to obtain a permit to legally carry a firearm, while other states require very minimal effort to do so. An increasing number of states do not require permits at all."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#rtc-laws-across-the-us",
    "href": "content/lectures/11-cs01-data.html#rtc-laws-across-the-us",
    "title": "11-cs01-data",
    "section": "RTC Laws Across the US",
    "text": "RTC Laws Across the US"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#rtc-laws-across-the-us-1",
    "href": "content/lectures/11-cs01-data.html#rtc-laws-across-the-us-1",
    "title": "11-cs01-data",
    "section": "RTC Laws Across the US",
    "text": "RTC Laws Across the US"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#the-data-source",
    "href": "content/lectures/11-cs01-data.html#the-data-source",
    "title": "11-cs01-data",
    "section": "The Data: Source",
    "text": "The Data: Source\nTwo contradictory analyses:\n\nJohn J. Donohue et al., Right‐to‐Carry Laws and Violent Crime: A Comprehensive Assessment Using Panel Data and a State‐Level Synthetic Control Analysis. Journal of Empirical Legal Studies, 16,2 (2019).\nDavid B. Mustard & John Lott. Crime, Deterrence, and Right-to-Carry Concealed Handguns. Coase-Sandor Institute for Law & Economics Working Paper No. 41, (1996)."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#the-data-1",
    "href": "content/lectures/11-cs01-data.html#the-data-1",
    "title": "11-cs01-data",
    "section": "The Data",
    "text": "The Data"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#two-analyses",
    "href": "content/lectures/11-cs01-data.html#two-analyses",
    "title": "11-cs01-data",
    "section": "Two Analyses",
    "text": "Two Analyses"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#limitations",
    "href": "content/lectures/11-cs01-data.html#limitations",
    "title": "11-cs01-data",
    "section": "Limitations",
    "text": "Limitations\n\n\nThe analyses differed in variables used; we will not be recreating either analysis in full\nWe’ll account for either the adoption or lack of adoption of a permissive right-to-carry law in each state; we will not account for differences in the level of permissiveness of the laws.\nRace is included here (as it was in initial analysis); however, any association between demographic variables (indicating the proportion of the population from specific race and age groups) and violent crime does not necessarily indicate that the two are linked causally."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#packages",
    "href": "content/lectures/11-cs01-data.html#packages",
    "title": "11-cs01-data",
    "section": "Packages",
    "text": "Packages\n\nlibrary(OCSdata) # will need to be installed\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(readxl)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#raw-data",
    "href": "content/lectures/11-cs01-data.html#raw-data",
    "title": "11-cs01-data",
    "section": "Raw Data",
    "text": "Raw Data\nThere are a whole bunch of different data files we’ll be using…\n\n# only get the data once\nOCSdata::load_raw_data(\"ocs-bp-RTC-wrangling\", outpath = '.')\n\n\ncreates a “data” sub-directory in your current working directory (if it does not already exist)\ncreates a “raw” sub-directory within “data”; contains the directories with the data\n\n👉 Your Turn: Load the data into RStudio. It will take a while…so just let it get started."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#the-goal",
    "href": "content/lectures/11-cs01-data.html#the-goal",
    "title": "11-cs01-data",
    "section": "The Goal",
    "text": "The Goal\nGet two datasets (Lott, Donohue) that contain demographic, population, police staffing, unemployment, violent crime, RTC, and poverty information at the state level across time.\n\n❓ What would be the tidy way to store these data?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#your-turn",
    "href": "content/lectures/11-cs01-data.html#your-turn",
    "title": "11-cs01-data",
    "section": "Your Turn",
    "text": "Your Turn\n🧠 Take a look in one of the data folders, open at least one of the data files to view it, and try to get a sense of the type of information contained within it.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#demographic-population-data",
    "href": "content/lectures/11-cs01-data.html#demographic-population-data",
    "title": "11-cs01-data",
    "section": "Demographic & Population Data",
    "text": "Demographic & Population Data\n\nCodeData\n\n\n\ndem_77_79 <- read_csv(\"data/raw/Demographics/Decade_1970/pe-19.csv\", skip = 5)\n\ndem_80_89 <- list.files(recursive = TRUE,\n                  path = \"data/raw/Demographics/Decade_1980\",\n                  pattern = \"*.csv\",\n                  full.names = TRUE) |> \n  purrr::map(~read_csv(., skip=5))\n\ndem_90_99 <- list.files(recursive = TRUE,\n                  path = \"data/raw/Demographics/Decade_1990\",\n                  pattern = \"*.txt\",\n                  full.names = TRUE) |> \n  map(~read_table2(., skip = 14))\n\ndem_00_10 <- list.files(recursive = TRUE,\n                  path = \"data/raw/Demographics/Decade_2000\",\n                  pattern = \"*.csv\",\n                   full.names = TRUE) |> \n   map(~read_csv(.))\n\nSource: US Census Bureau Data\n\n\n\nglimpse(dem_00_10[[1]])\n\nRows: 62,244\nColumns: 21\n$ REGION            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ DIVISION          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ STATE             <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ NAME              <chr> \"United States\", \"United States\", \"United States\", \"…\n$ SEX               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ORIGIN            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ RACE              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ AGEGRP            <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ ESTIMATESBASE2000 <dbl> 281424600, 19176154, 20549855, 20528425, 20218782, 1…\n$ POPESTIMATE2000   <dbl> 282162411, 19178293, 20463852, 20637696, 20294955, 1…\n$ POPESTIMATE2001   <dbl> 284968955, 19298217, 20173362, 20978678, 20456284, 1…\n$ POPESTIMATE2002   <dbl> 287625193, 19429192, 19872417, 21261421, 20610370, 2…\n$ POPESTIMATE2003   <dbl> 290107933, 19592446, 19620851, 21415353, 20797166, 2…\n$ POPESTIMATE2004   <dbl> 292805298, 19785885, 19454237, 21411680, 21102552, 2…\n$ POPESTIMATE2005   <dbl> 295516599, 19917400, 19389067, 21212579, 21486214, 2…\n$ POPESTIMATE2006   <dbl> 298379912, 19938883, 19544688, 21033138, 21807709, 2…\n$ POPESTIMATE2007   <dbl> 301231207, 20125962, 19714611, 20841042, 22067816, 2…\n$ POPESTIMATE2008   <dbl> 304093966, 20271127, 19929602, 20706655, 22210880, 2…\n$ POPESTIMATE2009   <dbl> 306771529, 20244518, 20182499, 20660564, 22192810, 2…\n$ CENSUS2010POP     <dbl> 308745538, 20201362, 20348657, 20677194, 22040343, 2…\n$ POPESTIMATE2010   <dbl> 309349689, 20200529, 20382409, 20694011, 21959087, 2…"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#state-fips-codes",
    "href": "content/lectures/11-cs01-data.html#state-fips-codes",
    "title": "11-cs01-data",
    "section": "State FIPS Codes",
    "text": "State FIPS Codes\n\nImageCodeDataWrangling\n\n\n\n\n\n\nSTATE_FIPS <- readxl::read_xls(\"data/raw/State_FIPS_codes/state-geocodes-v2014.xls\", skip = 5)\n\n\n\n\nglimpse(STATE_FIPS)\n\nRows: 64\nColumns: 4\n$ Region          <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",…\n$ Division        <chr> \"0\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\",…\n$ `State\\n(FIPS)` <chr> \"00\", \"00\", \"09\", \"23\", \"25\", \"33\", \"44\", \"50\", \"00\", …\n$ Name            <chr> \"Northeast Region\", \"New England Division\", \"Connectic…\n\n\n\n\n\nSTATE_FIPS <- STATE_FIPS |>\n  rename(STATEFP = `State\\n(FIPS)`,\n         STATE = Name) |>\n  select(STATEFP, STATE) |>\n  filter(STATEFP != \"00\")\n\nSTATE_FIPS\n\n# A tibble: 51 × 2\n   STATEFP STATE        \n   <chr>   <chr>        \n 1 09      Connecticut  \n 2 23      Maine        \n 3 25      Massachusetts\n 4 33      New Hampshire\n 5 44      Rhode Island \n 6 50      Vermont      \n 7 34      New Jersey   \n 8 36      New York     \n 9 42      Pennsylvania \n10 17      Illinois     \n# … with 41 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#police-staffing-data",
    "href": "content/lectures/11-cs01-data.html#police-staffing-data",
    "title": "11-cs01-data",
    "section": "Police Staffing Data",
    "text": "Police Staffing Data\n\nCodeData\n\n\nThere’s an issue currently with the ps_data file from OCS, so we’ll use this file instead:\n\nps_data <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/pe_1960_2018.csv\")\n\n\n\n\nglimpse(ps_data)\n\nRows: 2,242\nColumns: 3\n$ data_year           <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 19…\n$ state_abbr          <chr> \"AK\", \"AL\", \"AR\", \"AS\", \"AZ\", \"CA\", \"CO\", \"CT\", \"C…\n$ officer_state_total <dbl> 544, 7380, 3344, 0, 6414, 65596, 7337, 6051, 0, 47…"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#unemployment-data",
    "href": "content/lectures/11-cs01-data.html#unemployment-data",
    "title": "11-cs01-data",
    "section": "Unemployment Data",
    "text": "Unemployment Data\n\nImageCodeDataWrangle\n\n\n\n\n\nunemployment\n\n\n\n\n\nue_rate_data <- list.files(recursive = TRUE,\n                          path = \"data/raw/Unemployment\",\n                          pattern = \"*.xlsx\",\n                          full.names = TRUE) |> \nmap(~read_xlsx(., skip = 10))\n\nue_rate_names <- list.files(recursive = TRUE,\n                          path = \"data/raw/Unemployment\",\n                          pattern = \"*.xlsx\",\n                          full.names = TRUE) |>\nmap(~read_xlsx(., range = \"B4:B6\")) |>\n  map(c(1,2)) |>\nunlist()\n\nnames(ue_rate_data) <- ue_rate_names\n\n\n\n\nhead(ue_rate_data)[1]\n\n$Alabama\n# A tibble: 44 × 14\n    Year   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  1977   7.5   9     7.7   7.2   6.8   8.6   8     7.8   6.7   6.3   6.3   6  \n 2  1978   7.1   6.9   6.2   5.4   5.1   6.9   6.7   6.7   6.5   6.3   6.3   6.5\n 3  1979   6.7   7.5   6.9   6.6   6.4   8.4   7.7   7.8   7.1   7.2   6.9   6.7\n 4  1980   7.7   7.8   7.4   7.4   8.4   9.7  10.4  10.3   9.3   9.6   9.4   9  \n 5  1981  10    10.3   9.5   9.1   9.4  11.1  10.4  10.9  10.8  11.7  11.5  11.8\n 6  1982  13.2  13.2  12.9  12.6  12.8  14.5  14.7  14.8  14.7  15.1  15.4  15.3\n 7  1983  16    16    14.5  13.7  13.3  14.6  13.9  13.8  13.2  12.8  12.1  11.8\n 8  1984  12.5  12.4  11.4  10.8  10.1  11.3  11.5  11.3  10.8  10.2   9.7  10.1\n 9  1985  10.7  10.5   9.8   8.7   8.4   9.6   9.2   8.8   8.6   8.6   8.4   8.7\n10  1986   9.3  10.4  10.1   9.4   9.4  10.5   9.7   9.6   9.7   9.7   9.6   9  \n# … with 34 more rows, and 1 more variable: Annual <dbl>\n\n\n\n\n\nue_rate_data <- ue_rate_data |>\n  map_df(bind_rows, .id = \"STATE\") |>\n  select(STATE, Year, Annual) |>\n  rename(\"YEAR\" = Year,\n         \"VALUE\" = Annual) |>\n  mutate(VARIABLE = \"Unemployment_rate\")\n\nue_rate_data\n\n# A tibble: 2,244 × 4\n   STATE    YEAR VALUE VARIABLE         \n   <chr>   <dbl> <dbl> <chr>            \n 1 Alabama  1977   7.3 Unemployment_rate\n 2 Alabama  1978   6.4 Unemployment_rate\n 3 Alabama  1979   7.2 Unemployment_rate\n 4 Alabama  1980   8.9 Unemployment_rate\n 5 Alabama  1981  10.6 Unemployment_rate\n 6 Alabama  1982  14.1 Unemployment_rate\n 7 Alabama  1983  13.8 Unemployment_rate\n 8 Alabama  1984  11   Unemployment_rate\n 9 Alabama  1985   9.2 Unemployment_rate\n10 Alabama  1986   9.7 Unemployment_rate\n# … with 2,234 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#poverty-data",
    "href": "content/lectures/11-cs01-data.html#poverty-data",
    "title": "11-cs01-data",
    "section": "Poverty Data",
    "text": "Poverty Data\n\nCodeData\n\n\n\npoverty_rate_data <- read_xls(\"data/raw/Poverty/hstpov21.xls\", skip=2)\n\n\n\n\nhead(poverty_rate_data)\n\n# A tibble: 6 × 6\n  `NOTE: Number in thousands.` ...2  ...3   ...4              ...5         ...6 \n  <chr>                        <chr> <chr>  <chr>             <chr>        <chr>\n1 2018                         <NA>  <NA>    <NA>             <NA>          <NA>\n2 STATE                        Total Number \"Standard\\nerror\" Percent      \"Sta…\n3 Alabama                      4877  779    \"65\"              16           \"1.3\"\n4 Alaska                       720   94     \"9\"               13.1         \"1.2\"\n5 Arizona                      7241  929    \"80\"              12.80000000… \"1.1…\n6 Arkansas                     2912  462    \"38\"              15.9         \"1.3\"\n\n\n\n\n\nSource: US Census Bureau Data"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#violent-crime-data",
    "href": "content/lectures/11-cs01-data.html#violent-crime-data",
    "title": "11-cs01-data",
    "section": "Violent Crime Data",
    "text": "Violent Crime Data\n\nCodeData\n\n\n\ncrime_data <- read_lines(\"data/raw/Crime/CrimeStatebyState.csv\",\n                         skip = 2, \n                         skip_empty_rows = TRUE)\n\nDue to spaces and / in the column names, read_lines() from the readr package works better than read_csv()\n\n\n\nhead(crime_data)\n\n[1] \"Estimated crime in Alabama\"                                                                                                         \n[2] \",,National or state crime,,,,,,,\"                                                                                                   \n[3] \",,Violent crime,,,,,,,\"                                                                                                             \n[4] \"Year,Population,Violent crime total,Murder and nonnegligent Manslaughter,Legacy rape /1,Revised rape /2,Robbery,Aggravated assault,\"\n[5] \"1977,   3690000,      15293,         524,         929,,       3572,      10268 \"                                                    \n[6] \"1978,   3742000,      15682,         499,         954,,       3708,      10521 \""
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#right-to-carry-data",
    "href": "content/lectures/11-cs01-data.html#right-to-carry-data",
    "title": "11-cs01-data",
    "section": "Right-To-Carry Data",
    "text": "Right-To-Carry Data\n\nCodeData\n\n\n\nDAWpaper <- pdf_text(\"data/raw/w23510.pdf\")\n\n\n\n\nhead(DAWpaper[1])\n\n[1] \"                              NBER WORKING PAPER SERIES\\n\\n\\n\\n                  RIGHT-TO-CARRY LAWS AND VIOLENT CRIME:\\n             A COMPREHENSIVE ASSESSMENT USING PANEL DATA AND\\n                 A STATE-LEVEL SYNTHETIC CONTROL ANALYSIS\\n\\n                                        John J. Donohue\\n                                          Abhay Aneja\\n                                         Kyle D. Weber\\n\\n                                      Working Paper 23510\\n                              http://www.nber.org/papers/w23510\\n\\n\\n                     NATIONAL BUREAU OF ECONOMIC RESEARCH\\n                              1050 Massachusetts Avenue\\n                                 Cambridge, MA 02138\\n                           June 2017, Revised November 2018\\n\\n\\n\\nPreviously circulated as \\\"Right-to-Carry Laws and Violent Crime: A Comprehensive Assessment\\nUsing Panel Data and a State-Level Synthetic Controls Analysis.\\\" We thank Dan Ho, Stefano\\nDellaVigna, Rob Tibshirani, Trevor Hastie, StefanWager, Jeff Strnad, and participants at the\\n2011 Conference of Empirical Legal Studies (CELS), 2012 American Law and Economics\\nAssociation (ALEA) Annual Meeting, 2013 Canadian Law and Economics Association (CLEA)\\nAnnual Meeting, 2015 NBER Summer Institute (Crime), and the Stanford Law School faculty\\nworkshop for their comments and helpful suggestions. Financial support was provided by\\nStanford Law School. We are indebted to Alberto Abadie, Alexis Diamond, and Jens\\nHainmueller for their work developing the synthetic control algorithm and programming the Stata\\nmodule used in this paper and for their helpful comments. The authors would also like to thank\\nAlex Albright, Andrew Baker, Jacob Dorn, Bhargav Gopal, Crystal Huang, Mira Korb, Haksoo\\nLee, Isaac Rabbani, Akshay Rao, Vikram Rao, Henrik Sachs and Sidharth Sah who provided\\nexcellent research assistance, as well as Addis O’Connor and Alex Chekholko at the Research\\nComputing division of Stanford’s Information Technology Services for their technical support.\\nThe views expressed herein are those of the author and do not necessarily reflect the views of the\\nNational Bureau of Economic Research.\\n\\nNBER working papers are circulated for discussion and comment purposes. They have not been\\npeer-reviewed or been subject to the review by the NBER Board of Directors that accompanies\\nofficial NBER publications.\\n\\n© 2017 by John J. Donohue, Abhay Aneja, and Kyle D. Weber. All rights reserved. Short\\nsections of text, not to exceed two paragraphs, may be quoted without explicit permission\\nprovided that full credit, including © notice, is given to the source.\\n\""
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#save-imported-data",
    "href": "content/lectures/11-cs01-data.html#save-imported-data",
    "title": "11-cs01-data",
    "section": "Save (Imported) Data",
    "text": "Save (Imported) Data\n\nsave(dem_77_79, dem_80_89, dem_90_99, dem_00_10, \n     STATE_FIPS, \n     ps_data, \n     ue_rate_data, \n     poverty_rate_data,\n     crime_data,\n     DAWpaper, file = \"data/imported_data_rtc.rda\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangle-demo-data",
    "href": "content/lectures/11-cs01-data.html#wrangle-demo-data",
    "title": "11-cs01-data",
    "section": "Wrangle: Demo Data",
    "text": "Wrangle: Demo Data\n\n77-7980s90s00s\n\n\n\ndem_77_79 <- dem_77_79 |>\n  rename(\"race_sex\" =`Race/Sex Indicator`) |>\n  mutate(SEX = str_extract(race_sex, \"male|female\"),\n        RACE = str_extract(race_sex, \"Black|White|Other\"))|>\n  select(-`FIPS State Code`, -`race_sex`) |>\n  rename(\"YEAR\" = `Year of Estimate`,\n        \"STATE\" = `State Name`) |>\n  filter(YEAR %in% 1977:1979)\n\ndem_77_79 <- dem_77_79 |>\n  pivot_longer(cols=contains(\"years\"),\n               names_to = \"AGE_GROUP\",\n               values_to = \"SUB_POP\")\n\nglimpse(dem_77_79)\n\nRows: 16,524\nColumns: 6\n$ YEAR      <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, …\n$ STATE     <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alab…\n$ SEX       <chr> \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"male\", \"mal…\n$ RACE      <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"White…\n$ AGE_GROUP <chr> \"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to 19…\n$ SUB_POP   <dbl> 98814, 113365, 123107, 135343, 126053, 111547, 100674, 81038…\n\n\n\n\n\ndem_80_89 <- dem_80_89 |>\n  map_df(bind_rows)\n\ndem_80_89 <- dem_80_89 |>\n  rename(\"race_sex\" =`Race/Sex Indicator`) |>\n  mutate(SEX = str_extract(race_sex, \"male|female\"),\n        RACE = str_extract(race_sex, \"Black|White|Other\"))|>\n  select( -`race_sex`) |>\n  rename(\"YEAR\" = `Year of Estimate`) |> \n  rename(\"STATEFP_temp\" = \"FIPS State and County Codes\") |>\n  mutate(STATEFP = str_sub(STATEFP_temp, start = 1, end = 2)) |>\n    left_join(STATE_FIPS, by = \"STATEFP\") |>\n  select(-STATEFP)\n\ndem_80_89 <- dem_80_89 |>\n  pivot_longer(cols=contains(\"years\"),\n               names_to = \"AGE_GROUP\",\n               values_to = \"SUB_POP_temp\") |>\n  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) |>\n  summarize(SUB_POP = sum(SUB_POP_temp), .groups=\"drop\")\n\ndem_80_89\n\n# A tibble: 55,080 × 6\n    YEAR STATE   AGE_GROUP      SEX    RACE  SUB_POP\n   <dbl> <chr>   <chr>          <chr>  <chr>   <dbl>\n 1  1980 Alabama 10 to 14 years female Black   50108\n 2  1980 Alabama 10 to 14 years female Other     805\n 3  1980 Alabama 10 to 14 years female White  109066\n 4  1980 Alabama 10 to 14 years male   Black   50768\n 5  1980 Alabama 10 to 14 years male   Other     826\n 6  1980 Alabama 10 to 14 years male   White  115988\n 7  1980 Alabama 15 to 19 years female Black   58428\n 8  1980 Alabama 15 to 19 years female Other     743\n 9  1980 Alabama 15 to 19 years female White  126783\n10  1980 Alabama 15 to 19 years male   Black   56808\n# … with 55,070 more rows\n\n\n\n\n\ndem_90_99 <- dem_90_99 |>\n  map_df(bind_rows)\n\ncolnames(dem_90_99) <- c(\"YEAR\", \"STATEFP\", \"Age\", \"NH_W_M\", \"NH_W_F\", \"NH_B_M\",\n                         \"NH_B_F\", \"NH_AIAN_M\", \"NH_AIAN_F\", \"NH_API_M\", \"NH_API_F\",\n                         \"H_W_M\", \"H_W_F\", \"H_B_M\", \"H_B_F\", \"H_AIAN_M\", \"H_AIAN_F\",\n                         \"H_API_M\", \"H_API_F\")\n\ndem_90_99 <- dem_90_99 |>\n  drop_na() |>\n  mutate(W_M = NH_W_M + H_W_M, W_F = NH_W_F + H_W_F,\n         B_M = NH_B_M + H_B_M, B_F = NH_B_F + H_B_F,\n         AIAN_M = NH_AIAN_M + H_AIAN_M, AIAN_F = NH_AIAN_F + H_AIAN_F,\n         API_M = NH_API_M + H_API_M, API_F = NH_API_F + H_API_F) |>\n  select(-starts_with(\"NH_\"), -starts_with(\"H_\"))\n\ndem_90_99 <- dem_90_99 |>\n  mutate(AGE_GROUP = cut(Age,\n                         breaks = seq(0,90, by=5),\n                         right = FALSE, labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP))) |>\n  select(-Age) |>\n  pivot_longer(cols = c(starts_with(\"W_\"),\n                        starts_with(\"B_\"),\n                        starts_with(\"AIAN_\"),\n                        starts_with(\"API_\")),\n               names_to = \"RACE\",\n               values_to = \"SUB_POP_temp\") |>\n  mutate(SEX = case_when(str_detect(RACE, \"_M\") ~ \"Male\",\n                         TRUE ~ \"Female\"),\n         RACE = case_when(str_detect(RACE, \"W_\") ~ \"White\",\n                          str_detect(RACE, \"B_\") ~ \"Black\",\n                          TRUE ~ \"Other\"))\n\ndem_90_99 <- dem_90_99 |>\n  left_join(STATE_FIPS, by = \"STATEFP\") |>\n  select(-STATEFP) |>\n  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) |>\n  summarize(SUB_POP = sum(SUB_POP_temp), .groups=\"drop\")\n\nglimpse(dem_90_99)\n\nRows: 55,080\nColumns: 6\n$ YEAR      <dbl> 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, …\n$ STATE     <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alab…\n$ AGE_GROUP <fct> Under 5 years, Under 5 years, Under 5 years, Under 5 years, …\n$ SEX       <chr> \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Femal…\n$ RACE      <chr> \"Black\", \"Other\", \"White\", \"Black\", \"Other\", \"White\", \"Black…\n$ SUB_POP   <dbl> 45377, 1406, 92380, 46635, 1360, 98524, 46067, 1698, 92530, …\n\n\n\n\n\ndem_00_10 <- dem_00_10 |>\n  map_df(bind_rows)\n\ndem_00_10 <- dem_00_10 |>\n  select(-ESTIMATESBASE2000,-CENSUS2010POP) |>\n  filter(NAME != \"United States\",\n         SEX != 0,\n         RACE != 0,\n         AGEGRP != 0, \n         ORIGIN == 0) |>\n  select(-REGION, -DIVISION, -ORIGIN, -STATE) |>\n  rename(\"STATE\" = NAME,\n         \"AGE_GROUP\" = AGEGRP)\n\ndem_00_10 <- dem_00_10 |>\n  mutate(SEX = factor(SEX, levels = 1:2, labels = c(\"Male\", \"Female\")),\n         RACE = factor(RACE, levels = 1:6, labels = c(\"White\", \"Black\", rep(\"Other\",4))),\n         AGE_GROUP = factor(AGE_GROUP, levels = 1:18,\n                            labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP)))\n\ndem_00_10 <- dem_00_10 |>\n  pivot_longer(cols=contains(\"ESTIMATE\"), names_to = \"YEAR\", values_to = \"SUB_POP_temp\") |>\n   mutate(YEAR = str_sub(YEAR, start=-4),\n          YEAR = as.numeric(YEAR)) |> \n  group_by(YEAR, AGE_GROUP, STATE, SEX, RACE) |>\n  summarize(SUB_POP = sum(SUB_POP_temp), .groups = \"drop\")\n                            \nglimpse(dem_00_10)\n\nRows: 60,588\nColumns: 6\n$ YEAR      <dbl> 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, …\n$ AGE_GROUP <fct> Under 5 years, Under 5 years, Under 5 years, Under 5 years, …\n$ STATE     <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alab…\n$ SEX       <fct> Male, Male, Male, Female, Female, Female, Male, Male, Male, …\n$ RACE      <fct> White, Black, Other, White, Black, Other, White, Black, Othe…\n$ SUB_POP   <dbl> 99527, 46595, 4487, 94473, 45672, 4431, 14765, 1039, 8572, 1…"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangle-population-data",
    "href": "content/lectures/11-cs01-data.html#wrangle-population-data",
    "title": "11-cs01-data",
    "section": "Wrangle: Population Data",
    "text": "Wrangle: Population Data\n\n77-7980s90s00s\n\n\n\npop_77_79 <- dem_77_79 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\") \n\npop_77_79 \n\n# A tibble: 153 × 3\n    YEAR STATE                 TOT_POP\n   <dbl> <chr>                   <dbl>\n 1  1977 Alabama               3782571\n 2  1977 Alaska                 397220\n 3  1977 Arizona               2427296\n 4  1977 Arkansas              2207195\n 5  1977 California           22350332\n 6  1977 Colorado              2696179\n 7  1977 Connecticut           3088745\n 8  1977 Delaware               594815\n 9  1977 District of Columbia   681766\n10  1977 Florida               8888806\n# … with 143 more rows\n\n\n\n\n\npop_80_89 <- dem_80_89 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\") \n\n\n\n\npop_90_99 <- dem_90_99 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\")\n\n\n\n\npop_00_10 <- dem_00_10 |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\")"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#combine-demo-population",
    "href": "content/lectures/11-cs01-data.html#combine-demo-population",
    "title": "11-cs01-data",
    "section": "Combine: Demo + Population",
    "text": "Combine: Demo + Population\n\n77-7980s90s00sIdea\n\n\n\ndem_77_79 <- dem_77_79 |>\n  left_join(pop_77_79, by = c(\"YEAR\", \"STATE\")) |> \n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP) |>\n  mutate(SEX = str_to_title(SEX))\n\n\ndem_77_79\n\n# A tibble: 16,524 × 6\n    YEAR STATE   SEX   RACE  AGE_GROUP      PERC_SUB_POP\n   <dbl> <chr>   <chr> <chr> <chr>                 <dbl>\n 1  1977 Alabama Male  White Under 5 years          2.61\n 2  1977 Alabama Male  White 5 to 9 years           3.00\n 3  1977 Alabama Male  White 10 to 14 years         3.25\n 4  1977 Alabama Male  White 15 to 19 years         3.58\n 5  1977 Alabama Male  White 20 to 24 years         3.33\n 6  1977 Alabama Male  White 25 to 29 years         2.95\n 7  1977 Alabama Male  White 30 to 34 years         2.66\n 8  1977 Alabama Male  White 35 to 39 years         2.14\n 9  1977 Alabama Male  White 40 to 44 years         1.98\n10  1977 Alabama Male  White 45 to 49 years         2.02\n# … with 16,514 more rows\n\n\n\n\n\ndem_80_89 <- dem_80_89 |>\n  left_join(pop_80_89, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP) |>\n  mutate(SEX = str_to_title(SEX))\n\n\n\n\ndem_90_99 <- dem_90_99 |>\n  left_join(pop_90_99, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP)\n\ndem_90_99\n\n# A tibble: 55,080 × 6\n    YEAR STATE   AGE_GROUP     SEX    RACE  PERC_SUB_POP\n   <dbl> <chr>   <fct>         <chr>  <chr>        <dbl>\n 1  1990 Alabama Under 5 years Female Black       1.12  \n 2  1990 Alabama Under 5 years Female Other       0.0347\n 3  1990 Alabama Under 5 years Female White       2.28  \n 4  1990 Alabama Under 5 years Male   Black       1.15  \n 5  1990 Alabama Under 5 years Male   Other       0.0336\n 6  1990 Alabama Under 5 years Male   White       2.43  \n 7  1990 Alabama 5 to 9 years  Female Black       1.14  \n 8  1990 Alabama 5 to 9 years  Female Other       0.0419\n 9  1990 Alabama 5 to 9 years  Female White       2.29  \n10  1990 Alabama 5 to 9 years  Male   Black       1.16  \n# … with 55,070 more rows\n\n\n\n\n\ndem_00_10 <- dem_00_10 |>\n  left_join(pop_00_10, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP)\n\ndem_00_10\n\n# A tibble: 60,588 × 6\n    YEAR AGE_GROUP     STATE   SEX    RACE  PERC_SUB_POP\n   <dbl> <fct>         <chr>   <fct>  <fct>        <dbl>\n 1  2000 Under 5 years Alabama Male   White       2.24  \n 2  2000 Under 5 years Alabama Male   Black       1.05  \n 3  2000 Under 5 years Alabama Male   Other       0.101 \n 4  2000 Under 5 years Alabama Female White       2.12  \n 5  2000 Under 5 years Alabama Female Black       1.03  \n 6  2000 Under 5 years Alabama Female Other       0.0995\n 7  2000 Under 5 years Alaska  Male   White       2.35  \n 8  2000 Under 5 years Alaska  Male   Black       0.165 \n 9  2000 Under 5 years Alaska  Male   Other       1.37  \n10  2000 Under 5 years Alaska  Female White       2.26  \n# … with 60,578 more rows\n\n\n\n\n❗ This would be a good part of the code to write a user-defined function…"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#aside-udf",
    "href": "content/lectures/11-cs01-data.html#aside-udf",
    "title": "11-cs01-data",
    "section": "Aside: UDF",
    "text": "Aside: UDF\n\nThe WhatThe HowExampleUsing the function\n\n\n\nUser-defined functions (UDFs) are functions you write to make your code cleaner\nAny time you copy+paste very similar code, think to yourself…I should make this a function!\n\n\n\nThe general syntax for a function in R is:\n\nfunction_name <- function(parameters){\n  # code to carry out\n  # using the parameters\n}\n\nNote: by default the last object created within the function is returned from the function\n\n\n\ncombine_demo_pop <- function(df_dem, df_pop){\n  df_dem <- df_dem |>\n  group_by(YEAR, STATE) |>\n  summarize(TOT_POP = sum(SUB_POP), .groups = \"drop\")\n  \n  df_dem |>\n  left_join(df_pop, by = c(\"YEAR\", \"STATE\")) |>\n  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) |>\n  select(-SUB_POP, -TOT_POP) |>\n  mutate(SEX = str_to_title(SEX))\n}\n\n\n\n\ncombined_df <- combine_demo_pop(dem_00_10, pop_00_10)\n\n\nNote: if you’ve already combined the data into dem_00_10, this would not work.\nCleaning up/improving code in your case studies is encouraged!"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#combine-demopop-across-decades",
    "href": "content/lectures/11-cs01-data.html#combine-demopop-across-decades",
    "title": "11-cs01-data",
    "section": "Combine: Demo/Pop Across Decades",
    "text": "Combine: Demo/Pop Across Decades\n\ndem <- bind_rows(dem_77_79,\n                 dem_80_89,\n                 dem_90_99,\n                 dem_00_10)\n\nglimpse(dem)\n\nRows: 187,272\nColumns: 6\n$ YEAR         <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 197…\n$ STATE        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"A…\n$ SEX          <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n$ RACE         <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"Wh…\n$ AGE_GROUP    <chr> \"Under 5 years\", \"5 to 9 years\", \"10 to 14 years\", \"15 to…\n$ PERC_SUB_POP <dbl> 2.6123502, 2.9970356, 3.2545853, 3.5780690, 3.3324688, 2.…"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#demographic-data-donohue",
    "href": "content/lectures/11-cs01-data.html#demographic-data-donohue",
    "title": "11-cs01-data",
    "section": "Demographic Data (Donohue)",
    "text": "Demographic Data (Donohue)\n\nDONOHUE_AGE_GROUPS <- c(\"15 to 19 years\",\n                        \"20 to 24 years\",\n                        \"25 to 29 years\",\n                        \"30 to 34 years\",\n                        \"35 to 39 years\")\n\ndem_DONOHUE <- dem |>\n  filter(AGE_GROUP %in% DONOHUE_AGE_GROUPS,\n               SEX == \"Male\") |>\n  mutate(AGE_GROUP = fct_collapse(AGE_GROUP, \"20 to 39 years\"=c(\"20 to 24 years\",\n                                                                \"25 to 29 years\",\n                                                                \"30 to 34 years\",\n                                                                \"35 to 39 years\")),\n         AGE_GROUP = str_replace_all(string = AGE_GROUP, \n                                     pattern = \" \", \n                                     replacement = \"_\")) |>\n  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) |>\n  summarize(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = \"drop\") |>\n  unite(col = \"VARIABLE\", RACE, SEX, AGE_GROUP, sep = \"_\") |>\n  rename(\"VALUE\" = PERC_SUB_POP)\n\ndem_DONOHUE\n\n# A tibble: 10,404 × 4\n    YEAR STATE   VARIABLE                    VALUE\n   <dbl> <chr>   <chr>                       <dbl>\n 1  1977 Alabama Black_Male_15_to_19_years  1.55  \n 2  1977 Alabama Black_Male_20_to_39_years  3.04  \n 3  1977 Alabama Other_Male_15_to_19_years  0.0178\n 4  1977 Alabama Other_Male_20_to_39_years  0.0642\n 5  1977 Alabama White_Male_15_to_19_years  3.58  \n 6  1977 Alabama White_Male_20_to_39_years 11.1   \n 7  1977 Alaska  Black_Male_15_to_19_years  0.163 \n 8  1977 Alaska  Black_Male_20_to_39_years  0.968 \n 9  1977 Alaska  Other_Male_15_to_19_years  1.12  \n10  1977 Alaska  Other_Male_20_to_39_years  2.73  \n# … with 10,394 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#demographic-data-lott",
    "href": "content/lectures/11-cs01-data.html#demographic-data-lott",
    "title": "11-cs01-data",
    "section": "Demographic Data (Lott)",
    "text": "Demographic Data (Lott)\n\nLOTT_AGE_GROUPS_NULL <- c(\"Under 5 years\",\n                          \"5 to 9 years\")\n\ndem_LOTT <- dem |>\n  filter(!(AGE_GROUP %in% LOTT_AGE_GROUPS_NULL) )|>\n  mutate(AGE_GROUP = fct_collapse(AGE_GROUP,\n                                  \"10 to 19 years\"=c(\"10 to 14 years\", \"15 to 19 years\"),\n                                  \"20 to 29 years\"=c(\"20 to 24 years\", \"25 to 29 years\"),\n                                  \"30 to 39 years\"=c(\"30 to 34 years\", \"35 to 39 years\"),\n                                  \"40 to 49 years\"=c(\"40 to 44 years\", \"45 to 49 years\"),\n                                  \"50 to 64 years\"=c(\"50 to 54 years\", \"55 to 59 years\",\n                                                     \"60 to 64 years\"),\n                                  \"65 years and over\"=c(\"65 to 69 years\", \"70 to 74 years\", \n                                                        \"75 to 79 years\", \"80 to 84 years\",\n                                                        \"85 years and over\")),\n         AGE_GROUP = str_replace_all(AGE_GROUP, \" \", \"_\")) |>\n  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) |>\n  summarize(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = \"drop\") |>\n  unite(col = \"VARIABLE\", RACE, SEX, AGE_GROUP, sep = \"_\") |>\n  rename(\"VALUE\" = PERC_SUB_POP)\n\nglimpse(dem_LOTT)\n\nRows: 62,424\nColumns: 4\n$ YEAR     <dbl> 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1977, 1…\n$ STATE    <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alaba…\n$ VARIABLE <chr> \"Black_Female_10_to_19_years\", \"Black_Female_20_to_29_years\",…\n$ VALUE    <dbl> 3.01067713, 2.32860137, 1.29295656, 1.18231753, 1.73263106, 1…"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#combine-population-data",
    "href": "content/lectures/11-cs01-data.html#combine-population-data",
    "title": "11-cs01-data",
    "section": "Combine: Population Data",
    "text": "Combine: Population Data\n\npopulation_data <- bind_rows(pop_77_79,\n                             pop_80_89,\n                             pop_90_99,\n                             pop_00_10)\n\npopulation_data <- population_data |>\n  mutate(VARIABLE = \"Population\") |>\n  rename(\"VALUE\" = TOT_POP)"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangling-police-staffing",
    "href": "content/lectures/11-cs01-data.html#wrangling-police-staffing",
    "title": "11-cs01-data",
    "section": "Wrangling: Police staffing",
    "text": "Wrangling: Police staffing\n\n# the provided dataset has already had a bit of wrangling done for you\nps_data\n\n# A tibble: 2,242 × 3\n   data_year state_abbr officer_state_total\n       <dbl> <chr>                    <dbl>\n 1      1977 AK                         544\n 2      1977 AL                        7380\n 3      1977 AR                        3344\n 4      1977 AS                           0\n 5      1977 AZ                        6414\n 6      1977 CA                       65596\n 7      1977 CO                        7337\n 8      1977 CT                        6051\n 9      1977 CZ                           0\n10      1977 DC                        4751\n# … with 2,232 more rows\n\n\n\n\nremove territories\n\nstate_of_interest_NULL <- c(\"AS\", \"GM\", \"CZ\", \"FS\", \"MP\", \"OT\", \"PR\", \"VI\")\n\nps_data <- ps_data |>\n  filter(!(state_abbr %in% state_of_interest_NULL)) \n\n\n\nUse state abbreviations\n\nstate_abb_data <- tibble(\"state_abbr\" = state.abb, \"STATE\" = state.name)\nstate_abb_data <- state_abb_data |>\n  mutate(state_abbr = str_replace(string = state_abbr, \n                                  pattern = \"NE\", \n                                  replacement = \"NB\")) |>\n  add_row(state_abbr = \"DC\", STATE = \"District of Columbia\")\n\nps_data <- ps_data |> \n  left_join(state_abb_data, by = \"state_abbr\") |>\n  select(-state_abbr) |> \n  rename(YEAR = \"data_year\",\n         VALUE = \"officer_state_total\") |>\n  mutate(VARIABLE = \"officer_state_total\")\n\nps_data\n\n# A tibble: 1,938 × 4\n    YEAR VALUE STATE                VARIABLE           \n   <dbl> <dbl> <chr>                <chr>              \n 1  1977   544 Alaska               officer_state_total\n 2  1977  7380 Alabama              officer_state_total\n 3  1977  3344 Arkansas             officer_state_total\n 4  1977  6414 Arizona              officer_state_total\n 5  1977 65596 California           officer_state_total\n 6  1977  7337 Colorado             officer_state_total\n 7  1977  6051 Connecticut          officer_state_total\n 8  1977  4751 District of Columbia officer_state_total\n 9  1977  1018 Delaware             officer_state_total\n10  1977 24588 Florida              officer_state_total\n# … with 1,928 more rows\n\n\n\n\nScaling\n\ndenominator_temp <- population_data |> \n  select(-VARIABLE) |>\n  rename(\"Population_temp\"=VALUE) \n\nps_data <- ps_data |> \n  left_join(denominator_temp, by=c(\"STATE\",\"YEAR\")) |>\n  mutate(VALUE = (VALUE * 100000) / Population_temp) |>\n  mutate(VARIABLE = \"police_per_100k_lag\") |>\n  select(-Population_temp)\n\nhead(ps_data)\n\n# A tibble: 6 × 4\n   YEAR VALUE STATE      VARIABLE           \n  <dbl> <dbl> <chr>      <chr>              \n1  1977  137. Alaska     police_per_100k_lag\n2  1977  195. Alabama    police_per_100k_lag\n3  1977  152. Arkansas   police_per_100k_lag\n4  1977  264. Arizona    police_per_100k_lag\n5  1977  293. California police_per_100k_lag\n6  1977  272. Colorado   police_per_100k_lag"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangling-poverty-rate",
    "href": "content/lectures/11-cs01-data.html#wrangling-poverty-rate",
    "title": "11-cs01-data",
    "section": "Wrangling: Poverty Rate",
    "text": "Wrangling: Poverty Rate\n\n\n# A tibble: 5 × 6\n  `NOTE: Number in thousands.` ...2  ...3   ...4              ...5         ...6 \n  <chr>                        <chr> <chr>  <chr>             <chr>        <chr>\n1 2018                         <NA>  <NA>    <NA>             <NA>          <NA>\n2 STATE                        Total Number \"Standard\\nerror\" Percent      \"Sta…\n3 Alabama                      4877  779    \"65\"              16           \"1.3\"\n4 Alaska                       720   94     \"9\"               13.1         \"1.2\"\n5 Arizona                      7241  929    \"80\"              12.80000000… \"1.1…\n\n\n\n\ncolnames(poverty_rate_data) <- c(\"STATE\", \"Total\", \"Number\", \"Number_se\",\n                                 \"Percent\", \"Percent_se\")\n\npoverty_rate_data <- poverty_rate_data |>\n  filter(STATE != \"STATE\") |> \n  mutate(length_state = map_dbl(STATE, str_length)) |> # determine how long string in \"STATE\" column is\n  filter(length_state < 100) |> # filter to only include possible state lengths\n  mutate(STATE = str_replace(STATE, pattern = \"D.C.\", \n                              replacement = \"District of Columbia\" )) \n\nyear_values <- poverty_rate_data |>\n  filter(str_detect(STATE, \"[:digit:]\")) |>\n  distinct(STATE)\nyear_values <- rep(pull(year_values, STATE), each = 52) # repeat values from STATE column 52 times each\n\npoverty_rate_data <- poverty_rate_data |>\n  mutate(year_value = year_values) |>\n  select(-length_state) |>\n  filter(str_detect(STATE, \"[:alpha:]\"))\n\npoverty_rate_data <- poverty_rate_data |>\n  filter(year_value != \"2017\") |> \n  filter(year_value != \"2013 (18)\") |>\n  mutate(YEAR = str_sub(year_value, start = 1, end = 4)) |>\n  select(-c(Number, Number_se, Percent_se, Total, year_value)) |>\n  rename(\"VALUE\" = Percent) |>\n  mutate(VARIABLE = \"Poverty_rate\",\n         YEAR = as.numeric(YEAR),\n         VALUE = as.numeric(VALUE))\n\npoverty_rate_data\n\n# A tibble: 1,989 × 4\n   STATE                VALUE  YEAR VARIABLE    \n   <chr>                <dbl> <dbl> <chr>       \n 1 Alabama               16    2018 Poverty_rate\n 2 Alaska                13.1  2018 Poverty_rate\n 3 Arizona               12.8  2018 Poverty_rate\n 4 Arkansas              15.9  2018 Poverty_rate\n 5 California            11.9  2018 Poverty_rate\n 6 Colorado               9.1  2018 Poverty_rate\n 7 Connecticut           10.2  2018 Poverty_rate\n 8 Delaware               7.4  2018 Poverty_rate\n 9 District of Columbia  14.7  2018 Poverty_rate\n10 Florida               13.7  2018 Poverty_rate\n# … with 1,979 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangling-crime-data",
    "href": "content/lectures/11-cs01-data.html#wrangling-crime-data",
    "title": "11-cs01-data",
    "section": "Wrangling: Crime Data",
    "text": "Wrangling: Crime Data\n\ncrime_data <- crime_data[-((str_which(crime_data, \"The figures shown in this column for the offense of rape were estimated using the legacy UCR definition of rape\")-1): length(crime_data)+1)]\n\nn_rows <- 2014-1977+1 # determine how many rows there are for each state\nrep_cycle <- 4 + n_rows\nrep_cycle_cut <- 2 + n_rows\ncolnames_crime <- (crime_data[4])\n\n# specify which rows are to be deleted based on the file format\ndelete_rows <- c(seq(from = 2, \n                       to = length(crime_data),  \n                       by = rep_cycle),\n                 seq(from = 3, \n                       to = length(crime_data),\n                       by = rep_cycle), \n                 seq(from = 4,\n                       to = length(crime_data),\n                       by = rep_cycle))\nsort(delete_rows) # which rows are to be deleted\n\n  [1]    2    3    4   44   45   46   86   87   88  128  129  130  170  171  172\n [16]  212  213  214  254  255  256  296  297  298  338  339  340  380  381  382\n [31]  422  423  424  464  465  466  506  507  508  548  549  550  590  591  592\n [46]  632  633  634  674  675  676  716  717  718  758  759  760  800  801  802\n [61]  842  843  844  884  885  886  926  927  928  968  969  970 1010 1011 1012\n [76] 1052 1053 1054 1094 1095 1096 1136 1137 1138 1178 1179 1180 1220 1221 1222\n [91] 1262 1263 1264 1304 1305 1306 1346 1347 1348 1388 1389 1390 1430 1431 1432\n[106] 1472 1473 1474 1514 1515 1516 1556 1557 1558 1598 1599 1600 1640 1641 1642\n[121] 1682 1683 1684 1724 1725 1726 1766 1767 1768 1808 1809 1810 1850 1851 1852\n[136] 1892 1893 1894 1934 1935 1936 1976 1977 1978 2018 2019 2020 2060 2061 2062\n[151] 2102 2103 2104\n\n# convince yourself you did it right\n# should these rows be deleted?\ncrime_data[44:46]\n\n[1] \",,National or state crime,,,,,,,\"                                                                                                   \n[2] \",,Violent crime,,,,,,,\"                                                                                                             \n[3] \"Year,Population,Violent crime total,Murder and nonnegligent Manslaughter,Legacy rape /1,Revised rape /2,Robbery,Aggravated assault,\"\n\ncrime_data <- crime_data[-delete_rows]\n\n# extract state labels from data\nstate_labels <- crime_data[str_which(crime_data, \"Estimated crime in \")]\nstate_labels <- str_remove(state_labels, pattern = \"Estimated crime in \")\nstate_label_order <- rep(state_labels, each = n_rows) # repeat n_rows times\n\ncrime_data <- crime_data[-str_which(crime_data, \"Estimated crime\")]\ncrime_data_sep <- read_csv(I(crime_data), col_names = FALSE) |> \n  select(-X6) # remove random extra-comma column\n\n# get column names for later\ncolnames(crime_data_sep) <- c(\"Year\", \n                              \"Population\", \n                              \"Violent_crime_total\",\n                              \"Murder_and_nonnegligent_Manslaughter\",\n                              \"Legacy_rape\",\n                              \"Revised_rape\", \n                              \"Robbery\",\n                              \"Aggravated_assault\")\n# add column names in\ncrime_data_sep <- bind_cols(STATE = state_label_order, crime_data_sep)\n\ncrime_data <- crime_data_sep |>\n  mutate(VARIABLE = \"Viol_crime_count\") |>\n  rename(\"VALUE\" = Violent_crime_total) |>\n  rename(\"YEAR\" = Year) |>\n  select(YEAR,STATE, VARIABLE, VALUE)\n\ncrime_data\n\n# A tibble: 1,938 × 4\n    YEAR STATE   VARIABLE         VALUE\n   <dbl> <chr>   <chr>            <dbl>\n 1  1977 Alabama Viol_crime_count 15293\n 2  1978 Alabama Viol_crime_count 15682\n 3  1979 Alabama Viol_crime_count 15578\n 4  1980 Alabama Viol_crime_count 17320\n 5  1981 Alabama Viol_crime_count 18423\n 6  1982 Alabama Viol_crime_count 17653\n 7  1983 Alabama Viol_crime_count 16471\n 8  1984 Alabama Viol_crime_count 17204\n 9  1985 Alabama Viol_crime_count 18398\n10  1986 Alabama Viol_crime_count 22616\n# … with 1,928 more rows"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangling-rtc-laws",
    "href": "content/lectures/11-cs01-data.html#wrangling-rtc-laws",
    "title": "11-cs01-data",
    "section": "Wrangling: RTC Laws",
    "text": "Wrangling: RTC Laws\n\nDAWpaper_p_62 <- DAWpaper[[62]]\nstr(DAWpaper_p_62, nchar.max = 1000) # see data\n\n chr \"                                          Table A1: RTC Adoption Dates\\n\\n         State         Effective Date of RTC Law   Fraction of Year In Effect Year of Passage   RTC Date (Synthetic Controls Analysis)\\n      Alabama                      1975                                                                        1975\\n       Alaska                   10/1/1994                            0.252                                     1995\\n       Arizona                  7/17/1994                            0.460                                     1995\\n      Arkansas                  7/27/1995                            0.433                                     1996\\n     California                    N/A                                                                            0\\n      Colorado                  5/17/2003                            0.627                                     2003\\n    Connecticut                    1970                                \"| __truncated__\n\np_62 <- DAWpaper_p_62 |>\n  str_split(\"\\n\") |>\n  unlist() |>\n  as_tibble() |>\n  slice(-(1:2)) |> \n  rename(RTC = value) |>\n  slice(-c(53:54)) |>  # physical page 60 marking; empty line removal\n  mutate(RTC = str_replace_all(RTC, \"\\\\s{40,}\", \"|N/A|\"),\n         RTC = str_trim(RTC, side = \"left\"),\n         RTC = str_replace_all(RTC, \"\\\\s{2,15}\", \"|\"))\n\nhead(p_62)\n\n# A tibble: 6 × 1\n  RTC                                                                           \n  <chr>                                                                         \n1 State|Effective Date of RTC Law|Fraction of Year In Effect Year of Passage|RT…\n2 Alabama||1975|N/A|1975                                                        \n3 Alaska||10/1/1994||0.252|||1995                                               \n4 Arizona||7/17/1994||0.460|||1995                                              \n5 Arkansas||7/27/1995||0.433|||1996                                             \n6 California||N/A|N/A|0                                                         \n\np_62 <- pull(p_62, RTC) |>\n  str_split( \"\\\\|{1,}\")  # split data on \"|\" symbol\n\n# get the tibble!\np_62 <- as_tibble(do.call(rbind, p_62)) # rbind and not bind_cols here b/c we have no column names yet\n\ncolnames(p_62) <- c(\"STATE\",\n                    \"E_Date_RTC\",\n                    \"Frac_Yr_Eff_Yr_Pass\",\n                    \"RTC_Date_SA\")\n\np_62 <- p_62 |>\n  slice(-c(1, 53:nrow(p_62))) # remove unnecessary rows\n\nRTC <- p_62 |> \n  select(STATE, RTC_Date_SA) |>\n  rename(RTC_LAW_YEAR = RTC_Date_SA) |>\n  mutate(RTC_LAW_YEAR = as.numeric(RTC_LAW_YEAR)) |>\n  mutate(RTC_LAW_YEAR = case_when(RTC_LAW_YEAR == 0 ~ Inf,\n                                  TRUE ~ RTC_LAW_YEAR))"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangling-combining-donohue",
    "href": "content/lectures/11-cs01-data.html#wrangling-combining-donohue",
    "title": "11-cs01-data",
    "section": "Wrangling: Combining! (Donohue)",
    "text": "Wrangling: Combining! (Donohue)\n\n# combine after all that wrangling!\nDONOHUE_DF <- bind_rows(dem_DONOHUE,\n                        ue_rate_data,\n                        poverty_rate_data,\n                        crime_data,\n                        population_data,\n                        ps_data)\nDONOHUE_DF\n\n# A tibble: 20,247 × 4\n    YEAR STATE   VARIABLE                    VALUE\n   <dbl> <chr>   <chr>                       <dbl>\n 1  1977 Alabama Black_Male_15_to_19_years  1.55  \n 2  1977 Alabama Black_Male_20_to_39_years  3.04  \n 3  1977 Alabama Other_Male_15_to_19_years  0.0178\n 4  1977 Alabama Other_Male_20_to_39_years  0.0642\n 5  1977 Alabama White_Male_15_to_19_years  3.58  \n 6  1977 Alabama White_Male_20_to_39_years 11.1   \n 7  1977 Alaska  Black_Male_15_to_19_years  0.163 \n 8  1977 Alaska  Black_Male_20_to_39_years  0.968 \n 9  1977 Alaska  Other_Male_15_to_19_years  1.12  \n10  1977 Alaska  Other_Male_20_to_39_years  2.73  \n# … with 20,237 more rows\n\n# to wide format!\nDONOHUE_DF <- DONOHUE_DF |>\n  pivot_wider(names_from = \"VARIABLE\",\n              values_from = \"VALUE\")\n\n# add in RTC data!\nDONOHUE_DF <- DONOHUE_DF |>\n  left_join(RTC , by = c(\"STATE\")) |>\n  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,\n                              TRUE ~ FALSE)) |>\n drop_na() # drop rows with missing information\n\n# filter to only data where RTC laws were adopted between 1980-2010\n# have crime data pre- and post-adoption this way\nbaseline_year <- min(DONOHUE_DF$YEAR)\ncensoring_year <- max(DONOHUE_DF$YEAR)\n\nDONOHUE_DF <- DONOHUE_DF |>\n  mutate(TIME_0 = baseline_year,\n         TIME_INF = censoring_year) |>\n  filter(RTC_LAW_YEAR > TIME_0)\n\n# calculate violent crime rate; put population/crime on log scale\nDONOHUE_DF <- DONOHUE_DF |>\n  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,\n         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),\n         Population_log = log(Population))\n\nDONOHUE_DF |>\n  slice_sample(n = 10) |>\n  glimpse()\n\nRows: 10\nColumns: 20\n$ YEAR                      <dbl> 1980, 1986, 2003, 1980, 1995, 1983, 2005, 20…\n$ STATE                     <chr> \"North Dakota\", \"Iowa\", \"Maine\", \"Iowa\", \"Al…\n$ Black_Male_15_to_19_years <dbl> 0.03101600, 0.08728751, 0.04737802, 0.088765…\n$ Black_Male_20_to_39_years <dbl> 0.15095470, 0.28489324, 0.15790122, 0.239885…\n$ Other_Male_15_to_19_years <dbl> 0.21925100, 0.05376223, 0.13624051, 0.036459…\n$ Other_Male_20_to_39_years <dbl> 0.4945753, 0.2079212, 0.3423617, 0.1312268, …\n$ White_Male_15_to_19_years <dbl> 4.742850, 3.812856, 3.523271, 4.648058, 3.03…\n$ White_Male_20_to_39_years <dbl> 15.99432, 15.31951, 11.68813, 14.88125, 13.1…\n$ Unemployment_rate         <dbl> 4.9, 6.8, 5.0, 6.0, 7.3, 8.7, 3.8, 4.6, 10.5…\n$ Poverty_rate              <dbl> 15.5, 12.9, 11.6, 10.8, 7.1, 16.0, 11.8, 11.…\n$ Viol_crime_count          <dbl> 352, 6703, 1422, 5826, 4656, 161489, 1387, 5…\n$ Population                <dbl> 654501, 2791923, 1306513, 2915562, 601345, 1…\n$ police_per_100k_lag       <dbl> 163.9417, 200.2204, 231.8385, 155.9562, 294.…\n$ RTC_LAW_YEAR              <dbl> 1986, 2011, 1986, 2011, 1995, Inf, 1985, 198…\n$ RTC_LAW                   <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE…\n$ TIME_0                    <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 1980, 19…\n$ TIME_INF                  <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ Viol_crime_rate_1k        <dbl> 0.5378143, 2.4008542, 1.0883933, 1.9982425, …\n$ Viol_crime_rate_1k_log    <dbl> -0.62024194, 0.87582458, 0.08470258, 0.69226…\n$ Population_log            <dbl> 13.39163, 14.84224, 14.08287, 14.88557, 13.3…"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#wrangling-combining-lott",
    "href": "content/lectures/11-cs01-data.html#wrangling-combining-lott",
    "title": "11-cs01-data",
    "section": "Wrangling: Combining! (Lott)",
    "text": "Wrangling: Combining! (Lott)\n\nLOTT_DF <- bind_rows(dem_LOTT,\n                     ue_rate_data,\n                     poverty_rate_data,\n                     crime_data,\n                     population_data,\n                     ps_data) |>\n  pivot_wider(names_from = \"VARIABLE\",\n              values_from = \"VALUE\") |>\n  left_join(RTC , by = c(\"STATE\")) |>\n  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,\n                              TRUE ~ FALSE)) |>\n   drop_na()\n\nbaseline_year <- min(LOTT_DF$YEAR)\ncensoring_year <- max(LOTT_DF$YEAR)\n\nLOTT_DF <- LOTT_DF |>\n  mutate(TIME_0 = baseline_year,\n         TIME_INF = censoring_year) |>\n  filter(RTC_LAW_YEAR > TIME_0)\n\nLOTT_DF <- LOTT_DF |>\n  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,\n         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),\n         Population_log = log(Population))\n\nLOTT_DF\n\n# A tibble: 1,364 × 50\n    YEAR STATE   Black…¹ Black…² Black…³ Black…⁴ Black…⁵ Black…⁶ Black…⁷ Black…⁸\n   <dbl> <chr>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1  1980 Alaska   0.264    0.443  0.201   0.116   0.0924 0.0264    0.297   0.695\n 2  1980 Arizona  0.287    0.278  0.165   0.119   0.136  0.103     0.311   0.338\n 3  1980 Arkans…  1.82     1.50   0.842   0.634   1.02   1.16      1.81    1.26 \n 4  1980 Califo…  0.780    0.815  0.581   0.394   0.456  0.292     0.808   0.815\n 5  1980 Colora…  0.352    0.388  0.245   0.172   0.164  0.103     0.377   0.467\n 6  1980 Delawa…  1.87     1.68   1.14    0.783   0.952  0.670     1.81    1.36 \n 7  1980 Distri…  6.53     7.54   5.18    3.89    6.10   4.15      6.32    6.40 \n 8  1980 Florida  1.50     1.37   0.912   0.679   0.812  0.604     1.49    1.20 \n 9  1980 Georgia  2.90     2.78   1.85    1.22    1.56   1.35      2.92    2.45 \n10  1980 Hawaii   0.0930   0.215  0.0776  0.0253  0.0197 0.00738   0.180   0.656\n# … with 1,354 more rows, 40 more variables: Black_Male_30_to_39_years <dbl>,\n#   Black_Male_40_to_49_years <dbl>, Black_Male_50_to_64_years <dbl>,\n#   Black_Male_65_years_and_over <dbl>, Other_Female_10_to_19_years <dbl>,\n#   Other_Female_20_to_29_years <dbl>, Other_Female_30_to_39_years <dbl>,\n#   Other_Female_40_to_49_years <dbl>, Other_Female_50_to_64_years <dbl>,\n#   Other_Female_65_years_and_over <dbl>, Other_Male_10_to_19_years <dbl>,\n#   Other_Male_20_to_29_years <dbl>, Other_Male_30_to_39_years <dbl>, …"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#why",
    "href": "content/lectures/11-cs01-data.html#why",
    "title": "11-cs01-data",
    "section": "Why?",
    "text": "Why?\n❓ Why are there different dimensions for LOTT vs DONOHUE??\n\ndim(LOTT_DF)\n\n[1] 1364   50\n\n\n\ndim(DONOHUE_DF)\n\n[1] 1364   20"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#things-to-consider",
    "href": "content/lectures/11-cs01-data.html#things-to-consider",
    "title": "11-cs01-data",
    "section": "Things to Consider",
    "text": "Things to Consider\n\n\nHow RMarkdown documents work\nHow to control what is executed each time you render\nWhat to do with all this after this lecture?"
  },
  {
    "objectID": "content/lectures/11-cs01-data.html#save",
    "href": "content/lectures/11-cs01-data.html#save",
    "title": "11-cs01-data",
    "section": "Save",
    "text": "Save\n\nsave(LOTT_DF, DONOHUE_DF, file = \"data/wrangled/wrangled_data_rtc.rda\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#course-announcements",
    "href": "content/lectures/08-effective-communication-slides.html#course-announcements",
    "title": "08-effective-communication",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nLecture Participation survey “due” after class\nMidterm due Monday (2/13; 11:59 PM):\n\nreleased Friday after lab\ncompleted individually\n\n\n\n\nPractice Midterm Posted (Answer key posted Wed/tomorrow)\nLab03 & Lab04 Scores & Feedback Posted\nLab04 Notes:\n\nModel Interpretations\nText, code, & viz all matter\n\nLink for Later"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#agenda",
    "href": "content/lectures/08-effective-communication-slides.html#agenda",
    "title": "08-effective-communication",
    "section": "Agenda",
    "text": "Agenda\n\nCommunicating for your audience\nOral Communication\nWritten Communication\nVisual Communication"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#what-does-this-mean",
    "href": "content/lectures/08-effective-communication-slides.html#what-does-this-mean",
    "title": "08-effective-communication",
    "section": "What does this mean?",
    "text": "What does this mean?\n❓ What does it mean to “consider your audience?”\n\nSimply: You do the work so they don’t have to.\n\n\n…also the aesthetic-usability effect exists."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#whats-the-right-level",
    "href": "content/lectures/08-effective-communication-slides.html#whats-the-right-level",
    "title": "08-effective-communication",
    "section": "What’s the right level?",
    "text": "What’s the right level?\n\n\nGeneral Audience\n✔ background\n🚫 limit technical details\n🎉 emphasize take-home\n\n\n\nTechnical Audience\n⬇ limit background\n💻 all-the-details\n🎉 emphasize take-home"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#considerations",
    "href": "content/lectures/08-effective-communication-slides.html#considerations",
    "title": "08-effective-communication",
    "section": "Considerations",
    "text": "Considerations\n\nPlatform: written? oral?\n\n\n\nSetting: informal? formal?\n\n\n\n\nTiming: never go over your time limit!"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#storytelling",
    "href": "content/lectures/08-effective-communication-slides.html#storytelling",
    "title": "08-effective-communication",
    "section": "Storytelling",
    "text": "Storytelling\n\nStories have a beginning, a middle, and an end.\n\n\n\nStories do not need every detail of what you’ve tried\n\n\n\n\nReports and presentations should tell a story\nPlanning out your report/presentation can help\n\n\n\n\nHold the audience’s attention with what needs to be said; do so effectively\nTell your audience why they should care; why it matters\nYou should explain your choices and the “why”"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#choose-informative-titles",
    "href": "content/lectures/08-effective-communication-slides.html#choose-informative-titles",
    "title": "08-effective-communication",
    "section": "Choose informative titles",
    "text": "Choose informative titles\nOn presentations: Balance b/w short and informative (goal: concise)\n\n\nAvoid: “Analyzing NHANES”\n\nBetter: “Data from the NHANES study shows that diet is related to overall health”\n\n\nOn visualizations: emphasize the take-home! (what’s learned or what action to take)\n\n\n\nAvoid: “Boxplot of gender”\n\nBetter: “Twice as many females as males included for analysis”\n\n\n\nAvoid: “Tickets vs. Time”\n\nBetter: “Staff unable to respond to incoming tickets; need to hire 2 FTEs”"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\nAdvice you've received\n\n\n\n\nSpeak loudly and clearly enough for everyone to hear (project your voice!); be engaging so that people are inclined to pay attention\n\n\nMake eye contact with your audience as you speak\n\n\nDon't just read off slides, and (most of the time) don't read from a script\n\n\nloud and clear voice\n\n\nPosture, eye contact, and mannerisms can all influence oral presentation performance. While presenting don't stand in one spot, get up, move around and engage with the audience.\n\n\ntry harder to better word choice\n\n\nLook at the audience when I am talking to them. Speak loud and clear.\n\n\nEye contact!!!\n\n\neye contact, try not to use filler words, talk slower than you would normally, don't just read off of the slides\n\n\ndont overcomplicate the wording, what might make sense to you might be revolutionary to someone else. Using simple words for simple explainations for begginer simple thought processes\n\n\ndrink water constantly\n\n\nmaking enough eye contact"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#presentations-are-for-listening",
    "href": "content/lectures/08-effective-communication-slides.html#presentations-are-for-listening",
    "title": "08-effective-communication",
    "section": "Presentations are for listening",
    "text": "Presentations are for listening\n\nAdvantage: words to explain out loud what you’re showing\n\n\n\nYou are presenting for the person in the back of the room.\n\n\n\nTo accomplish:\n\ndon’t read directly off slides\nrepetition is ok: tell what you’re going to tell them, tell them, tell them what you told them\nuse animation to build your story (not to distract)\nintroduce your axes\ntext/labels larger\nwatch your speech speed\npractice!"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "href": "content/lectures/08-effective-communication-slides.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "title": "08-effective-communication",
    "section": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood",
    "text": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood\n\nRed Riding Hood (RRH) has to walk 0.54 mi from Point A (home) to Point B (Grandma’s)\nRRH meets Wolf who (1) runs ahead to Grandma’s, (2) eats her, and (3) dresses in her clothes\nRRH arrives at Grandmas at 2PM, asks her three questions\nIdentified problem: after third question, Wolf eats RRH\nSolution: vendor (Woodsman) employs tool (ax)\nExpected outcome: Grandma and RRH alive, wolf is not"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses-1",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses-1",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\nAdvice you've received\n\n\n\n\nYour rough draft should be rough. Don't polish it because it will flush all your great ideas away.\n\n\nInclude data/metrics that highlight quantifiable impact of work.\n\n\nbe concise when writing\n\n\nshorter sentences = good\n\n\nState your main point clearly; use good grammar\n\n\ndon't use crazy fonts and coloring. If things need to be emphasized stick to italics bold and font size generally speaking.\n\n\nDisplay contents of slides in a left to right fashion.\n\n\nUse clear fonts.\n\n\nas concise as possible\n\n\nIf it's a presentation, less wording is often better. Use oral communication to fill in gaps between the main points\n\n\nRead what you write in between every iteration. Try to avoid repeating words - show some variety. Use punctuation well. Split up paragraphs; monolithic walls of text are intimidating and readers will skip them. It's cliche, but for a reason - the topic, argument, evidence structure is effective.\n\n\nExplicitly write out the opposing arguments/concerns in your paper and answer them as best you can. That way the paper feels more like a conversation with the author than just something to read.\n\n\nMake sentences flow well. Include a conclusion to summarize\n\n\nstay on topic, summarize main points at end,\n\n\nRepeat the point you want the readers to remember"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#benefits-of-written-communciation",
    "href": "content/lectures/08-effective-communication-slides.html#benefits-of-written-communciation",
    "title": "08-effective-communication",
    "section": "Benefits of written communciation",
    "text": "Benefits of written communciation\nYour audience has time to process…but the explanation has to be there!\n\nVisually: more on a single visualization\n\n\nYes, often there are different visualizations for reports/papers than for presentations/lectures."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#when-you-have-time-to-digest-read",
    "href": "content/lectures/08-effective-communication-slides.html#when-you-have-time-to-digest-read",
    "title": "08-effective-communication",
    "section": "When you have time to digest (read)",
    "text": "When you have time to digest (read)\n\n\n❓ What makes this an effective visualization for a written communication?”\nSource: Storytelling wtih data by cole nussbaumer knaflic"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#data-science-reports-in-.rmd",
    "href": "content/lectures/08-effective-communication-slides.html#data-science-reports-in-.rmd",
    "title": "08-effective-communication",
    "section": "Data Science Reports in .Rmd",
    "text": "Data Science Reports in .Rmd\n\nAs concise as possible\nNecessary details (for your audience); nothing more\nTypical Sections: Introduction/Background, Setup, Data, Analysis, Conclusion, References"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#controlling-html-document-settings",
    "href": "content/lectures/08-effective-communication-slides.html#controlling-html-document-settings",
    "title": "08-effective-communication",
    "section": "Controlling HTML document settings",
    "text": "Controlling HTML document settings\n\nTable of Contents\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\nTheme\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    theme: united\n    highlight: tango\n---\n\n\n\nFigure Options\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    fig_width: 7\n    fig_height: 6\n    fig_caption: true\n---\n\n\n\nCode Folding\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    code_folding: hide\n---"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#controlling-code-chunk-output",
    "href": "content/lectures/08-effective-communication-slides.html#controlling-code-chunk-output",
    "title": "08-effective-communication",
    "section": "Controlling code chunk output",
    "text": "Controlling code chunk output\n\nSpecified in the curly braces, separated by commas\n\n\n{r, chunk-label, results='hide', fig.height=4}\n\n\n\neval: whether to execute the code chunk\necho: whether to include the code in the output\nwarning, message, and error: whether to show warnings, messages, or errors in the knit document\nfig.width and fig.height: control the width/height of plots\n\n\n\n\nControlling for the whole document:\n\nknitr::opts_chunk$set(fig.width = 8, collapse = TRUE)"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#editing-proofreading",
    "href": "content/lectures/08-effective-communication-slides.html#editing-proofreading",
    "title": "08-effective-communication",
    "section": "Editing & Proofreading",
    "text": "Editing & Proofreading\n\nDid you end up telling a story?\n\nThings missing?\nThings to delete?\n\n\n\n\nDo not fall in love with your words/code/plots\n\n\n\n\nDo spell check\nDo read it over before sending/presenting/submitting"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#aside-citing-sources",
    "href": "content/lectures/08-effective-communication-slides.html#aside-citing-sources",
    "title": "08-effective-communication",
    "section": "Aside: Citing Sources",
    "text": "Aside: Citing Sources\nWhen are citations needed?\n\n\n\n“We will be doing our analysis using two different data sets created by two different groups: Donohue and Mustard + Lott, or simply Lott”\n\n\n\n\n\n\n“What turned from the idea of carrying firearms to protect oneself from enemies such as the British monarchy and the unknown frontier of North America has now become a nationwide issue.”\n\n\n\n\n\n\n“Right to Carry Laws refer to laws that specify how citizens are allowed to carry concealed handguns when they’re away from home without a permit”\n\n\n\n\n\n\n“In this case study, we are examining the relationship between unemployment rate, poverty rate, police staffing, and violent crime rate.”\n\n\n\n\n\n\n“In the United States, the second amendment permits the right to bear arms, and this law has not been changed since its creation in 1791.”\n\n\n\n\n\n\n“The Right to Carry Laws (RTC) is defined as “a law that specifies if and how citizens are allowed to have a firearm on their person or nearby in public.””\n\n\n\n\nReminder: You do NOT get docked points for citing others’ work. You can be at risk of AI Violation if you don’t. When in doubt, give credit."
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#footnotes-in-.rmd",
    "href": "content/lectures/08-effective-communication-slides.html#footnotes-in-.rmd",
    "title": "08-effective-communication",
    "section": "Footnotes in .Rmd",
    "text": "Footnotes in .Rmd\nHow to specify a footnote in text:\nHere is some body text.[^1]\nHow to include the footnote’s reference:\n[^1]: This footnote will appear at the bottom of the page.\n\n\nNote: .bib files can be included with BibTeX references using the bibliography parameter in your YAML"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#student-responses-2",
    "href": "content/lectures/08-effective-communication-slides.html#student-responses-2",
    "title": "08-effective-communication",
    "section": "Student Responses",
    "text": "Student Responses\n\n\n\n\n\nAdvice you've received\n\n\n\n\nTry to make the title of the plot something descriptive rather than simply, eg, \"Time vs Length\" or \"linear regression of pet sizes\".\"\n\n\nshow the kind of plot that can be understood by audience\n\n\nI heard that when making a visual graphic, you should try remove as much as you can while still showing the necessary information. The less on the screen, the better.\n\n\nWhen giving a presentation, try to keep bullet points shorter than 7 words so it's easier digest/remember\n\n\nWhen displaying data and visualizations. Take note of people with eye color visualization disabilities. Also present things based on intuitive knowledge. Like reading left to right\n\n\nhighlighting only the important bits with color usage\n\n\n(picture / plot / visualization) > words\n\n\nUse appropriate colors and use them to highlight findings.\n\n\nMake plots big and clear\n\n\nSometimes simple is better, try to stay away from distracting visualizations\n\n\nUse appropriate (non-misleading) scales\n\n\nColors choices are important, and also remember about color blind people\n\n\nlabel axis, always include a title, use ticks if they help understand the data, think about color choices, big text\n\n\nInclude visuals only if they are meaningful and/or strengthen your argument"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#the-glamour-of-graphics",
    "href": "content/lectures/08-effective-communication-slides.html#the-glamour-of-graphics",
    "title": "08-effective-communication",
    "section": "The Glamour of Graphics",
    "text": "The Glamour of Graphics\n\nbuilds on top of the grammar (components) of a graphic\nconsiderations for the design of a graphic\ncolor, typography, layout\ngoing from accurate to 😍effective\n\n\n\nThese ideas and slides are all modified from Will Chase’s rstudio::conf2020 slides/talk"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#left-align-titles-at-top-left",
    "href": "content/lectures/08-effective-communication-slides.html#left-align-titles-at-top-left",
    "title": "08-effective-communication",
    "section": "Left-align titles at top-left",
    "text": "Left-align titles at top-left\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#avoid-head-tilting",
    "href": "content/lectures/08-effective-communication-slides.html#avoid-head-tilting",
    "title": "08-effective-communication",
    "section": "Avoid head-tilting",
    "text": "Avoid head-tilting\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#borders-backgrounds",
    "href": "content/lectures/08-effective-communication-slides.html#borders-backgrounds",
    "title": "08-effective-communication",
    "section": "Borders & Backgrounds: 👎",
    "text": "Borders & Backgrounds: 👎\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_bw() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#organize-removelighten-as-much-as-possible",
    "href": "content/lectures/08-effective-communication-slides.html#organize-removelighten-as-much-as-possible",
    "title": "08-effective-communication",
    "section": "Organize & Remove/Lighten as much as possible",
    "text": "Organize & Remove/Lighten as much as possible\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#legends-suck",
    "href": "content/lectures/08-effective-communication-slides.html#legends-suck",
    "title": "08-effective-communication",
    "section": "Legends suck",
    "text": "Legends suck\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 7) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 20) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#additional-guidance",
    "href": "content/lectures/08-effective-communication-slides.html#additional-guidance",
    "title": "08-effective-communication",
    "section": "Additional Guidance",
    "text": "Additional Guidance\n\nWhite space is like garlic - take the amount you need and triple it\nFonts Matter\nUse Color Effectively"
  },
  {
    "objectID": "content/lectures/08-effective-communication-slides.html#suggested-reading",
    "href": "content/lectures/08-effective-communication-slides.html#suggested-reading",
    "title": "08-effective-communication",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nBookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html",
    "href": "content/lectures/08-effective-communication.html",
    "title": "08-effective-communication",
    "section": "",
    "text": "Lecture Participation survey “due” after class\nMidterm due Monday (2/13; 11:59 PM):\n\nreleased Friday after lab\ncompleted individually\n\n\n\n\nPractice Midterm Posted (Answer key posted Wed/tomorrow)\nLab03 & Lab04 Scores & Feedback Posted\nLab04 Notes:\n\nModel Interpretations\nText, code, & viz all matter\n\nLink for Later\n\n\n\n\n\n\nCommunicating for your audience\nOral Communication\nWritten Communication\nVisual Communication"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "href": "content/lectures/08-effective-communication.html#what-does-this-mean",
    "title": "08-effective-communication",
    "section": "What does this mean?",
    "text": "What does this mean?\n❓ What does it mean to “consider your audience?”\n\nSimply: You do the work so they don’t have to.\n\n\n…also the aesthetic-usability effect exists."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "href": "content/lectures/08-effective-communication.html#whats-the-right-level",
    "title": "08-effective-communication",
    "section": "What’s the right level?",
    "text": "What’s the right level?\n\n\nGeneral Audience\n✔ background\n🚫 limit technical details\n🎉 emphasize take-home\n\n\n\nTechnical Audience\n⬇ limit background\n💻 all-the-details\n🎉 emphasize take-home"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#considerations",
    "href": "content/lectures/08-effective-communication.html#considerations",
    "title": "08-effective-communication",
    "section": "Considerations",
    "text": "Considerations\n\nPlatform: written? oral?\n\n\n\nSetting: informal? formal?\n\n\n\n\nTiming: never go over your time limit!"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#storytelling",
    "href": "content/lectures/08-effective-communication.html#storytelling",
    "title": "08-effective-communication",
    "section": "Storytelling",
    "text": "Storytelling\n\nStories have a beginning, a middle, and an end.\n\n\n\nStories do not need every detail of what you’ve tried\n\n\n\n\nReports and presentations should tell a story\nPlanning out your report/presentation can help\n\n\n\n\nHold the audience’s attention with what needs to be said; do so effectively\nTell your audience why they should care; why it matters\nYou should explain your choices and the “why”"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "href": "content/lectures/08-effective-communication.html#choose-informative-titles",
    "title": "08-effective-communication",
    "section": "Choose informative titles",
    "text": "Choose informative titles\nOn presentations: Balance b/w short and informative (goal: concise)\n\n\nAvoid: “Analyzing NHANES”\n\nBetter: “Data from the NHANES study shows that diet is related to overall health”\n\n\nOn visualizations: emphasize the take-home! (what’s learned or what action to take)\n\n\n\nAvoid: “Boxplot of gender”\n\nBetter: “Twice as many females as males included for analysis”\n\n\n\nAvoid: “Tickets vs. Time”\n\nBetter: “Staff unable to respond to incoming tickets; need to hire 2 FTEs”"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses",
    "href": "content/lectures/08-effective-communication.html#student-responses",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\nAdvice you've received\n\n\n\n\nSpeak loudly and clearly enough for everyone to hear (project your voice!); be engaging so that people are inclined to pay attention\n\n\nMake eye contact with your audience as you speak\n\n\nDon't just read off slides, and (most of the time) don't read from a script\n\n\nloud and clear voice\n\n\nPosture, eye contact, and mannerisms can all influence oral presentation performance. While presenting don't stand in one spot, get up, move around and engage with the audience.\n\n\ntry harder to better word choice\n\n\nLook at the audience when I am talking to them. Speak loud and clear.\n\n\nEye contact!!!\n\n\neye contact, try not to use filler words, talk slower than you would normally, don't just read off of the slides\n\n\ndont overcomplicate the wording, what might make sense to you might be revolutionary to someone else. Using simple words for simple explainations for begginer simple thought processes\n\n\ndrink water constantly\n\n\nmaking enough eye contact"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "href": "content/lectures/08-effective-communication.html#presentations-are-for-listening",
    "title": "08-effective-communication",
    "section": "Presentations are for listening",
    "text": "Presentations are for listening\n\nAdvantage: words to explain out loud what you’re showing\n\n\n\nYou are presenting for the person in the back of the room.\n\n\n\nTo accomplish:\n\ndon’t read directly off slides\nrepetition is ok: tell what you’re going to tell them, tell them, tell them what you told them\nuse animation to build your story (not to distract)\nintroduce your axes\ntext/labels larger\nwatch your speech speed\npractice!"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "href": "content/lectures/08-effective-communication.html#for-example-a-happy-ending-for-almost-everyone-in-little-red-riding-hood",
    "title": "08-effective-communication",
    "section": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood",
    "text": "For Example: A Happy Ending for (almost) everyone in Little Red Riding Hood\n\nRed Riding Hood (RRH) has to walk 0.54 mi from Point A (home) to Point B (Grandma’s)\nRRH meets Wolf who (1) runs ahead to Grandma’s, (2) eats her, and (3) dresses in her clothes\nRRH arrives at Grandmas at 2PM, asks her three questions\nIdentified problem: after third question, Wolf eats RRH\nSolution: vendor (Woodsman) employs tool (ax)\nExpected outcome: Grandma and RRH alive, wolf is not"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-1",
    "href": "content/lectures/08-effective-communication.html#student-responses-1",
    "title": "08-effective-communication",
    "section": "Student responses",
    "text": "Student responses\n\n\n\n\n\nAdvice you've received\n\n\n\n\nYour rough draft should be rough. Don't polish it because it will flush all your great ideas away.\n\n\nInclude data/metrics that highlight quantifiable impact of work.\n\n\nbe concise when writing\n\n\nshorter sentences = good\n\n\nState your main point clearly; use good grammar\n\n\ndon't use crazy fonts and coloring. If things need to be emphasized stick to italics bold and font size generally speaking.\n\n\nDisplay contents of slides in a left to right fashion.\n\n\nUse clear fonts.\n\n\nas concise as possible\n\n\nIf it's a presentation, less wording is often better. Use oral communication to fill in gaps between the main points\n\n\nRead what you write in between every iteration. Try to avoid repeating words - show some variety. Use punctuation well. Split up paragraphs; monolithic walls of text are intimidating and readers will skip them. It's cliche, but for a reason - the topic, argument, evidence structure is effective.\n\n\nExplicitly write out the opposing arguments/concerns in your paper and answer them as best you can. That way the paper feels more like a conversation with the author than just something to read.\n\n\nMake sentences flow well. Include a conclusion to summarize\n\n\nstay on topic, summarize main points at end,\n\n\nRepeat the point you want the readers to remember"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "href": "content/lectures/08-effective-communication.html#benefits-of-written-communciation",
    "title": "08-effective-communication",
    "section": "Benefits of written communciation",
    "text": "Benefits of written communciation\nYour audience has time to process…but the explanation has to be there!\n\nVisually: more on a single visualization\n\n\nYes, often there are different visualizations for reports/papers than for presentations/lectures."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "href": "content/lectures/08-effective-communication.html#when-you-have-time-to-digest-read",
    "title": "08-effective-communication",
    "section": "When you have time to digest (read)",
    "text": "When you have time to digest (read)\n\n\n❓ What makes this an effective visualization for a written communication?”\nSource: Storytelling wtih data by cole nussbaumer knaflic"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#data-science-reports-in-.rmd",
    "title": "08-effective-communication",
    "section": "Data Science Reports in .Rmd",
    "text": "Data Science Reports in .Rmd\n\nAs concise as possible\nNecessary details (for your audience); nothing more\nTypical Sections: Introduction/Background, Setup, Data, Analysis, Conclusion, References"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "href": "content/lectures/08-effective-communication.html#controlling-html-document-settings",
    "title": "08-effective-communication",
    "section": "Controlling HTML document settings",
    "text": "Controlling HTML document settings\n\nTable of Contents\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    toc: true\n    toc_float: true\n---\n\n\nTheme\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    theme: united\n    highlight: tango\n---\n\n\n\nFigure Options\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    fig_width: 7\n    fig_height: 6\n    fig_caption: true\n---\n\n\n\nCode Folding\n\n---\ntitle: \"Document Title\"\noutput:\n  html_document:\n    code_folding: hide\n---"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "href": "content/lectures/08-effective-communication.html#controlling-code-chunk-output",
    "title": "08-effective-communication",
    "section": "Controlling code chunk output",
    "text": "Controlling code chunk output\n\nSpecified in the curly braces, separated by commas\n\n\n{r, chunk-label, results='hide', fig.height=4}\n\n\n\neval: whether to execute the code chunk\necho: whether to include the code in the output\nwarning, message, and error: whether to show warnings, messages, or errors in the knit document\nfig.width and fig.height: control the width/height of plots\n\n\n\n\nControlling for the whole document:\n\nknitr::opts_chunk$set(fig.width = 8, collapse = TRUE)"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#editing-proofreading",
    "href": "content/lectures/08-effective-communication.html#editing-proofreading",
    "title": "08-effective-communication",
    "section": "Editing & Proofreading",
    "text": "Editing & Proofreading\n\nDid you end up telling a story?\n\nThings missing?\nThings to delete?\n\n\n\n\nDo not fall in love with your words/code/plots\n\n\n\n\nDo spell check\nDo read it over before sending/presenting/submitting"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "href": "content/lectures/08-effective-communication.html#aside-citing-sources",
    "title": "08-effective-communication",
    "section": "Aside: Citing Sources",
    "text": "Aside: Citing Sources\nWhen are citations needed?\n\n\n\n“We will be doing our analysis using two different data sets created by two different groups: Donohue and Mustard + Lott, or simply Lott”\n\n\n\n\n\n\n“What turned from the idea of carrying firearms to protect oneself from enemies such as the British monarchy and the unknown frontier of North America has now become a nationwide issue.”\n\n\n\n\n\n\n“Right to Carry Laws refer to laws that specify how citizens are allowed to carry concealed handguns when they’re away from home without a permit”\n\n\n\n\n\n\n“In this case study, we are examining the relationship between unemployment rate, poverty rate, police staffing, and violent crime rate.”\n\n\n\n\n\n\n“In the United States, the second amendment permits the right to bear arms, and this law has not been changed since its creation in 1791.”\n\n\n\n\n\n\n“The Right to Carry Laws (RTC) is defined as “a law that specifies if and how citizens are allowed to have a firearm on their person or nearby in public.””\n\n\n\n\nReminder: You do NOT get docked points for citing others’ work. You can be at risk of AI Violation if you don’t. When in doubt, give credit."
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "href": "content/lectures/08-effective-communication.html#footnotes-in-.rmd",
    "title": "08-effective-communication",
    "section": "Footnotes in .Rmd",
    "text": "Footnotes in .Rmd\nHow to specify a footnote in text:\nHere is some body text.[^1]\nHow to include the footnote’s reference:\n[^1]: This footnote will appear at the bottom of the page.\n\n\nNote: .bib files can be included with BibTeX references using the bibliography parameter in your YAML"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#student-responses-2",
    "href": "content/lectures/08-effective-communication.html#student-responses-2",
    "title": "08-effective-communication",
    "section": "Student Responses",
    "text": "Student Responses\n\n\n\n\n\nAdvice you've received\n\n\n\n\nTry to make the title of the plot something descriptive rather than simply, eg, \"Time vs Length\" or \"linear regression of pet sizes\".\"\n\n\nshow the kind of plot that can be understood by audience\n\n\nI heard that when making a visual graphic, you should try remove as much as you can while still showing the necessary information. The less on the screen, the better.\n\n\nWhen giving a presentation, try to keep bullet points shorter than 7 words so it's easier digest/remember\n\n\nWhen displaying data and visualizations. Take note of people with eye color visualization disabilities. Also present things based on intuitive knowledge. Like reading left to right\n\n\nhighlighting only the important bits with color usage\n\n\n(picture / plot / visualization) > words\n\n\nUse appropriate colors and use them to highlight findings.\n\n\nMake plots big and clear\n\n\nSometimes simple is better, try to stay away from distracting visualizations\n\n\nUse appropriate (non-misleading) scales\n\n\nColors choices are important, and also remember about color blind people\n\n\nlabel axis, always include a title, use ticks if they help understand the data, think about color choices, big text\n\n\nInclude visuals only if they are meaningful and/or strengthen your argument"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "href": "content/lectures/08-effective-communication.html#the-glamour-of-graphics",
    "title": "08-effective-communication",
    "section": "The Glamour of Graphics",
    "text": "The Glamour of Graphics\n\nbuilds on top of the grammar (components) of a graphic\nconsiderations for the design of a graphic\ncolor, typography, layout\ngoing from accurate to 😍effective\n\n\n\nThese ideas and slides are all modified from Will Chase’s rstudio::conf2020 slides/talk"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "href": "content/lectures/08-effective-communication.html#left-align-titles-at-top-left",
    "title": "08-effective-communication",
    "section": "Left-align titles at top-left",
    "text": "Left-align titles at top-left\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "href": "content/lectures/08-effective-communication.html#avoid-head-tilting",
    "title": "08-effective-communication",
    "section": "Avoid head-tilting",
    "text": "Avoid head-tilting\n\n\n😬 Accurate\n\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),\n        plot.title.position = \"plot\")\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "href": "content/lectures/08-effective-communication.html#borders-backgrounds",
    "title": "08-effective-communication",
    "section": "Borders & Backgrounds: 👎",
    "text": "Borders & Backgrounds: 👎\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_bw() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "href": "content/lectures/08-effective-communication.html#organize-removelighten-as-much-as-possible",
    "title": "08-effective-communication",
    "section": "Organize & Remove/Lighten as much as possible",
    "text": "Organize & Remove/Lighten as much as possible\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = species, fill = species)) +\n  geom_bar() +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\") \n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#legends-suck",
    "href": "content/lectures/08-effective-communication.html#legends-suck",
    "title": "08-effective-communication",
    "section": "Legends suck",
    "text": "Legends suck\n\n\n😬 Accurate\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 6) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 18) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n😍 Effective\n\nggplot(penguins, aes(y = fct_rev(fct_infreq(species)), fill = species)) +\n  geom_bar() +\n  geom_text(stat='count', aes(label=..count..), hjust = 1.5, color = \"white\", size = 7) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#454545\", rep(\"#adadad\", 2))) +\n  labs(title = \"Adelie Penguins are the most common in Antarctica\", \n       subtitle = \"Frequency of each penguin species studied near Palmer Station, Antarctica\") +\n  theme_minimal(base_size = 20) +\n  theme(axis.text.x = element_blank(),\n        plot.title.position = \"plot\", \n        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\")"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#additional-guidance",
    "href": "content/lectures/08-effective-communication.html#additional-guidance",
    "title": "08-effective-communication",
    "section": "Additional Guidance",
    "text": "Additional Guidance\n\nWhite space is like garlic - take the amount you need and triple it\nFonts Matter\nUse Color Effectively"
  },
  {
    "objectID": "content/lectures/08-effective-communication.html#suggested-reading",
    "href": "content/lectures/08-effective-communication.html#suggested-reading",
    "title": "08-effective-communication",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nBookdown Section 2.6 R Code Chunks & inline R code\nBookdown Chapter 3: Documents\nWill Chase’s rstudio::conf2020 talk: “The Glamour of Graphics” [slides] [video]"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#qa",
    "href": "content/lectures/06-analysis-slides.html#qa",
    "title": "06-analysis",
    "section": "Q&A",
    "text": "Q&A\n\nQ: What if we’re not good at coming up with ideas of what plots to generate from a given dataset?\nA: We’re going to discuss this and practice this! If you’re not sure where to start, the DatatoViz resource is a great place to help with ideas! If you’re still struggling, be sure when we get to case studies to really think to yourself for each plot, why did prof choose this plot? Could another have been better? And, discuss with your teammates (once you have those)!\n\n\nQ: maybe i missed it… but i am confused about the difference between “count” and “proportions”. Would they end up being the same thing? is the code different for doing count vs. proportion?\nA: Another student had this question, and it’s a good one! Count is for the value across the dataset. Proportions are calculated within the group, such that each group’s values are displayed out of 100%. Happy to discuss more if there are questions. We’ll see an example of this in today’s lecture!\n\n\nQ: Not the lecture, but the way to think in the way to get the “right” plot for the data.\nA: So, the good news is that there isn’t typically one right way. There’s room for personalization, personal preferences, and sometimes, there’s just more than one way! The bad news is that it takes time to learn best practices…but that’s part of what this class is all about!\n\n\nQ: Why is ChatGPT mentioned in the lab answers?\nA: Because I assume that some students are using this as a resource. And, that’s great! It’s there so that you get a sense of ChatGPT’s strengths (sometimes it really does give helpful dplyr code or get you started on the right path) and its weaknesses (it’s still important to think critically if you’re choosing to use it as a tool!) ChatGPT is by no means required in this course. In fact, I think labs/assignments may take longer if you are using it. I don’t have a sense yet who would learn the material better!"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#agenda",
    "href": "content/lectures/06-analysis-slides.html#agenda",
    "title": "06-analysis",
    "section": "Agenda",
    "text": "Agenda\n\nDiscuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#course-announcements",
    "href": "content/lectures/06-analysis-slides.html#course-announcements",
    "title": "06-analysis",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 04 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\nHW02 due Monday (2/6; 11:59 PM)\n\nNotes:\n\nHW01 Grades (Canvas) & Feedback (GitHub Issue) Posted\nLab03 Answer Key Posted"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#what-is-eda",
    "href": "content/lectures/06-analysis-slides.html#what-is-eda",
    "title": "06-analysis",
    "section": "What is EDA?",
    "text": "What is EDA?\n\nExploratory data analysis (EDA) is an aproach to analyzing data sets to summarize and understand its main characteristics.\nOften, this is visual….but the visuals do not have to be perfect. (Save that for communication)\nCalculating summary statistics is also part of EDA.\n\n\n\nData tidying/wrangling/manipulation/transformation typically happens before this stage of the analysis.\n\n\n\nThe Goal: KNOW YOUR DATA!"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#consider-no.-of-variables-involved",
    "href": "content/lectures/06-analysis-slides.html#consider-no.-of-variables-involved",
    "title": "06-analysis",
    "section": "Consider: No. of variables involved",
    "text": "Consider: No. of variables involved\n\nUnivariate data analysis - distribution of single variable\nBivariate data analysis - relationship between two variables\nMultivariate data analysis - relationship between many variables at once, usually focusing on the relationship between two while conditioning for others"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#consider-types-of-variables",
    "href": "content/lectures/06-analysis-slides.html#consider-types-of-variables",
    "title": "06-analysis",
    "section": "Consider: Types of variables",
    "text": "Consider: Types of variables\n\nNumerical variables can be classified as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.\nIf the variable is categorical, we can determine if it is ordinal based on whether or not the levels have a natural ordering."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#data-visualization",
    "href": "content/lectures/06-analysis-slides.html#data-visualization",
    "title": "06-analysis",
    "section": "Data visualization",
    "text": "Data visualization\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\n\nData visualization is the creation and study of the visual representation of data.\nThere are many tools for visualizing data (R is one of them), and many approaches/systems within R for making data visualizations (ggplot2 is what we’ll continue to use).\nEDA will involve making plots/visualizing your data"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#paris-paintings",
    "href": "content/lectures/06-analysis-slides.html#paris-paintings",
    "title": "06-analysis",
    "section": "Paris Paintings",
    "text": "Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nSource: Printed catalogs of 28 auction sales in Paris, 1764 - 1780\nData curators Sandra van Ginhoven and Hilary Coe Cronheim (who were PhD students in the Duke Art, Law, and Markets Initiative at the time of putting together this dataset) translated and tabulated the catalogues\n3393 paintings, their prices, and descriptive details from sales catalogues over 60 variables"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auctions-today",
    "href": "content/lectures/06-analysis-slides.html#auctions-today",
    "title": "06-analysis",
    "section": "Auctions today",
    "text": "Auctions today\n\n\n\n\n\n\n\nSource: Sothebys"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auctions-back-in-the-day",
    "href": "content/lectures/06-analysis-slides.html#auctions-back-in-the-day",
    "title": "06-analysis",
    "section": "Auctions back in the day",
    "text": "Auctions back in the day\n\nSource: Pierre-Antoine de Machy, Public Sale at the Hôtel Bullion, Musée Carnavalet, Paris (18th century)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#paris-auction-market",
    "href": "content/lectures/06-analysis-slides.html#paris-auction-market",
    "title": "06-analysis",
    "section": "Paris auction market",
    "text": "Paris auction market"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse",
    "href": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse",
    "title": "06-analysis",
    "section": "Depart pour la chasse",
    "text": "Depart pour la chasse"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#auction-catalogue-text",
    "href": "content/lectures/06-analysis-slides.html#auction-catalogue-text",
    "title": "06-analysis",
    "section": "Auction catalogue text",
    "text": "Auction catalogue text\n\n\n\n\nTwo paintings very rich in composition, of a beautiful execution, and whose merit is very remarkable, each 17 inches 3 lines high, 23 inches wide; the first, painted on wood, comes from the Cabinet of Madame la Comtesse de Verrue; it represents a departure for the hunt: it shows in the front a child on a white horse, a man who gives the horn to gather the dogs, a falconer and other figures nicely distributed across the width of the painting; two horses drinking from a fountain; on the right in the corner a lovely country house topped by a terrace, on which people are at the table, others who play instruments; trees and fabriques pleasantly enrich the background."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse-as-data",
    "href": "content/lectures/06-analysis-slides.html#depart-pour-la-chasse-as-data",
    "title": "06-analysis",
    "section": "Depart pour la chasse as Data",
    "text": "Depart pour la chasse as Data"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#section",
    "href": "content/lectures/06-analysis-slides.html#section",
    "title": "06-analysis",
    "section": "",
    "text": "pp |>\n  filter(name == \"R1777-89a\") |>\n  glimpse()\n\nRows: 1\nColumns: 61\n$ name              <chr> \"R1777-89a\"\n$ sale              <chr> \"R1777\"\n$ lot               <chr> \"89\"\n$ position          <dbl> 0.3755274\n$ dealer            <chr> \"R\"\n$ year              <dbl> 1777\n$ origin_author     <chr> \"D/FL\"\n$ origin_cat        <chr> \"D/FL\"\n$ school_pntg       <chr> \"D/FL\"\n$ diff_origin       <dbl> 0\n$ logprice          <dbl> 8.575462\n$ price             <dbl> 5300\n$ count             <dbl> 1\n$ subject           <chr> \"D\\x8epart pour la chasse\"\n$ authorstandard    <chr> \"Wouwerman, Philips\"\n$ artistliving      <dbl> 0\n$ authorstyle       <chr> NA\n$ author            <chr> \"Philippe Wouwermans\"\n$ winningbidder     <chr> \"Langlier, Jacques for Poullain, Antoine\"\n$ winningbiddertype <chr> \"DC\"\n$ endbuyer          <chr> \"C\"\n$ Interm            <dbl> 1\n$ type_intermed     <chr> \"D\"\n$ Height_in         <dbl> 17.25\n$ Width_in          <dbl> 23\n$ Surface_Rect      <dbl> 396.75\n$ Diam_in           <dbl> NA\n$ Surface_Rnd       <dbl> NA\n$ Shape             <chr> \"squ_rect\"\n$ Surface           <dbl> 396.75\n$ material          <chr> \"bois\"\n$ mat               <chr> \"b\"\n$ materialCat       <chr> \"wood\"\n$ quantity          <dbl> 1\n$ nfigures          <dbl> 0\n$ engraved          <dbl> 0\n$ original          <dbl> 0\n$ prevcoll          <dbl> 1\n$ othartist         <dbl> 0\n$ paired            <dbl> 1\n$ figures           <dbl> 0\n$ finished          <dbl> 0\n$ lrgfont           <dbl> 0\n$ relig             <dbl> 0\n$ landsALL          <dbl> 1\n$ lands_sc          <dbl> 0\n$ lands_elem        <dbl> 1\n$ lands_figs        <dbl> 1\n$ lands_ment        <dbl> 0\n$ arch              <dbl> 1\n$ mytho             <dbl> 0\n$ peasant           <dbl> 0\n$ othgenre          <dbl> 0\n$ singlefig         <dbl> 0\n$ portrait          <dbl> 0\n$ still_life        <dbl> 0\n$ discauth          <dbl> 0\n$ history           <dbl> 0\n$ allegory          <dbl> 0\n$ pastorale         <dbl> 0\n$ other             <dbl> 0"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-numerical-data",
    "href": "content/lectures/06-analysis-slides.html#visualizing-numerical-data",
    "title": "06-analysis",
    "section": "Visualizing numerical data",
    "text": "Visualizing numerical data\nDescribing shapes of numerical distributions\n\nshape:\n\nskewness: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail)\nmodality: unimodal, bimodal, multimodal, uniform\n\ncenter: mean (mean), median (median), mode (not always useful)\nspread: range (range), standard deviation (sd), inter-quartile range (IQR)\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#histograms",
    "href": "content/lectures/06-analysis-slides.html#histograms",
    "title": "06-analysis",
    "section": "Histograms",
    "text": "Histograms\n\nHeightsWidthsPrices\n\n\n\nggplot(data = pp, aes(x = Height_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Height, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = Width_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Width, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 100) +\n  labs(x = \"Price\", y = NULL)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#density-plots",
    "href": "content/lectures/06-analysis-slides.html#density-plots",
    "title": "06-analysis",
    "section": "Density plots",
    "text": "Density plots\n\nggplot(data = pp, mapping = aes(x = Height_in)) +\n  geom_density()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#side-by-side-box-plots",
    "href": "content/lectures/06-analysis-slides.html#side-by-side-box-plots",
    "title": "06-analysis",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = pp, mapping = aes(y = Height_in, x = as.factor(landsALL))) +\n  geom_boxplot()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-categorical-data",
    "href": "content/lectures/06-analysis-slides.html#visualizing-categorical-data",
    "title": "06-analysis",
    "section": "Visualizing categorical data",
    "text": "Visualizing categorical data\n\ncount/proportion of values\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#bar-plots",
    "href": "content/lectures/06-analysis-slides.html#bar-plots",
    "title": "06-analysis",
    "section": "Bar plots",
    "text": "Bar plots\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL))) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#segmented-bar-plots-counts",
    "href": "content/lectures/06-analysis-slides.html#segmented-bar-plots-counts",
    "title": "06-analysis",
    "section": "Segmented bar plots, counts",
    "text": "Segmented bar plots, counts\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#segmented-bar-plots-proportions",
    "href": "content/lectures/06-analysis-slides.html#segmented-bar-plots-proportions",
    "title": "06-analysis",
    "section": "Segmented bar plots, proportions",
    "text": "Segmented bar plots, proportions\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\")"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#your-turn",
    "href": "content/lectures/06-analysis-slides.html#your-turn",
    "title": "06-analysis",
    "section": "Your Turn",
    "text": "Your Turn\n❓ Which of the previous two bar plots is a more useful representation for visualizing the relationship between landscape and painting material?\n\n❓ What else would you want to do/know to complete EDA?\n\n\n🧠 Try to answer at least one thing you’d want to know from the dataset.\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#modelling-1",
    "href": "content/lectures/06-analysis-slides.html#modelling-1",
    "title": "06-analysis",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we focus on linear models (but remember there are other types of models too!)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#packages",
    "href": "content/lectures/06-analysis-slides.html#packages",
    "title": "06-analysis",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nYou’re familiar with the tidyverse:\n\n\nlibrary(tidyverse)\n\n\nThe broom package takes the messy output of built-in functions in R, such as lm, and turns them into tidy data frames.\n\n\nlibrary(broom)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#modelling-the-relationship-between-variables",
    "href": "content/lectures/06-analysis-slides.html#modelling-the-relationship-between-variables",
    "title": "06-analysis",
    "section": "Modelling the relationship between variables",
    "text": "Modelling the relationship between variables\nEDA: Prices\n❗ Describe the distribution of prices of paintings.\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 1000)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#models-as-functions",
    "href": "content/lectures/06-analysis-slides.html#models-as-functions",
    "title": "06-analysis",
    "section": "Models as functions",
    "text": "Models as functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs.\n\nPlug in the inputs and receive back the output\nExample: the formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\), when \\(x\\) is \\(5\\), the output \\(y\\) is \\(22\\)\n\n\ny = 3 * 5 + 7 = 22"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#height-as-a-function-of-width",
    "href": "content/lectures/06-analysis-slides.html#height-as-a-function-of-width",
    "title": "06-analysis",
    "section": "Height as a function of width",
    "text": "Height as a function of width\n❗ Describe the relationship between height and width of paintings."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-1",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-1",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… without the measure of uncertainty around the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-2",
    "href": "content/lectures/06-analysis-slides.html#visualizing-the-linear-model-2",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… with different cosmetic choices for the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, \n              col = \"pink\",      # color\n              lty = 2,           # line type\n              linewidth = 3)     # line weight"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#vocabulary",
    "href": "content/lectures/06-analysis-slides.html#vocabulary",
    "title": "06-analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis (dependent variable)\n\n\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis (independent variables)\n\n\n\n\nPredicted value: Output of the function model function\n\nThe model function gives the typical value of the response variable conditioning on the explanatory variables\n\n\n\n\n\nResiduals: Show how far each case is from its model value\n\nResidual = Observed value - Predicted value\nTells how far above/below the model function each case is"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#residuals",
    "href": "content/lectures/06-analysis-slides.html#residuals",
    "title": "06-analysis",
    "section": "Residuals",
    "text": "Residuals\n❓ What does a negative residual mean? Which paintings on the plot have have negative residuals?"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#section-1",
    "href": "content/lectures/06-analysis-slides.html#section-1",
    "title": "06-analysis",
    "section": "",
    "text": "The plot below displays the relationship between height and width of paintings. It uses a lower alpha level for the points than the previous plots we looked at.\n\n❓ What feature is apparent in this plot that was not (as) apparent in the previous plots? What might be the reason for this feature?"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#landscape-paintings",
    "href": "content/lectures/06-analysis-slides.html#landscape-paintings",
    "title": "06-analysis",
    "section": "Landscape paintings",
    "text": "Landscape paintings\n\nLandscape painting is the depiction in art of landscapes – natural scenery such as mountains, valleys, trees, rivers, and forests, especially where the main subject is a wide view – with its elements arranged into a coherent composition.1\n\nLandscape paintings tend to be wider than longer.\n\nPortrait painting is a genre in painting, where the intent is to depict a human subject.2\n\nPortrait paintings tend to be longer than wider.\n\n\nSource: Wikipedia, Landscape paintingSource: Wikipedia, Portait painting"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#multiple-explanatory-variables",
    "href": "content/lectures/06-analysis-slides.html#multiple-explanatory-variables",
    "title": "06-analysis",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\n❓ How, if at all, does the relationship between width and height of paintings vary by whether or not they have any landscape elements?\n\nggplot(data = pp, aes(x = Width_in, y = Height_in, \n                      color = factor(landsALL))) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"landscape\")"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#models---upsides-and-downsides",
    "href": "content/lectures/06-analysis-slides.html#models---upsides-and-downsides",
    "title": "06-analysis",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modelling over simple visual inspection of data.\n\n\n\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#variation-around-the-model",
    "href": "content/lectures/06-analysis-slides.html#variation-around-the-model",
    "title": "06-analysis",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\n\nStatistics is the explanation of variation in the context of what remains unexplained.\n\n\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#how-do-we-use-models",
    "href": "content/lectures/06-analysis-slides.html#how-do-we-use-models",
    "title": "06-analysis",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nExplanation: Characterize the relationship between \\(y\\) and \\(x\\) via slopes for numerical explanatory variables or differences for categorical explanatory variables (Inference)\nPrediction: Plug in \\(x\\), get the predicted \\(y\\) (Machine Learning)"
  },
  {
    "objectID": "content/lectures/06-analysis-slides.html#suggested-reading",
    "href": "content/lectures/06-analysis-slides.html#suggested-reading",
    "title": "06-analysis",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nIntroduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/06-analysis.html",
    "href": "content/lectures/06-analysis.html",
    "title": "06-analysis",
    "section": "",
    "text": "Q: What if we’re not good at coming up with ideas of what plots to generate from a given dataset?\nA: We’re going to discuss this and practice this! If you’re not sure where to start, the DatatoViz resource is a great place to help with ideas! If you’re still struggling, be sure when we get to case studies to really think to yourself for each plot, why did prof choose this plot? Could another have been better? And, discuss with your teammates (once you have those)!\n\n\nQ: maybe i missed it… but i am confused about the difference between “count” and “proportions”. Would they end up being the same thing? is the code different for doing count vs. proportion?\nA: Another student had this question, and it’s a good one! Count is for the value across the dataset. Proportions are calculated within the group, such that each group’s values are displayed out of 100%. Happy to discuss more if there are questions. We’ll see an example of this in today’s lecture!\n\n\nQ: Not the lecture, but the way to think in the way to get the “right” plot for the data.\nA: So, the good news is that there isn’t typically one right way. There’s room for personalization, personal preferences, and sometimes, there’s just more than one way! The bad news is that it takes time to learn best practices…but that’s part of what this class is all about!\n\n\nQ: Why is ChatGPT mentioned in the lab answers?\nA: Because I assume that some students are using this as a resource. And, that’s great! It’s there so that you get a sense of ChatGPT’s strengths (sometimes it really does give helpful dplyr code or get you started on the right path) and its weaknesses (it’s still important to think critically if you’re choosing to use it as a tool!) ChatGPT is by no means required in this course. In fact, I think labs/assignments may take longer if you are using it. I don’t have a sense yet who would learn the material better!\n\n\n\n\n\nDiscuss Exploratory Data Analysis (EDA)\nIntroduce modelling as a concept\nPaintings example\n\nEDA\nModelling (Linear)\n\n\n\n\n\nDue Dates:\n\nLab 04 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\nHW02 due Monday (2/6; 11:59 PM)\n\nNotes:\n\nHW01 Grades (Canvas) & Feedback (GitHub Issue) Posted\nLab03 Answer Key Posted"
  },
  {
    "objectID": "content/lectures/06-analysis.html#what-is-eda",
    "href": "content/lectures/06-analysis.html#what-is-eda",
    "title": "06-analysis",
    "section": "What is EDA?",
    "text": "What is EDA?\n\nExploratory data analysis (EDA) is an aproach to analyzing data sets to summarize and understand its main characteristics.\nOften, this is visual….but the visuals do not have to be perfect. (Save that for communication)\nCalculating summary statistics is also part of EDA.\n\n\n\nData tidying/wrangling/manipulation/transformation typically happens before this stage of the analysis.\n\n\n\nThe Goal: KNOW YOUR DATA!"
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "href": "content/lectures/06-analysis.html#consider-no.-of-variables-involved",
    "title": "06-analysis",
    "section": "Consider: No. of variables involved",
    "text": "Consider: No. of variables involved\n\nUnivariate data analysis - distribution of single variable\nBivariate data analysis - relationship between two variables\nMultivariate data analysis - relationship between many variables at once, usually focusing on the relationship between two while conditioning for others"
  },
  {
    "objectID": "content/lectures/06-analysis.html#consider-types-of-variables",
    "href": "content/lectures/06-analysis.html#consider-types-of-variables",
    "title": "06-analysis",
    "section": "Consider: Types of variables",
    "text": "Consider: Types of variables\n\nNumerical variables can be classified as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.\nIf the variable is categorical, we can determine if it is ordinal based on whether or not the levels have a natural ordering."
  },
  {
    "objectID": "content/lectures/06-analysis.html#data-visualization",
    "href": "content/lectures/06-analysis.html#data-visualization",
    "title": "06-analysis",
    "section": "Data visualization",
    "text": "Data visualization\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\n\nData visualization is the creation and study of the visual representation of data.\nThere are many tools for visualizing data (R is one of them), and many approaches/systems within R for making data visualizations (ggplot2 is what we’ll continue to use).\nEDA will involve making plots/visualizing your data"
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-paintings",
    "href": "content/lectures/06-analysis.html#paris-paintings",
    "title": "06-analysis",
    "section": "Paris Paintings",
    "text": "Paris Paintings\n\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n\n\nSource: Printed catalogs of 28 auction sales in Paris, 1764 - 1780\nData curators Sandra van Ginhoven and Hilary Coe Cronheim (who were PhD students in the Duke Art, Law, and Markets Initiative at the time of putting together this dataset) translated and tabulated the catalogues\n3393 paintings, their prices, and descriptive details from sales catalogues over 60 variables"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-today",
    "href": "content/lectures/06-analysis.html#auctions-today",
    "title": "06-analysis",
    "section": "Auctions today",
    "text": "Auctions today\n\n\n\n\n\n\n\nSource: Sothebys"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "href": "content/lectures/06-analysis.html#auctions-back-in-the-day",
    "title": "06-analysis",
    "section": "Auctions back in the day",
    "text": "Auctions back in the day\n\n\n\n\n\nSource: Pierre-Antoine de Machy, Public Sale at the Hôtel Bullion, Musée Carnavalet, Paris (18th century)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#paris-auction-market",
    "href": "content/lectures/06-analysis.html#paris-auction-market",
    "title": "06-analysis",
    "section": "Paris auction market",
    "text": "Paris auction market"
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse",
    "title": "06-analysis",
    "section": "Depart pour la chasse",
    "text": "Depart pour la chasse"
  },
  {
    "objectID": "content/lectures/06-analysis.html#auction-catalogue-text",
    "href": "content/lectures/06-analysis.html#auction-catalogue-text",
    "title": "06-analysis",
    "section": "Auction catalogue text",
    "text": "Auction catalogue text\n\n\n\n\nTwo paintings very rich in composition, of a beautiful execution, and whose merit is very remarkable, each 17 inches 3 lines high, 23 inches wide; the first, painted on wood, comes from the Cabinet of Madame la Comtesse de Verrue; it represents a departure for the hunt: it shows in the front a child on a white horse, a man who gives the horn to gather the dogs, a falconer and other figures nicely distributed across the width of the painting; two horses drinking from a fountain; on the right in the corner a lovely country house topped by a terrace, on which people are at the table, others who play instruments; trees and fabriques pleasantly enrich the background."
  },
  {
    "objectID": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "href": "content/lectures/06-analysis.html#depart-pour-la-chasse-as-data",
    "title": "06-analysis",
    "section": "Depart pour la chasse as Data",
    "text": "Depart pour la chasse as Data"
  },
  {
    "objectID": "content/lectures/06-analysis.html#section",
    "href": "content/lectures/06-analysis.html#section",
    "title": "06-analysis",
    "section": "",
    "text": "pp |>\n  filter(name == \"R1777-89a\") |>\n  glimpse()\n\nRows: 1\nColumns: 61\n$ name              <chr> \"R1777-89a\"\n$ sale              <chr> \"R1777\"\n$ lot               <chr> \"89\"\n$ position          <dbl> 0.3755274\n$ dealer            <chr> \"R\"\n$ year              <dbl> 1777\n$ origin_author     <chr> \"D/FL\"\n$ origin_cat        <chr> \"D/FL\"\n$ school_pntg       <chr> \"D/FL\"\n$ diff_origin       <dbl> 0\n$ logprice          <dbl> 8.575462\n$ price             <dbl> 5300\n$ count             <dbl> 1\n$ subject           <chr> \"D\\x8epart pour la chasse\"\n$ authorstandard    <chr> \"Wouwerman, Philips\"\n$ artistliving      <dbl> 0\n$ authorstyle       <chr> NA\n$ author            <chr> \"Philippe Wouwermans\"\n$ winningbidder     <chr> \"Langlier, Jacques for Poullain, Antoine\"\n$ winningbiddertype <chr> \"DC\"\n$ endbuyer          <chr> \"C\"\n$ Interm            <dbl> 1\n$ type_intermed     <chr> \"D\"\n$ Height_in         <dbl> 17.25\n$ Width_in          <dbl> 23\n$ Surface_Rect      <dbl> 396.75\n$ Diam_in           <dbl> NA\n$ Surface_Rnd       <dbl> NA\n$ Shape             <chr> \"squ_rect\"\n$ Surface           <dbl> 396.75\n$ material          <chr> \"bois\"\n$ mat               <chr> \"b\"\n$ materialCat       <chr> \"wood\"\n$ quantity          <dbl> 1\n$ nfigures          <dbl> 0\n$ engraved          <dbl> 0\n$ original          <dbl> 0\n$ prevcoll          <dbl> 1\n$ othartist         <dbl> 0\n$ paired            <dbl> 1\n$ figures           <dbl> 0\n$ finished          <dbl> 0\n$ lrgfont           <dbl> 0\n$ relig             <dbl> 0\n$ landsALL          <dbl> 1\n$ lands_sc          <dbl> 0\n$ lands_elem        <dbl> 1\n$ lands_figs        <dbl> 1\n$ lands_ment        <dbl> 0\n$ arch              <dbl> 1\n$ mytho             <dbl> 0\n$ peasant           <dbl> 0\n$ othgenre          <dbl> 0\n$ singlefig         <dbl> 0\n$ portrait          <dbl> 0\n$ still_life        <dbl> 0\n$ discauth          <dbl> 0\n$ history           <dbl> 0\n$ allegory          <dbl> 0\n$ pastorale         <dbl> 0\n$ other             <dbl> 0"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "href": "content/lectures/06-analysis.html#visualizing-numerical-data",
    "title": "06-analysis",
    "section": "Visualizing numerical data",
    "text": "Visualizing numerical data\nDescribing shapes of numerical distributions\n\nshape:\n\nskewness: right-skewed, left-skewed, symmetric (skew is to the side of the longer tail)\nmodality: unimodal, bimodal, multimodal, uniform\n\ncenter: mean (mean), median (median), mode (not always useful)\nspread: range (range), standard deviation (sd), inter-quartile range (IQR)\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis.html#histograms",
    "href": "content/lectures/06-analysis.html#histograms",
    "title": "06-analysis",
    "section": "Histograms",
    "text": "Histograms\n\nHeightsWidthsPrices\n\n\n\nggplot(data = pp, aes(x = Height_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Height, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = Width_in)) +\n  geom_histogram(binwidth = 5) +\n  labs(x = \"Width, in inches\", y = NULL)\n\n\n\n\n\n\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 100) +\n  labs(x = \"Price\", y = NULL)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#density-plots",
    "href": "content/lectures/06-analysis.html#density-plots",
    "title": "06-analysis",
    "section": "Density plots",
    "text": "Density plots\n\nggplot(data = pp, mapping = aes(x = Height_in)) +\n  geom_density()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "href": "content/lectures/06-analysis.html#side-by-side-box-plots",
    "title": "06-analysis",
    "section": "Side-by-side box plots",
    "text": "Side-by-side box plots\n\nggplot(data = pp, mapping = aes(y = Height_in, x = as.factor(landsALL))) +\n  geom_boxplot()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "href": "content/lectures/06-analysis.html#visualizing-categorical-data",
    "title": "06-analysis",
    "section": "Visualizing categorical data",
    "text": "Visualizing categorical data\n\ncount/proportion of values\nunusual observations"
  },
  {
    "objectID": "content/lectures/06-analysis.html#bar-plots",
    "href": "content/lectures/06-analysis.html#bar-plots",
    "title": "06-analysis",
    "section": "Bar plots",
    "text": "Bar plots\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL))) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-counts",
    "title": "06-analysis",
    "section": "Segmented bar plots, counts",
    "text": "Segmented bar plots, counts\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "href": "content/lectures/06-analysis.html#segmented-bar-plots-proportions",
    "title": "06-analysis",
    "section": "Segmented bar plots, proportions",
    "text": "Segmented bar plots, proportions\n\nggplot(data = pp, mapping = aes(x = as.factor(landsALL), fill = materialCat)) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\")"
  },
  {
    "objectID": "content/lectures/06-analysis.html#your-turn",
    "href": "content/lectures/06-analysis.html#your-turn",
    "title": "06-analysis",
    "section": "Your Turn",
    "text": "Your Turn\n❓ Which of the previous two bar plots is a more useful representation for visualizing the relationship between landscape and painting material?\n\n❓ What else would you want to do/know to complete EDA?\n\n\n🧠 Try to answer at least one thing you’d want to know from the dataset.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-1",
    "href": "content/lectures/06-analysis.html#modelling-1",
    "title": "06-analysis",
    "section": "Modelling",
    "text": "Modelling\n\nUse models to explain the relationship between variables and to make predictions\nFor now we focus on linear models (but remember there are other types of models too!)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#packages",
    "href": "content/lectures/06-analysis.html#packages",
    "title": "06-analysis",
    "section": "Packages",
    "text": "Packages\n\n\n\n\n\n\nYou’re familiar with the tidyverse:\n\n\nlibrary(tidyverse)\n\n\nThe broom package takes the messy output of built-in functions in R, such as lm, and turns them into tidy data frames.\n\n\nlibrary(broom)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "href": "content/lectures/06-analysis.html#modelling-the-relationship-between-variables",
    "title": "06-analysis",
    "section": "Modelling the relationship between variables",
    "text": "Modelling the relationship between variables\nEDA: Prices\n❗ Describe the distribution of prices of paintings.\n\nggplot(data = pp, aes(x = price)) +\n  geom_histogram(binwidth = 1000)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#models-as-functions",
    "href": "content/lectures/06-analysis.html#models-as-functions",
    "title": "06-analysis",
    "section": "Models as functions",
    "text": "Models as functions\n\nWe can represent relationships between variables using functions\nA function is a mathematical concept: the relationship between an output and one or more inputs.\n\nPlug in the inputs and receive back the output\nExample: the formula \\(y = 3x + 7\\) is a function with input \\(x\\) and output \\(y\\), when \\(x\\) is \\(5\\), the output \\(y\\) is \\(22\\)\n\n\ny = 3 * 5 + 7 = 22"
  },
  {
    "objectID": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "href": "content/lectures/06-analysis.html#height-as-a-function-of-width",
    "title": "06-analysis",
    "section": "Height as a function of width",
    "text": "Height as a function of width\n❗ Describe the relationship between height and width of paintings."
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-1",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… without the measure of uncertainty around the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) # lm for linear model"
  },
  {
    "objectID": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "href": "content/lectures/06-analysis.html#visualizing-the-linear-model-2",
    "title": "06-analysis",
    "section": "Visualizing the linear model",
    "text": "Visualizing the linear model\n… with different cosmetic choices for the line\n\nggplot(data = pp, aes(x = Width_in, y = Height_in)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, \n              col = \"pink\",      # color\n              lty = 2,           # line type\n              linewidth = 3)     # line weight"
  },
  {
    "objectID": "content/lectures/06-analysis.html#vocabulary",
    "href": "content/lectures/06-analysis.html#vocabulary",
    "title": "06-analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nResponse variable: Variable whose behavior or variation you are trying to understand, on the y-axis (dependent variable)\n\n\n\nExplanatory variables: Other variables that you want to use to explain the variation in the response, on the x-axis (independent variables)\n\n\n\n\nPredicted value: Output of the function model function\n\nThe model function gives the typical value of the response variable conditioning on the explanatory variables\n\n\n\n\n\nResiduals: Show how far each case is from its model value\n\nResidual = Observed value - Predicted value\nTells how far above/below the model function each case is"
  },
  {
    "objectID": "content/lectures/06-analysis.html#residuals",
    "href": "content/lectures/06-analysis.html#residuals",
    "title": "06-analysis",
    "section": "Residuals",
    "text": "Residuals\n❓ What does a negative residual mean? Which paintings on the plot have have negative residuals?"
  },
  {
    "objectID": "content/lectures/06-analysis.html#section-1",
    "href": "content/lectures/06-analysis.html#section-1",
    "title": "06-analysis",
    "section": "",
    "text": "The plot below displays the relationship between height and width of paintings. It uses a lower alpha level for the points than the previous plots we looked at.\n\n\n\n\n\n❓ What feature is apparent in this plot that was not (as) apparent in the previous plots? What might be the reason for this feature?"
  },
  {
    "objectID": "content/lectures/06-analysis.html#landscape-paintings",
    "href": "content/lectures/06-analysis.html#landscape-paintings",
    "title": "06-analysis",
    "section": "Landscape paintings",
    "text": "Landscape paintings\n\nLandscape painting is the depiction in art of landscapes – natural scenery such as mountains, valleys, trees, rivers, and forests, especially where the main subject is a wide view – with its elements arranged into a coherent composition.1\n\nLandscape paintings tend to be wider than longer.\n\nPortrait painting is a genre in painting, where the intent is to depict a human subject.2\n\nPortrait paintings tend to be longer than wider."
  },
  {
    "objectID": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "href": "content/lectures/06-analysis.html#multiple-explanatory-variables",
    "title": "06-analysis",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\n❓ How, if at all, does the relationship between width and height of paintings vary by whether or not they have any landscape elements?\n\nggplot(data = pp, aes(x = Width_in, y = Height_in, \n                      color = factor(landsALL))) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(color = \"landscape\")"
  },
  {
    "objectID": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "href": "content/lectures/06-analysis.html#models---upsides-and-downsides",
    "title": "06-analysis",
    "section": "Models - upsides and downsides",
    "text": "Models - upsides and downsides\n\nModels can sometimes reveal patterns that are not evident in a graph of the data. This is a great advantage of modelling over simple visual inspection of data.\n\n\n\nThere is a real risk, however, that a model is imposing structure that is not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted."
  },
  {
    "objectID": "content/lectures/06-analysis.html#variation-around-the-model",
    "href": "content/lectures/06-analysis.html#variation-around-the-model",
    "title": "06-analysis",
    "section": "Variation around the model…",
    "text": "Variation around the model…\nis just as important as the model, if not more!\n\nStatistics is the explanation of variation in the context of what remains unexplained.\n\n\n\nThe scatter suggests that there might be other factors that account for large parts of painting-to-painting variability, or perhaps just that randomness plays a big role.\nAdding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We’ll talk more about this later.)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#how-do-we-use-models",
    "href": "content/lectures/06-analysis.html#how-do-we-use-models",
    "title": "06-analysis",
    "section": "How do we use models?",
    "text": "How do we use models?\n\nExplanation: Characterize the relationship between \\(y\\) and \\(x\\) via slopes for numerical explanatory variables or differences for categorical explanatory variables (Inference)\nPrediction: Plug in \\(x\\), get the predicted \\(y\\) (Machine Learning)"
  },
  {
    "objectID": "content/lectures/06-analysis.html#suggested-reading",
    "href": "content/lectures/06-analysis.html#suggested-reading",
    "title": "06-analysis",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nIntroduction to Modern Statistics Chapter 4: Exploring Categorical Data\nIntroduction to Modern Statistics Chapter 5: Exploring Numerical Data\nR4DS Chapter 22: Introduction to Modelling\nR4DS Chapter 23: Model Basics\nR4DS Chapter 24: Model Building"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#qa",
    "href": "content/lectures/05-viz-slides.html#qa",
    "title": "05-viz",
    "section": "Q&A",
    "text": "Q&A\n\nQ: What were those columns in the NC Bike Accident Data we used?\nA: Variables are described here\n\n\nQ: I was confused by wrap vs grid and how should I choose between them.\nA: When you want to generate a plot that uses two other variables in the dataset to determine which subset of the data to plot, grid! When you want to use a single variable to facet your data and want to specify how many columns/rows to display, wrap!\n\n\nQ: When programming in R so far, I often find myself stuck at getting going on a problem and have a different time identifying on where to start. Any tips/advice on how to get past this initial bump in order to start getting through the problem?\nA: This is a common struggle! This may sound like an old-person response, but jotting down what you have and what you want (like on actual paper/iPad) can be really helpful. For example, if you have 3 columns and you know you want to have 3 columns at the end, but you want fewer rows, you can draw a picture of this and help yourself realize you need a filter. Of course when there are multiple steps, the drawings become a bit more complex…but also more helpful! The same can be said for data visualization. Drawing out quickly what you want can help you get started.\n\n\nQ:In what time frame will the lecture survey be available, how many hours after class will the survey be closed\nA: It will be open for at least 2h.\n\n\nQ:For HW1 Q7, I used read_csv and got an error message. I tried read.csv and it worked. Is there any difference between read_csv and read.csv?\nA: Hmm…I’d love to take a look to see what error you got. They are similar and often behave the same way. The difference is read.csv() was made before the tidyverse, so it reads your data in as a dataframe. read_csv() is a function that “plays nicely” with the tidyverse and reads the data in as a tibble/data frame. What does that mean practically? It means that typically each one will read the data in and you’ll get the same number of rows and columns. What could differ would be the column names and/or the column types (depending upon the data). All that said, read_csv() is what I’ll recommend in this course…so that’s why I’m curious about the error you got!"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#course-announcements",
    "href": "content/lectures/05-viz-slides.html#course-announcements",
    "title": "05-viz",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nLab02 Grades (Canvas) & Feedback (GitHub Issue) Posted\nHW02 Now Available\nDiscord? - Campuswire post\n“Vote” on posts when grades released (pink: send message; green: announce in class)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#keep-it-simple",
    "href": "content/lectures/05-viz-slides.html#keep-it-simple",
    "title": "05-viz",
    "section": "Keep it simple",
    "text": "Keep it simple"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-color-to-draw-attention",
    "href": "content/lectures/05-viz-slides.html#use-color-to-draw-attention",
    "title": "05-viz",
    "section": "Use color to draw attention",
    "text": "Use color to draw attention"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#tell-a-story",
    "href": "content/lectures/05-viz-slides.html#tell-a-story",
    "title": "05-viz",
    "section": "Tell a story",
    "text": "Tell a story\n\n\n\n\n\n\n\nCredit: Angela Zoss and Eric Monson, Duke DVS"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#principles-for-effective-visualizations-1",
    "href": "content/lectures/05-viz-slides.html#principles-for-effective-visualizations-1",
    "title": "05-viz",
    "section": "Principles for effective visualizations",
    "text": "Principles for effective visualizations\n\nOrder matters\nPut long categories on the y-axis\nKeep scales consistent\nSelect meaningful colors\nUse meaningful and nonredundant labels"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#data",
    "href": "content/lectures/05-viz-slides.html#data",
    "title": "05-viz",
    "section": "Data",
    "text": "Data\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\n\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\n\n\n\n\n\n\n\n\n\n\nSource: YouGov Survey Results, retrieved Oct 7, 2019"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#the-data-code",
    "href": "content/lectures/05-viz-slides.html#the-data-code",
    "title": "05-viz",
    "section": "The Data: Code",
    "text": "The Data: Code\n\nbrexit <- tibble(\n  opinion = c(\n    rep(\"Right\", 664), rep(\"Wrong\", 787), rep(\"Don't know\", 188)\n  ),\n  region = c(\n    rep(\"london\", 63), rep(\"rest_of_south\", 241), rep(\"midlands_wales\", 145), rep(\"north\", 176), rep(\"scot\", 39),\n    rep(\"london\", 110), rep(\"rest_of_south\", 257), rep(\"midlands_wales\", 152), rep(\"north\", 176), rep(\"scot\", 92),\n    rep(\"london\", 24), rep(\"rest_of_south\", 49), rep(\"midlands_wales\", 57), rep(\"north\", 48), rep(\"scot\", 10)\n  )\n)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#order-matters",
    "href": "content/lectures/05-viz-slides.html#order-matters",
    "title": "05-viz",
    "section": "Order matters",
    "text": "Order matters\nAlphabetical is rarely ideal\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#order-by-frequency",
    "href": "content/lectures/05-viz-slides.html#order-by-frequency",
    "title": "05-viz",
    "section": "Order by frequency",
    "text": "Order by frequency\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_infreq: Reorder factors’ levels by frequency\n\nggplot(brexit, aes(x = fct_infreq(opinion))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar() +\n  labs( \n    x = \"Opinion\", \n    y = \"Count\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#avoiding-alphabetical-order",
    "href": "content/lectures/05-viz-slides.html#avoiding-alphabetical-order",
    "title": "05-viz",
    "section": "Avoiding Alphabetical Order",
    "text": "Avoiding Alphabetical Order\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = region)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-inherent-level-order",
    "href": "content/lectures/05-viz-slides.html#use-inherent-level-order",
    "title": "05-viz",
    "section": "Use inherent level order",
    "text": "Use inherent level order\n\nRelevelPlot\n\n\nfct_relevel: Reorder factor levels using a custom order\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_relevel( \n      region,\n      \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels-1",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels-1",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nRecodePlot\n\n\nfct_recode: Change factor levels by hand\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_recode( \n      region,\n      London = \"london\",\n      `Rest of South` = \"rest_of_south\",\n      `Midlands / Wales` = \"midlands_wales\",\n      North = \"north\",\n      Scotland = \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#put-long-categories-on-the-y-axis",
    "href": "content/lectures/05-viz-slides.html#put-long-categories-on-the-y-axis",
    "title": "05-viz",
    "section": "Put long categories on the y-axis",
    "text": "Put long categories on the y-axis\nLong categories can be hard to read"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#move-them-to-the-y-axis",
    "href": "content/lectures/05-viz-slides.html#move-them-to-the-y-axis",
    "title": "05-viz",
    "section": "Move them to the y-axis",
    "text": "Move them to the y-axis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#and-reverse-the-order-of-levels",
    "href": "content/lectures/05-viz-slides.html#and-reverse-the-order-of-levels",
    "title": "05-viz",
    "section": "And reverse the order of levels",
    "text": "And reverse the order of levels\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_rev: Reverse order of factor levels\n\nggplot(brexit, aes(y = fct_rev(region))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#clean-up-labels-2",
    "href": "content/lectures/05-viz-slides.html#clean-up-labels-2",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = fct_rev(region))) +\n  geom_bar() +\n  labs( \n    x = \"Count\", \n    y = \"Region\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#segmented-bar-plots-can-be-hard-to-read",
    "href": "content/lectures/05-viz-slides.html#segmented-bar-plots-can-be-hard-to-read",
    "title": "05-viz",
    "section": "Segmented bar plots can be hard to read",
    "text": "Segmented bar plots can be hard to read\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region, fill = opinion)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-facets",
    "href": "content/lectures/05-viz-slides.html#use-facets",
    "title": "05-viz",
    "section": "Use facets",
    "text": "Use facets\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = region)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#avoid-redundancy",
    "href": "content/lectures/05-viz-slides.html#avoid-redundancy",
    "title": "05-viz",
    "section": "Avoid redundancy?",
    "text": "Avoid redundancy?"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#redundancy-can-help-tell-a-story",
    "href": "content/lectures/05-viz-slides.html#redundancy-can-help-tell-a-story",
    "title": "05-viz",
    "section": "Redundancy can help tell a story",
    "text": "Redundancy can help tell a story\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#be-selective-with-redundancy",
    "href": "content/lectures/05-viz-slides.html#be-selective-with-redundancy",
    "title": "05-viz",
    "section": "Be selective with redundancy",
    "text": "Be selective with redundancy\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-informative-labels",
    "href": "content/lectures/05-viz-slides.html#use-informative-labels",
    "title": "05-viz",
    "section": "Use informative labels",
    "text": "Use informative labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#a-bit-more-info",
    "href": "content/lectures/05-viz-slides.html#a-bit-more-info",
    "title": "05-viz",
    "section": "A bit more info",
    "text": "A bit more info\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\", \n    caption = \"Source: https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#lets-do-better",
    "href": "content/lectures/05-viz-slides.html#lets-do-better",
    "title": "05-viz",
    "section": "Let’s do better",
    "text": "Let’s do better\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#fix-up-facet-labels",
    "href": "content/lectures/05-viz-slides.html#fix-up-facet-labels",
    "title": "05-viz",
    "section": "Fix up facet labels",
    "text": "Fix up facet labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1,\n    labeller = label_wrap_gen(width = 12) \n  ) + \n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#rainbow-colors-not-always-best",
    "href": "content/lectures/05-viz-slides.html#rainbow-colors-not-always-best",
    "title": "05-viz",
    "section": "Rainbow colors not always best",
    "text": "Rainbow colors not always best"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#manually-choose-colors-when-needed",
    "href": "content/lectures/05-viz-slides.html#manually-choose-colors-when-needed",
    "title": "05-viz",
    "section": "Manually choose colors when needed",
    "text": "Manually choose colors when needed\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c( \n    \"Wrong\" = \"red\", \n    \"Right\" = \"green\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#choosing-better-colors",
    "href": "content/lectures/05-viz-slides.html#choosing-better-colors",
    "title": "05-viz",
    "section": "Choosing better colors",
    "text": "Choosing better colors\ncolorbrewer2.org"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#use-better-colors",
    "href": "content/lectures/05-viz-slides.html#use-better-colors",
    "title": "05-viz",
    "section": "Use better colors",
    "text": "Use better colors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\", \n    \"Right\" = \"#67a9cf\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#select-theme",
    "href": "content/lectures/05-viz-slides.html#select-theme",
    "title": "05-viz",
    "section": "Select theme",
    "text": "Select theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal() \n\n\n\n\n\n\nggthemes described here"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#customize-theme",
    "href": "content/lectures/05-viz-slides.html#customize-theme",
    "title": "05-viz",
    "section": "Customize theme",
    "text": "Customize theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal(base_size = 16) + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#your-turn",
    "href": "content/lectures/05-viz-slides.html#your-turn",
    "title": "05-viz",
    "section": "Your Turn",
    "text": "Your Turn\n\nRead in the data (Data slide)\nThink of at least three different ways to tell slightly different stories with these data\nTry to implement at least one of these ideas!"
  },
  {
    "objectID": "content/lectures/05-viz-slides.html#suggested-reading",
    "href": "content/lectures/05-viz-slides.html#suggested-reading",
    "title": "05-viz",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/05-viz.html",
    "href": "content/lectures/05-viz.html",
    "title": "05-viz",
    "section": "",
    "text": "Slides modified from datascienceinabox.org\n\n\n\nQ: What were those columns in the NC Bike Accident Data we used?\nA: Variables are described here\n\n\nQ: I was confused by wrap vs grid and how should I choose between them.\nA: When you want to generate a plot that uses two other variables in the dataset to determine which subset of the data to plot, grid! When you want to use a single variable to facet your data and want to specify how many columns/rows to display, wrap!\n\n\nQ: When programming in R so far, I often find myself stuck at getting going on a problem and have a different time identifying on where to start. Any tips/advice on how to get past this initial bump in order to start getting through the problem?\nA: This is a common struggle! This may sound like an old-person response, but jotting down what you have and what you want (like on actual paper/iPad) can be really helpful. For example, if you have 3 columns and you know you want to have 3 columns at the end, but you want fewer rows, you can draw a picture of this and help yourself realize you need a filter. Of course when there are multiple steps, the drawings become a bit more complex…but also more helpful! The same can be said for data visualization. Drawing out quickly what you want can help you get started.\n\n\nQ:In what time frame will the lecture survey be available, how many hours after class will the survey be closed\nA: It will be open for at least 2h.\n\n\nQ:For HW1 Q7, I used read_csv and got an error message. I tried read.csv and it worked. Is there any difference between read_csv and read.csv?\nA: Hmm…I’d love to take a look to see what error you got. They are similar and often behave the same way. The difference is read.csv() was made before the tidyverse, so it reads your data in as a dataframe. read_csv() is a function that “plays nicely” with the tidyverse and reads the data in as a tibble/data frame. What does that mean practically? It means that typically each one will read the data in and you’ll get the same number of rows and columns. What could differ would be the column names and/or the column types (depending upon the data). All that said, read_csv() is what I’ll recommend in this course…so that’s why I’m curious about the error you got!\n\n\n\n\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nLab02 Grades (Canvas) & Feedback (GitHub Issue) Posted\nHW02 Now Available\nDiscord? - Campuswire post\n“Vote” on posts when grades released (pink: send message; green: announce in class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit: Angela Zoss and Eric Monson, Duke DVS"
  },
  {
    "objectID": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "href": "content/lectures/05-viz.html#principles-for-effective-visualizations-1",
    "title": "05-viz",
    "section": "Principles for effective visualizations",
    "text": "Principles for effective visualizations\n\nOrder matters\nPut long categories on the y-axis\nKeep scales consistent\nSelect meaningful colors\nUse meaningful and nonredundant labels"
  },
  {
    "objectID": "content/lectures/05-viz.html#data",
    "href": "content/lectures/05-viz.html#data",
    "title": "05-viz",
    "section": "Data",
    "text": "Data\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\n\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\n\n\n\n\n\n\n\n\n\n\nSource: YouGov Survey Results, retrieved Oct 7, 2019"
  },
  {
    "objectID": "content/lectures/05-viz.html#the-data-code",
    "href": "content/lectures/05-viz.html#the-data-code",
    "title": "05-viz",
    "section": "The Data: Code",
    "text": "The Data: Code\n\nbrexit <- tibble(\n  opinion = c(\n    rep(\"Right\", 664), rep(\"Wrong\", 787), rep(\"Don't know\", 188)\n  ),\n  region = c(\n    rep(\"london\", 63), rep(\"rest_of_south\", 241), rep(\"midlands_wales\", 145), rep(\"north\", 176), rep(\"scot\", 39),\n    rep(\"london\", 110), rep(\"rest_of_south\", 257), rep(\"midlands_wales\", 152), rep(\"north\", 176), rep(\"scot\", 92),\n    rep(\"london\", 24), rep(\"rest_of_south\", 49), rep(\"midlands_wales\", 57), rep(\"north\", 48), rep(\"scot\", 10)\n  )\n)"
  },
  {
    "objectID": "content/lectures/05-viz.html#order-matters",
    "href": "content/lectures/05-viz.html#order-matters",
    "title": "05-viz",
    "section": "Order matters",
    "text": "Order matters\nAlphabetical is rarely ideal\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#order-by-frequency",
    "href": "content/lectures/05-viz.html#order-by-frequency",
    "title": "05-viz",
    "section": "Order by frequency",
    "text": "Order by frequency\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_infreq: Reorder factors’ levels by frequency\n\nggplot(brexit, aes(x = fct_infreq(opinion))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels",
    "href": "content/lectures/05-viz.html#clean-up-labels",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = opinion)) +\n  geom_bar() +\n  labs( \n    x = \"Opinion\", \n    y = \"Count\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "href": "content/lectures/05-viz.html#avoiding-alphabetical-order",
    "title": "05-viz",
    "section": "Avoiding Alphabetical Order",
    "text": "Avoiding Alphabetical Order\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(x = region)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-inherent-level-order",
    "href": "content/lectures/05-viz.html#use-inherent-level-order",
    "title": "05-viz",
    "section": "Use inherent level order",
    "text": "Use inherent level order\n\nRelevelPlot\n\n\nfct_relevel: Reorder factor levels using a custom order\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_relevel( \n      region,\n      \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-1",
    "href": "content/lectures/05-viz.html#clean-up-labels-1",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nRecodePlot\n\n\nfct_recode: Change factor levels by hand\n\nbrexit <- brexit |>\n  mutate(\n    region = fct_recode( \n      region,\n      London = \"london\",\n      `Rest of South` = \"rest_of_south\",\n      `Midlands / Wales` = \"midlands_wales\",\n      North = \"north\",\n      Scotland = \"scot\"\n    )\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "href": "content/lectures/05-viz.html#put-long-categories-on-the-y-axis",
    "title": "05-viz",
    "section": "Put long categories on the y-axis",
    "text": "Put long categories on the y-axis\nLong categories can be hard to read"
  },
  {
    "objectID": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "href": "content/lectures/05-viz.html#move-them-to-the-y-axis",
    "title": "05-viz",
    "section": "Move them to the y-axis",
    "text": "Move them to the y-axis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "href": "content/lectures/05-viz.html#and-reverse-the-order-of-levels",
    "title": "05-viz",
    "section": "And reverse the order of levels",
    "text": "And reverse the order of levels\n\nPlotCode\n\n\n\n\n\n\n\n\n\nfct_rev: Reverse order of factor levels\n\nggplot(brexit, aes(y = fct_rev(region))) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#clean-up-labels-2",
    "href": "content/lectures/05-viz.html#clean-up-labels-2",
    "title": "05-viz",
    "section": "Clean up labels",
    "text": "Clean up labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = fct_rev(region))) +\n  geom_bar() +\n  labs( \n    x = \"Count\", \n    y = \"Region\" \n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "href": "content/lectures/05-viz.html#segmented-bar-plots-can-be-hard-to-read",
    "title": "05-viz",
    "section": "Segmented bar plots can be hard to read",
    "text": "Segmented bar plots can be hard to read\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = region, fill = opinion)) + \n  geom_bar()"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-facets",
    "href": "content/lectures/05-viz.html#use-facets",
    "title": "05-viz",
    "section": "Use facets",
    "text": "Use facets\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = region)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz.html#avoid-redundancy",
    "href": "content/lectures/05-viz.html#avoid-redundancy",
    "title": "05-viz",
    "section": "Avoid redundancy?",
    "text": "Avoid redundancy?"
  },
  {
    "objectID": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "href": "content/lectures/05-viz.html#redundancy-can-help-tell-a-story",
    "title": "05-viz",
    "section": "Redundancy can help tell a story",
    "text": "Redundancy can help tell a story\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1)"
  },
  {
    "objectID": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "href": "content/lectures/05-viz.html#be-selective-with-redundancy",
    "title": "05-viz",
    "section": "Be selective with redundancy",
    "text": "Be selective with redundancy\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-informative-labels",
    "href": "content/lectures/05-viz.html#use-informative-labels",
    "title": "05-viz",
    "section": "Use informative labels",
    "text": "Use informative labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#a-bit-more-info",
    "href": "content/lectures/05-viz.html#a-bit-more-info",
    "title": "05-viz",
    "section": "A bit more info",
    "text": "A bit more info\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\", \n    caption = \"Source: https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/x0msmggx08/YouGov%20-%20Brexit%20and%202019%20election.pdf\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#lets-do-better",
    "href": "content/lectures/05-viz.html#lets-do-better",
    "title": "05-viz",
    "section": "Let’s do better",
    "text": "Let’s do better\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\", \n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#fix-up-facet-labels",
    "href": "content/lectures/05-viz.html#fix-up-facet-labels",
    "title": "05-viz",
    "section": "Fix up facet labels",
    "text": "Fix up facet labels\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1,\n    labeller = label_wrap_gen(width = 12) \n  ) + \n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  )"
  },
  {
    "objectID": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "href": "content/lectures/05-viz.html#rainbow-colors-not-always-best",
    "title": "05-viz",
    "section": "Rainbow colors not always best",
    "text": "Rainbow colors not always best"
  },
  {
    "objectID": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "href": "content/lectures/05-viz.html#manually-choose-colors-when-needed",
    "title": "05-viz",
    "section": "Manually choose colors when needed",
    "text": "Manually choose colors when needed\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c( \n    \"Wrong\" = \"red\", \n    \"Right\" = \"green\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz.html#choosing-better-colors",
    "href": "content/lectures/05-viz.html#choosing-better-colors",
    "title": "05-viz",
    "section": "Choosing better colors",
    "text": "Choosing better colors\ncolorbrewer2.org"
  },
  {
    "objectID": "content/lectures/05-viz.html#use-better-colors",
    "href": "content/lectures/05-viz.html#use-better-colors",
    "title": "05-viz",
    "section": "Use better colors",
    "text": "Use better colors\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\", \n    \"Right\" = \"#67a9cf\", \n    \"Don't know\" = \"gray\" \n  ))"
  },
  {
    "objectID": "content/lectures/05-viz.html#select-theme",
    "href": "content/lectures/05-viz.html#select-theme",
    "title": "05-viz",
    "section": "Select theme",
    "text": "Select theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal() \n\n\n\n\n\n\nggthemes described here"
  },
  {
    "objectID": "content/lectures/05-viz.html#customize-theme",
    "href": "content/lectures/05-viz.html#customize-theme",
    "title": "05-viz",
    "section": "Customize theme",
    "text": "Customize theme\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(title = \"Was Britain right/wrong to vote to leave EU?\",\n       subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n       caption = \"Source: bit.ly/2lCJZVg\",\n       x = NULL, y = NULL) +\n  scale_fill_manual(values = c(\"Wrong\" = \"#ef8a62\",\n                               \"Right\" = \"#67a9cf\",\n                               \"Don't know\" = \"gray\")) +\n  theme_minimal(base_size = 16) + \n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank())"
  },
  {
    "objectID": "content/lectures/05-viz.html#your-turn",
    "href": "content/lectures/05-viz.html#your-turn",
    "title": "05-viz",
    "section": "Your Turn",
    "text": "Your Turn\n\nRead in the data (Data slide)\nThink of at least three different ways to tell slightly different stories with these data\nTry to implement at least one of these ideas!"
  },
  {
    "objectID": "content/lectures/05-viz.html#suggested-reading",
    "href": "content/lectures/05-viz.html#suggested-reading",
    "title": "05-viz",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 28: Graphics for Communication\nThe Glamour of Graphics: [video] [slides] [Prof’s slides inspired by Will’s talk]"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#qa",
    "href": "content/lectures/09-projects-slides.html#qa",
    "title": "09-projects",
    "section": "Q&A",
    "text": "Q&A\n\nQ: When is the presentation?\nA: Discussing this today! It’s at the end of the quarter. While written reports will be completed throughout the rest of the quarter as we do case studies, an oral presentation will be part of your final project. These will be able to be recorded or given live in person during finals week.\n\n\nQ: Will we have another lab with as many questions as Lab 4? The turnaround was pretty stressful, so just want to be prepared.\nA: The next lab (multiple linear regression) is also a tady lengthy, but after that I don’t plan on the rest being quite as long. Just as a reminder that you do not need to complete the entire lab to receive credit!\n\n\nQ: I’m not really comfortable with log transformations yet. Will we get more practice on that?\nA: Yup! Last lecture was a first introduction. We’ll return to this in upcoming case studies. The midterm does not require any transformations."
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#course-announcements",
    "href": "content/lectures/09-projects-slides.html#course-announcements",
    "title": "09-projects",
    "section": "Course Announcements",
    "text": "Course Announcements\n\nLecture Participation survey “due” after class\nMidterm due Monday (2/13; 11:59 PM):\n\nreleased Friday (tomorrow) after lab\ncompleted individually\n\nPractice Midterm Answer Key Posted"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#agenda",
    "href": "content/lectures/09-projects-slides.html#agenda",
    "title": "09-projects",
    "section": "Agenda",
    "text": "Agenda\n\nExam chat\nHW02 recap\nCase Studies\nFinal Project"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#midterm-details",
    "href": "content/lectures/09-projects-slides.html#midterm-details",
    "title": "09-projects",
    "section": "Midterm Details",
    "text": "Midterm Details\n\n\nInstructions will be posted on the website at 2PM Fri (tomorrow)\nYou’ll be provided a template (link on Canvas) and “submit” on GitHub\nYou’ll be provided with data and a data dictionary\nCovers data wrangling/tidying, dplyr, viz/ggplot2, and linear regression/tidymodels"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#hw02-q1",
    "href": "content/lectures/09-projects-slides.html#hw02-q1",
    "title": "09-projects",
    "section": "HW02 : Q1",
    "text": "HW02 : Q1\nGenerate a visualization that will allow readers to determine whether male or female penguins are larger (by mass)."
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#boxplot",
    "href": "content/lectures/09-projects-slides.html#boxplot",
    "title": "09-projects",
    "section": "Boxplot",
    "text": "Boxplot\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npenguins |>\n  drop_na() |> \n  ggplot(aes(x = sex, y = body_mass_g)) +\n  geom_boxplot() +\n  labs(title = \"Penguin body mass by sex\", \n       y = \"body mass (g)\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#histogram",
    "href": "content/lectures/09-projects-slides.html#histogram",
    "title": "09-projects",
    "section": "Histogram",
    "text": "Histogram\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npenguins |>\n  filter(!is.na(sex)) |>\n  ggplot(mapping = aes(x = body_mass_g, fill = sex)) +\n  geom_histogram() +\n  labs(\n    title = 'Body Mass Distribution by Sex',\n    x = 'Body Mass (g)',\n    y = 'Count',\n    color = 'Sex'\n  )"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#faceted-histograms",
    "href": "content/lectures/09-projects-slides.html#faceted-histograms",
    "title": "09-projects",
    "section": "Faceted Histograms",
    "text": "Faceted Histograms\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npenguins |>\n  filter(!is.na(body_mass_g)) |>\n  ggplot(., mapping=aes(y=body_mass_g)) + \n  geom_histogram(binwidth=100) +\n  facet_grid(. ~ sex) + \n  labs(\n    title='Frequency of Penguins based on their Body mass and Female/Male Penguins',\n    x='Frequency / Count',\n    y='Body Mass (of penguins, in g (grams))')"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#hw02-q2",
    "href": "content/lectures/09-projects-slides.html#hw02-q2",
    "title": "09-projects",
    "section": "HW02 : Q2",
    "text": "HW02 : Q2\nGenerate a barplot that visualizes how many penguins there are from each species on each island. Each island should be a different panel (in a 1 row x 3 columns visualization), and each chart should visualize the species count."
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#barplot",
    "href": "content/lectures/09-projects-slides.html#barplot",
    "title": "09-projects",
    "section": "Barplot",
    "text": "Barplot\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar() +\n  facet_wrap(~ island) +\n  labs(title = \"Count of species per island\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#barplot-with-color",
    "href": "content/lectures/09-projects-slides.html#barplot-with-color",
    "title": "09-projects",
    "section": "Barplot with color",
    "text": "Barplot with color\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n  ggplot(penguins, aes(x = fct_infreq(species), fill = species)) +\n  geom_bar() +\n  facet_wrap(~island, nrow = 1) +\n  guides(fill = \"none\") + \n  labs(\n    title = \"Count of Penguin Species Across the Palmer Archipelago Islands\",\n    x = \"Species\",\n    y = \"Number of Penguins\"\n  )"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#hw02-q3",
    "href": "content/lectures/09-projects-slides.html#hw02-q3",
    "title": "09-projects",
    "section": "HW02 : Q3",
    "text": "HW02 : Q3\nGenerate a scatterplot that will allow the viewer to determine whether flipper length has differed over time. Be sure to color the points on this plot by species."
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#scatterplot-no-jitter",
    "href": "content/lectures/09-projects-slides.html#scatterplot-no-jitter",
    "title": "09-projects",
    "section": "Scatterplot (no jitter)",
    "text": "Scatterplot (no jitter)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = year, \n           y = flipper_length_mm,\n           color = species)) +\ngeom_point() + \nscale_color_viridis_d() +\nscale_x_continuous(n.breaks = 3) +\nlabs(\n  title = \"Flipper Lengths of Penguin Species Over Time\",\n  color = \"Species\",\n  x = \"Year\",\n  y = \"Flipper Length (mm)\"\n)"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#scatterplot-w-jitter",
    "href": "content/lectures/09-projects-slides.html#scatterplot-w-jitter",
    "title": "09-projects",
    "section": "Scatterplot (w/ jitter)",
    "text": "Scatterplot (w/ jitter)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(penguins,\n       mapping = aes(x = factor(year),\n                     y = flipper_length_mm,\n                     color = species)) +\n  scale_color_viridis_d() +\n  geom_jitter(na.rm = TRUE) +\n  labs(title = \"Flipper length of different penguin species by year\",\n       y = \"Flipper length (mm)\",\n       x = \"Year\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#hw02-part-ii",
    "href": "content/lectures/09-projects-slides.html#hw02-part-ii",
    "title": "09-projects",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery\nExample from: Eric Ko\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Eric890916\nchessData <- data.frame(country = c(\"United States\", \"Germany\", \"Canada\", \"Spain\", \"Russia\", \"France\", \"Bosnia and Herzegovina\", \"Croatia\", \"Turkey\", \"Austria\"),\n                        num = c(89, 55, 44, 41, 36, 34, 32, 32, 31, 29))\n\nggplot(chessData, aes(y = reorder(country, num), x = num)) + \n  geom_col(fill = \"#008080\") + \n  geom_text(aes(label = num), hjust = 1, nudge_x = -.5) +\n  labs(title = \"More players transfer to the U.S. than to any other country\",\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       caption = \"2017 data as of April 11. SOURCE: FIDE\",\n       x = \"NUMBER OF TRANSFERS\", y = \"COUNTRY\")"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#section",
    "href": "content/lectures/09-projects-slides.html#section",
    "title": "09-projects",
    "section": "",
    "text": "Example from: Christine Kwon\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommon_first_names <- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/most-common-name/new-top-firstNames.csv\")\n\n# editing data\ncommon_first_names <- common_first_names[1:20, ]\ncommon_first_names <- common_first_names %>%\n  mutate(sex = case_when (name == \"Mary\" | \n                          name == \"Jennifer\" |\n                          name == \"Patricia\" |\n                          name == \"Linda\" |\n                          name == \"Elizabeth\" ~ \"female\",\n                          name != \"Mary\" | \n                          name != \"Jennifer\" |\n                          name != \"Patricia\" |\n                          name != \"Linda\" |\n                          name != \"Elizabeth\" ~ \"male\",),\n         percentage = round(newPerct2013 * 1000, digits = 1))\n\n# creating visualization\ncommon_first_names %>%\n  ggplot(aes(y = reorder(name, percentage),  x = percentage, fill = sex)) +\n  geom_histogram(stat = \"identity\") +\n  guides(fill = \"none\") +\n  annotate(\"text\", x = 9.65, y = 21.7, label = expression(bold(\"MALE\")), cex = 3.85, hjust = 1, vjust = 1, color = \"dodgerblue\") +\n  annotate(\"text\", x = 11.5, y = 21.7, label = expression(bold(\"FEMALE\")), cex = 3.85, hjust = 1, vjust = 1, color = \"gold1\") +\n  geom_text(aes(label = signif(percentage)), nudge_x = 0.5) +\n  labs(title = \"Most Common First Names\",\n       subtitle = \"Per 1,000 Americans as of 2013\") +\n  scale_fill_manual(values = c(\"male\" = \"dodgerblue\",\n                               \"female\" = \"gold1\")) +\n  theme_classic() +\n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank(),\n        plot.title = element_text(size = 16,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 11),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(color = \"black\"),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank())"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#section-1",
    "href": "content/lectures/09-projects-slides.html#section-1",
    "title": "09-projects",
    "section": "",
    "text": "Example by: Cheng Chang (FA21)\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get data\npoll <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls_adjusted.csv\")\npoll_mean <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_toplines.csv\")\n\npoll <- poll |>\n  filter(subject == \"Biden\", party != \"all\") |>\n  mutate(Party=case_when(party == \"D\" ~ \"Democrats\",\n                         party == \"I\" ~ \"Independents\",\n                         party == \"R\" ~ \"Republicans\")) |>\n  mutate(enddate=as.Date(enddate, format=\"%m/%d/%Y\"))\n\npoll_mean <- poll_mean |>\n  filter(subject == \"Biden\", party != \"all\") |>\n  mutate(Party=case_when(party == \"D\" ~ \"Democrats\",\n                         party == \"I\" ~ \"Independents\",\n                         party == \"R\" ~ \"Republicans\")) |>\n  mutate(modeldate=as.Date(modeldate, format=\"%m/%d/%Y\"))\n\nggplot() +\n  geom_point(data=poll,\n             aes(x=enddate, y=approve_adjusted, color=Party),\n             size=1,\n             alpha = 0.5) +\n  geom_path(data=poll_mean, aes(x=modeldate, y=approve_estimate, color=Party)) +\n  labs(title=\"Approval of Biden’s response varies widely by party\",\n       subtitle=\n         \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s\\nhandling of the coronavirus outbreak\",\n       x=NULL,\n       y=NULL) +\n  scale_color_manual(values = c(\"Democrats\" = \"#2acaea\",\n                                \"Independents\" = \"#ce7e00\",\n                                \"Republicans\" = \"#f44336\")) +\n  theme(plot.title.position = \"plot\",\n        panel.grid.major = element_line(color=\"grey\"),\n        panel.border = element_rect(fill=NA, color=\"grey\"),\n        panel.background = element_rect(fill=\"white\"))"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#hw03-part-iii",
    "href": "content/lectures/09-projects-slides.html#hw03-part-iii",
    "title": "09-projects",
    "section": "HW03 : Part III",
    "text": "HW03 : Part III\nTake a Sad Plot & Make It Better\nExample from: Christine Kwon\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nmedals <- tibble(\n  country = c(\n    rep(\"USA\", 79), rep(\"CHN\", 70), rep(\"ROC\", 53), rep(\"GBR\", 48), rep(\"JPN\", 40)),\n  medal_type = c(\n    rep(\"gold\", 25), rep(\"silver\", 31), rep(\"bronze\", 23),\n    rep(\"gold\", 32), rep(\"silver\", 22), rep(\"bronze\", 16),\n    rep(\"gold\", 14), rep(\"silver\", 21), rep(\"bronze\", 18),\n    rep(\"gold\", 15), rep(\"silver\", 18), rep(\"bronze\", 15),\n    rep(\"gold\", 21), rep(\"silver\", 7), rep(\"bronze\", 12)))\n\n# creating visualization\nmedal_viz <- medals %>%\n   mutate(country = factor(country, levels = c(\"JPN\", \"GBR\",\"ROC\", \"CHN\", \"USA\"))) %>%\n  ggplot(aes(y = country, fill = factor(medal_type, levels = c(\"bronze\", \"silver\", \"gold\")))) + \n  geom_bar() +\n  annotate(\"text\", x = 4.5, y = 5.05, label = \"25\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 30.5, y = 5.05, label = \"31\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 61.5, y = 5.05, label = \"23\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 86.5, y = 5.05, label = expression(bold(\"79\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 4.05, label = \"32\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 37.5, y = 4.05, label = \"22\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 58.5, y = 4.05, label = \"16\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 76.5, y = 4.05, label = expression(bold(\"70\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 3.05, label = \"14\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 18.5, y = 3.05, label = \"21\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 39.5, y = 3.05, label = \"18\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 59.5, y = 3.05, label = expression(bold(\"53\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 2.05, label = \"15\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 19.5, y = 2.05, label = \"18\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 38, y = 2.05, label = \"15\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 54.5, y = 2.05, label = expression(bold(\"48\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 1.05, label = \"21\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 23.5, y = 1.05, label = \"7\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 32.5, y = 1.05, label = \"12\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 46.5, y = 1.05, label = expression(bold(\"40\")) , cex = 5, hjust = 1, vjust = 1) +\n  labs(title = \"Medals Won at the Tokyo Olympics (ongoing)\", \n       subtitle = \"Distribution of medals won by the top 5 countries (ordered by total)\", \n       fill = \"Medal Type\") +\n  scale_fill_manual(values = c(\"gold\" = \"gold\",\n                               \"silver\" = \"gray75\",\n                               \"bronze\" = \"tan3\")) +\n  theme(#legend.title = element_text(face = \"bold\"),\n        legend.position = \"top\") + \n  guides(fill = guide_legend(title.position = \"top\")) +\n  theme_classic() +\n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank(),\n        plot.title = element_text(size = 16,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 11),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(color = \"black\", \n                                   #face = \"bold\", \n                                   size = 11),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank())\n\n\nmedal_viz +\n  theme(legend.position = c(0.8, 0.25))"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#opencasestudies",
    "href": "content/lectures/09-projects-slides.html#opencasestudies",
    "title": "09-projects",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\n\nOpenCaseStudies\nUses R/the tidyverse\nasks public health-centric questions\ngoal: to teach statistical analysis/data science through case studies"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#what-well-do",
    "href": "content/lectures/09-projects-slides.html#what-well-do",
    "title": "09-projects",
    "section": "What We’ll Do",
    "text": "What We’ll Do\nFor each case study (2), during lecture:\n\nStats: (1-2d)\nBackground, Data & Wrangling (1-2d)\nEDA & Analysis (1d)\n\n\nFor each case study:\n\nyou’ll also work with case study data in lab.\nyou’ll work in assigned groups of ~3 students to complete a data science report"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#data-science-reports",
    "href": "content/lectures/09-projects-slides.html#data-science-reports",
    "title": "09-projects",
    "section": "Data Science Reports",
    "text": "Data Science Reports\nWith your group, you will:\n\ncarry out all steps of the analysis\n\nsome code will be taken directly from lecture\n\nadd text/organize into a report\n\n\n\nhave to extend the case study"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#what-does-extend-the-case-study-mean",
    "href": "content/lectures/09-projects-slides.html#what-does-extend-the-case-study-mean",
    "title": "09-projects",
    "section": "What does extend the case study mean?",
    "text": "What does extend the case study mean?\nYou’ll need to do something more on the topic beyond what is presented in class.\n\nExamples:\n\nAsking an additional question and answering it from the data provided\nFinding an additional dataset and using it to add to the case study\nGenerating a handful of additional and very informative visualizations (beyond what’s presented in class)"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#grading",
    "href": "content/lectures/09-projects-slides.html#grading",
    "title": "09-projects",
    "section": "Grading",
    "text": "Grading\nGraded on:\n\ncontent (code, text, viz)\neffective written communication\nextension carried out"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#final-project-logistics",
    "href": "content/lectures/09-projects-slides.html#final-project-logistics",
    "title": "09-projects",
    "section": "Final Project Logistics",
    "text": "Final Project Logistics\n\nwill be completed in groups of 3-4 students\nyou get to choose the group\nI will ask at the end of week 7 for your final project groups"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#final-project-details",
    "href": "content/lectures/09-projects-slides.html#final-project-details",
    "title": "09-projects",
    "section": "Final Project Details",
    "text": "Final Project Details\nTwo possible Paths:\n\nCreate a technical presentation on a statistics topic and/or an R package.\nCarry out a data analysis"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#option-1-technical-presentation",
    "href": "content/lectures/09-projects-slides.html#option-1-technical-presentation",
    "title": "09-projects",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\n\n.Rmd document used to make slides\n“Teaches” the details of the R package/statistics topic\nDemonstrates how to use the package and/or carry out the statistical analysis in R\nTopic/Package must go beyond what was taught in this course or what you should have learned in an intro stats course\nPresentation Length: 10-15min"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#option-2-data-analysis",
    "href": "content/lectures/09-projects-slides.html#option-2-data-analysis",
    "title": "09-projects",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\n\n.Rmd document used for data science report\nAsks a question, finds data, analyzes data (basically: a mini case report, but you find the data and formulate the question)\nPresentation Length: 3-5min (brief summary of the full report)"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#wherewhen-for-this-presentation",
    "href": "content/lectures/09-projects-slides.html#wherewhen-for-this-presentation",
    "title": "09-projects",
    "section": "Where/when for this presentation?",
    "text": "Where/when for this presentation?\nYou get to choose:\n\nRecord ahead of time: submit by Th 3/23 of finals week at 11:59 PM\nPresent in-person Th of finals week (slots to sign up for a time will be released later; want this option for those interested in getting more practice)"
  },
  {
    "objectID": "content/lectures/09-projects-slides.html#should-i-be-working-on-my-final-project-now",
    "href": "content/lectures/09-projects-slides.html#should-i-be-working-on-my-final-project-now",
    "title": "09-projects",
    "section": "Should I be working on my final project now?",
    "text": "Should I be working on my final project now?\n…probably not\n\nBut, you should start thinking about/getting a group of 3-4 people together.\n\n\nI’d recommend you start planning/working on your final project around wk 8\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/09-projects.html",
    "href": "content/lectures/09-projects.html",
    "title": "09-projects",
    "section": "",
    "text": "Q: When is the presentation?\nA: Discussing this today! It’s at the end of the quarter. While written reports will be completed throughout the rest of the quarter as we do case studies, an oral presentation will be part of your final project. These will be able to be recorded or given live in person during finals week.\n\n\nQ: Will we have another lab with as many questions as Lab 4? The turnaround was pretty stressful, so just want to be prepared.\nA: The next lab (multiple linear regression) is also a tady lengthy, but after that I don’t plan on the rest being quite as long. Just as a reminder that you do not need to complete the entire lab to receive credit!\n\n\nQ: I’m not really comfortable with log transformations yet. Will we get more practice on that?\nA: Yup! Last lecture was a first introduction. We’ll return to this in upcoming case studies. The midterm does not require any transformations.\n\n\n\n\n\nLecture Participation survey “due” after class\nMidterm due Monday (2/13; 11:59 PM):\n\nreleased Friday (tomorrow) after lab\ncompleted individually\n\nPractice Midterm Answer Key Posted\n\n\n\n\n\nExam chat\nHW02 recap\nCase Studies\nFinal Project"
  },
  {
    "objectID": "content/lectures/09-projects.html#midterm-details",
    "href": "content/lectures/09-projects.html#midterm-details",
    "title": "09-projects",
    "section": "Midterm Details",
    "text": "Midterm Details\n\n\nInstructions will be posted on the website at 2PM Fri (tomorrow)\nYou’ll be provided a template (link on Canvas) and “submit” on GitHub\nYou’ll be provided with data and a data dictionary\nCovers data wrangling/tidying, dplyr, viz/ggplot2, and linear regression/tidymodels"
  },
  {
    "objectID": "content/lectures/09-projects.html#hw02-q1",
    "href": "content/lectures/09-projects.html#hw02-q1",
    "title": "09-projects",
    "section": "HW02 : Q1",
    "text": "HW02 : Q1\nGenerate a visualization that will allow readers to determine whether male or female penguins are larger (by mass)."
  },
  {
    "objectID": "content/lectures/09-projects.html#boxplot",
    "href": "content/lectures/09-projects.html#boxplot",
    "title": "09-projects",
    "section": "Boxplot",
    "text": "Boxplot\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npenguins |>\n  drop_na() |> \n  ggplot(aes(x = sex, y = body_mass_g)) +\n  geom_boxplot() +\n  labs(title = \"Penguin body mass by sex\", \n       y = \"body mass (g)\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/09-projects.html#histogram",
    "href": "content/lectures/09-projects.html#histogram",
    "title": "09-projects",
    "section": "Histogram",
    "text": "Histogram\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npenguins |>\n  filter(!is.na(sex)) |>\n  ggplot(mapping = aes(x = body_mass_g, fill = sex)) +\n  geom_histogram() +\n  labs(\n    title = 'Body Mass Distribution by Sex',\n    x = 'Body Mass (g)',\n    y = 'Count',\n    color = 'Sex'\n  )"
  },
  {
    "objectID": "content/lectures/09-projects.html#faceted-histograms",
    "href": "content/lectures/09-projects.html#faceted-histograms",
    "title": "09-projects",
    "section": "Faceted Histograms",
    "text": "Faceted Histograms\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npenguins |>\n  filter(!is.na(body_mass_g)) |>\n  ggplot(., mapping=aes(y=body_mass_g)) + \n  geom_histogram(binwidth=100) +\n  facet_grid(. ~ sex) + \n  labs(\n    title='Frequency of Penguins based on their Body mass and Female/Male Penguins',\n    x='Frequency / Count',\n    y='Body Mass (of penguins, in g (grams))')"
  },
  {
    "objectID": "content/lectures/09-projects.html#hw02-q2",
    "href": "content/lectures/09-projects.html#hw02-q2",
    "title": "09-projects",
    "section": "HW02 : Q2",
    "text": "HW02 : Q2\nGenerate a barplot that visualizes how many penguins there are from each species on each island. Each island should be a different panel (in a 1 row x 3 columns visualization), and each chart should visualize the species count."
  },
  {
    "objectID": "content/lectures/09-projects.html#barplot",
    "href": "content/lectures/09-projects.html#barplot",
    "title": "09-projects",
    "section": "Barplot",
    "text": "Barplot\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar() +\n  facet_wrap(~ island) +\n  labs(title = \"Count of species per island\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/09-projects.html#barplot-with-color",
    "href": "content/lectures/09-projects.html#barplot-with-color",
    "title": "09-projects",
    "section": "Barplot with color",
    "text": "Barplot with color\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n  ggplot(penguins, aes(x = fct_infreq(species), fill = species)) +\n  geom_bar() +\n  facet_wrap(~island, nrow = 1) +\n  guides(fill = \"none\") + \n  labs(\n    title = \"Count of Penguin Species Across the Palmer Archipelago Islands\",\n    x = \"Species\",\n    y = \"Number of Penguins\"\n  )"
  },
  {
    "objectID": "content/lectures/09-projects.html#hw02-q3",
    "href": "content/lectures/09-projects.html#hw02-q3",
    "title": "09-projects",
    "section": "HW02 : Q3",
    "text": "HW02 : Q3\nGenerate a scatterplot that will allow the viewer to determine whether flipper length has differed over time. Be sure to color the points on this plot by species."
  },
  {
    "objectID": "content/lectures/09-projects.html#scatterplot-no-jitter",
    "href": "content/lectures/09-projects.html#scatterplot-no-jitter",
    "title": "09-projects",
    "section": "Scatterplot (no jitter)",
    "text": "Scatterplot (no jitter)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = year, \n           y = flipper_length_mm,\n           color = species)) +\ngeom_point() + \nscale_color_viridis_d() +\nscale_x_continuous(n.breaks = 3) +\nlabs(\n  title = \"Flipper Lengths of Penguin Species Over Time\",\n  color = \"Species\",\n  x = \"Year\",\n  y = \"Flipper Length (mm)\"\n)"
  },
  {
    "objectID": "content/lectures/09-projects.html#scatterplot-w-jitter",
    "href": "content/lectures/09-projects.html#scatterplot-w-jitter",
    "title": "09-projects",
    "section": "Scatterplot (w/ jitter)",
    "text": "Scatterplot (w/ jitter)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(penguins,\n       mapping = aes(x = factor(year),\n                     y = flipper_length_mm,\n                     color = species)) +\n  scale_color_viridis_d() +\n  geom_jitter(na.rm = TRUE) +\n  labs(title = \"Flipper length of different penguin species by year\",\n       y = \"Flipper length (mm)\",\n       x = \"Year\") +\n  theme(plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/09-projects.html#hw02-part-ii",
    "href": "content/lectures/09-projects.html#hw02-part-ii",
    "title": "09-projects",
    "section": "HW02 : Part II",
    "text": "HW02 : Part II\nImitation is the highest form of flattery\nExample from: Eric Ko\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Eric890916\nchessData <- data.frame(country = c(\"United States\", \"Germany\", \"Canada\", \"Spain\", \"Russia\", \"France\", \"Bosnia and Herzegovina\", \"Croatia\", \"Turkey\", \"Austria\"),\n                        num = c(89, 55, 44, 41, 36, 34, 32, 32, 31, 29))\n\nggplot(chessData, aes(y = reorder(country, num), x = num)) + \n  geom_col(fill = \"#008080\") + \n  geom_text(aes(label = num), hjust = 1, nudge_x = -.5) +\n  labs(title = \"More players transfer to the U.S. than to any other country\",\n       subtitle = \"Nations that received the highest number of player transfers, 2000-17\",\n       caption = \"2017 data as of April 11. SOURCE: FIDE\",\n       x = \"NUMBER OF TRANSFERS\", y = \"COUNTRY\")"
  },
  {
    "objectID": "content/lectures/09-projects.html#section",
    "href": "content/lectures/09-projects.html#section",
    "title": "09-projects",
    "section": "",
    "text": "Example from: Christine Kwon\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\ncommon_first_names <- read.csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/most-common-name/new-top-firstNames.csv\")\n\n# editing data\ncommon_first_names <- common_first_names[1:20, ]\ncommon_first_names <- common_first_names %>%\n  mutate(sex = case_when (name == \"Mary\" | \n                          name == \"Jennifer\" |\n                          name == \"Patricia\" |\n                          name == \"Linda\" |\n                          name == \"Elizabeth\" ~ \"female\",\n                          name != \"Mary\" | \n                          name != \"Jennifer\" |\n                          name != \"Patricia\" |\n                          name != \"Linda\" |\n                          name != \"Elizabeth\" ~ \"male\",),\n         percentage = round(newPerct2013 * 1000, digits = 1))\n\n# creating visualization\ncommon_first_names %>%\n  ggplot(aes(y = reorder(name, percentage),  x = percentage, fill = sex)) +\n  geom_histogram(stat = \"identity\") +\n  guides(fill = \"none\") +\n  annotate(\"text\", x = 9.65, y = 21.7, label = expression(bold(\"MALE\")), cex = 3.85, hjust = 1, vjust = 1, color = \"dodgerblue\") +\n  annotate(\"text\", x = 11.5, y = 21.7, label = expression(bold(\"FEMALE\")), cex = 3.85, hjust = 1, vjust = 1, color = \"gold1\") +\n  geom_text(aes(label = signif(percentage)), nudge_x = 0.5) +\n  labs(title = \"Most Common First Names\",\n       subtitle = \"Per 1,000 Americans as of 2013\") +\n  scale_fill_manual(values = c(\"male\" = \"dodgerblue\",\n                               \"female\" = \"gold1\")) +\n  theme_classic() +\n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank(),\n        plot.title = element_text(size = 16,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 11),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(color = \"black\"),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank())"
  },
  {
    "objectID": "content/lectures/09-projects.html#section-1",
    "href": "content/lectures/09-projects.html#section-1",
    "title": "09-projects",
    "section": "",
    "text": "Example by: Cheng Chang (FA21)\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n# get data\npoll <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_polls_adjusted.csv\")\npoll_mean <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/covid-19-polls/master/covid_approval_toplines.csv\")\n\npoll <- poll |>\n  filter(subject == \"Biden\", party != \"all\") |>\n  mutate(Party=case_when(party == \"D\" ~ \"Democrats\",\n                         party == \"I\" ~ \"Independents\",\n                         party == \"R\" ~ \"Republicans\")) |>\n  mutate(enddate=as.Date(enddate, format=\"%m/%d/%Y\"))\n\npoll_mean <- poll_mean |>\n  filter(subject == \"Biden\", party != \"all\") |>\n  mutate(Party=case_when(party == \"D\" ~ \"Democrats\",\n                         party == \"I\" ~ \"Independents\",\n                         party == \"R\" ~ \"Republicans\")) |>\n  mutate(modeldate=as.Date(modeldate, format=\"%m/%d/%Y\"))\n\nggplot() +\n  geom_point(data=poll,\n             aes(x=enddate, y=approve_adjusted, color=Party),\n             size=1,\n             alpha = 0.5) +\n  geom_path(data=poll_mean, aes(x=modeldate, y=approve_estimate, color=Party)) +\n  labs(title=\"Approval of Biden’s response varies widely by party\",\n       subtitle=\n         \"A calculation of the share of Democrats, Republicans and independents who approve of the president’s\\nhandling of the coronavirus outbreak\",\n       x=NULL,\n       y=NULL) +\n  scale_color_manual(values = c(\"Democrats\" = \"#2acaea\",\n                                \"Independents\" = \"#ce7e00\",\n                                \"Republicans\" = \"#f44336\")) +\n  theme(plot.title.position = \"plot\",\n        panel.grid.major = element_line(color=\"grey\"),\n        panel.border = element_rect(fill=NA, color=\"grey\"),\n        panel.background = element_rect(fill=\"white\"))"
  },
  {
    "objectID": "content/lectures/09-projects.html#hw03-part-iii",
    "href": "content/lectures/09-projects.html#hw03-part-iii",
    "title": "09-projects",
    "section": "HW03 : Part III",
    "text": "HW03 : Part III\nTake a Sad Plot & Make It Better\nExample from: Christine Kwon\n\nOriginalPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\nmedals <- tibble(\n  country = c(\n    rep(\"USA\", 79), rep(\"CHN\", 70), rep(\"ROC\", 53), rep(\"GBR\", 48), rep(\"JPN\", 40)),\n  medal_type = c(\n    rep(\"gold\", 25), rep(\"silver\", 31), rep(\"bronze\", 23),\n    rep(\"gold\", 32), rep(\"silver\", 22), rep(\"bronze\", 16),\n    rep(\"gold\", 14), rep(\"silver\", 21), rep(\"bronze\", 18),\n    rep(\"gold\", 15), rep(\"silver\", 18), rep(\"bronze\", 15),\n    rep(\"gold\", 21), rep(\"silver\", 7), rep(\"bronze\", 12)))\n\n# creating visualization\nmedal_viz <- medals %>%\n   mutate(country = factor(country, levels = c(\"JPN\", \"GBR\",\"ROC\", \"CHN\", \"USA\"))) %>%\n  ggplot(aes(y = country, fill = factor(medal_type, levels = c(\"bronze\", \"silver\", \"gold\")))) + \n  geom_bar() +\n  annotate(\"text\", x = 4.5, y = 5.05, label = \"25\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 30.5, y = 5.05, label = \"31\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 61.5, y = 5.05, label = \"23\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 86.5, y = 5.05, label = expression(bold(\"79\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 4.05, label = \"32\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 37.5, y = 4.05, label = \"22\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 58.5, y = 4.05, label = \"16\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 76.5, y = 4.05, label = expression(bold(\"70\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 3.05, label = \"14\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 18.5, y = 3.05, label = \"21\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 39.5, y = 3.05, label = \"18\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 59.5, y = 3.05, label = expression(bold(\"53\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 2.05, label = \"15\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 19.5, y = 2.05, label = \"18\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 38, y = 2.05, label = \"15\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 54.5, y = 2.05, label = expression(bold(\"48\")), cex = 5, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 4.5, y = 1.05, label = \"21\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 23.5, y = 1.05, label = \"7\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 32.5, y = 1.05, label = \"12\", cex = 4, hjust = 1, vjust = 1) +\n  annotate(\"text\", x = 46.5, y = 1.05, label = expression(bold(\"40\")) , cex = 5, hjust = 1, vjust = 1) +\n  labs(title = \"Medals Won at the Tokyo Olympics (ongoing)\", \n       subtitle = \"Distribution of medals won by the top 5 countries (ordered by total)\", \n       fill = \"Medal Type\") +\n  scale_fill_manual(values = c(\"gold\" = \"gold\",\n                               \"silver\" = \"gray75\",\n                               \"bronze\" = \"tan3\")) +\n  theme(#legend.title = element_text(face = \"bold\"),\n        legend.position = \"top\") + \n  guides(fill = guide_legend(title.position = \"top\")) +\n  theme_classic() +\n  theme(plot.title.position = \"plot\", \n        panel.grid.major.y = element_blank(),\n        plot.title = element_text(size = 16,\n                                  face = \"bold\"),\n        plot.subtitle = element_text(size = 11),\n        axis.ticks.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.line.x = element_blank(),\n        axis.line.y = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_text(color = \"black\", \n                                   #face = \"bold\", \n                                   size = 11),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank())\n\n\nmedal_viz +\n  theme(legend.position = c(0.8, 0.25))"
  },
  {
    "objectID": "content/lectures/09-projects.html#opencasestudies",
    "href": "content/lectures/09-projects.html#opencasestudies",
    "title": "09-projects",
    "section": "OpenCaseStudies",
    "text": "OpenCaseStudies\n\nOpenCaseStudies\nUses R/the tidyverse\nasks public health-centric questions\ngoal: to teach statistical analysis/data science through case studies"
  },
  {
    "objectID": "content/lectures/09-projects.html#what-well-do",
    "href": "content/lectures/09-projects.html#what-well-do",
    "title": "09-projects",
    "section": "What We’ll Do",
    "text": "What We’ll Do\nFor each case study (2), during lecture:\n\nStats: (1-2d)\nBackground, Data & Wrangling (1-2d)\nEDA & Analysis (1d)\n\n\nFor each case study:\n\nyou’ll also work with case study data in lab.\nyou’ll work in assigned groups of ~3 students to complete a data science report"
  },
  {
    "objectID": "content/lectures/09-projects.html#data-science-reports",
    "href": "content/lectures/09-projects.html#data-science-reports",
    "title": "09-projects",
    "section": "Data Science Reports",
    "text": "Data Science Reports\nWith your group, you will:\n\ncarry out all steps of the analysis\n\nsome code will be taken directly from lecture\n\nadd text/organize into a report\n\n\n\nhave to extend the case study"
  },
  {
    "objectID": "content/lectures/09-projects.html#what-does-extend-the-case-study-mean",
    "href": "content/lectures/09-projects.html#what-does-extend-the-case-study-mean",
    "title": "09-projects",
    "section": "What does extend the case study mean?",
    "text": "What does extend the case study mean?\nYou’ll need to do something more on the topic beyond what is presented in class.\n\nExamples:\n\nAsking an additional question and answering it from the data provided\nFinding an additional dataset and using it to add to the case study\nGenerating a handful of additional and very informative visualizations (beyond what’s presented in class)"
  },
  {
    "objectID": "content/lectures/09-projects.html#grading",
    "href": "content/lectures/09-projects.html#grading",
    "title": "09-projects",
    "section": "Grading",
    "text": "Grading\nGraded on:\n\ncontent (code, text, viz)\neffective written communication\nextension carried out"
  },
  {
    "objectID": "content/lectures/09-projects.html#final-project-logistics",
    "href": "content/lectures/09-projects.html#final-project-logistics",
    "title": "09-projects",
    "section": "Final Project Logistics",
    "text": "Final Project Logistics\n\nwill be completed in groups of 3-4 students\nyou get to choose the group\nI will ask at the end of week 7 for your final project groups"
  },
  {
    "objectID": "content/lectures/09-projects.html#final-project-details",
    "href": "content/lectures/09-projects.html#final-project-details",
    "title": "09-projects",
    "section": "Final Project Details",
    "text": "Final Project Details\nTwo possible Paths:\n\nCreate a technical presentation on a statistics topic and/or an R package.\nCarry out a data analysis"
  },
  {
    "objectID": "content/lectures/09-projects.html#option-1-technical-presentation",
    "href": "content/lectures/09-projects.html#option-1-technical-presentation",
    "title": "09-projects",
    "section": "Option 1: Technical Presentation",
    "text": "Option 1: Technical Presentation\n\n.Rmd document used to make slides\n“Teaches” the details of the R package/statistics topic\nDemonstrates how to use the package and/or carry out the statistical analysis in R\nTopic/Package must go beyond what was taught in this course or what you should have learned in an intro stats course\nPresentation Length: 10-15min"
  },
  {
    "objectID": "content/lectures/09-projects.html#option-2-data-analysis",
    "href": "content/lectures/09-projects.html#option-2-data-analysis",
    "title": "09-projects",
    "section": "Option 2: Data Analysis",
    "text": "Option 2: Data Analysis\n\n.Rmd document used for data science report\nAsks a question, finds data, analyzes data (basically: a mini case report, but you find the data and formulate the question)\nPresentation Length: 3-5min (brief summary of the full report)"
  },
  {
    "objectID": "content/lectures/09-projects.html#wherewhen-for-this-presentation",
    "href": "content/lectures/09-projects.html#wherewhen-for-this-presentation",
    "title": "09-projects",
    "section": "Where/when for this presentation?",
    "text": "Where/when for this presentation?\nYou get to choose:\n\nRecord ahead of time: submit by Th 3/23 of finals week at 11:59 PM\nPresent in-person Th of finals week (slots to sign up for a time will be released later; want this option for those interested in getting more practice)"
  },
  {
    "objectID": "content/lectures/09-projects.html#should-i-be-working-on-my-final-project-now",
    "href": "content/lectures/09-projects.html#should-i-be-working-on-my-final-project-now",
    "title": "09-projects",
    "section": "Should I be working on my final project now?",
    "text": "Should I be working on my final project now?\n…probably not\n\nBut, you should start thinking about/getting a group of 3-4 people together.\n\n\nI’d recommend you start planning/working on your final project around wk 8"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#qa",
    "href": "content/lectures/12-cs01-eda-slides.html#qa",
    "title": "12-cs01-eda",
    "section": "Q&A",
    "text": "Q&A\n\nQ: I’m curious about how we would collaborate for the case study as a group with a single file - would it be a bunch of pushes and pulls or is it a little more complicated than that?\nA: As long as you’re working on separate lines/parts of the file, you can all push and pull to the same file! So as long as tasks are well delineated and you always remember to pull before you get started (and nobody pushes while you’re working on your part), there won’t be any issues. However, if you’re all working on similar parts/pushing and pulling at the same time, you will run into merge conflicts. These can certainly be handled via git/GitHub but make things a tad more complicated. For those who are less comfortable using GitHub, some groups choose to work in separate .Rmd files, pushing those to your group repo, and then combine them all at the end! This is very much something for your group to discuss!"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#course-announcements",
    "href": "content/lectures/12-cs01-eda-slides.html#course-announcements",
    "title": "12-cs01-eda",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLecture Participation survey “due” after class\nLab06 due Friday (2/24; 11:59 PM)\nHW03 due Mon (2/27; 11:59 PM)\n\nNotes:\n\nFinal Project Groups survey (link also on canvas; “due” Friday)\n\nif you’re in a group: one reply per group\nif you need a group: one reply per individual\n\nLab05 Answers posted\nMid-course survey credit posted\nMidterm grades to be finalized/posted tomorrow"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#agenda",
    "href": "content/lectures/12-cs01-eda-slides.html#agenda",
    "title": "12-cs01-eda",
    "section": "Agenda",
    "text": "Agenda\n\nMid-course survey summary\nSee/Discuss some EDA\nBrainstorm some EDA\nDo some EDA"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#time-spent",
    "href": "content/lectures/12-cs01-eda-slides.html#time-spent",
    "title": "12-cs01-eda",
    "section": "Time Spent",
    "text": "Time Spent"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#difficulty",
    "href": "content/lectures/12-cs01-eda-slides.html#difficulty",
    "title": "12-cs01-eda",
    "section": "Difficulty",
    "text": "Difficulty"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#what-would-you-change",
    "href": "content/lectures/12-cs01-eda-slides.html#what-would-you-change",
    "title": "12-cs01-eda",
    "section": "What would you change?",
    "text": "What would you change?"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#lott-data",
    "href": "content/lectures/12-cs01-eda-slides.html#lott-data",
    "title": "12-cs01-eda",
    "section": "LOTT data",
    "text": "LOTT data\n\nglimpseskim\n\n\n\nglimpse(LOTT_DF)\n\nRows: 1,364\nColumns: 50\n$ YEAR                           <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 198…\n$ STATE                          <chr> \"Alaska\", \"Arizona\", \"Arkansas\", \"Calif…\n$ Black_Female_10_to_19_years    <dbl> 0.26391223, 0.28748026, 1.81933049, 0.7…\n$ Black_Female_20_to_29_years    <dbl> 0.44331324, 0.27753816, 1.50296508, 0.8…\n$ Black_Female_30_to_39_years    <dbl> 0.201146585, 0.165433651, 0.842359498, …\n$ Black_Female_40_to_49_years    <dbl> 0.115646931, 0.119305223, 0.633866784, …\n$ Black_Female_50_to_64_years    <dbl> 0.092418701, 0.136484590, 1.015244173, …\n$ Black_Female_65_years_and_over <dbl> 0.026440644, 0.103332066, 1.156103458, …\n$ Black_Male_10_to_19_years      <dbl> 0.29677770, 0.31145827, 1.81159721, 0.8…\n$ Black_Male_20_to_29_years      <dbl> 0.69462291, 0.33792181, 1.26270912, 0.8…\n$ Black_Male_30_to_39_years      <dbl> 0.29875457, 0.18879028, 0.71111220, 0.5…\n$ Black_Male_40_to_49_years      <dbl> 0.147771078, 0.127310077, 0.476448668, …\n$ Black_Male_50_to_64_years      <dbl> 0.102797272, 0.130636295, 0.741127809, …\n$ Black_Male_65_years_and_over   <dbl> 0.027181971, 0.085421662, 0.870583784, …\n$ Other_Female_10_to_19_years    <dbl> 2.04383711, 0.80253231, 0.06531781, 0.5…\n$ Other_Female_20_to_29_years    <dbl> 1.76559257, 0.65515527, 0.07942996, 0.6…\n$ Other_Female_30_to_39_years    <dbl> 1.24839379, 0.44180215, 0.06702176, 0.6…\n$ Other_Female_40_to_49_years    <dbl> 0.79124246, 0.31098310, 0.04216167, 0.3…\n$ Other_Female_50_to_64_years    <dbl> 0.74651577, 0.28875958, 0.04390930, 0.4…\n$ Other_Female_65_years_and_over <dbl> 0.37906494, 0.16250950, 0.03158848, 0.2…\n$ Other_Male_10_to_19_years      <dbl> 2.15157655, 0.81174338, 0.07034226, 0.5…\n$ Other_Male_20_to_29_years      <dbl> 1.76361570, 0.59561232, 0.07497349, 0.6…\n$ Other_Male_30_to_39_years      <dbl> 1.19971335, 0.38931370, 0.04928327, 0.5…\n$ Other_Male_40_to_49_years      <dbl> 0.79519620, 0.25710568, 0.03552066, 0.3…\n$ Other_Male_50_to_64_years      <dbl> 0.74058515, 0.23513802, 0.03281182, 0.3…\n$ Other_Male_65_years_and_over   <dbl> 0.393397252, 0.150630154, 0.019486117, …\n$ White_Female_10_to_19_years    <dbl> 6.121874, 7.373713, 6.669014, 6.720429,…\n$ White_Female_20_to_29_years    <dbl> 8.608777, 8.195326, 6.657261, 7.997032,…\n$ White_Female_30_to_39_years    <dbl> 7.054710, 6.259248, 5.710656, 6.373367,…\n$ White_Female_40_to_49_years    <dbl> 3.749629, 4.414842, 4.319801, 4.342865,…\n$ White_Female_50_to_64_years    <dbl> 3.352525, 7.079325, 6.767843, 6.587129,…\n$ White_Female_65_years_and_over <dbl> 1.048977, 6.082958, 6.700472, 5.556054,…\n$ White_Male_10_to_19_years      <dbl> 6.873085, 7.641858, 6.993288, 7.029783,…\n$ White_Male_20_to_29_years      <dbl> 9.804784, 8.406997, 6.564418, 8.471549,…\n$ White_Male_30_to_39_years      <dbl> 8.483740, 6.285382, 5.560709, 6.519398,…\n$ White_Male_40_to_49_years      <dbl> 4.666650, 4.336730, 4.170641, 4.353268,…\n$ White_Male_50_to_64_years      <dbl> 4.103242, 6.210707, 5.993248, 6.065005,…\n$ White_Male_65_years_and_over   <dbl> 1.020807, 4.797064, 4.924526, 3.754192,…\n$ Unemployment_rate              <dbl> 9.6, 6.6, 7.6, 6.8, 5.8, 7.6, 7.4, 6.1,…\n$ Poverty_rate                   <dbl> 9.6, 12.8, 21.5, 11.0, 8.6, 11.8, 20.9,…\n$ Viol_crime_count               <dbl> 1919, 17673, 7656, 210290, 15215, 2824,…\n$ Population                     <dbl> 404680, 2735840, 2288809, 23792840, 290…\n$ police_per_100k_lag            <dbl> 194.72176, 262.66156, 152.00045, 243.92…\n$ RTC_LAW_YEAR                   <dbl> 1995, 1995, 1996, Inf, 2003, Inf, Inf, …\n$ RTC_LAW                        <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ TIME_0                         <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 198…\n$ TIME_INF                       <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 201…\n$ Viol_crime_rate_1k             <dbl> 4.742018, 6.459808, 3.344971, 8.838373,…\n$ Viol_crime_rate_1k_log         <dbl> 1.5564629, 1.8655995, 1.2074581, 2.1791…\n$ Population_log                 <dbl> 12.91085, 14.82195, 14.64354, 16.98490,…\n\n\n\n\n\nskimr::skim(LOTT_DF)\n\n\nData summary\n\n\nName\nLOTT_DF\n\n\nNumber of rows\n1364\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nlogical\n1\n\n\nnumeric\n48\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTATE\n0\n1\n4\n20\n0\n44\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nRTC_LAW\n0\n1\n0.36\nFAL: 868, TRU: 496\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYEAR\n0\n1\n1995.00\n8.95\n1980.00\n1987.00\n1995.00\n2003.00\n2010.00\n▇▇▇▇▇\n\n\nBlack_Female_10_to_19_years\n0\n1\n1.02\n1.02\n0.02\n0.26\n0.64\n1.44\n6.53\n▇▂▁▁▁\n\n\nBlack_Female_20_to_29_years\n0\n1\n1.01\n1.09\n0.02\n0.26\n0.61\n1.37\n7.73\n▇▂▁▁▁\n\n\nBlack_Female_30_to_39_years\n0\n1\n0.93\n1.00\n0.01\n0.21\n0.58\n1.29\n6.11\n▇▂▁▁▁\n\n\nBlack_Female_40_to_49_years\n0\n1\n0.76\n0.87\n0.01\n0.14\n0.49\n1.10\n5.45\n▇▂▁▁▁\n\n\nBlack_Female_50_to_64_years\n0\n1\n0.78\n0.97\n0.00\n0.14\n0.45\n1.08\n6.10\n▇▂▁▁▁\n\n\nBlack_Female_65_years_and_over\n0\n1\n0.62\n0.86\n0.00\n0.08\n0.35\n0.82\n6.12\n▇▁▁▁▁\n\n\nBlack_Male_10_to_19_years\n0\n1\n1.04\n1.02\n0.03\n0.29\n0.68\n1.47\n6.32\n▇▃▁▁▁\n\n\nBlack_Male_20_to_29_years\n0\n1\n0.95\n0.93\n0.04\n0.31\n0.66\n1.25\n6.57\n▇▂▁▁▁\n\n\nBlack_Male_30_to_39_years\n0\n1\n0.82\n0.84\n0.02\n0.24\n0.55\n1.10\n5.37\n▇▂▁▁▁\n\n\nBlack_Male_40_to_49_years\n0\n1\n0.66\n0.72\n0.01\n0.16\n0.44\n0.93\n4.45\n▇▂▁▁▁\n\n\nBlack_Male_50_to_64_years\n0\n1\n0.64\n0.76\n0.00\n0.14\n0.40\n0.87\n4.79\n▇▂▁▁▁\n\n\nBlack_Male_65_years_and_over\n0\n1\n0.39\n0.51\n0.00\n0.06\n0.24\n0.52\n3.56\n▇▁▁▁▁\n\n\nOther_Female_10_to_19_years\n0\n1\n0.51\n0.78\n0.03\n0.15\n0.27\n0.56\n5.33\n▇▁▁▁▁\n\n\nOther_Female_20_to_29_years\n0\n1\n0.49\n0.71\n0.04\n0.17\n0.30\n0.56\n5.55\n▇▁▁▁▁\n\n\nOther_Female_30_to_39_years\n0\n1\n0.48\n0.75\n0.04\n0.15\n0.28\n0.52\n5.36\n▇▁▁▁▁\n\n\nOther_Female_40_to_49_years\n0\n1\n0.39\n0.70\n0.02\n0.11\n0.21\n0.38\n5.46\n▇▁▁▁▁\n\n\nOther_Female_50_to_64_years\n0\n1\n0.38\n0.84\n0.02\n0.09\n0.18\n0.35\n7.10\n▇▁▁▁▁\n\n\nOther_Female_65_years_and_over\n0\n1\n0.25\n0.72\n0.01\n0.04\n0.09\n0.18\n6.20\n▇▁▁▁▁\n\n\nOther_Male_10_to_19_years\n0\n1\n0.53\n0.81\n0.03\n0.15\n0.28\n0.58\n5.58\n▇▁▁▁▁\n\n\nOther_Male_20_to_29_years\n0\n1\n0.48\n0.71\n0.03\n0.16\n0.29\n0.54\n5.33\n▇▁▁▁▁\n\n\nOther_Male_30_to_39_years\n0\n1\n0.44\n0.71\n0.03\n0.14\n0.26\n0.48\n5.06\n▇▁▁▁▁\n\n\nOther_Male_40_to_49_years\n0\n1\n0.35\n0.66\n0.02\n0.09\n0.19\n0.34\n5.13\n▇▁▁▁▁\n\n\nOther_Male_50_to_64_years\n0\n1\n0.33\n0.74\n0.01\n0.08\n0.16\n0.30\n6.50\n▇▁▁▁▁\n\n\nOther_Male_65_years_and_over\n0\n1\n0.19\n0.59\n0.01\n0.03\n0.07\n0.14\n4.51\n▇▁▁▁▁\n\n\nWhite_Female_10_to_19_years\n0\n1\n5.69\n1.37\n0.94\n4.96\n5.79\n6.57\n9.45\n▁▁▇▆▁\n\n\nWhite_Female_20_to_29_years\n0\n1\n6.07\n1.36\n1.59\n5.23\n5.90\n6.93\n9.65\n▁▂▇▅▂\n\n\nWhite_Female_30_to_39_years\n0\n1\n6.15\n1.22\n1.53\n5.45\n6.28\n7.00\n8.95\n▁▁▅▇▂\n\n\nWhite_Female_40_to_49_years\n0\n1\n5.56\n1.22\n1.20\n4.84\n5.66\n6.39\n8.33\n▁▁▇▇▂\n\n\nWhite_Female_50_to_64_years\n0\n1\n6.55\n1.45\n1.72\n6.00\n6.57\n7.32\n11.40\n▁▂▇▂▁\n\n\nWhite_Female_65_years_and_over\n0\n1\n6.40\n1.71\n1.05\n5.37\n6.67\n7.54\n9.90\n▁▁▆▇▂\n\n\nWhite_Male_10_to_19_years\n0\n1\n6.00\n1.42\n1.02\n5.26\n6.11\n6.91\n9.74\n▁▁▇▇▁\n\n\nWhite_Male_20_to_29_years\n0\n1\n6.26\n1.32\n2.41\n5.42\n6.10\n7.13\n9.96\n▁▃▇▃▁\n\n\nWhite_Male_30_to_39_years\n0\n1\n6.25\n1.18\n1.93\n5.57\n6.31\n7.04\n9.67\n▁▂▇▆▁\n\n\nWhite_Male_40_to_49_years\n0\n1\n5.56\n1.21\n1.35\n4.77\n5.66\n6.40\n8.24\n▁▁▇▇▃\n\n\nWhite_Male_50_to_64_years\n0\n1\n6.23\n1.39\n1.78\n5.62\n6.16\n6.92\n10.93\n▁▂▇▂▁\n\n\nWhite_Male_65_years_and_over\n0\n1\n4.56\n1.19\n1.02\n3.80\n4.78\n5.34\n7.51\n▁▂▇▇▁\n\n\nUnemployment_rate\n0\n1\n6.04\n2.11\n2.30\n4.50\n5.60\n7.20\n17.80\n▇▇▂▁▁\n\n\nPoverty_rate\n0\n1\n13.39\n3.86\n5.70\n10.40\n12.80\n15.60\n27.20\n▃▇▅▂▁\n\n\nViol_crime_count\n0\n1\n32452.11\n46790.78\n322.00\n5598.75\n14684.00\n39119.00\n345624.00\n▇▁▁▁▁\n\n\nPopulation\n0\n1\n5559352.78\n6092703.87\n404680.00\n1570224.75\n3659637.00\n6487139.00\n37349363.00\n▇▂▁▁▁\n\n\npolice_per_100k_lag\n0\n1\n315.19\n116.43\n83.76\n247.63\n298.45\n354.02\n1021.14\n▆▇▁▁▁\n\n\nRTC_LAW_YEAR\n0\n1\nInf\nNaN\n1985.00\n1994.25\n1997.00\n2011.25\nInf\n▇▇▃▅▂\n\n\nTIME_0\n0\n1\n1980.00\n0.00\n1980.00\n1980.00\n1980.00\n1980.00\n1980.00\n▁▁▇▁▁\n\n\nTIME_INF\n0\n1\n2010.00\n0.00\n2010.00\n2010.00\n2010.00\n2010.00\n2010.00\n▁▁▇▁▁\n\n\nViol_crime_rate_1k\n0\n1\n5.10\n3.21\n0.48\n2.87\n4.63\n6.47\n29.30\n▇▃▁▁▁\n\n\nViol_crime_rate_1k_log\n0\n1\n1.46\n0.60\n-0.74\n1.05\n1.53\n1.87\n3.38\n▁▂▇▅▁\n\n\nPopulation_log\n0\n1\n15.04\n1.02\n12.91\n14.27\n15.11\n15.69\n17.44\n▃▅▇▅▂"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#donohue-data",
    "href": "content/lectures/12-cs01-eda-slides.html#donohue-data",
    "title": "12-cs01-eda",
    "section": "DONOHUE data",
    "text": "DONOHUE data\n\nglimpseskim\n\n\n\nglimpse(DONOHUE_DF)\n\nRows: 1,364\nColumns: 20\n$ YEAR                      <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 1980, 19…\n$ STATE                     <chr> \"Alaska\", \"Arizona\", \"Arkansas\", \"California…\n$ Black_Male_15_to_19_years <dbl> 0.16704557, 0.17475437, 0.95451390, 0.433886…\n$ Black_Male_20_to_39_years <dbl> 0.99337748, 0.52671209, 1.97382132, 1.353260…\n$ Other_Male_15_to_19_years <dbl> 1.12978156, 0.41504620, 0.03849163, 0.312308…\n$ Other_Male_20_to_39_years <dbl> 2.96332905, 0.98492602, 0.12425676, 1.213007…\n$ White_Male_15_to_19_years <dbl> 3.6278047, 4.0915770, 3.7401985, 3.8358473, …\n$ White_Male_20_to_39_years <dbl> 18.288524, 14.692380, 12.125127, 14.990947, …\n$ Unemployment_rate         <dbl> 9.6, 6.6, 7.6, 6.8, 5.8, 7.6, 7.4, 6.1, 6.3,…\n$ Poverty_rate              <dbl> 9.6, 12.8, 21.5, 11.0, 8.6, 11.8, 20.9, 16.7…\n$ Viol_crime_count          <dbl> 1919, 17673, 7656, 210290, 15215, 2824, 1277…\n$ Population                <dbl> 404680, 2735840, 2288809, 23792840, 2909545,…\n$ police_per_100k_lag       <dbl> 194.72176, 262.66156, 152.00045, 243.92632, …\n$ RTC_LAW_YEAR              <dbl> 1995, 1995, 1996, Inf, 2003, Inf, Inf, 1988,…\n$ RTC_LAW                   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ TIME_0                    <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 1980, 19…\n$ TIME_INF                  <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ Viol_crime_rate_1k        <dbl> 4.742018, 6.459808, 3.344971, 8.838373, 5.22…\n$ Viol_crime_rate_1k_log    <dbl> 1.5564629, 1.8655995, 1.2074581, 2.1791028, …\n$ Population_log            <dbl> 12.91085, 14.82195, 14.64354, 16.98490, 14.8…\n\n\n\n\n\nskimr::skim(DONOHUE_DF)\n\n\nData summary\n\n\nName\nDONOHUE_DF\n\n\nNumber of rows\n1364\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nlogical\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTATE\n0\n1\n4\n20\n0\n44\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nRTC_LAW\n0\n1\n0.36\nFAL: 868, TRU: 496\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYEAR\n0\n1\n1995.00\n8.95\n1980.00\n1987.00\n1995.00\n2003.00\n2010.00\n▇▇▇▇▇\n\n\nBlack_Male_15_to_19_years\n0\n1\n0.53\n0.51\n0.02\n0.15\n0.36\n0.74\n3.46\n▇▂▁▁▁\n\n\nBlack_Male_20_to_39_years\n0\n1\n1.77\n1.76\n0.07\n0.57\n1.19\n2.32\n11.33\n▇▂▁▁▁\n\n\nOther_Male_15_to_19_years\n0\n1\n0.26\n0.40\n0.01\n0.08\n0.14\n0.29\n2.90\n▇▁▁▁▁\n\n\nOther_Male_20_to_39_years\n0\n1\n0.93\n1.42\n0.07\n0.31\n0.55\n1.01\n9.90\n▇▁▁▁▁\n\n\nWhite_Male_15_to_19_years\n0\n1\n3.07\n0.72\n0.55\n2.67\n3.13\n3.52\n4.99\n▁▁▇▇▁\n\n\nWhite_Male_20_to_39_years\n0\n1\n12.51\n2.28\n4.41\n11.13\n12.61\n14.13\n18.29\n▁▂▇▇▂\n\n\nUnemployment_rate\n0\n1\n6.04\n2.11\n2.30\n4.50\n5.60\n7.20\n17.80\n▇▇▂▁▁\n\n\nPoverty_rate\n0\n1\n13.39\n3.86\n5.70\n10.40\n12.80\n15.60\n27.20\n▃▇▅▂▁\n\n\nViol_crime_count\n0\n1\n32452.11\n46790.78\n322.00\n5598.75\n14684.00\n39119.00\n345624.00\n▇▁▁▁▁\n\n\nPopulation\n0\n1\n5559352.78\n6092703.87\n404680.00\n1570224.75\n3659637.00\n6487139.00\n37349363.00\n▇▂▁▁▁\n\n\npolice_per_100k_lag\n0\n1\n315.19\n116.43\n83.76\n247.63\n298.45\n354.02\n1021.14\n▆▇▁▁▁\n\n\nRTC_LAW_YEAR\n0\n1\nInf\nNaN\n1985.00\n1994.25\n1997.00\n2011.25\nInf\n▇▇▃▅▂\n\n\nTIME_0\n0\n1\n1980.00\n0.00\n1980.00\n1980.00\n1980.00\n1980.00\n1980.00\n▁▁▇▁▁\n\n\nTIME_INF\n0\n1\n2010.00\n0.00\n2010.00\n2010.00\n2010.00\n2010.00\n2010.00\n▁▁▇▁▁\n\n\nViol_crime_rate_1k\n0\n1\n5.10\n3.21\n0.48\n2.87\n4.63\n6.47\n29.30\n▇▃▁▁▁\n\n\nViol_crime_rate_1k_log\n0\n1\n1.46\n0.60\n-0.74\n1.05\n1.53\n1.87\n3.38\n▁▂▇▅▁\n\n\nPopulation_log\n0\n1\n15.04\n1.02\n12.91\n14.27\n15.11\n15.69\n17.44\n▃▅▇▅▂"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#population-over-time",
    "href": "content/lectures/12-cs01-eda-slides.html#population-over-time",
    "title": "12-cs01-eda",
    "section": "Population over time",
    "text": "Population over time\n\nCodePlot\n\n\n\nDONOHUE_DF |>\n  group_by(YEAR) |>\n  summarise(Population = sum(Population)) |>\nggplot(aes(x = YEAR, y = Population)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = seq(1980, 2010, by = 1),\n    limits = c(1980, 2010),\n    labels = c(seq(1980, 2010, by = 1))\n  ) +\n  labs(\n    title = \"Population has steadily increased\",\n    x = \"Year\",\n    y = \"Population\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#crime-over-time",
    "href": "content/lectures/12-cs01-eda-slides.html#crime-over-time",
    "title": "12-cs01-eda",
    "section": "Crime over time",
    "text": "Crime over time\n\nCode-1Code-2Plot\n\n\n\ndf <- DONOHUE_DF |>\n  group_by(YEAR) |>\n  summarize(Viol_crime_count = sum(Viol_crime_count),\n            Population = sum(Population),\n            .groups = \"drop\") |>\n  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population))\n\n\n\n\ndf |>\n  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = seq(1980, 2010, by = 1),\n    limits = c(1980, 2010),\n    labels = c(seq(1980, 2010, by = 1))\n  ) +\n  scale_y_continuous(\n    breaks = seq(5.75, 6.75, by = 0.25),\n    limits = c(5.75, 6.75)\n  ) +\n  labs(\n    title = \"Crime rates fluctuate over time\",\n    x = \"Year\",\n    y = \"ln(violent crimes per 100,000 people)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90), \n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#crime-over-time-by-state",
    "href": "content/lectures/12-cs01-eda-slides.html#crime-over-time-by-state",
    "title": "12-cs01-eda",
    "section": "Crime over time by state",
    "text": "Crime over time by state\n\nCode-1Code-2Plot\n\n\n\np <- DONOHUE_DF |>\n  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population)) |>\n  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log, color = STATE)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(group = STATE),\n    size = 0.5,\n    show.legend = FALSE\n  ) +\n  geom_text_repel(data = DONOHUE_DF |>\n      mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population)) |>\n      filter(YEAR == last(YEAR)),\n      aes(label = STATE,x = YEAR, y = Viol_crime_rate_100k_log),\n      size = 3, alpha = 1, nudge_x = 1, direction = \"y\",\n      hjust = 1, vjust = 1, segment.size = 0.25, segment.alpha = 0.25,\n      force = 1, max.iter = 9999)\n\n\n\n\np + \n  guides(color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(1980, 2015, by = 1),\n    limits = c(1980, 2015),\n    labels = c(seq(1980, 2010, by = 1), rep(\"\", 5))\n  ) +\n  scale_y_continuous(\n    breaks = seq(3.5, 8.5, by = 0.5),\n    limits = c(3.5, 8.5)\n  ) +\n  labs(\n    title = \"States have different levels of crime\",\n    x = \"Year\", y = \"ln(violent crimes per 100,000 people)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90), plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#police-presence-over-time",
    "href": "content/lectures/12-cs01-eda-slides.html#police-presence-over-time",
    "title": "12-cs01-eda",
    "section": "Police Presence over time",
    "text": "Police Presence over time\n\nCode-1Plot\n\n\n\nDONOHUE_DF |>\n  group_by(YEAR) |>\n  summarise(Police = sum(police_per_100k_lag)) |> \n  ggplot(aes(x = YEAR, y = Police)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = seq(1980, 2010, by = 1),\n    limits = c(1980, 2010),\n    labels = c(seq(1980, 2010, by = 1))\n  ) +\n  labs(\n    title = \"Police Presence has increased over time with fluctuations\",\n    x = \"Year\",\n    y = \"Police Presence per 100K people\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#your-turn",
    "href": "content/lectures/12-cs01-eda-slides.html#your-turn",
    "title": "12-cs01-eda",
    "section": "Your Turn",
    "text": "Your Turn\n🧠 Consider the data we’re working with and our questions of interest, what would you like to know that you don’t know yet?\n\n❗ Do some EDA! Try to learn something from the data that we haven’t yet discussed. (Summarize data, make a plot, make a table, etc.)\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/12-cs01-eda-slides.html#where-to-go-from-here",
    "href": "content/lectures/12-cs01-eda-slides.html#where-to-go-from-here",
    "title": "12-cs01-eda",
    "section": "Where to go from here?",
    "text": "Where to go from here?\n\nImplement some of these ideas\nThis week’s lab - continues the EDA!\n\nConsider what variables we have that we haven’t looked at\nConsider the variables we have looked at but look at them differently\n\nEventually: incorporate some of this and likely some of lab into your final case study\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html",
    "href": "content/lectures/12-cs01-eda.html",
    "title": "12-cs01-eda",
    "section": "",
    "text": "Q: I’m curious about how we would collaborate for the case study as a group with a single file - would it be a bunch of pushes and pulls or is it a little more complicated than that?\nA: As long as you’re working on separate lines/parts of the file, you can all push and pull to the same file! So as long as tasks are well delineated and you always remember to pull before you get started (and nobody pushes while you’re working on your part), there won’t be any issues. However, if you’re all working on similar parts/pushing and pulling at the same time, you will run into merge conflicts. These can certainly be handled via git/GitHub but make things a tad more complicated. For those who are less comfortable using GitHub, some groups choose to work in separate .Rmd files, pushing those to your group repo, and then combine them all at the end! This is very much something for your group to discuss!\n\n\n\n\nDue Dates:\n\nLecture Participation survey “due” after class\nLab06 due Friday (2/24; 11:59 PM)\nHW03 due Mon (2/27; 11:59 PM)\n\nNotes:\n\nFinal Project Groups survey (link also on canvas; “due” Friday)\n\nif you’re in a group: one reply per group\nif you need a group: one reply per individual\n\nLab05 Answers posted\nMid-course survey credit posted\nMidterm grades to be finalized/posted tomorrow\n\n\n\n\n\nMid-course survey summary\nSee/Discuss some EDA\nBrainstorm some EDA\nDo some EDA"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#time-spent",
    "href": "content/lectures/12-cs01-eda.html#time-spent",
    "title": "12-cs01-eda",
    "section": "Time Spent",
    "text": "Time Spent"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#difficulty",
    "href": "content/lectures/12-cs01-eda.html#difficulty",
    "title": "12-cs01-eda",
    "section": "Difficulty",
    "text": "Difficulty"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#what-would-you-change",
    "href": "content/lectures/12-cs01-eda.html#what-would-you-change",
    "title": "12-cs01-eda",
    "section": "What would you change?",
    "text": "What would you change?"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#lott-data",
    "href": "content/lectures/12-cs01-eda.html#lott-data",
    "title": "12-cs01-eda",
    "section": "LOTT data",
    "text": "LOTT data\n\nglimpseskim\n\n\n\nglimpse(LOTT_DF)\n\nRows: 1,364\nColumns: 50\n$ YEAR                           <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 198…\n$ STATE                          <chr> \"Alaska\", \"Arizona\", \"Arkansas\", \"Calif…\n$ Black_Female_10_to_19_years    <dbl> 0.26391223, 0.28748026, 1.81933049, 0.7…\n$ Black_Female_20_to_29_years    <dbl> 0.44331324, 0.27753816, 1.50296508, 0.8…\n$ Black_Female_30_to_39_years    <dbl> 0.201146585, 0.165433651, 0.842359498, …\n$ Black_Female_40_to_49_years    <dbl> 0.115646931, 0.119305223, 0.633866784, …\n$ Black_Female_50_to_64_years    <dbl> 0.092418701, 0.136484590, 1.015244173, …\n$ Black_Female_65_years_and_over <dbl> 0.026440644, 0.103332066, 1.156103458, …\n$ Black_Male_10_to_19_years      <dbl> 0.29677770, 0.31145827, 1.81159721, 0.8…\n$ Black_Male_20_to_29_years      <dbl> 0.69462291, 0.33792181, 1.26270912, 0.8…\n$ Black_Male_30_to_39_years      <dbl> 0.29875457, 0.18879028, 0.71111220, 0.5…\n$ Black_Male_40_to_49_years      <dbl> 0.147771078, 0.127310077, 0.476448668, …\n$ Black_Male_50_to_64_years      <dbl> 0.102797272, 0.130636295, 0.741127809, …\n$ Black_Male_65_years_and_over   <dbl> 0.027181971, 0.085421662, 0.870583784, …\n$ Other_Female_10_to_19_years    <dbl> 2.04383711, 0.80253231, 0.06531781, 0.5…\n$ Other_Female_20_to_29_years    <dbl> 1.76559257, 0.65515527, 0.07942996, 0.6…\n$ Other_Female_30_to_39_years    <dbl> 1.24839379, 0.44180215, 0.06702176, 0.6…\n$ Other_Female_40_to_49_years    <dbl> 0.79124246, 0.31098310, 0.04216167, 0.3…\n$ Other_Female_50_to_64_years    <dbl> 0.74651577, 0.28875958, 0.04390930, 0.4…\n$ Other_Female_65_years_and_over <dbl> 0.37906494, 0.16250950, 0.03158848, 0.2…\n$ Other_Male_10_to_19_years      <dbl> 2.15157655, 0.81174338, 0.07034226, 0.5…\n$ Other_Male_20_to_29_years      <dbl> 1.76361570, 0.59561232, 0.07497349, 0.6…\n$ Other_Male_30_to_39_years      <dbl> 1.19971335, 0.38931370, 0.04928327, 0.5…\n$ Other_Male_40_to_49_years      <dbl> 0.79519620, 0.25710568, 0.03552066, 0.3…\n$ Other_Male_50_to_64_years      <dbl> 0.74058515, 0.23513802, 0.03281182, 0.3…\n$ Other_Male_65_years_and_over   <dbl> 0.393397252, 0.150630154, 0.019486117, …\n$ White_Female_10_to_19_years    <dbl> 6.121874, 7.373713, 6.669014, 6.720429,…\n$ White_Female_20_to_29_years    <dbl> 8.608777, 8.195326, 6.657261, 7.997032,…\n$ White_Female_30_to_39_years    <dbl> 7.054710, 6.259248, 5.710656, 6.373367,…\n$ White_Female_40_to_49_years    <dbl> 3.749629, 4.414842, 4.319801, 4.342865,…\n$ White_Female_50_to_64_years    <dbl> 3.352525, 7.079325, 6.767843, 6.587129,…\n$ White_Female_65_years_and_over <dbl> 1.048977, 6.082958, 6.700472, 5.556054,…\n$ White_Male_10_to_19_years      <dbl> 6.873085, 7.641858, 6.993288, 7.029783,…\n$ White_Male_20_to_29_years      <dbl> 9.804784, 8.406997, 6.564418, 8.471549,…\n$ White_Male_30_to_39_years      <dbl> 8.483740, 6.285382, 5.560709, 6.519398,…\n$ White_Male_40_to_49_years      <dbl> 4.666650, 4.336730, 4.170641, 4.353268,…\n$ White_Male_50_to_64_years      <dbl> 4.103242, 6.210707, 5.993248, 6.065005,…\n$ White_Male_65_years_and_over   <dbl> 1.020807, 4.797064, 4.924526, 3.754192,…\n$ Unemployment_rate              <dbl> 9.6, 6.6, 7.6, 6.8, 5.8, 7.6, 7.4, 6.1,…\n$ Poverty_rate                   <dbl> 9.6, 12.8, 21.5, 11.0, 8.6, 11.8, 20.9,…\n$ Viol_crime_count               <dbl> 1919, 17673, 7656, 210290, 15215, 2824,…\n$ Population                     <dbl> 404680, 2735840, 2288809, 23792840, 290…\n$ police_per_100k_lag            <dbl> 194.72176, 262.66156, 152.00045, 243.92…\n$ RTC_LAW_YEAR                   <dbl> 1995, 1995, 1996, Inf, 2003, Inf, Inf, …\n$ RTC_LAW                        <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ TIME_0                         <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 198…\n$ TIME_INF                       <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 201…\n$ Viol_crime_rate_1k             <dbl> 4.742018, 6.459808, 3.344971, 8.838373,…\n$ Viol_crime_rate_1k_log         <dbl> 1.5564629, 1.8655995, 1.2074581, 2.1791…\n$ Population_log                 <dbl> 12.91085, 14.82195, 14.64354, 16.98490,…\n\n\n\n\n\nskimr::skim(LOTT_DF)\n\nWarning: There was 1 warning in `dplyr::summarize()`.\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nℹ In group 0: .\nCaused by warning:\n! There was 1 warning in `dplyr::summarize()`.\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nCaused by warning in `inline_hist()`:\n! Variable contains Inf or -Inf value(s) that were converted to NA.\n\n\n\nData summary\n\n\nName\nLOTT_DF\n\n\nNumber of rows\n1364\n\n\nNumber of columns\n50\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nlogical\n1\n\n\nnumeric\n48\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTATE\n0\n1\n4\n20\n0\n44\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nRTC_LAW\n0\n1\n0.36\nFAL: 868, TRU: 496\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYEAR\n0\n1\n1995.00\n8.95\n1980.00\n1987.00\n1995.00\n2003.00\n2010.00\n▇▇▇▇▇\n\n\nBlack_Female_10_to_19_years\n0\n1\n1.02\n1.02\n0.02\n0.26\n0.64\n1.44\n6.53\n▇▂▁▁▁\n\n\nBlack_Female_20_to_29_years\n0\n1\n1.01\n1.09\n0.02\n0.26\n0.61\n1.37\n7.73\n▇▂▁▁▁\n\n\nBlack_Female_30_to_39_years\n0\n1\n0.93\n1.00\n0.01\n0.21\n0.58\n1.29\n6.11\n▇▂▁▁▁\n\n\nBlack_Female_40_to_49_years\n0\n1\n0.76\n0.87\n0.01\n0.14\n0.49\n1.10\n5.45\n▇▂▁▁▁\n\n\nBlack_Female_50_to_64_years\n0\n1\n0.78\n0.97\n0.00\n0.14\n0.45\n1.08\n6.10\n▇▂▁▁▁\n\n\nBlack_Female_65_years_and_over\n0\n1\n0.62\n0.86\n0.00\n0.08\n0.35\n0.82\n6.12\n▇▁▁▁▁\n\n\nBlack_Male_10_to_19_years\n0\n1\n1.04\n1.02\n0.03\n0.29\n0.68\n1.47\n6.32\n▇▃▁▁▁\n\n\nBlack_Male_20_to_29_years\n0\n1\n0.95\n0.93\n0.04\n0.31\n0.66\n1.25\n6.57\n▇▂▁▁▁\n\n\nBlack_Male_30_to_39_years\n0\n1\n0.82\n0.84\n0.02\n0.24\n0.55\n1.10\n5.37\n▇▂▁▁▁\n\n\nBlack_Male_40_to_49_years\n0\n1\n0.66\n0.72\n0.01\n0.16\n0.44\n0.93\n4.45\n▇▂▁▁▁\n\n\nBlack_Male_50_to_64_years\n0\n1\n0.64\n0.76\n0.00\n0.14\n0.40\n0.87\n4.79\n▇▂▁▁▁\n\n\nBlack_Male_65_years_and_over\n0\n1\n0.39\n0.51\n0.00\n0.06\n0.24\n0.52\n3.56\n▇▁▁▁▁\n\n\nOther_Female_10_to_19_years\n0\n1\n0.51\n0.78\n0.03\n0.15\n0.27\n0.56\n5.33\n▇▁▁▁▁\n\n\nOther_Female_20_to_29_years\n0\n1\n0.49\n0.71\n0.04\n0.17\n0.30\n0.56\n5.55\n▇▁▁▁▁\n\n\nOther_Female_30_to_39_years\n0\n1\n0.48\n0.75\n0.04\n0.15\n0.28\n0.52\n5.36\n▇▁▁▁▁\n\n\nOther_Female_40_to_49_years\n0\n1\n0.39\n0.70\n0.02\n0.11\n0.21\n0.38\n5.46\n▇▁▁▁▁\n\n\nOther_Female_50_to_64_years\n0\n1\n0.38\n0.84\n0.02\n0.09\n0.18\n0.35\n7.10\n▇▁▁▁▁\n\n\nOther_Female_65_years_and_over\n0\n1\n0.25\n0.72\n0.01\n0.04\n0.09\n0.18\n6.20\n▇▁▁▁▁\n\n\nOther_Male_10_to_19_years\n0\n1\n0.53\n0.81\n0.03\n0.15\n0.28\n0.58\n5.58\n▇▁▁▁▁\n\n\nOther_Male_20_to_29_years\n0\n1\n0.48\n0.71\n0.03\n0.16\n0.29\n0.54\n5.33\n▇▁▁▁▁\n\n\nOther_Male_30_to_39_years\n0\n1\n0.44\n0.71\n0.03\n0.14\n0.26\n0.48\n5.06\n▇▁▁▁▁\n\n\nOther_Male_40_to_49_years\n0\n1\n0.35\n0.66\n0.02\n0.09\n0.19\n0.34\n5.13\n▇▁▁▁▁\n\n\nOther_Male_50_to_64_years\n0\n1\n0.33\n0.74\n0.01\n0.08\n0.16\n0.30\n6.50\n▇▁▁▁▁\n\n\nOther_Male_65_years_and_over\n0\n1\n0.19\n0.59\n0.01\n0.03\n0.07\n0.14\n4.51\n▇▁▁▁▁\n\n\nWhite_Female_10_to_19_years\n0\n1\n5.69\n1.37\n0.94\n4.96\n5.79\n6.57\n9.45\n▁▁▇▆▁\n\n\nWhite_Female_20_to_29_years\n0\n1\n6.07\n1.36\n1.59\n5.23\n5.90\n6.93\n9.65\n▁▂▇▅▂\n\n\nWhite_Female_30_to_39_years\n0\n1\n6.15\n1.22\n1.53\n5.45\n6.28\n7.00\n8.95\n▁▁▅▇▂\n\n\nWhite_Female_40_to_49_years\n0\n1\n5.56\n1.22\n1.20\n4.84\n5.66\n6.39\n8.33\n▁▁▇▇▂\n\n\nWhite_Female_50_to_64_years\n0\n1\n6.55\n1.45\n1.72\n6.00\n6.57\n7.32\n11.40\n▁▂▇▂▁\n\n\nWhite_Female_65_years_and_over\n0\n1\n6.40\n1.71\n1.05\n5.37\n6.67\n7.54\n9.90\n▁▁▆▇▂\n\n\nWhite_Male_10_to_19_years\n0\n1\n6.00\n1.42\n1.02\n5.26\n6.11\n6.91\n9.74\n▁▁▇▇▁\n\n\nWhite_Male_20_to_29_years\n0\n1\n6.26\n1.32\n2.41\n5.42\n6.10\n7.13\n9.96\n▁▃▇▃▁\n\n\nWhite_Male_30_to_39_years\n0\n1\n6.25\n1.18\n1.93\n5.57\n6.31\n7.04\n9.67\n▁▂▇▆▁\n\n\nWhite_Male_40_to_49_years\n0\n1\n5.56\n1.21\n1.35\n4.77\n5.66\n6.40\n8.24\n▁▁▇▇▃\n\n\nWhite_Male_50_to_64_years\n0\n1\n6.23\n1.39\n1.78\n5.62\n6.16\n6.92\n10.93\n▁▂▇▂▁\n\n\nWhite_Male_65_years_and_over\n0\n1\n4.56\n1.19\n1.02\n3.80\n4.78\n5.34\n7.51\n▁▂▇▇▁\n\n\nUnemployment_rate\n0\n1\n6.04\n2.11\n2.30\n4.50\n5.60\n7.20\n17.80\n▇▇▂▁▁\n\n\nPoverty_rate\n0\n1\n13.39\n3.86\n5.70\n10.40\n12.80\n15.60\n27.20\n▃▇▅▂▁\n\n\nViol_crime_count\n0\n1\n32452.11\n46790.78\n322.00\n5598.75\n14684.00\n39119.00\n345624.00\n▇▁▁▁▁\n\n\nPopulation\n0\n1\n5559352.78\n6092703.87\n404680.00\n1570224.75\n3659637.00\n6487139.00\n37349363.00\n▇▂▁▁▁\n\n\npolice_per_100k_lag\n0\n1\n315.19\n116.43\n83.76\n247.63\n298.45\n354.02\n1021.14\n▆▇▁▁▁\n\n\nRTC_LAW_YEAR\n0\n1\nInf\nNaN\n1985.00\n1994.25\n1997.00\n2011.25\nInf\n▇▇▃▅▂\n\n\nTIME_0\n0\n1\n1980.00\n0.00\n1980.00\n1980.00\n1980.00\n1980.00\n1980.00\n▁▁▇▁▁\n\n\nTIME_INF\n0\n1\n2010.00\n0.00\n2010.00\n2010.00\n2010.00\n2010.00\n2010.00\n▁▁▇▁▁\n\n\nViol_crime_rate_1k\n0\n1\n5.10\n3.21\n0.48\n2.87\n4.63\n6.47\n29.30\n▇▃▁▁▁\n\n\nViol_crime_rate_1k_log\n0\n1\n1.46\n0.60\n-0.74\n1.05\n1.53\n1.87\n3.38\n▁▂▇▅▁\n\n\nPopulation_log\n0\n1\n15.04\n1.02\n12.91\n14.27\n15.11\n15.69\n17.44\n▃▅▇▅▂"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#donohue-data",
    "href": "content/lectures/12-cs01-eda.html#donohue-data",
    "title": "12-cs01-eda",
    "section": "DONOHUE data",
    "text": "DONOHUE data\n\nglimpseskim\n\n\n\nglimpse(DONOHUE_DF)\n\nRows: 1,364\nColumns: 20\n$ YEAR                      <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 1980, 19…\n$ STATE                     <chr> \"Alaska\", \"Arizona\", \"Arkansas\", \"California…\n$ Black_Male_15_to_19_years <dbl> 0.16704557, 0.17475437, 0.95451390, 0.433886…\n$ Black_Male_20_to_39_years <dbl> 0.99337748, 0.52671209, 1.97382132, 1.353260…\n$ Other_Male_15_to_19_years <dbl> 1.12978156, 0.41504620, 0.03849163, 0.312308…\n$ Other_Male_20_to_39_years <dbl> 2.96332905, 0.98492602, 0.12425676, 1.213007…\n$ White_Male_15_to_19_years <dbl> 3.6278047, 4.0915770, 3.7401985, 3.8358473, …\n$ White_Male_20_to_39_years <dbl> 18.288524, 14.692380, 12.125127, 14.990947, …\n$ Unemployment_rate         <dbl> 9.6, 6.6, 7.6, 6.8, 5.8, 7.6, 7.4, 6.1, 6.3,…\n$ Poverty_rate              <dbl> 9.6, 12.8, 21.5, 11.0, 8.6, 11.8, 20.9, 16.7…\n$ Viol_crime_count          <dbl> 1919, 17673, 7656, 210290, 15215, 2824, 1277…\n$ Population                <dbl> 404680, 2735840, 2288809, 23792840, 2909545,…\n$ police_per_100k_lag       <dbl> 194.72176, 262.66156, 152.00045, 243.92632, …\n$ RTC_LAW_YEAR              <dbl> 1995, 1995, 1996, Inf, 2003, Inf, Inf, 1988,…\n$ RTC_LAW                   <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ TIME_0                    <dbl> 1980, 1980, 1980, 1980, 1980, 1980, 1980, 19…\n$ TIME_INF                  <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20…\n$ Viol_crime_rate_1k        <dbl> 4.742018, 6.459808, 3.344971, 8.838373, 5.22…\n$ Viol_crime_rate_1k_log    <dbl> 1.5564629, 1.8655995, 1.2074581, 2.1791028, …\n$ Population_log            <dbl> 12.91085, 14.82195, 14.64354, 16.98490, 14.8…\n\n\n\n\n\nskimr::skim(DONOHUE_DF)\n\nWarning: There was 1 warning in `dplyr::summarize()`.\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nℹ In group 0: .\nCaused by warning:\n! There was 1 warning in `dplyr::summarize()`.\nℹ In argument: `dplyr::across(tidyselect::any_of(variable_names),\n  mangled_skimmers$funs)`.\nCaused by warning in `inline_hist()`:\n! Variable contains Inf or -Inf value(s) that were converted to NA.\n\n\n\nData summary\n\n\nName\nDONOHUE_DF\n\n\nNumber of rows\n1364\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nlogical\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nSTATE\n0\n1\n4\n20\n0\n44\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nRTC_LAW\n0\n1\n0.36\nFAL: 868, TRU: 496\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYEAR\n0\n1\n1995.00\n8.95\n1980.00\n1987.00\n1995.00\n2003.00\n2010.00\n▇▇▇▇▇\n\n\nBlack_Male_15_to_19_years\n0\n1\n0.53\n0.51\n0.02\n0.15\n0.36\n0.74\n3.46\n▇▂▁▁▁\n\n\nBlack_Male_20_to_39_years\n0\n1\n1.77\n1.76\n0.07\n0.57\n1.19\n2.32\n11.33\n▇▂▁▁▁\n\n\nOther_Male_15_to_19_years\n0\n1\n0.26\n0.40\n0.01\n0.08\n0.14\n0.29\n2.90\n▇▁▁▁▁\n\n\nOther_Male_20_to_39_years\n0\n1\n0.93\n1.42\n0.07\n0.31\n0.55\n1.01\n9.90\n▇▁▁▁▁\n\n\nWhite_Male_15_to_19_years\n0\n1\n3.07\n0.72\n0.55\n2.67\n3.13\n3.52\n4.99\n▁▁▇▇▁\n\n\nWhite_Male_20_to_39_years\n0\n1\n12.51\n2.28\n4.41\n11.13\n12.61\n14.13\n18.29\n▁▂▇▇▂\n\n\nUnemployment_rate\n0\n1\n6.04\n2.11\n2.30\n4.50\n5.60\n7.20\n17.80\n▇▇▂▁▁\n\n\nPoverty_rate\n0\n1\n13.39\n3.86\n5.70\n10.40\n12.80\n15.60\n27.20\n▃▇▅▂▁\n\n\nViol_crime_count\n0\n1\n32452.11\n46790.78\n322.00\n5598.75\n14684.00\n39119.00\n345624.00\n▇▁▁▁▁\n\n\nPopulation\n0\n1\n5559352.78\n6092703.87\n404680.00\n1570224.75\n3659637.00\n6487139.00\n37349363.00\n▇▂▁▁▁\n\n\npolice_per_100k_lag\n0\n1\n315.19\n116.43\n83.76\n247.63\n298.45\n354.02\n1021.14\n▆▇▁▁▁\n\n\nRTC_LAW_YEAR\n0\n1\nInf\nNaN\n1985.00\n1994.25\n1997.00\n2011.25\nInf\n▇▇▃▅▂\n\n\nTIME_0\n0\n1\n1980.00\n0.00\n1980.00\n1980.00\n1980.00\n1980.00\n1980.00\n▁▁▇▁▁\n\n\nTIME_INF\n0\n1\n2010.00\n0.00\n2010.00\n2010.00\n2010.00\n2010.00\n2010.00\n▁▁▇▁▁\n\n\nViol_crime_rate_1k\n0\n1\n5.10\n3.21\n0.48\n2.87\n4.63\n6.47\n29.30\n▇▃▁▁▁\n\n\nViol_crime_rate_1k_log\n0\n1\n1.46\n0.60\n-0.74\n1.05\n1.53\n1.87\n3.38\n▁▂▇▅▁\n\n\nPopulation_log\n0\n1\n15.04\n1.02\n12.91\n14.27\n15.11\n15.69\n17.44\n▃▅▇▅▂"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#population-over-time",
    "href": "content/lectures/12-cs01-eda.html#population-over-time",
    "title": "12-cs01-eda",
    "section": "Population over time",
    "text": "Population over time\n\nCodePlot\n\n\n\nDONOHUE_DF |>\n  group_by(YEAR) |>\n  summarise(Population = sum(Population)) |>\nggplot(aes(x = YEAR, y = Population)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = seq(1980, 2010, by = 1),\n    limits = c(1980, 2010),\n    labels = c(seq(1980, 2010, by = 1))\n  ) +\n  labs(\n    title = \"Population has steadily increased\",\n    x = \"Year\",\n    y = \"Population\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#crime-over-time",
    "href": "content/lectures/12-cs01-eda.html#crime-over-time",
    "title": "12-cs01-eda",
    "section": "Crime over time",
    "text": "Crime over time\n\nCode-1Code-2Plot\n\n\n\ndf <- DONOHUE_DF |>\n  group_by(YEAR) |>\n  summarize(Viol_crime_count = sum(Viol_crime_count),\n            Population = sum(Population),\n            .groups = \"drop\") |>\n  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population))\n\n\n\n\ndf |>\n  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = seq(1980, 2010, by = 1),\n    limits = c(1980, 2010),\n    labels = c(seq(1980, 2010, by = 1))\n  ) +\n  scale_y_continuous(\n    breaks = seq(5.75, 6.75, by = 0.25),\n    limits = c(5.75, 6.75)\n  ) +\n  labs(\n    title = \"Crime rates fluctuate over time\",\n    x = \"Year\",\n    y = \"ln(violent crimes per 100,000 people)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90), \n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#crime-over-time-by-state",
    "href": "content/lectures/12-cs01-eda.html#crime-over-time-by-state",
    "title": "12-cs01-eda",
    "section": "Crime over time by state",
    "text": "Crime over time by state\n\nCode-1Code-2Plot\n\n\n\np <- DONOHUE_DF |>\n  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population)) |>\n  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log, color = STATE)) +\n  geom_point(size = 0.5) +\n  geom_line(aes(group = STATE),\n    size = 0.5,\n    show.legend = FALSE\n  ) +\n  geom_text_repel(data = DONOHUE_DF |>\n      mutate(Viol_crime_rate_100k_log = log((Viol_crime_count * 100000) / Population)) |>\n      filter(YEAR == last(YEAR)),\n      aes(label = STATE,x = YEAR, y = Viol_crime_rate_100k_log),\n      size = 3, alpha = 1, nudge_x = 1, direction = \"y\",\n      hjust = 1, vjust = 1, segment.size = 0.25, segment.alpha = 0.25,\n      force = 1, max.iter = 9999)\n\n\n\n\np + \n  guides(color = \"none\") +\n  scale_x_continuous(\n    breaks = seq(1980, 2015, by = 1),\n    limits = c(1980, 2015),\n    labels = c(seq(1980, 2010, by = 1), rep(\"\", 5))\n  ) +\n  scale_y_continuous(\n    breaks = seq(3.5, 8.5, by = 0.5),\n    limits = c(3.5, 8.5)\n  ) +\n  labs(\n    title = \"States have different levels of crime\",\n    x = \"Year\", y = \"ln(violent crimes per 100,000 people)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90), plot.title.position = \"plot\")\n\nWarning: ggrepel: 42 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\nWarning: ggrepel: 42 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#police-presence-over-time",
    "href": "content/lectures/12-cs01-eda.html#police-presence-over-time",
    "title": "12-cs01-eda",
    "section": "Police Presence over time",
    "text": "Police Presence over time\n\nCode-1Plot\n\n\n\nDONOHUE_DF |>\n  group_by(YEAR) |>\n  summarise(Police = sum(police_per_100k_lag)) |> \n  ggplot(aes(x = YEAR, y = Police)) +\n  geom_line() +\n  scale_x_continuous(\n    breaks = seq(1980, 2010, by = 1),\n    limits = c(1980, 2010),\n    labels = c(seq(1980, 2010, by = 1))\n  ) +\n  labs(\n    title = \"Police Presence has increased over time with fluctuations\",\n    x = \"Year\",\n    y = \"Police Presence per 100K people\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90),\n        plot.title.position = \"plot\")"
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#your-turn",
    "href": "content/lectures/12-cs01-eda.html#your-turn",
    "title": "12-cs01-eda",
    "section": "Your Turn",
    "text": "Your Turn\n🧠 Consider the data we’re working with and our questions of interest, what would you like to know that you don’t know yet?\n\n❗ Do some EDA! Try to learn something from the data that we haven’t yet discussed. (Summarize data, make a plot, make a table, etc.)\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/12-cs01-eda.html#where-to-go-from-here",
    "href": "content/lectures/12-cs01-eda.html#where-to-go-from-here",
    "title": "12-cs01-eda",
    "section": "Where to go from here?",
    "text": "Where to go from here?\n\nImplement some of these ideas\nThis week’s lab - continues the EDA!\n\nConsider what variables we have that we haven’t looked at\nConsider the variables we have looked at but look at them differently\n\nEventually: incorporate some of this and likely some of lab into your final case study"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#qa",
    "href": "content/lectures/02-dplyr-slides.html#qa",
    "title": "02-dplyr",
    "section": "Q&A",
    "text": "Q&A\n\nQ: what does |> mean?\nA: It means “and then” and is called a “pipe.” Discussing this in more detail in today’s lecture!\n\n\nQ: I think the lab is much more harder than the lecture.\nA: This first week there is a lot to learn regarding tooling, but once you’ve got that down the “process” of completing a lab will become second nature. Then, it’s just content…and applying something is always more difficult than listening to it being explained. So, labs will be a bit harder than lecture…but that’s by design b/c 1) we grade on effort (it’s ok to be wrong in labs!) and 2) answer keys will be posted (learning from mistakes/places we struggle is a great way to learn!)\n\n\nQ: Just curious about the oral presentation portion of the final project. Will this be a requirement in the form of a recorded video (like in COGS 108), or will we be presenting our project findings live in front of the class on the due date?\nA: There are going to be two options (most likely). A recording will definitely be an option. We’re also hoping to have an option where you can come present live, to the instructional staff and any students who want to learn from one another.\n\n\nQ: I am curious about the similarities and differences between both of Python and R!\nA: Very briefly, some similarities: both are object-oriented programming languages (although R is not only object-oriented); both are great for data analysis; differences: python arguably has a simpler syntax; R is, at its core, designed for statistical analysis so I’d argue it’s better for that; Python is a general-programming language so it’s arguablly better for software development; R has an implementation of the grammar of graphics, so I’d argue it’s better for data visualization (but some would disagree).\n\n\nQ: How can I understand dataframes in R easier?\nA: With practice! Today’s lecture, this week 2 lab, and your first hw will help you get additional practice. Then, we’ll use them throughout the course and build your understanding.\n\n\nQ: For very large data sets, how would you be able to find every type coercion?\nA: Each column would have a single type. glimpse() would help you identify the type of each column. From there, you can edit any that are not what you wanted.\n\n\nQ: Where would be a good location to get some practice on most of the packages we will be using in this class?\nA: The labs and homeworks are one place. But, there are also exercises in the “textbook” for this class: https://r4ds.had.co.nz/\n\n\nQ: How do we grasp Markdown easier?\nA: Trying things out and learning the basic rules. I would recommend starting with an RMarkdown document and just typing a sentence. Then knit to see what that looks like. Then, try to bold and italicize some text. Then try to add some headers. Then, add a bulleted list. Each time knitting to see the output. With that, you’ll have most of the markdown you’ll need! reference for basics\n\n\nQ: I am still not sure whether this class focuses more on the programming aspect or the mathematical concepts\nA: Both! First 5 weeks, more programming; Last 5: combination of both are used, but new programming concepts aren’t introduced a ton, so we focus on the stats while continuing to use what we learned the first five weeks!\n\n\nQ: Is there any rules for coercion?\nA: There are rules! Summarized here\n\n\nQ: When do we use R markdown?\nA: When completing labs, homework assignments, case studies, and likely on the final project. You can also take notes in RMarkdown, but that’s up to you!\n\n\nQ: Never heard about ggplot2. I think everyone had already heard of it previously…\nA: While some students have, you’re not alone! We’ll have two lectures on this and get plenty of practice soon!"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#course-announcements",
    "href": "content/lectures/02-dplyr-slides.html#course-announcements",
    "title": "02-dplyr",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 02 due Friday (1/20; 11:59 PM)\nHW01 due Monday (1/23; 11:59 PM)\nLecture Participation survey “due” after class\n\nNotes (1/19):\n\nLab01 Graded - see GitHub issue for comments; 2pts == full credit\n\nCommon issues:\n\nDid not knit file to HTML\nDid not include name in file\nYou do not need to version control the .RProj file\nYou should replace the “instruction” text with your explanations/interpretations\n\nIf you received a zero, you should have an email from me\n\nLab02 released\nHW01 released - let’s take a look"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#agenda",
    "href": "content/lectures/02-dplyr-slides.html#agenda",
    "title": "02-dplyr",
    "section": "Agenda",
    "text": "Agenda\n\ndplyr\n\nphilosophy\npipes\ncommon operations"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#philosophy",
    "href": "content/lectures/02-dplyr-slides.html#philosophy",
    "title": "02-dplyr",
    "section": "Philosophy",
    "text": "Philosophy\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#the-pipe-in-baser",
    "href": "content/lectures/02-dplyr-slides.html#the-pipe-in-baser",
    "title": "02-dplyr",
    "section": "The pipe in baseR",
    "text": "The pipe in baseR\n\n\n\n\n|> should be read as “and then”\nfor example “Wake up |> brush teeth” would be read as “wake up and then brush teeth”"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#where-does-the-name-come-from",
    "href": "content/lectures/02-dplyr-slides.html#where-does-the-name-come-from",
    "title": "02-dplyr",
    "section": "Where does the name come from?",
    "text": "Where does the name come from?\nThe pipe operator was first implemented in the package magrittr.\n\n\n\n\n\n\n\nYou will see this frequently in code online. It’s equivalent to |>."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#review-how-does-a-pipe-work",
    "href": "content/lectures/02-dplyr-slides.html#review-how-does-a-pipe-work",
    "title": "02-dplyr",
    "section": "Review: How does a pipe work?",
    "text": "Review: How does a pipe work?\n\nYou can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.\n\n\n\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"campus\"))\n\n\n\n\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") |>\n  start_car() |>\n  drive(to = \"campus\") |>\n  park()"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#nc-dot-fatal-crashes-in-north-carolina",
    "href": "content/lectures/02-dplyr-slides.html#nc-dot-fatal-crashes-in-north-carolina",
    "title": "02-dplyr",
    "section": "NC DOT Fatal Crashes in North Carolina",
    "text": "NC DOT Fatal Crashes in North Carolina\nFrom OpenDurham’s Data Portal\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#variables",
    "href": "content/lectures/02-dplyr-slides.html#variables",
    "title": "02-dplyr",
    "section": "Variables",
    "text": "Variables\nView the names of variables via\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#viewing-your-data",
    "href": "content/lectures/02-dplyr-slides.html#viewing-your-data",
    "title": "02-dplyr",
    "section": "Viewing your data",
    "text": "Viewing your data\n\nIn the Environment, click on the name of the data frame to view it in the data viewer (or use the View function)\nUse the glimpse function to take a peek\n\n\nglimpse(bike)\n\nRows: 5,716\nColumns: 54\n$ FID        <dbl> 18, 29, 33, 35, 49, 53, 56, 60, 63, 66, 72, 75, 82, 84, 85,…\n$ OBJECTID   <dbl> 19, 30, 34, 36, 50, 54, 57, 61, 64, 67, 73, 76, 83, 85, 86,…\n$ AmbulanceR <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", …\n$ BikeAge_Gr <chr> NA, \"50-59\", NA, \"16-19\", NA, \"50-59\", \"16-19\", \"40-49\", \"1…\n$ Bike_Age   <dbl> 6, 51, 10, 17, 6, 52, 18, 40, 6, 7, 45, 30, 17, 20, 14, 15,…\n$ Bike_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bike_Dir   <chr> \"Not Applicable\", \"With Traffic\", \"With Traffic\", NA, \"Faci…\n$ Bike_Injur <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"Injury\", \"B: E…\n$ Bike_Pos   <chr> \"Driveway / Alley\", \"Travel Lane\", \"Travel Lane\", \"Travel L…\n$ Bike_Race  <chr> \"Black\", \"Black\", \"Black\", \"White\", \"Black\", \"White\", \"Blac…\n$ Bike_Sex   <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ City       <chr> \"Durham\", \"Greenville\", \"Farmville\", \"Charlotte\", \"Charlott…\n$ County     <chr> \"Durham\", \"Pitt\", \"Pitt\", \"Mecklenburg\", \"Mecklenburg\", \"Du…\n$ CrashAlcoh <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ CrashDay   <chr> \"01-01-06\", \"01-01-02\", \"01-01-07\", \"01-01-05\", NA, NA, NA,…\n$ Crash_Date <date> 2007-01-06, 2007-01-09, 2007-01-14, 2007-01-12, 2007-01-15…\n$ Crash_Grp  <chr> \"Bicyclist Failed to Yield - Midblock\", \"Crossing Paths - O…\n$ Crash_Hour <dbl> 13, 23, 16, 19, 12, 20, 19, 14, 16, 0, 17, 18, 14, 17, 19, …\n$ Crash_Loc  <chr> \"Non-Intersection\", \"Intersection-Related\", \"Intersection\",…\n$ Crash_Mont <chr> NA, NA, NA, NA, NA, \"01-04-01\", \"01-04-01\", NA, \"01-02-01\",…\n$ Crash_Time <dttm> 0001-01-01 13:17:58, 0001-01-01 23:08:58, 0001-01-01 16:44…\n$ Crash_Type <chr> \"Bicyclist Ride Out - Residential Driveway\", \"Crossing Path…\n$ Crash_Ty_1 <dbl> 353311, 211180, 111144, 119139, 112114, 311231, 119144, 132…\n$ Crash_Year <dbl> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n$ Crsh_Sevri <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"O: No Injury\",…\n$ Developmen <chr> \"Residential\", \"Commercial\", \"Residential\", \"Residential\", …\n$ DrvrAge_Gr <chr> \"60-69\", \"30-39\", \"50-59\", \"30-39\", NA, \"20-24\", \"40-49\", N…\n$ Drvr_Age   <dbl> 66, 34, 52, 33, NA, 20, 40, NA, 17, 51, NA, 64, 50, 66, 30,…\n$ Drvr_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"Missing\", \"No\", \"No\", \"Missing\", \"…\n$ Drvr_EstSp <chr> \"11-15 mph\", \"0-5 mph\", \"21-25 mph\", \"46-50 mph\", \"16-20 mp…\n$ Drvr_Injur <chr> \"O: No Injury\", \"O: No Injury\", \"O: No Injury\", \"O: No Inju…\n$ Drvr_Race  <chr> \"Black\", \"Black\", \"White\", \"White\", \"/Missing\", \"White\", \"B…\n$ Drvr_Sex   <chr> \"Male\", \"Male\", \"Female\", \"Female\", NA, \"Female\", \"Male\", N…\n$ Drvr_VehTy <chr> \"Pickup\", \"Passenger Car\", \"Passenger Car\", \"Sport Utility\"…\n$ ExcsSpdInd <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Hit_Run    <chr> \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n$ Light_Cond <chr> \"Daylight\", \"Dark - Lighted Roadway\", \"Daylight\", \"Dark - R…\n$ Locality   <chr> \"Mixed (30% To 70% Developed)\", \"Urban (>70% Developed)\", \"…\n$ Num_Lanes  <chr> \"2 lanes\", \"5 lanes\", \"2 lanes\", \"4 lanes\", \"2 lanes\", \"4 l…\n$ Num_Units  <dbl> 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Rd_Charact <chr> \"Straight - Level\", \"Straight - Level\", \"Straight - Level\",…\n$ Rd_Class   <chr> \"Local Street\", \"Local Street\", \"Local Street\", \"NC Route\",…\n$ Rd_Conditi <chr> \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr…\n$ Rd_Config  <chr> \"Two-Way, Not Divided\", \"Two-Way, Divided, Unprotected Medi…\n$ Rd_Defects <chr> \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ Rd_Feature <chr> \"No Special Feature\", \"Four-Way Intersection\", \"Four-Way In…\n$ Rd_Surface <chr> \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smoo…\n$ Region     <chr> \"Piedmont\", \"Coastal\", \"Coastal\", \"Piedmont\", \"Piedmont\", \"…\n$ Rural_Urba <chr> \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Urban\", \"Urban\", \"Urba…\n$ Speed_Limi <chr> \"20 - 25  MPH\", \"40 - 45  MPH\", \"30 - 35  MPH\", \"40 - 45  M…\n$ Traff_Cntr <chr> \"No Control Present\", \"Stop And Go Signal\", \"Stop Sign\", \"S…\n$ Weather    <chr> \"Clear\", \"Clear\", \"Clear\", \"Cloudy\", \"Clear\", \"Clear\", \"Cle…\n$ Workzone_I <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Location   <chr> \"36.002743, -78.8785\", \"35.612984, -77.39265\", \"35.595676, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#a-grammar-of-data-manipulation",
    "href": "content/lectures/02-dplyr-slides.html#a-grammar-of-data-manipulation",
    "title": "02-dplyr",
    "section": "A Grammar of Data Manipulation",
    "text": "A Grammar of Data Manipulation\ndplyr is based on the concepts of functions as verbs that manipulate data frames.\nSingle data frame functions / verbs:\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#dplyr-rules-for-functions",
    "href": "content/lectures/02-dplyr-slides.html#dplyr-rules-for-functions",
    "title": "02-dplyr",
    "section": "dplyr rules for functions",
    "text": "dplyr rules for functions\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDo not modify in place\nPerformance via lazy evaluation"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter-rows-with-filter",
    "href": "content/lectures/02-dplyr-slides.html#filter-rows-with-filter",
    "title": "02-dplyr",
    "section": "Filter rows with filter",
    "text": "Filter rows with filter\n\nSelect a subset of rows in a data frame.\nEasily filter for many conditions at once."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter",
    "href": "content/lectures/02-dplyr-slides.html#filter",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County\n\nbike |>\n  filter(County == \"Durham\")\n\n# A tibble: 253 × 54\n     FID OBJEC…¹ Ambul…² BikeA…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl>   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1    18      19 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black  \n 2    53      54 Yes     50-59        52 No      With T… A: Dis… Travel… White  \n 3    56      57 Yes     16-19        18 No      <NA>    C: Pos… Travel… Black  \n 4   209     210 No      16-19        16 No      Facing… C: Pos… <NA>    Black  \n 5   228     229 Yes     40-49        40 No      With T… B: Evi… Bike L… Black  \n 6   620     621 Yes     50-59        55 No      With T… B: Evi… Travel… White  \n 7   667     668 Yes     60-69        61 No      Not Ap… B: Evi… Sidewa… Black  \n 8   458     459 Yes     60-69        62 No      With T… B: Evi… Travel… White  \n 9   576     577 No      40-49        49 No      With T… C: Pos… Travel… Black  \n10   618     619 No      20-24        23 No      With T… C: Pos… Travel… Asian  \n# … with 243 more rows, 44 more variables: Bike_Sex <chr>, City <chr>,\n#   County <chr>, CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>,\n#   Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>,\n#   Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>,\n#   Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>,\n#   Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>,\n#   Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#filter-1",
    "href": "content/lectures/02-dplyr-slides.html#filter-1",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County where biker was < 10 yrs old\n\nbike |>\n  filter(County == \"Durham\", Bike_Age < 10)\n\n# A tibble: 20 × 54\n     FID OBJEC…¹ Ambul…² BikeA…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl>   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1    18      19 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black  \n 2    47      48 No      10-Jun        9 No      Not Ap… O: No … Non-Ro… Black  \n 3   124     125 Yes     10-Jun        8 No      With T… C: Pos… Travel… Black  \n 4   531     532 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n 5   704     705 Yes     10-Jun        9 No      Not Ap… C: Pos… Non-Ro… Black  \n 6    42      43 No      10-Jun        8 No      With T… O: No … Travel… Black  \n 7   392     393 Yes     0-5           2 No      Not Ap… B: Evi… Drivew… Black  \n 8   941     942 No      10-Jun        9 No      With T… C: Pos… Travel… Black  \n 9   436     437 Yes     10-Jun        6 No      Not Ap… O: No … Drivew… Black  \n10   160     161 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n11   273     274 Yes     10-Jun        7 No      Facing… C: Pos… Travel… Black  \n12    78      79 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n13   422     423 No      10-Jun        9 No      Not Ap… O: No … Drivew… Black  \n14   570     571 No      <NA>          0 Missing Not Ap… Injury  Non-Ro… /Missi…\n15   683     684 Yes     10-Jun        8 No      Not Ap… C: Pos… Non-Ro… Black  \n16    62      63 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n17   248     249 No      0-5           4 No      Not Ap… O: No … Drivew… Hispan…\n18   306     307 Yes     10-Jun        8 No      With T… C: Pos… <NA>    Black  \n19   231     232 Yes     10-Jun        8 No      With T… C: Pos… Travel… Black  \n20   361     362 Yes     10-Jun        9 No      With T… B: Evi… Travel… Hispan…\n# … with 44 more variables: Bike_Sex <chr>, City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#aside-real-data-is-messy",
    "href": "content/lectures/02-dplyr-slides.html#aside-real-data-is-messy",
    "title": "02-dplyr",
    "section": "Aside: real data is messy!",
    "text": "Aside: real data is messy!\n   What in the world does a BikeAge_gr of 10-Jun or 15-Nov mean?\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 10-Jun             421\n 3 15-Nov             747\n 4 16-19              605\n 5 20-24              680\n 6 25-29              430\n 7 30-39              658\n 8 40-49              920\n 9 50-59              739\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#careful-data-scientists-clean-up-their-data-first",
    "href": "content/lectures/02-dplyr-slides.html#careful-data-scientists-clean-up-their-data-first",
    "title": "02-dplyr",
    "section": "Careful data scientists clean up their data first!",
    "text": "Careful data scientists clean up their data first!\n\nWe’re going to need to do some text parsing to clean up these data\n\n10-Jun should be 6-10\n15-Nov should be 11-15"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#correct-and-overwrite-mutate",
    "href": "content/lectures/02-dplyr-slides.html#correct-and-overwrite-mutate",
    "title": "02-dplyr",
    "section": "Correct and overwrite mutate",
    "text": "Correct and overwrite mutate\n\nRemember we want to do the following in the BikeAge_Gr variable\n\n10-Jun should be 6-10\n15-Nov should be 11-15\n\n\n\nbike <- bike |>\n  mutate(\n    BikeAge_Gr = case_when(\n      BikeAge_Gr == \"10-Jun\" ~ \"6-10\",\n      BikeAge_Gr == \"15-Nov\" ~ \"11-15\",\n      TRUE                   ~ BikeAge_Gr     # everything else\n    )\n  )\n\n\nNote that we’re overwriting existing data and columns, so be careful!\n\nBut remember, it’s easy to revert if you make a mistake since we didn’t touch the raw data, we can always reload it and start over"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#check-before-you-move-on",
    "href": "content/lectures/02-dplyr-slides.html#check-before-you-move-on",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr count\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#mutate-to-add-new-variables",
    "href": "content/lectures/02-dplyr-slides.html#mutate-to-add-new-variables",
    "title": "02-dplyr",
    "section": "mutate to add new variables",
    "text": "mutate to add new variables\n   How is the new alcohol variable determined?\n\nbike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#save-when-you-mutate",
    "href": "content/lectures/02-dplyr-slides.html#save-when-you-mutate",
    "title": "02-dplyr",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame.\n\nbike <- bike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#transmute-to-create-a-new-dataset",
    "href": "content/lectures/02-dplyr-slides.html#transmute-to-create-a-new-dataset",
    "title": "02-dplyr",
    "section": "transmute to create a new dataset",
    "text": "transmute to create a new dataset\nYou’ll use this much less often than mutate but when you need it, you need it.\n\nbike |> \n  transmute(ID = paste(FID, OBJECTID, sep = \"-\"))\n\n# A tibble: 5,716 × 1\n   ID   \n   <chr>\n 1 18-19\n 2 29-30\n 3 33-34\n 4 35-36\n 5 49-50\n 6 53-54\n 7 56-57\n 8 60-61\n 9 63-64\n10 66-67\n# … with 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#mutate-vs.-transmute",
    "href": "content/lectures/02-dplyr-slides.html#mutate-vs.-transmute",
    "title": "02-dplyr",
    "section": "mutate vs. transmute",
    "text": "mutate vs. transmute\n\nmutate adds new and keeps original\ntransmute adds new; drops existing"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#your-turn",
    "href": "content/lectures/02-dplyr-slides.html#your-turn",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nHow many accidents in our dataset required an ambulance ride (AmbulanceR) and had the Crash_Type “Bicyclist Lost Control - Mechanical Problems”?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers",
    "href": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nFirst five\n\nbike |>\n  slice(1:5)\n\n# A tibble: 5 × 54\n    FID OBJECTID Ambul…¹ BikeA…² Bike_…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸\n  <dbl>    <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n1    18       19 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black  \n2    29       30 Yes     50-59        51 No      With T… C: Pos… Travel… Black  \n3    33       34 No      <NA>         10 No      With T… Injury  Travel… Black  \n4    35       36 Yes     16-19        17 No      <NA>    B: Evi… Travel… White  \n5    49       50 No      <NA>          6 No      Facing… O: No … Travel… Black  \n# … with 44 more variables: Bike_Sex <chr>, City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers-1",
    "href": "content/lectures/02-dplyr-slides.html#slice-for-certain-row-numbers-1",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nLast five\n\nlast_row <- nrow(bike)\nbike |>\n  slice((last_row - 4):last_row)\n\n# A tibble: 5 × 54\n    FID OBJECTID Ambul…¹ BikeA…² Bike_…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸\n  <dbl>    <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n1   460      461 Yes     6-10          7 No      Not Ap… C: Pos… Drivew… Black  \n2   474      475 Yes     50-59        50 No      With T… B: Evi… Travel… White  \n3   479      480 Yes     16-19        16 No      Not Ap… C: Pos… Sidewa… White  \n4   487      488 No      40-49        47 Yes     With T… C: Pos… Travel… White  \n5   488      489 Yes     30-39        35 No      Facing… C: Pos… Travel… Black  \n# … with 44 more variables: Bike_Sex <chr>, City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#select-to-keep-only-the-variables-you-mention",
    "href": "content/lectures/02-dplyr-slides.html#select-to-keep-only-the-variables-you-mention",
    "title": "02-dplyr",
    "section": "select to keep only the variables you mention",
    "text": "select to keep only the variables you mention\n\nbike |>\n  select(Crash_Loc, Hit_Run) |>\n  table()\n\n                      Hit_Run\nCrash_Loc                No  Yes\n  Intersection         2223  275\n  Intersection-Related  252   42\n  Location                3    7\n  Non-Intersection     2213  462\n  Non-Roadway           205   30"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#or-select-to-exclude-variables",
    "href": "content/lectures/02-dplyr-slides.html#or-select-to-exclude-variables",
    "title": "02-dplyr",
    "section": "or select to exclude variables",
    "text": "or select to exclude variables\n\nbike |>\n  select(-OBJECTID)\n\n# A tibble: 5,716 × 53\n     FID Ambul…¹ BikeA…² Bike_…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n 1    18 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black   Female \n 2    29 Yes     50-59        51 No      With T… C: Pos… Travel… Black   Male   \n 3    33 No      <NA>         10 No      With T… Injury  Travel… Black   Male   \n 4    35 Yes     16-19        17 No      <NA>    B: Evi… Travel… White   Male   \n 5    49 No      <NA>          6 No      Facing… O: No … Travel… Black   Male   \n 6    53 Yes     50-59        52 No      With T… A: Dis… Travel… White   Male   \n 7    56 Yes     16-19        18 No      <NA>    C: Pos… Travel… Black   Female \n 8    60 No      40-49        40 No      Facing… B: Evi… Sidewa… Hispan… Male   \n 9    63 Yes     6-10          6 No      Facing… B: Evi… Travel… White   Male   \n10    66 Yes     6-10          7 No      <NA>    B: Evi… Non-Ro… Black   Female \n# … with 5,706 more rows, 43 more variables: City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#or-select-a-range-of-variables",
    "href": "content/lectures/02-dplyr-slides.html#or-select-a-range-of-variables",
    "title": "02-dplyr",
    "section": "or select a range of variables",
    "text": "or select a range of variables\n\nbike |>\n  select(OBJECTID:Bike_Injur)\n\n# A tibble: 5,716 × 7\n   OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir       Bike_Injur \n      <dbl> <chr>      <chr>         <dbl> <chr>      <chr>          <chr>      \n 1       19 No         <NA>              6 No         Not Applicable C: Possibl…\n 2       30 Yes        50-59            51 No         With Traffic   C: Possibl…\n 3       34 No         <NA>             10 No         With Traffic   Injury     \n 4       36 Yes        16-19            17 No         <NA>           B: Evident…\n 5       50 No         <NA>              6 No         Facing Traffic O: No Inju…\n 6       54 Yes        50-59            52 No         With Traffic   A: Disabli…\n 7       57 Yes        16-19            18 No         <NA>           C: Possibl…\n 8       61 No         40-49            40 No         Facing Traffic B: Evident…\n 9       64 Yes        6-10              6 No         Facing Traffic B: Evident…\n10       67 Yes        6-10              7 No         <NA>           B: Evident…\n# … with 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#pull-to-extract-a-column-as-a-vector",
    "href": "content/lectures/02-dplyr-slides.html#pull-to-extract-a-column-as-a-vector",
    "title": "02-dplyr",
    "section": "pull to extract a column as a vector",
    "text": "pull to extract a column as a vector\n\nbike |>\n  slice(1:6) |>\n  pull(Location)\n\n[1] \"36.002743, -78.8785\"  \"35.612984, -77.39265\" \"35.595676, -77.59074\"\n[4] \"35.076767, -80.7728\"  \"35.19999, -80.75713\"  \"35.966644, -78.96749\"\n\n\n\nbike |>\n  slice(1:6) |>\n  select(Location)\n\n# A tibble: 6 × 1\n  Location            \n  <chr>               \n1 36.002743, -78.8785 \n2 35.612984, -77.39265\n3 35.595676, -77.59074\n4 35.076767, -80.7728 \n5 35.19999, -80.75713 \n6 35.966644, -78.96749"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#the-two-pulls-in-your-lives",
    "href": "content/lectures/02-dplyr-slides.html#the-two-pulls-in-your-lives",
    "title": "02-dplyr",
    "section": "The two pulls in your lives",
    "text": "The two pulls in your lives\n\n\n\n\n\n\n\n\nDon’t get pull happy when wrangling data! Only extract out variables if you truly need to, otherwise keep in data frame.\nBut always ⬇️ Pull before starting your work when collaborating on GitHub."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#rename-specific-columns",
    "href": "content/lectures/02-dplyr-slides.html#rename-specific-columns",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\nUseful for correcting typos, and renaming to make variable names shorter and/or more informative\n\nOriginal names:\n\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#rename-specific-columns-1",
    "href": "content/lectures/02-dplyr-slides.html#rename-specific-columns-1",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\n\nRename Speed_Limi to Speed_Limit:\n\n\nbike <- bike |>\n  rename(Speed_Limit = Speed_Limi)"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#check-before-you-move-on-1",
    "href": "content/lectures/02-dplyr-slides.html#check-before-you-move-on-1",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nnames(bike)\n\n [1] \"FID\"         \"OBJECTID\"    \"AmbulanceR\"  \"BikeAge_Gr\"  \"Bike_Age\"   \n [6] \"Bike_Alc_D\"  \"Bike_Dir\"    \"Bike_Injur\"  \"Bike_Pos\"    \"Bike_Race\"  \n[11] \"Bike_Sex\"    \"City\"        \"County\"      \"CrashAlcoh\"  \"CrashDay\"   \n[16] \"Crash_Date\"  \"Crash_Grp\"   \"Crash_Hour\"  \"Crash_Loc\"   \"Crash_Mont\" \n[21] \"Crash_Time\"  \"Crash_Type\"  \"Crash_Ty_1\"  \"Crash_Year\"  \"Crsh_Sevri\" \n[26] \"Developmen\"  \"DrvrAge_Gr\"  \"Drvr_Age\"    \"Drvr_Alc_D\"  \"Drvr_EstSp\" \n[31] \"Drvr_Injur\"  \"Drvr_Race\"   \"Drvr_Sex\"    \"Drvr_VehTy\"  \"ExcsSpdInd\" \n[36] \"Hit_Run\"     \"Light_Cond\"  \"Locality\"    \"Num_Lanes\"   \"Num_Units\"  \n[41] \"Rd_Charact\"  \"Rd_Class\"    \"Rd_Conditi\"  \"Rd_Config\"   \"Rd_Defects\" \n[46] \"Rd_Feature\"  \"Rd_Surface\"  \"Region\"      \"Rural_Urba\"  \"Speed_Limit\"\n[51] \"Traff_Cntr\"  \"Weather\"     \"Workzone_I\"  \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#your-turn-1",
    "href": "content/lectures/02-dplyr-slides.html#your-turn-1",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nYour boss in Cumberland County gets overwhelmed by data easily, but he wants some data from you. He wants all bike accidents from his County, but he only wants to know the road’s speed limit, the age of the biker, and to know if alcohol was involved. If you have time, mine as well make the column names very clear to your boss while you’re at it…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#summarize-to-reduce-variables-to-values",
    "href": "content/lectures/02-dplyr-slides.html#summarize-to-reduce-variables-to-values",
    "title": "02-dplyr",
    "section": "summarize to reduce variables to values",
    "text": "summarize to reduce variables to values\nThe values are summarized in a data frame\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 11-15              747\n 3 16-19              605\n 4 20-24              680\n 5 25-29              430\n 6 30-39              658\n 7 40-49              920\n 8 50-59              739\n 9 6-10               421\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#and-arrange-to-order-rows",
    "href": "content/lectures/02-dplyr-slides.html#and-arrange-to-order-rows",
    "title": "02-dplyr",
    "section": "and arrange to order rows",
    "text": "and arrange to order rows\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n()) |>\n  arrange(desc(crash_count))\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 40-49              920\n 2 11-15              747\n 3 50-59              739\n 4 20-24              680\n 5 30-39              658\n 6 16-19              605\n 7 25-29              430\n 8 6-10               421\n 9 60-69              274\n10 <NA>               112\n11 0-5                 60\n12 70+                 58\n13 70                  12"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#count-to-group-by-then-count",
    "href": "content/lectures/02-dplyr-slides.html#count-to-group-by-then-count",
    "title": "02-dplyr",
    "section": "count to group by then count",
    "text": "count to group by then count\n\nbike |>\n  count(BikeAge_Gr)\n\n# A tibble: 13 × 2\n   BikeAge_Gr     n\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112\n\n\n   If you wanted to arrange these in ascending order what would you add to the pipe?"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#select-rows-with-sample_n-or-sample_frac",
    "href": "content/lectures/02-dplyr-slides.html#select-rows-with-sample_n-or-sample_frac",
    "title": "02-dplyr",
    "section": "Select rows with sample_n or sample_frac",
    "text": "Select rows with sample_n or sample_frac\n\nsample_n: randomly sample 5 observations\n\n\nbike_n5 <- bike |>\n  sample_n(5, replace = FALSE)\n\ndim(bike_n5)\n\n[1]  5 54\n\n\n\nsample_frac: randomly sample 20% of observations\n\n\nbike_perc20 <- bike |>\n  sample_frac(0.2, replace = FALSE)\n\ndim(bike_perc20)\n\n[1] 1143   54"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#distinct-to-filter-for-unique-rows",
    "href": "content/lectures/02-dplyr-slides.html#distinct-to-filter-for-unique-rows",
    "title": "02-dplyr",
    "section": "distinct to filter for unique rows",
    "text": "distinct to filter for unique rows\n\nbike |> \n  select(County, City) |> \n  distinct() |> \n  arrange(County, City)\n\n# A tibble: 360 × 2\n   County    City              \n   <chr>     <chr>             \n 1 Alamance  Alamance          \n 2 Alamance  Burlington        \n 3 Alamance  Elon College      \n 4 Alamance  Gibsonville       \n 5 Alamance  Graham            \n 6 Alamance  Green Level       \n 7 Alamance  Mebane            \n 8 Alamance  None - Rural Crash\n 9 Alexander None - Rural Crash\n10 Alleghany None - Rural Crash\n# … with 350 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#distinct-has-a-.keep_all-parameter",
    "href": "content/lectures/02-dplyr-slides.html#distinct-has-a-.keep_all-parameter",
    "title": "02-dplyr",
    "section": "distinct has a .keep_all parameter",
    "text": "distinct has a .keep_all parameter\n\nbike |> \n  distinct(County, City, .keep_all = TRUE) |> \n  arrange(County, City)\n\n# A tibble: 360 × 54\n     FID OBJEC…¹ Ambul…² BikeA…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl>   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1   524     525 Yes     11-15        12 No      <NA>    B: Evi… <NA>    Black  \n 2    84      85 Yes     20-24        20 No      With T… B: Evi… Travel… White  \n 3   571     572 Yes     16-19        16 No      Not Ap… B: Evi… Non-Ro… White  \n 4   509     510 Yes     40-49        43 Yes     With T… K: Kil… Travel… Black  \n 5   855     856 Yes     30-39        30 No      With T… A: Dis… Travel… Black  \n 6     5       6 Yes     40-49        44 Yes     With T… C: Pos… Travel… Black  \n 7   163     164 Yes     30-39        35 No      Not Ap… C: Pos… <NA>    White  \n 8    96      97 Yes     30-39        36 No      With T… C: Pos… Travel… White  \n 9    46      47 Yes     50-59        53 No      With T… B: Evi… Travel… White  \n10   485     486 Yes     60-69        62 No      With T… C: Pos… Travel… White  \n# … with 350 more rows, 44 more variables: Bike_Sex <chr>, City <chr>,\n#   County <chr>, CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>,\n#   Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>,\n#   Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>,\n#   Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>,\n#   Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>,\n#   Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#factors-1",
    "href": "content/lectures/02-dplyr-slides.html#factors-1",
    "title": "02-dplyr",
    "section": "Factors",
    "text": "Factors\nFactor objects are how R stores data for categorical variables (fixed numbers of discrete values).\n\n(x = factor(c(\"BS\", \"MS\", \"PhD\", \"MS\")))\n\n[1] BS  MS  PhD MS \nLevels: BS MS PhD\n\n\n\nglimpse(x)\n\n Factor w/ 3 levels \"BS\",\"MS\",\"PhD\": 1 2 3 2\n\n\n\ntypeof(x)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#returning-to-cat-lovers",
    "href": "content/lectures/02-dplyr-slides.html#returning-to-cat-lovers",
    "title": "02-dplyr",
    "section": "Returning to: Cat lovers",
    "text": "Returning to: Cat lovers\nReading in the cat-lovers data…\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#read-data-in-as-character-strings",
    "href": "content/lectures/02-dplyr-slides.html#read-data-in-as-character-strings",
    "title": "02-dplyr",
    "section": "Read data in as character strings",
    "text": "Read data in as character strings\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#but-coerce-when-plotting",
    "href": "content/lectures/02-dplyr-slides.html#but-coerce-when-plotting",
    "title": "02-dplyr",
    "section": "But coerce when plotting",
    "text": "But coerce when plotting\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#use-forcats-to-manipulate-factors",
    "href": "content/lectures/02-dplyr-slides.html#use-forcats-to-manipulate-factors",
    "title": "02-dplyr",
    "section": "Use forcats to manipulate factors",
    "text": "Use forcats to manipulate factors\n\ncat_lovers <- cat_lovers |>\n  mutate(handedness = fct_relevel(handedness, \n                                  \"right\", \"left\", \"ambidextrous\"))\n\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#forcats-functionality",
    "href": "content/lectures/02-dplyr-slides.html#forcats-functionality",
    "title": "02-dplyr",
    "section": "forcats functionality ",
    "text": "forcats functionality \n\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values. Historically, factors were much easier to work with than character vectors, so many base R functions automatically convert character vectors to factors.\nfactors are still useful when you have true categorical data, and when you want to override the ordering of character vectors to improve display. The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors.\n\n\n\nSource: forcats.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr-slides.html#suggested-reading",
    "href": "content/lectures/02-dplyr-slides.html#suggested-reading",
    "title": "02-dplyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/02-dplyr.html",
    "href": "content/lectures/02-dplyr.html",
    "title": "02-dplyr",
    "section": "",
    "text": "Q: what does |> mean?\nA: It means “and then” and is called a “pipe.” Discussing this in more detail in today’s lecture!\n\n\nQ: I think the lab is much more harder than the lecture.\nA: This first week there is a lot to learn regarding tooling, but once you’ve got that down the “process” of completing a lab will become second nature. Then, it’s just content…and applying something is always more difficult than listening to it being explained. So, labs will be a bit harder than lecture…but that’s by design b/c 1) we grade on effort (it’s ok to be wrong in labs!) and 2) answer keys will be posted (learning from mistakes/places we struggle is a great way to learn!)\n\n\nQ: Just curious about the oral presentation portion of the final project. Will this be a requirement in the form of a recorded video (like in COGS 108), or will we be presenting our project findings live in front of the class on the due date?\nA: There are going to be two options (most likely). A recording will definitely be an option. We’re also hoping to have an option where you can come present live, to the instructional staff and any students who want to learn from one another.\n\n\nQ: I am curious about the similarities and differences between both of Python and R!\nA: Very briefly, some similarities: both are object-oriented programming languages (although R is not only object-oriented); both are great for data analysis; differences: python arguably has a simpler syntax; R is, at its core, designed for statistical analysis so I’d argue it’s better for that; Python is a general-programming language so it’s arguablly better for software development; R has an implementation of the grammar of graphics, so I’d argue it’s better for data visualization (but some would disagree).\n\n\nQ: How can I understand dataframes in R easier?\nA: With practice! Today’s lecture, this week 2 lab, and your first hw will help you get additional practice. Then, we’ll use them throughout the course and build your understanding.\n\n\nQ: For very large data sets, how would you be able to find every type coercion?\nA: Each column would have a single type. glimpse() would help you identify the type of each column. From there, you can edit any that are not what you wanted.\n\n\nQ: Where would be a good location to get some practice on most of the packages we will be using in this class?\nA: The labs and homeworks are one place. But, there are also exercises in the “textbook” for this class: https://r4ds.had.co.nz/\n\n\nQ: How do we grasp Markdown easier?\nA: Trying things out and learning the basic rules. I would recommend starting with an RMarkdown document and just typing a sentence. Then knit to see what that looks like. Then, try to bold and italicize some text. Then try to add some headers. Then, add a bulleted list. Each time knitting to see the output. With that, you’ll have most of the markdown you’ll need! reference for basics\n\n\nQ: I am still not sure whether this class focuses more on the programming aspect or the mathematical concepts\nA: Both! First 5 weeks, more programming; Last 5: combination of both are used, but new programming concepts aren’t introduced a ton, so we focus on the stats while continuing to use what we learned the first five weeks!\n\n\nQ: Is there any rules for coercion?\nA: There are rules! Summarized here\n\n\nQ: When do we use R markdown?\nA: When completing labs, homework assignments, case studies, and likely on the final project. You can also take notes in RMarkdown, but that’s up to you!\n\n\nQ: Never heard about ggplot2. I think everyone had already heard of it previously…\nA: While some students have, you’re not alone! We’ll have two lectures on this and get plenty of practice soon!\n\n\n\n\nDue Dates:\n\nLab 02 due Friday (1/20; 11:59 PM)\nHW01 due Monday (1/23; 11:59 PM)\nLecture Participation survey “due” after class\n\nNotes (1/19):\n\nLab01 Graded - see GitHub issue for comments; 2pts == full credit\n\nCommon issues:\n\nDid not knit file to HTML\nDid not include name in file\nYou do not need to version control the .RProj file\nYou should replace the “instruction” text with your explanations/interpretations\n\nIf you received a zero, you should have an email from me\n\nLab02 released\nHW01 released - let’s take a look\n\n\n\n\n\ndplyr\n\nphilosophy\npipes\ncommon operations\n\n\n\n\n\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\n\n\nSource: dplyr.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "href": "content/lectures/02-dplyr.html#the-pipe-in-baser",
    "title": "02-dplyr",
    "section": "The pipe in baseR",
    "text": "The pipe in baseR\n\n\n\n\n|> should be read as “and then”\nfor example “Wake up |> brush teeth” would be read as “wake up and then brush teeth”"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "href": "content/lectures/02-dplyr.html#where-does-the-name-come-from",
    "title": "02-dplyr",
    "section": "Where does the name come from?",
    "text": "Where does the name come from?\nThe pipe operator was first implemented in the package magrittr.\n\n\n\n\n\n\n\nYou will see this frequently in code online. It’s equivalent to |>."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "href": "content/lectures/02-dplyr.html#review-how-does-a-pipe-work",
    "title": "02-dplyr",
    "section": "Review: How does a pipe work?",
    "text": "Review: How does a pipe work?\n\nYou can think about the following sequence of actions - find key, unlock car, start car, drive to school, park.\n\n\n\nExpressed as a set of nested functions in R pseudocode this would look like:\n\n\npark(drive(start_car(find(\"keys\")), to = \"campus\"))\n\n\n\n\nWriting it out using pipes give it a more natural (and easier to read) structure:\n\n\nfind(\"keys\") |>\n  start_car() |>\n  drive(to = \"campus\") |>\n  park()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "href": "content/lectures/02-dplyr.html#nc-dot-fatal-crashes-in-north-carolina",
    "title": "02-dplyr",
    "section": "NC DOT Fatal Crashes in North Carolina",
    "text": "NC DOT Fatal Crashes in North Carolina\nFrom OpenDurham’s Data Portal\n\nbike <- read_csv2(\"https://raw.githubusercontent.com/COGS137/datasets/main/nc_bike_crash.csv\", \n                  na = c(\"NA\", \"\", \".\"))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#variables",
    "href": "content/lectures/02-dplyr.html#variables",
    "title": "02-dplyr",
    "section": "Variables",
    "text": "Variables\nView the names of variables via\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#viewing-your-data",
    "href": "content/lectures/02-dplyr.html#viewing-your-data",
    "title": "02-dplyr",
    "section": "Viewing your data",
    "text": "Viewing your data\n\nIn the Environment, click on the name of the data frame to view it in the data viewer (or use the View function)\nUse the glimpse function to take a peek\n\n\nglimpse(bike)\n\nRows: 5,716\nColumns: 54\n$ FID        <dbl> 18, 29, 33, 35, 49, 53, 56, 60, 63, 66, 72, 75, 82, 84, 85,…\n$ OBJECTID   <dbl> 19, 30, 34, 36, 50, 54, 57, 61, 64, 67, 73, 76, 83, 85, 86,…\n$ AmbulanceR <chr> \"No\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", …\n$ BikeAge_Gr <chr> NA, \"50-59\", NA, \"16-19\", NA, \"50-59\", \"16-19\", \"40-49\", \"1…\n$ Bike_Age   <dbl> 6, 51, 10, 17, 6, 52, 18, 40, 6, 7, 45, 30, 17, 20, 14, 15,…\n$ Bike_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bike_Dir   <chr> \"Not Applicable\", \"With Traffic\", \"With Traffic\", NA, \"Faci…\n$ Bike_Injur <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"Injury\", \"B: E…\n$ Bike_Pos   <chr> \"Driveway / Alley\", \"Travel Lane\", \"Travel Lane\", \"Travel L…\n$ Bike_Race  <chr> \"Black\", \"Black\", \"Black\", \"White\", \"Black\", \"White\", \"Blac…\n$ Bike_Sex   <chr> \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Female\",…\n$ City       <chr> \"Durham\", \"Greenville\", \"Farmville\", \"Charlotte\", \"Charlott…\n$ County     <chr> \"Durham\", \"Pitt\", \"Pitt\", \"Mecklenburg\", \"Mecklenburg\", \"Du…\n$ CrashAlcoh <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ CrashDay   <chr> \"01-01-06\", \"01-01-02\", \"01-01-07\", \"01-01-05\", NA, NA, NA,…\n$ Crash_Date <date> 2007-01-06, 2007-01-09, 2007-01-14, 2007-01-12, 2007-01-15…\n$ Crash_Grp  <chr> \"Bicyclist Failed to Yield - Midblock\", \"Crossing Paths - O…\n$ Crash_Hour <dbl> 13, 23, 16, 19, 12, 20, 19, 14, 16, 0, 17, 18, 14, 17, 19, …\n$ Crash_Loc  <chr> \"Non-Intersection\", \"Intersection-Related\", \"Intersection\",…\n$ Crash_Mont <chr> NA, NA, NA, NA, NA, \"01-04-01\", \"01-04-01\", NA, \"01-02-01\",…\n$ Crash_Time <dttm> 0001-01-01 13:17:58, 0001-01-01 23:08:58, 0001-01-01 16:44…\n$ Crash_Type <chr> \"Bicyclist Ride Out - Residential Driveway\", \"Crossing Path…\n$ Crash_Ty_1 <dbl> 353311, 211180, 111144, 119139, 112114, 311231, 119144, 132…\n$ Crash_Year <dbl> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007,…\n$ Crsh_Sevri <chr> \"C: Possible Injury\", \"C: Possible Injury\", \"O: No Injury\",…\n$ Developmen <chr> \"Residential\", \"Commercial\", \"Residential\", \"Residential\", …\n$ DrvrAge_Gr <chr> \"60-69\", \"30-39\", \"50-59\", \"30-39\", NA, \"20-24\", \"40-49\", N…\n$ Drvr_Age   <dbl> 66, 34, 52, 33, NA, 20, 40, NA, 17, 51, NA, 64, 50, 66, 30,…\n$ Drvr_Alc_D <chr> \"No\", \"No\", \"No\", \"No\", \"Missing\", \"No\", \"No\", \"Missing\", \"…\n$ Drvr_EstSp <chr> \"11-15 mph\", \"0-5 mph\", \"21-25 mph\", \"46-50 mph\", \"16-20 mp…\n$ Drvr_Injur <chr> \"O: No Injury\", \"O: No Injury\", \"O: No Injury\", \"O: No Inju…\n$ Drvr_Race  <chr> \"Black\", \"Black\", \"White\", \"White\", \"/Missing\", \"White\", \"B…\n$ Drvr_Sex   <chr> \"Male\", \"Male\", \"Female\", \"Female\", NA, \"Female\", \"Male\", N…\n$ Drvr_VehTy <chr> \"Pickup\", \"Passenger Car\", \"Passenger Car\", \"Sport Utility\"…\n$ ExcsSpdInd <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Hit_Run    <chr> \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n$ Light_Cond <chr> \"Daylight\", \"Dark - Lighted Roadway\", \"Daylight\", \"Dark - R…\n$ Locality   <chr> \"Mixed (30% To 70% Developed)\", \"Urban (>70% Developed)\", \"…\n$ Num_Lanes  <chr> \"2 lanes\", \"5 lanes\", \"2 lanes\", \"4 lanes\", \"2 lanes\", \"4 l…\n$ Num_Units  <dbl> 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Rd_Charact <chr> \"Straight - Level\", \"Straight - Level\", \"Straight - Level\",…\n$ Rd_Class   <chr> \"Local Street\", \"Local Street\", \"Local Street\", \"NC Route\",…\n$ Rd_Conditi <chr> \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr…\n$ Rd_Config  <chr> \"Two-Way, Not Divided\", \"Two-Way, Divided, Unprotected Medi…\n$ Rd_Defects <chr> \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"None\", \"No…\n$ Rd_Feature <chr> \"No Special Feature\", \"Four-Way Intersection\", \"Four-Way In…\n$ Rd_Surface <chr> \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smooth Asphalt\", \"Smoo…\n$ Region     <chr> \"Piedmont\", \"Coastal\", \"Coastal\", \"Piedmont\", \"Piedmont\", \"…\n$ Rural_Urba <chr> \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Urban\", \"Urban\", \"Urba…\n$ Speed_Limi <chr> \"20 - 25  MPH\", \"40 - 45  MPH\", \"30 - 35  MPH\", \"40 - 45  M…\n$ Traff_Cntr <chr> \"No Control Present\", \"Stop And Go Signal\", \"Stop Sign\", \"S…\n$ Weather    <chr> \"Clear\", \"Clear\", \"Clear\", \"Cloudy\", \"Clear\", \"Clear\", \"Cle…\n$ Workzone_I <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Location   <chr> \"36.002743, -78.8785\", \"35.612984, -77.39265\", \"35.595676, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "href": "content/lectures/02-dplyr.html#a-grammar-of-data-manipulation",
    "title": "02-dplyr",
    "section": "A Grammar of Data Manipulation",
    "text": "A Grammar of Data Manipulation\ndplyr is based on the concepts of functions as verbs that manipulate data frames.\nSingle data frame functions / verbs:\n\nfilter: pick rows matching criteria\nslice: pick rows using index(es)\nselect: pick columns by name\npull: grab a column as a vector\nrename: rename specific columns\narrange: reorder rows\nmutate: add new variables\ntransmute: create new data frame with variables\ndistinct: filter for unique rows\nsample_n / sample_frac: randomly sample rows\nsummarize: reduce variables to values\n… (many more)"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "href": "content/lectures/02-dplyr.html#dplyr-rules-for-functions",
    "title": "02-dplyr",
    "section": "dplyr rules for functions",
    "text": "dplyr rules for functions\n\nFirst argument is always a data frame\nSubsequent arguments say what to do with that data frame\nAlways return a data frame\nDo not modify in place\nPerformance via lazy evaluation"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "href": "content/lectures/02-dplyr.html#filter-rows-with-filter",
    "title": "02-dplyr",
    "section": "Filter rows with filter",
    "text": "Filter rows with filter\n\nSelect a subset of rows in a data frame.\nEasily filter for many conditions at once."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter",
    "href": "content/lectures/02-dplyr.html#filter",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County\n\nbike |>\n  filter(County == \"Durham\")\n\n# A tibble: 253 × 54\n     FID OBJEC…¹ Ambul…² BikeA…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl>   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1    18      19 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black  \n 2    53      54 Yes     50-59        52 No      With T… A: Dis… Travel… White  \n 3    56      57 Yes     16-19        18 No      <NA>    C: Pos… Travel… Black  \n 4   209     210 No      16-19        16 No      Facing… C: Pos… <NA>    Black  \n 5   228     229 Yes     40-49        40 No      With T… B: Evi… Bike L… Black  \n 6   620     621 Yes     50-59        55 No      With T… B: Evi… Travel… White  \n 7   667     668 Yes     60-69        61 No      Not Ap… B: Evi… Sidewa… Black  \n 8   458     459 Yes     60-69        62 No      With T… B: Evi… Travel… White  \n 9   576     577 No      40-49        49 No      With T… C: Pos… Travel… Black  \n10   618     619 No      20-24        23 No      With T… C: Pos… Travel… Asian  \n# … with 243 more rows, 44 more variables: Bike_Sex <chr>, City <chr>,\n#   County <chr>, CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>,\n#   Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>,\n#   Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>,\n#   Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>,\n#   Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>,\n#   Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#filter-1",
    "href": "content/lectures/02-dplyr.html#filter-1",
    "title": "02-dplyr",
    "section": "filter",
    "text": "filter\nfor crashes in Durham County where biker was < 10 yrs old\n\nbike |>\n  filter(County == \"Durham\", Bike_Age < 10)\n\n# A tibble: 20 × 54\n     FID OBJEC…¹ Ambul…² BikeA…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl>   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1    18      19 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black  \n 2    47      48 No      10-Jun        9 No      Not Ap… O: No … Non-Ro… Black  \n 3   124     125 Yes     10-Jun        8 No      With T… C: Pos… Travel… Black  \n 4   531     532 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n 5   704     705 Yes     10-Jun        9 No      Not Ap… C: Pos… Non-Ro… Black  \n 6    42      43 No      10-Jun        8 No      With T… O: No … Travel… Black  \n 7   392     393 Yes     0-5           2 No      Not Ap… B: Evi… Drivew… Black  \n 8   941     942 No      10-Jun        9 No      With T… C: Pos… Travel… Black  \n 9   436     437 Yes     10-Jun        6 No      Not Ap… O: No … Drivew… Black  \n10   160     161 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n11   273     274 Yes     10-Jun        7 No      Facing… C: Pos… Travel… Black  \n12    78      79 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n13   422     423 No      10-Jun        9 No      Not Ap… O: No … Drivew… Black  \n14   570     571 No      <NA>          0 Missing Not Ap… Injury  Non-Ro… /Missi…\n15   683     684 Yes     10-Jun        8 No      Not Ap… C: Pos… Non-Ro… Black  \n16    62      63 Yes     10-Jun        7 No      With T… C: Pos… Travel… Black  \n17   248     249 No      0-5           4 No      Not Ap… O: No … Drivew… Hispan…\n18   306     307 Yes     10-Jun        8 No      With T… C: Pos… <NA>    Black  \n19   231     232 Yes     10-Jun        8 No      With T… C: Pos… Travel… Black  \n20   361     362 Yes     10-Jun        9 No      With T… B: Evi… Travel… Hispan…\n# … with 44 more variables: Bike_Sex <chr>, City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "href": "content/lectures/02-dplyr.html#aside-real-data-is-messy",
    "title": "02-dplyr",
    "section": "Aside: real data is messy!",
    "text": "Aside: real data is messy!\n   What in the world does a BikeAge_gr of 10-Jun or 15-Nov mean?\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 10-Jun             421\n 3 15-Nov             747\n 4 16-19              605\n 5 20-24              680\n 6 25-29              430\n 7 30-39              658\n 8 40-49              920\n 9 50-59              739\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "href": "content/lectures/02-dplyr.html#careful-data-scientists-clean-up-their-data-first",
    "title": "02-dplyr",
    "section": "Careful data scientists clean up their data first!",
    "text": "Careful data scientists clean up their data first!\n\nWe’re going to need to do some text parsing to clean up these data\n\n10-Jun should be 6-10\n15-Nov should be 11-15"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "href": "content/lectures/02-dplyr.html#correct-and-overwrite-mutate",
    "title": "02-dplyr",
    "section": "Correct and overwrite mutate",
    "text": "Correct and overwrite mutate\n\nRemember we want to do the following in the BikeAge_Gr variable\n\n10-Jun should be 6-10\n15-Nov should be 11-15\n\n\n\nbike <- bike |>\n  mutate(\n    BikeAge_Gr = case_when(\n      BikeAge_Gr == \"10-Jun\" ~ \"6-10\",\n      BikeAge_Gr == \"15-Nov\" ~ \"11-15\",\n      TRUE                   ~ BikeAge_Gr     # everything else\n    )\n  )\n\n\nNote that we’re overwriting existing data and columns, so be careful!\n\nBut remember, it’s easy to revert if you make a mistake since we didn’t touch the raw data, we can always reload it and start over"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr count\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "href": "content/lectures/02-dplyr.html#mutate-to-add-new-variables",
    "title": "02-dplyr",
    "section": "mutate to add new variables",
    "text": "mutate to add new variables\n   How is the new alcohol variable determined?\n\nbike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "href": "content/lectures/02-dplyr.html#save-when-you-mutate",
    "title": "02-dplyr",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame.\n\nbike <- bike |>\n  mutate(alcohol = case_when(\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"No\"      ~ \"No\",\n    Bike_Alc_D == \"Yes\" | Drvr_Alc_D == \"Yes\"    ~ \"Yes\",\n    Bike_Alc_D == \"Missing\" & Drvr_Alc_D == \"No\" ~ \"Missing\",\n    Bike_Alc_D == \"No\" & Drvr_Alc_D == \"Missing\" ~ \"Missing\"\n  ))"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "href": "content/lectures/02-dplyr.html#transmute-to-create-a-new-dataset",
    "title": "02-dplyr",
    "section": "transmute to create a new dataset",
    "text": "transmute to create a new dataset\nYou’ll use this much less often than mutate but when you need it, you need it.\n\nbike |> \n  transmute(ID = paste(FID, OBJECTID, sep = \"-\"))\n\n# A tibble: 5,716 × 1\n   ID   \n   <chr>\n 1 18-19\n 2 29-30\n 3 33-34\n 4 35-36\n 5 49-50\n 6 53-54\n 7 56-57\n 8 60-61\n 9 63-64\n10 66-67\n# … with 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "href": "content/lectures/02-dplyr.html#mutate-vs.-transmute",
    "title": "02-dplyr",
    "section": "mutate vs. transmute",
    "text": "mutate vs. transmute\n\nmutate adds new and keeps original\ntransmute adds new; drops existing"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn",
    "href": "content/lectures/02-dplyr.html#your-turn",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nHow many accidents in our dataset required an ambulance ride (AmbulanceR) and had the Crash_Type “Bicyclist Lost Control - Mechanical Problems”?\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nFirst five\n\nbike |>\n  slice(1:5)\n\n# A tibble: 5 × 54\n    FID OBJECTID Ambul…¹ BikeA…² Bike_…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸\n  <dbl>    <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n1    18       19 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black  \n2    29       30 Yes     50-59        51 No      With T… C: Pos… Travel… Black  \n3    33       34 No      <NA>         10 No      With T… Injury  Travel… Black  \n4    35       36 Yes     16-19        17 No      <NA>    B: Evi… Travel… White  \n5    49       50 No      <NA>          6 No      Facing… O: No … Travel… Black  \n# … with 44 more variables: Bike_Sex <chr>, City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "href": "content/lectures/02-dplyr.html#slice-for-certain-row-numbers-1",
    "title": "02-dplyr",
    "section": "slice for certain row numbers",
    "text": "slice for certain row numbers\nLast five\n\nlast_row <- nrow(bike)\nbike |>\n  slice((last_row - 4):last_row)\n\n# A tibble: 5 × 54\n    FID OBJECTID Ambul…¹ BikeA…² Bike_…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸\n  <dbl>    <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n1   460      461 Yes     6-10          7 No      Not Ap… C: Pos… Drivew… Black  \n2   474      475 Yes     50-59        50 No      With T… B: Evi… Travel… White  \n3   479      480 Yes     16-19        16 No      Not Ap… C: Pos… Sidewa… White  \n4   487      488 No      40-49        47 Yes     With T… C: Pos… Travel… White  \n5   488      489 Yes     30-39        35 No      Facing… C: Pos… Travel… Black  \n# … with 44 more variables: Bike_Sex <chr>, City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "href": "content/lectures/02-dplyr.html#select-to-keep-only-the-variables-you-mention",
    "title": "02-dplyr",
    "section": "select to keep only the variables you mention",
    "text": "select to keep only the variables you mention\n\nbike |>\n  select(Crash_Loc, Hit_Run) |>\n  table()\n\n                      Hit_Run\nCrash_Loc                No  Yes\n  Intersection         2223  275\n  Intersection-Related  252   42\n  Location                3    7\n  Non-Intersection     2213  462\n  Non-Roadway           205   30"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "href": "content/lectures/02-dplyr.html#or-select-to-exclude-variables",
    "title": "02-dplyr",
    "section": "or select to exclude variables",
    "text": "or select to exclude variables\n\nbike |>\n  select(-OBJECTID)\n\n# A tibble: 5,716 × 53\n     FID Ambul…¹ BikeA…² Bike_…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n 1    18 No      <NA>          6 No      Not Ap… C: Pos… Drivew… Black   Female \n 2    29 Yes     50-59        51 No      With T… C: Pos… Travel… Black   Male   \n 3    33 No      <NA>         10 No      With T… Injury  Travel… Black   Male   \n 4    35 Yes     16-19        17 No      <NA>    B: Evi… Travel… White   Male   \n 5    49 No      <NA>          6 No      Facing… O: No … Travel… Black   Male   \n 6    53 Yes     50-59        52 No      With T… A: Dis… Travel… White   Male   \n 7    56 Yes     16-19        18 No      <NA>    C: Pos… Travel… Black   Female \n 8    60 No      40-49        40 No      Facing… B: Evi… Sidewa… Hispan… Male   \n 9    63 Yes     6-10          6 No      Facing… B: Evi… Travel… White   Male   \n10    66 Yes     6-10          7 No      <NA>    B: Evi… Non-Ro… Black   Female \n# … with 5,706 more rows, 43 more variables: City <chr>, County <chr>,\n#   CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>, Crash_Grp <chr>,\n#   Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>, Crash_Time <dttm>,\n#   Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>, Crsh_Sevri <chr>,\n#   Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>, Drvr_Alc_D <chr>,\n#   Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>, Drvr_Sex <chr>,\n#   Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, Light_Cond <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "href": "content/lectures/02-dplyr.html#or-select-a-range-of-variables",
    "title": "02-dplyr",
    "section": "or select a range of variables",
    "text": "or select a range of variables\n\nbike |>\n  select(OBJECTID:Bike_Injur)\n\n# A tibble: 5,716 × 7\n   OBJECTID AmbulanceR BikeAge_Gr Bike_Age Bike_Alc_D Bike_Dir       Bike_Injur \n      <dbl> <chr>      <chr>         <dbl> <chr>      <chr>          <chr>      \n 1       19 No         <NA>              6 No         Not Applicable C: Possibl…\n 2       30 Yes        50-59            51 No         With Traffic   C: Possibl…\n 3       34 No         <NA>             10 No         With Traffic   Injury     \n 4       36 Yes        16-19            17 No         <NA>           B: Evident…\n 5       50 No         <NA>              6 No         Facing Traffic O: No Inju…\n 6       54 Yes        50-59            52 No         With Traffic   A: Disabli…\n 7       57 Yes        16-19            18 No         <NA>           C: Possibl…\n 8       61 No         40-49            40 No         Facing Traffic B: Evident…\n 9       64 Yes        6-10              6 No         Facing Traffic B: Evident…\n10       67 Yes        6-10              7 No         <NA>           B: Evident…\n# … with 5,706 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "href": "content/lectures/02-dplyr.html#pull-to-extract-a-column-as-a-vector",
    "title": "02-dplyr",
    "section": "pull to extract a column as a vector",
    "text": "pull to extract a column as a vector\n\nbike |>\n  slice(1:6) |>\n  pull(Location)\n\n[1] \"36.002743, -78.8785\"  \"35.612984, -77.39265\" \"35.595676, -77.59074\"\n[4] \"35.076767, -80.7728\"  \"35.19999, -80.75713\"  \"35.966644, -78.96749\"\n\n\n\nbike |>\n  slice(1:6) |>\n  select(Location)\n\n# A tibble: 6 × 1\n  Location            \n  <chr>               \n1 36.002743, -78.8785 \n2 35.612984, -77.39265\n3 35.595676, -77.59074\n4 35.076767, -80.7728 \n5 35.19999, -80.75713 \n6 35.966644, -78.96749"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "href": "content/lectures/02-dplyr.html#the-two-pulls-in-your-lives",
    "title": "02-dplyr",
    "section": "The two pulls in your lives",
    "text": "The two pulls in your lives\n\n\n\n\n\n\n\n\nDon’t get pull happy when wrangling data! Only extract out variables if you truly need to, otherwise keep in data frame.\nBut always ⬇️ Pull before starting your work when collaborating on GitHub."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\nUseful for correcting typos, and renaming to make variable names shorter and/or more informative\n\nOriginal names:\n\n\nnames(bike)\n\n [1] \"FID\"        \"OBJECTID\"   \"AmbulanceR\" \"BikeAge_Gr\" \"Bike_Age\"  \n [6] \"Bike_Alc_D\" \"Bike_Dir\"   \"Bike_Injur\" \"Bike_Pos\"   \"Bike_Race\" \n[11] \"Bike_Sex\"   \"City\"       \"County\"     \"CrashAlcoh\" \"CrashDay\"  \n[16] \"Crash_Date\" \"Crash_Grp\"  \"Crash_Hour\" \"Crash_Loc\"  \"Crash_Mont\"\n[21] \"Crash_Time\" \"Crash_Type\" \"Crash_Ty_1\" \"Crash_Year\" \"Crsh_Sevri\"\n[26] \"Developmen\" \"DrvrAge_Gr\" \"Drvr_Age\"   \"Drvr_Alc_D\" \"Drvr_EstSp\"\n[31] \"Drvr_Injur\" \"Drvr_Race\"  \"Drvr_Sex\"   \"Drvr_VehTy\" \"ExcsSpdInd\"\n[36] \"Hit_Run\"    \"Light_Cond\" \"Locality\"   \"Num_Lanes\"  \"Num_Units\" \n[41] \"Rd_Charact\" \"Rd_Class\"   \"Rd_Conditi\" \"Rd_Config\"  \"Rd_Defects\"\n[46] \"Rd_Feature\" \"Rd_Surface\" \"Region\"     \"Rural_Urba\" \"Speed_Limi\"\n[51] \"Traff_Cntr\" \"Weather\"    \"Workzone_I\" \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "href": "content/lectures/02-dplyr.html#rename-specific-columns-1",
    "title": "02-dplyr",
    "section": "rename specific columns",
    "text": "rename specific columns\n\nRename Speed_Limi to Speed_Limit:\n\n\nbike <- bike |>\n  rename(Speed_Limit = Speed_Limi)"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "href": "content/lectures/02-dplyr.html#check-before-you-move-on-1",
    "title": "02-dplyr",
    "section": "Check before you move on",
    "text": "Check before you move on\nAlways check your changes and confirm code did what you wanted it to do\n\nnames(bike)\n\n [1] \"FID\"         \"OBJECTID\"    \"AmbulanceR\"  \"BikeAge_Gr\"  \"Bike_Age\"   \n [6] \"Bike_Alc_D\"  \"Bike_Dir\"    \"Bike_Injur\"  \"Bike_Pos\"    \"Bike_Race\"  \n[11] \"Bike_Sex\"    \"City\"        \"County\"      \"CrashAlcoh\"  \"CrashDay\"   \n[16] \"Crash_Date\"  \"Crash_Grp\"   \"Crash_Hour\"  \"Crash_Loc\"   \"Crash_Mont\" \n[21] \"Crash_Time\"  \"Crash_Type\"  \"Crash_Ty_1\"  \"Crash_Year\"  \"Crsh_Sevri\" \n[26] \"Developmen\"  \"DrvrAge_Gr\"  \"Drvr_Age\"    \"Drvr_Alc_D\"  \"Drvr_EstSp\" \n[31] \"Drvr_Injur\"  \"Drvr_Race\"   \"Drvr_Sex\"    \"Drvr_VehTy\"  \"ExcsSpdInd\" \n[36] \"Hit_Run\"     \"Light_Cond\"  \"Locality\"    \"Num_Lanes\"   \"Num_Units\"  \n[41] \"Rd_Charact\"  \"Rd_Class\"    \"Rd_Conditi\"  \"Rd_Config\"   \"Rd_Defects\" \n[46] \"Rd_Feature\"  \"Rd_Surface\"  \"Region\"      \"Rural_Urba\"  \"Speed_Limit\"\n[51] \"Traff_Cntr\"  \"Weather\"     \"Workzone_I\"  \"Location\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#your-turn-1",
    "href": "content/lectures/02-dplyr.html#your-turn-1",
    "title": "02-dplyr",
    "section": "Your Turn",
    "text": "Your Turn\nYour boss in Cumberland County gets overwhelmed by data easily, but he wants some data from you. He wants all bike accidents from his County, but he only wants to know the road’s speed limit, the age of the biker, and to know if alcohol was involved. If you have time, mine as well make the column names very clear to your boss while you’re at it…\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "href": "content/lectures/02-dplyr.html#summarize-to-reduce-variables-to-values",
    "title": "02-dplyr",
    "section": "summarize to reduce variables to values",
    "text": "summarize to reduce variables to values\nThe values are summarized in a data frame\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n())\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 0-5                 60\n 2 11-15              747\n 3 16-19              605\n 4 20-24              680\n 5 25-29              430\n 6 30-39              658\n 7 40-49              920\n 8 50-59              739\n 9 6-10               421\n10 60-69              274\n11 70                  12\n12 70+                 58\n13 <NA>               112"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "href": "content/lectures/02-dplyr.html#and-arrange-to-order-rows",
    "title": "02-dplyr",
    "section": "and arrange to order rows",
    "text": "and arrange to order rows\n\nbike |>\n  group_by(BikeAge_Gr) |>\n  summarize(crash_count = n()) |>\n  arrange(desc(crash_count))\n\n# A tibble: 13 × 2\n   BikeAge_Gr crash_count\n   <chr>            <int>\n 1 40-49              920\n 2 11-15              747\n 3 50-59              739\n 4 20-24              680\n 5 30-39              658\n 6 16-19              605\n 7 25-29              430\n 8 6-10               421\n 9 60-69              274\n10 <NA>               112\n11 0-5                 60\n12 70+                 58\n13 70                  12"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "href": "content/lectures/02-dplyr.html#count-to-group-by-then-count",
    "title": "02-dplyr",
    "section": "count to group by then count",
    "text": "count to group by then count\n\nbike |>\n  count(BikeAge_Gr)\n\n# A tibble: 13 × 2\n   BikeAge_Gr     n\n   <chr>      <int>\n 1 0-5           60\n 2 11-15        747\n 3 16-19        605\n 4 20-24        680\n 5 25-29        430\n 6 30-39        658\n 7 40-49        920\n 8 50-59        739\n 9 6-10         421\n10 60-69        274\n11 70            12\n12 70+           58\n13 <NA>         112\n\n\n   If you wanted to arrange these in ascending order what would you add to the pipe?"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "href": "content/lectures/02-dplyr.html#select-rows-with-sample_n-or-sample_frac",
    "title": "02-dplyr",
    "section": "Select rows with sample_n or sample_frac",
    "text": "Select rows with sample_n or sample_frac\n\nsample_n: randomly sample 5 observations\n\n\nbike_n5 <- bike |>\n  sample_n(5, replace = FALSE)\n\ndim(bike_n5)\n\n[1]  5 54\n\n\n\nsample_frac: randomly sample 20% of observations\n\n\nbike_perc20 <- bike |>\n  sample_frac(0.2, replace = FALSE)\n\ndim(bike_perc20)\n\n[1] 1143   54"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "href": "content/lectures/02-dplyr.html#distinct-to-filter-for-unique-rows",
    "title": "02-dplyr",
    "section": "distinct to filter for unique rows",
    "text": "distinct to filter for unique rows\n\nbike |> \n  select(County, City) |> \n  distinct() |> \n  arrange(County, City)\n\n# A tibble: 360 × 2\n   County    City              \n   <chr>     <chr>             \n 1 Alamance  Alamance          \n 2 Alamance  Burlington        \n 3 Alamance  Elon College      \n 4 Alamance  Gibsonville       \n 5 Alamance  Graham            \n 6 Alamance  Green Level       \n 7 Alamance  Mebane            \n 8 Alamance  None - Rural Crash\n 9 Alexander None - Rural Crash\n10 Alleghany None - Rural Crash\n# … with 350 more rows"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "href": "content/lectures/02-dplyr.html#distinct-has-a-.keep_all-parameter",
    "title": "02-dplyr",
    "section": "distinct has a .keep_all parameter",
    "text": "distinct has a .keep_all parameter\n\nbike |> \n  distinct(County, City, .keep_all = TRUE) |> \n  arrange(County, City)\n\n# A tibble: 360 × 54\n     FID OBJEC…¹ Ambul…² BikeA…³ Bike_…⁴ Bike_…⁵ Bike_…⁶ Bike_…⁷ Bike_…⁸ Bike_…⁹\n   <dbl>   <dbl> <chr>   <chr>     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1   524     525 Yes     11-15        12 No      <NA>    B: Evi… <NA>    Black  \n 2    84      85 Yes     20-24        20 No      With T… B: Evi… Travel… White  \n 3   571     572 Yes     16-19        16 No      Not Ap… B: Evi… Non-Ro… White  \n 4   509     510 Yes     40-49        43 Yes     With T… K: Kil… Travel… Black  \n 5   855     856 Yes     30-39        30 No      With T… A: Dis… Travel… Black  \n 6     5       6 Yes     40-49        44 Yes     With T… C: Pos… Travel… Black  \n 7   163     164 Yes     30-39        35 No      Not Ap… C: Pos… <NA>    White  \n 8    96      97 Yes     30-39        36 No      With T… C: Pos… Travel… White  \n 9    46      47 Yes     50-59        53 No      With T… B: Evi… Travel… White  \n10   485     486 Yes     60-69        62 No      With T… C: Pos… Travel… White  \n# … with 350 more rows, 44 more variables: Bike_Sex <chr>, City <chr>,\n#   County <chr>, CrashAlcoh <chr>, CrashDay <chr>, Crash_Date <date>,\n#   Crash_Grp <chr>, Crash_Hour <dbl>, Crash_Loc <chr>, Crash_Mont <chr>,\n#   Crash_Time <dttm>, Crash_Type <chr>, Crash_Ty_1 <dbl>, Crash_Year <dbl>,\n#   Crsh_Sevri <chr>, Developmen <chr>, DrvrAge_Gr <chr>, Drvr_Age <dbl>,\n#   Drvr_Alc_D <chr>, Drvr_EstSp <chr>, Drvr_Injur <chr>, Drvr_Race <chr>,\n#   Drvr_Sex <chr>, Drvr_VehTy <chr>, ExcsSpdInd <chr>, Hit_Run <chr>, …"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#factors-1",
    "href": "content/lectures/02-dplyr.html#factors-1",
    "title": "02-dplyr",
    "section": "Factors",
    "text": "Factors\nFactor objects are how R stores data for categorical variables (fixed numbers of discrete values).\n\n(x = factor(c(\"BS\", \"MS\", \"PhD\", \"MS\")))\n\n[1] BS  MS  PhD MS \nLevels: BS MS PhD\n\n\n\nglimpse(x)\n\n Factor w/ 3 levels \"BS\",\"MS\",\"PhD\": 1 2 3 2\n\n\n\ntypeof(x)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "href": "content/lectures/02-dplyr.html#returning-to-cat-lovers",
    "title": "02-dplyr",
    "section": "Returning to: Cat lovers",
    "text": "Returning to: Cat lovers\nReading in the cat-lovers data…\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "href": "content/lectures/02-dplyr.html#read-data-in-as-character-strings",
    "title": "02-dplyr",
    "section": "Read data in as character strings",
    "text": "Read data in as character strings\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "href": "content/lectures/02-dplyr.html#but-coerce-when-plotting",
    "title": "02-dplyr",
    "section": "But coerce when plotting",
    "text": "But coerce when plotting\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "href": "content/lectures/02-dplyr.html#use-forcats-to-manipulate-factors",
    "title": "02-dplyr",
    "section": "Use forcats to manipulate factors",
    "text": "Use forcats to manipulate factors\n\ncat_lovers <- cat_lovers |>\n  mutate(handedness = fct_relevel(handedness, \n                                  \"right\", \"left\", \"ambidextrous\"))\n\n\nggplot(cat_lovers, mapping = aes(x = handedness)) +\n  geom_bar()"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#forcats-functionality",
    "href": "content/lectures/02-dplyr.html#forcats-functionality",
    "title": "02-dplyr",
    "section": "forcats functionality ",
    "text": "forcats functionality \n\nR uses factors to handle categorical variables, variables that have a fixed and known set of possible values. Historically, factors were much easier to work with than character vectors, so many base R functions automatically convert character vectors to factors.\nfactors are still useful when you have true categorical data, and when you want to override the ordering of character vectors to improve display. The goal of the forcats package is to provide a suite of useful tools that solve common problems with factors.\n\n\n\nSource: forcats.tidyverse.org"
  },
  {
    "objectID": "content/lectures/02-dplyr.html#suggested-reading",
    "href": "content/lectures/02-dplyr.html#suggested-reading",
    "title": "02-dplyr",
    "section": "Suggested Reading",
    "text": "Suggested Reading\nR4DS:\n\nChapter 5: Data Transformation\nChapter 15: Factors"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#ad-calpirg",
    "href": "content/lectures/01-intro-to-r-slides.html#ad-calpirg",
    "title": "01-intro-to-r",
    "section": "[ad] CALPIRG",
    "text": "[ad] CALPIRG\n\nAPPLY NOW: Volunteer or Intern to fight plastic pollution or turn out the vote on campus!\nCALPIRG Students is a student organization on campus that works to protect the environment, address food insecurity, and promote civic engagement. We helped nearly 10,000 students register to vote in California and got the UCs to release new policy to phase out single-use plastics to protect our oceans! Get involved to protect our oceans!\n\nThis Winter, we’re working to protect our oceans. They give us food, provide most of the oxygen we breathe, and are home to awesome biodiversity like whales and sea turtles, but our oceans are at risk from things like overfishing, oil drilling, and pollution. That’s why we’re calling on Governor Newsom to increase protections for marine areas in California!\nWe’re also working on campaigns addressing hunger & homelessness, fighting plastic pollution, and promoting voter participation.\nAs a volunteer or intern with CALPIRG you can:\n\nBuild grassroots support\nWork with the media and help organize press conferences with experts and elected officials\nLobby elected officials\nPlan big events like rallies or a benefit concert\nLearn key nonprofit management and fundraising skills\n\nLearn skills, build your resume, and work with issues that matter. Apply today."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#qa",
    "href": "content/lectures/01-intro-to-r-slides.html#qa",
    "title": "01-intro-to-r",
    "section": "Q&A",
    "text": "Q&A\n\nQ: Curious about why we’re using RStudio on DataHub instead of running it locally. Are there tasks that require the hardware used in DataHub or can most of the tasks be also done on a laptop?\nA: You could run this whole class locally and are free to do so! The one place where you would run into issues is that I assume packages (which have been installed on datahub) are installed. So, you would just have to install those packages before proceeding.\n\n\nQ: How exactly do students make appointments for the Wednesday office hour? Is it simply through email or is there a separate form?\nA: I use Calendly for this. The link to sign up is on Canvas!\n\n\nQ: I am still not sure how to clone GitHub code into datahub.\nA: This will be covered in lab this week!\n\n\nQ: How will we submit the lab. What will the lab being release and when will it due?\nA: Labs, homework, and exams will all be “submitted” by pushing to GitHub (covered in lab on Friday). Once it’s on GitHub, it’s submitted! This weeks lab will be released today. Typically, each week’s lab will be released on Monday of the week it’s due.\n\n\nQ: Do we need to bring our own computer to lab?\nA: Good question! I should have covered this. Lab is in the same room as lecture, so you will need your own computer. If you don’t have one, please reach out to me or campus resources (listed in the syllabus) and I’ll see what I can do!\n\n\nQ: After class today, I am a little confused on how we will be writing code for assignments. Will we be doing everything through RStudio once we have opened DataHub, or will there be a cell format similar to Jupyter Notebooks when using Python? In other words, can we expect all of our assignments to be conducted in RStudio as we did today?\nA: The code you submit will typically be in R Markdown documents (discussing more today!) These are similar to Jupyter Notebooks. And yes, everything can be done in RStudio as demo-ed on Tuesday.\n\n\nQ: I know you mentioned R has changed since you last used it. Would it be possible for you to share any resources or materials that you and other R practitioners find useful. I.E. Frameworks/Conventions/Blogs. A: Ah, so I meant to say that R has changed since I first used it many years ago. I have continued to use and learn R since then! I’ve never stopped using R. That said, I have/know of TONS of resources. Two big compilations I can recommend are: learnr4free and the Big Book of R. If there’s something specific you’re interested, feel free to let me know and I can point to more specific resources!\n\n\nQ: I’m anxious to learn Github - have put it off for most of my programming life. It seems so complicated! :(\nA: It is complicated, but that’s b/c it’s doing something REALLY hard. That said, the basics are not too complicated. So this course will force you to learn the basics, and then you’ll be on your way to learn the harder stuff!\n\n\nQ: Could you talk about the prevalence of R in industry (use cases / types of jobs / etc.)? My general understanding of it is that it’s kind of out the door when compared to Python.\nA: Yes! I’ll try to discuss this more throughout the course, but R is super popular among data scientists, particularly those who have a focus in statistics, biological data (bioinformatics), econometrics, data analysis, and/or data journalism. If you’re trying to do data engineering, you likely won’t see/use R, but if you’re analyzing and visualizing data in your job, you’re likely to encounter R. That said, Python is certainly a more popular language. That’s why I/we teach it in our intro programming course!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#course-announcements",
    "href": "content/lectures/01-intro-to-r-slides.html#course-announcements",
    "title": "01-intro-to-r",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 01 due tomorrow (Friday) 11:59 PM\nLecture Participation survey “due” after class (both Tu and today’s lectures available)\nStudent survey “due” Sunday (1/15) 11:59 PM\n\n\nUpdate: Lab will be podcast but restricted to UCSD students and will not be archived."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#agenda",
    "href": "content/lectures/01-intro-to-r-slides.html#agenda",
    "title": "01-intro-to-r",
    "section": "Agenda",
    "text": "Agenda\n\nVariables\nOperators\nData in R\nRMarkdown"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variables-assignment-1",
    "href": "content/lectures/01-intro-to-r-slides.html#variables-assignment-1",
    "title": "01-intro-to-r",
    "section": "Variables & Assignment",
    "text": "Variables & Assignment\nVariables are how we store information so that we can access it later.\n\nVariables are created and stored using the assignment operator <- 1\n\nfirst_variable <- 3\n\nThe above stores the value 3 in the variable first_variable\n\n\nThis means that if we ever want to reference the information stored in that variable later, we can “call” (mean, type in our code) the variable’s name:\n\nfirst_variable\n\n[1] 3\n\n\n\nOther programming languages use = for assignment. R also uses that for assignment, but it is more typical to see <- in R code, so we’ll stick with that."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-type",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-type",
    "title": "01-intro-to-r",
    "section": "Variable Type",
    "text": "Variable Type\n\nEvery variable you create in R will be of a specific type.\n\n\n\nThe type of the variable is determined dynamically on assignment.\n\n\n\n\nDetermining the type of a variable with class():\n\n\nclass(first_variable)\n\n[1] \"numeric\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#basic-variable-types",
    "href": "content/lectures/01-intro-to-r-slides.html#basic-variable-types",
    "title": "01-intro-to-r",
    "section": "Basic Variable Types",
    "text": "Basic Variable Types\n\n\n\n\n\n\n\n\nVariable Type\nExplanation\nExample\n\n\n\n\ncharacter\nstores a string\n\"cogs137\", \"hi!\"\n\n\nnumeric\nstores whole numbers and decimals\n9, 9.29\n\n\ninteger\nspecifies integer\n9L (the L specifies this is an integer)\n\n\nlogical\nBooleans\nTRUE, FALSE\n\n\nlist\nstore multiple elements\nlist(7, \"a\", TRUE)\n\n\n\n\n\nThere are many more. We’ll get to some but not all in this course."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#logical-character",
    "href": "content/lectures/01-intro-to-r-slides.html#logical-character",
    "title": "01-intro-to-r",
    "section": "logical & character",
    "text": "logical & character\nlogical - Boolean values TRUE and FALSE\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\ncharacter - character strings\n\nclass(\"hello\")\n\n[1] \"character\"\n\nclass('students') # equivalent...but we'll use double quotes!\n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#numeric-double-integer",
    "href": "content/lectures/01-intro-to-r-slides.html#numeric-double-integer",
    "title": "01-intro-to-r",
    "section": "numeric: double & integer",
    "text": "numeric: double & integer\ndouble - floating point numerical values (default numerical type)\n\nclass(1.335)\n\n[1] \"numeric\"\n\nclass(7)\n\n[1] \"numeric\"\n\n\n\ninteger - integer numerical values (indicated with an L)\n\nclass(7L)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#lists",
    "href": "content/lectures/01-intro-to-r-slides.html#lists",
    "title": "01-intro-to-r",
    "section": "lists",
    "text": "lists\nSo far, every variable has been an atomic vector, meaning it only stores a single piece of information.\n\nLists are 1d objects that can contain any combination of R objects\n\n\n\nmylist <- list(\"A\", 7L, TRUE, 18.4)\nmylist\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 18.4\n\n\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#your-turn",
    "href": "content/lectures/01-intro-to-r-slides.html#your-turn",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nDefine variables of each of the following types: charachter, numeric, integer, logical, list\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#functions",
    "href": "content/lectures/01-intro-to-r-slides.html#functions",
    "title": "01-intro-to-r",
    "section": "Functions",
    "text": "Functions\n\nclass() (and View() & median()) were our first functions…but we’ll show a few more.\n\n\n\nFunctions are (most often) verbs, followed by what they will be applied to in parentheses.\n\n\n\nFunctions are:\n\navailable from base R\navailable from packages you import\ndefined by you\n\n\n\nWe’ll start by getting comfortable with available functions, but in a few days, you’ll learn how to write your own!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#helpful-functions",
    "href": "content/lectures/01-intro-to-r-slides.html#helpful-functions",
    "title": "01-intro-to-r",
    "section": "Helpful Functions",
    "text": "Helpful Functions\n\n\n\nclass() - determine high-level variable type\n\n\nclass(mylist)\n\n[1] \"list\"\n\n\n\nlength()- determine how long an object is\n\n\n# contains 4 elements\nlength(mylist)\n\n[1] 4\n\n\n\n\nstr() - display the structure of an R object\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#coercion",
    "href": "content/lectures/01-intro-to-r-slides.html#coercion",
    "title": "01-intro-to-r",
    "section": "Coercion",
    "text": "Coercion\nR is a dynamically typed language – it will happily convert between the various types without complaint.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"\n\nc(FALSE, 3L)\n\n[1] 0 3\n\nc(1.2, 3L)\n\n[1] 1.2 3.0"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#missing-values",
    "href": "content/lectures/01-intro-to-r-slides.html#missing-values",
    "title": "01-intro-to-r",
    "section": "Missing Values",
    "text": "Missing Values\nR uses NA to represent missing values in its data structures.\n\nclass(NA)\n\n[1] \"logical\"\n\n\n\nOther Special Values\nNaN | Not a number\nInf | Positive infinity\n-Inf | Negative infinity"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#activity",
    "href": "content/lectures/01-intro-to-r-slides.html#activity",
    "title": "01-intro-to-r",
    "section": "Activity",
    "text": "Activity\nWhat is the type of the following vectors? Chat about why they have that type.\n\nc(1, NA+1L, \"C\")\nc(1L / 0, NA)\nc(1:3, 5)\nc(3L, NaN+1L)\nc(NA, TRUE)\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#operators-1",
    "href": "content/lectures/01-intro-to-r-slides.html#operators-1",
    "title": "01-intro-to-r",
    "section": "Operators",
    "text": "Operators\nAt its simplest, R is a calculator. To carry out mathematical operations, R uses operators."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators",
    "href": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\n\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\nx %% y\nmodulus (x mod y) 9%%2 is 1\n\n\nx %/% y\ninteger division 9%/%2 is 4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators-examples",
    "href": "content/lectures/01-intro-to-r-slides.html#arithmetic-operators-examples",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators: Examples",
    "text": "Arithmetic Operators: Examples\n\n7 + 6  \n\n[1] 13\n\n2 - 3\n\n[1] -1\n\n4 * 2\n\n[1] 8\n\n9 / 2\n\n[1] 4.5"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#reminder",
    "href": "content/lectures/01-intro-to-r-slides.html#reminder",
    "title": "01-intro-to-r",
    "section": "Reminder",
    "text": "Reminder\nOutput can be stored to a variable\n\nmy_addition <- 7 + 6\n\n\n\nmy_addition\n\n[1] 13"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#comparison-operators",
    "href": "content/lectures/01-intro-to-r-slides.html#comparison-operators",
    "title": "01-intro-to-r",
    "section": "Comparison Operators",
    "text": "Comparison Operators\nThese operators return a Boolean.\n\n\n\nOperator\nDescription\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#comparison-operators-examples",
    "href": "content/lectures/01-intro-to-r-slides.html#comparison-operators-examples",
    "title": "01-intro-to-r",
    "section": "Comparison Operators: Examples",
    "text": "Comparison Operators: Examples\n\n4 < 12\n\n[1] TRUE\n\n4 >= 3\n\n[1] TRUE\n\n6 == 6\n\n[1] TRUE\n\n7 != 6\n\n[1] TRUE"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#your-turn-1",
    "href": "content/lectures/01-intro-to-r-slides.html#your-turn-1",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nUse arithmetic and comparison operators to store the value 30 in the variable var_30 and TRUE in the variable true_var.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#packages",
    "href": "content/lectures/01-intro-to-r-slides.html#packages",
    "title": "01-intro-to-r",
    "section": "Packages",
    "text": "Packages\n\nPackages are installed with the install.packages function and loaded with the library function, once per session:\n\n\ninstall.packages(\"package_name\")\nlibrary(package_name)\n\n\nIn this course, most packages we’ll use have been installed for you already on datahub, so you will only have to load the package in (using library)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-sets-in-r",
    "href": "content/lectures/01-intro-to-r-slides.html#data-sets-in-r",
    "title": "01-intro-to-r",
    "section": "Data “sets” in R",
    "text": "Data “sets” in R\n\n“set” is in quotation marks because it is not a formal data class\nA tidy data “set” can be one of the following types:\n\ntibble\ndata.frame\n\nWe’ll often work with tibbles:\n\nreadr package (e.g. read_csv function) loads data as a tibble by default\ntibbles are part of the tidyverse, so they work well with other packages we are using\nthey make minimal assumptions about your data, so are less likely to cause hard to track bugs in your code"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-frames",
    "href": "content/lectures/01-intro-to-r-slides.html#data-frames",
    "title": "01-intro-to-r",
    "section": "Data frames",
    "text": "Data frames\n\nA data frame is the most commonly used data structure in R, they are list of equal length vectors (usually atomic, but can be generic). Each vector is treated as a column and elements of the vectors as rows.\nA tibble is a type of data frame that … makes your life (i.e. data analysis) easier.\nMost often a data frame will be constructed by reading in from a file, but we can create them from scratch.\n\n\ndf <- tibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\nclass(df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(df)\n\nRows: 3\nColumns: 2\n$ x <int> 1, 2, 3\n$ y <chr> \"a\", \"b\", \"c\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#data-frames-cont.",
    "href": "content/lectures/01-intro-to-r-slides.html#data-frames-cont.",
    "title": "01-intro-to-r",
    "section": "Data frames (cont.)",
    "text": "Data frames (cont.)\n\nattributes(df)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n$names\n[1] \"x\" \"y\"\n\n\n\nColumns (variables) in data frames are accessed with $:\n\ndataframe$var_name\n\n\n\n\nclass(df$x)  # access variable type for column\n\n[1] \"integer\"\n\nclass(df$y)  \n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-types",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-types",
    "title": "01-intro-to-r",
    "section": "Variable Types",
    "text": "Variable Types\nData stored in columns can include different kinds of information…which would require a different type (class) of variable to be used in R.\n\n\n\n\nR Data Types:\n\nContinuous: numeric, integer\nDiscrete: factors (we haven’t talked about these yet, but will today!)\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#variable-types-cont.",
    "href": "content/lectures/01-intro-to-r-slides.html#variable-types-cont.",
    "title": "01-intro-to-r",
    "section": "Variable Types (cont.)",
    "text": "Variable Types (cont.)\nSometimes data are non-numeric and store words. Even when that is the case, the data can be conveying different information.\n\n\n\n\nR Data Types:\n\nNominal: character\nOrdinal: factors\nBinary: logical OR numeric OR factors 😱\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#example-cat-lovers",
    "href": "content/lectures/01-intro-to-r-slides.html#example-cat-lovers",
    "title": "01-intro-to-r",
    "section": "Example: Cat lovers",
    "text": "Example: Cat lovers\nA survey asked respondents their name and number of cats. The instructions said to enter the number of cats as a numerical value.\n\n🚨 There is code ahead that we’re not going to discuss in detail today, but we will in coming lectures.\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#the-data",
    "href": "content/lectures/01-intro-to-r-slides.html#the-data",
    "title": "01-intro-to-r",
    "section": "The Data",
    "text": "The Data\n\ncat_lovers |>\n  datatable()"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#the-question",
    "href": "content/lectures/01-intro-to-r-slides.html#the-question",
    "title": "01-intro-to-r",
    "section": "The Question",
    "text": "The Question\nHow many respondents have a below average number of cats?\n\nGiving it a first shot…\n\ncat_lovers |>\n  summarise(mean = mean(number_of_cats))\n\n# A tibble: 1 × 1\n   mean\n  <dbl>\n1    NA\n\n\n\n\n💡 maybe there is missing data in the number_of_cats column!\nOh why will you still not work??!!\n\ncat_lovers |>\n  summarise(mean_cats = mean(number_of_cats, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1        NA\n\n\n\n\n💡What is the type of the number_of_cats variable?"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#take-a-breath-and-look-at-your-data",
    "href": "content/lectures/01-intro-to-r-slides.html#take-a-breath-and-look-at-your-data",
    "title": "01-intro-to-r",
    "section": "Take a breath and look at your data",
    "text": "Take a breath and look at your data\n\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#lets-take-another-look",
    "href": "content/lectures/01-intro-to-r-slides.html#lets-take-another-look",
    "title": "01-intro-to-r",
    "section": "Let’s take another look",
    "text": "Let’s take another look"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#sometimes-you-need-to-babysit-your-respondents",
    "href": "content/lectures/01-intro-to-r-slides.html#sometimes-you-need-to-babysit-your-respondents",
    "title": "01-intro-to-r",
    "section": "Sometimes you need to babysit your respondents",
    "text": "Sometimes you need to babysit your respondents\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n    name == \"Ginger Clark\" ~ 2,\n    name == \"Doug Bass\"    ~ 3,\n    TRUE                   ~ as.numeric(number_of_cats))) \n\n# A tibble: 60 × 3\n   name           number_of_cats handedness\n   <chr>                   <dbl> <chr>     \n 1 Bernice Warren              0 left      \n 2 Woodrow Stone               0 left      \n 3 Willie Bass                 1 left      \n 4 Tyrone Estrada              3 left      \n 5 Alex Daniels                3 left      \n 6 Jane Bates                  2 left      \n 7 Latoya Simpson              1 left      \n 8 Darin Woods                 1 left      \n 9 Agnes Cobb                  0 left      \n10 Tabitha Grant               0 left      \n# … with 50 more rows"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#always-respect-check-data-types",
    "href": "content/lectures/01-intro-to-r-slides.html#always-respect-check-data-types",
    "title": "01-intro-to-r",
    "section": "Always respect (& check!) data types",
    "text": "Always respect (& check!) data types\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats)) |>\n  summarise(mean_cats = mean(number_of_cats))\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1     0.817"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#now-that-we-know-what-were-doing",
    "href": "content/lectures/01-intro-to-r-slides.html#now-that-we-know-what-were-doing",
    "title": "01-intro-to-r",
    "section": "Now that we know what we’re doing…",
    "text": "Now that we know what we’re doing…\n\ncat_lovers <- cat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats))\n\n… store your data in a variable (here we’re overwriting the old cat_lovers tibble)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#moral-of-the-story",
    "href": "content/lectures/01-intro-to-r-slides.html#moral-of-the-story",
    "title": "01-intro-to-r",
    "section": "Moral of the story",
    "text": "Moral of the story\n\nIf your data does not behave how you expect it to, type coercion upon reading in the data might be the reason.\nGo in and investigate your data, apply the fix, save your data, live happily ever after."
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#r-markdown-tour",
    "href": "content/lectures/01-intro-to-r-slides.html#r-markdown-tour",
    "title": "01-intro-to-r",
    "section": "R Markdown: tour",
    "text": "R Markdown: tour\n\n[DEMO]\n\nBefore we move on…\n   What is the Bechdel test?\n\nThe Bechdel test asks whether a work of fiction features at least two women who talk to each other about something other than a man, and there must be two women named characters.\n\n\nConcepts introduced:\n\nKnitting documents\nR Markdown and (some) R syntax"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#giving-the-demo-a-go",
    "href": "content/lectures/01-intro-to-r-slides.html#giving-the-demo-a-go",
    "title": "01-intro-to-r",
    "section": "Giving the demo a go…",
    "text": "Giving the demo a go…\n\nNavigate to the demo URL (on Canvas)\nAccept the “assignment” (this is NOT graded)\nClone the repo\nEdit the document\nKnit the document\nPush your changes\n\nTry to play around with this after finishing your lab tomorrow!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r-slides.html#recap",
    "href": "content/lectures/01-intro-to-r-slides.html#recap",
    "title": "01-intro-to-r",
    "section": "Recap",
    "text": "Recap\n\nAlways best to think of data as part of a tibble\n\nThis plays nicely with the tidyverse as well\nRows are observations, columns are variables\n\nWhat are the common variable types in R\n\nHow do I create a variable of each type?\nWhen would I use each one?\n\nDo I know how to determine the class/type of a variable?\nCan I explain dynamic typing?\nCan I operate on variables and values using…\n\narithmetic operators?\ncomparison operators?\n\nWhat are dataframes/tibbles? and why are they useful?\nWhat is the difference between installing and loading a package?\nWhat are the components of an R Markdown file?\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html",
    "href": "content/lectures/01-intro-to-r.html",
    "title": "01-intro-to-r",
    "section": "",
    "text": "APPLY NOW: Volunteer or Intern to fight plastic pollution or turn out the vote on campus!\nCALPIRG Students is a student organization on campus that works to protect the environment, address food insecurity, and promote civic engagement. We helped nearly 10,000 students register to vote in California and got the UCs to release new policy to phase out single-use plastics to protect our oceans! Get involved to protect our oceans!\n\nThis Winter, we’re working to protect our oceans. They give us food, provide most of the oxygen we breathe, and are home to awesome biodiversity like whales and sea turtles, but our oceans are at risk from things like overfishing, oil drilling, and pollution. That’s why we’re calling on Governor Newsom to increase protections for marine areas in California!\nWe’re also working on campaigns addressing hunger & homelessness, fighting plastic pollution, and promoting voter participation.\nAs a volunteer or intern with CALPIRG you can:\n\nBuild grassroots support\nWork with the media and help organize press conferences with experts and elected officials\nLobby elected officials\nPlan big events like rallies or a benefit concert\nLearn key nonprofit management and fundraising skills\n\nLearn skills, build your resume, and work with issues that matter. Apply today.\n\n\n\n\n\nQ: Curious about why we’re using RStudio on DataHub instead of running it locally. Are there tasks that require the hardware used in DataHub or can most of the tasks be also done on a laptop?\nA: You could run this whole class locally and are free to do so! The one place where you would run into issues is that I assume packages (which have been installed on datahub) are installed. So, you would just have to install those packages before proceeding.\n\n\nQ: How exactly do students make appointments for the Wednesday office hour? Is it simply through email or is there a separate form?\nA: I use Calendly for this. The link to sign up is on Canvas!\n\n\nQ: I am still not sure how to clone GitHub code into datahub.\nA: This will be covered in lab this week!\n\n\nQ: How will we submit the lab. What will the lab being release and when will it due?\nA: Labs, homework, and exams will all be “submitted” by pushing to GitHub (covered in lab on Friday). Once it’s on GitHub, it’s submitted! This weeks lab will be released today. Typically, each week’s lab will be released on Monday of the week it’s due.\n\n\nQ: Do we need to bring our own computer to lab?\nA: Good question! I should have covered this. Lab is in the same room as lecture, so you will need your own computer. If you don’t have one, please reach out to me or campus resources (listed in the syllabus) and I’ll see what I can do!\n\n\nQ: After class today, I am a little confused on how we will be writing code for assignments. Will we be doing everything through RStudio once we have opened DataHub, or will there be a cell format similar to Jupyter Notebooks when using Python? In other words, can we expect all of our assignments to be conducted in RStudio as we did today?\nA: The code you submit will typically be in R Markdown documents (discussing more today!) These are similar to Jupyter Notebooks. And yes, everything can be done in RStudio as demo-ed on Tuesday.\n\n\nQ: I know you mentioned R has changed since you last used it. Would it be possible for you to share any resources or materials that you and other R practitioners find useful. I.E. Frameworks/Conventions/Blogs. A: Ah, so I meant to say that R has changed since I first used it many years ago. I have continued to use and learn R since then! I’ve never stopped using R. That said, I have/know of TONS of resources. Two big compilations I can recommend are: learnr4free and the Big Book of R. If there’s something specific you’re interested, feel free to let me know and I can point to more specific resources!\n\n\nQ: I’m anxious to learn Github - have put it off for most of my programming life. It seems so complicated! :(\nA: It is complicated, but that’s b/c it’s doing something REALLY hard. That said, the basics are not too complicated. So this course will force you to learn the basics, and then you’ll be on your way to learn the harder stuff!\n\n\nQ: Could you talk about the prevalence of R in industry (use cases / types of jobs / etc.)? My general understanding of it is that it’s kind of out the door when compared to Python.\nA: Yes! I’ll try to discuss this more throughout the course, but R is super popular among data scientists, particularly those who have a focus in statistics, biological data (bioinformatics), econometrics, data analysis, and/or data journalism. If you’re trying to do data engineering, you likely won’t see/use R, but if you’re analyzing and visualizing data in your job, you’re likely to encounter R. That said, Python is certainly a more popular language. That’s why I/we teach it in our intro programming course!\n\n\n\n\nDue Dates:\n\nLab 01 due tomorrow (Friday) 11:59 PM\nLecture Participation survey “due” after class (both Tu and today’s lectures available)\nStudent survey “due” Sunday (1/15) 11:59 PM\n\n\nUpdate: Lab will be podcast but restricted to UCSD students and will not be archived.\n\n\n\n\n\nVariables\nOperators\nData in R\nRMarkdown"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variables-assignment-1",
    "href": "content/lectures/01-intro-to-r.html#variables-assignment-1",
    "title": "01-intro-to-r",
    "section": "Variables & Assignment",
    "text": "Variables & Assignment\nVariables are how we store information so that we can access it later.\n\nVariables are created and stored using the assignment operator <- 1\n\nfirst_variable <- 3\n\nThe above stores the value 3 in the variable first_variable\n\n\nThis means that if we ever want to reference the information stored in that variable later, we can “call” (mean, type in our code) the variable’s name:\n\nfirst_variable\n\n[1] 3"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-type",
    "href": "content/lectures/01-intro-to-r.html#variable-type",
    "title": "01-intro-to-r",
    "section": "Variable Type",
    "text": "Variable Type\n\nEvery variable you create in R will be of a specific type.\n\n\n\nThe type of the variable is determined dynamically on assignment.\n\n\n\n\nDetermining the type of a variable with class():\n\n\nclass(first_variable)\n\n[1] \"numeric\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#basic-variable-types",
    "href": "content/lectures/01-intro-to-r.html#basic-variable-types",
    "title": "01-intro-to-r",
    "section": "Basic Variable Types",
    "text": "Basic Variable Types\n\n\n\n\n\n\n\n\nVariable Type\nExplanation\nExample\n\n\n\n\ncharacter\nstores a string\n\"cogs137\", \"hi!\"\n\n\nnumeric\nstores whole numbers and decimals\n9, 9.29\n\n\ninteger\nspecifies integer\n9L (the L specifies this is an integer)\n\n\nlogical\nBooleans\nTRUE, FALSE\n\n\nlist\nstore multiple elements\nlist(7, \"a\", TRUE)\n\n\n\n\n\nThere are many more. We’ll get to some but not all in this course."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#logical-character",
    "href": "content/lectures/01-intro-to-r.html#logical-character",
    "title": "01-intro-to-r",
    "section": "logical & character",
    "text": "logical & character\nlogical - Boolean values TRUE and FALSE\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\ncharacter - character strings\n\nclass(\"hello\")\n\n[1] \"character\"\n\nclass('students') # equivalent...but we'll use double quotes!\n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#numeric-double-integer",
    "href": "content/lectures/01-intro-to-r.html#numeric-double-integer",
    "title": "01-intro-to-r",
    "section": "numeric: double & integer",
    "text": "numeric: double & integer\ndouble - floating point numerical values (default numerical type)\n\nclass(1.335)\n\n[1] \"numeric\"\n\nclass(7)\n\n[1] \"numeric\"\n\n\n\ninteger - integer numerical values (indicated with an L)\n\nclass(7L)\n\n[1] \"integer\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#lists",
    "href": "content/lectures/01-intro-to-r.html#lists",
    "title": "01-intro-to-r",
    "section": "lists",
    "text": "lists\nSo far, every variable has been an atomic vector, meaning it only stores a single piece of information.\n\nLists are 1d objects that can contain any combination of R objects\n\n\n\nmylist <- list(\"A\", 7L, TRUE, 18.4)\nmylist\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] 7\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 18.4\n\n\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#your-turn",
    "href": "content/lectures/01-intro-to-r.html#your-turn",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nDefine variables of each of the following types: charachter, numeric, integer, logical, list\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#functions",
    "href": "content/lectures/01-intro-to-r.html#functions",
    "title": "01-intro-to-r",
    "section": "Functions",
    "text": "Functions\n\nclass() (and View() & median()) were our first functions…but we’ll show a few more.\n\n\n\nFunctions are (most often) verbs, followed by what they will be applied to in parentheses.\n\n\n\nFunctions are:\n\navailable from base R\navailable from packages you import\ndefined by you\n\n\n\nWe’ll start by getting comfortable with available functions, but in a few days, you’ll learn how to write your own!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#helpful-functions",
    "href": "content/lectures/01-intro-to-r.html#helpful-functions",
    "title": "01-intro-to-r",
    "section": "Helpful Functions",
    "text": "Helpful Functions\n\n\n\nclass() - determine high-level variable type\n\n\nclass(mylist)\n\n[1] \"list\"\n\n\n\nlength()- determine how long an object is\n\n\n# contains 4 elements\nlength(mylist)\n\n[1] 4\n\n\n\n\nstr() - display the structure of an R object\n\n\nstr(mylist)\n\nList of 4\n $ : chr \"A\"\n $ : int 7\n $ : logi TRUE\n $ : num 18.4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#coercion",
    "href": "content/lectures/01-intro-to-r.html#coercion",
    "title": "01-intro-to-r",
    "section": "Coercion",
    "text": "Coercion\nR is a dynamically typed language – it will happily convert between the various types without complaint.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"\n\nc(FALSE, 3L)\n\n[1] 0 3\n\nc(1.2, 3L)\n\n[1] 1.2 3.0"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#missing-values",
    "href": "content/lectures/01-intro-to-r.html#missing-values",
    "title": "01-intro-to-r",
    "section": "Missing Values",
    "text": "Missing Values\nR uses NA to represent missing values in its data structures.\n\nclass(NA)\n\n[1] \"logical\"\n\n\n\n\nOther Special Values\nNaN | Not a number\nInf | Positive infinity\n-Inf | Negative infinity"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#activity",
    "href": "content/lectures/01-intro-to-r.html#activity",
    "title": "01-intro-to-r",
    "section": "Activity",
    "text": "Activity\nWhat is the type of the following vectors? Chat about why they have that type.\n\nc(1, NA+1L, \"C\")\nc(1L / 0, NA)\nc(1:3, 5)\nc(3L, NaN+1L)\nc(NA, TRUE)\n\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#operators-1",
    "href": "content/lectures/01-intro-to-r.html#operators-1",
    "title": "01-intro-to-r",
    "section": "Operators",
    "text": "Operators\nAt its simplest, R is a calculator. To carry out mathematical operations, R uses operators."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#arithmetic-operators",
    "href": "content/lectures/01-intro-to-r.html#arithmetic-operators",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators",
    "text": "Arithmetic Operators\n\n\n\nOperator\nDescription\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\nx %% y\nmodulus (x mod y) 9%%2 is 1\n\n\nx %/% y\ninteger division 9%/%2 is 4"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#arithmetic-operators-examples",
    "href": "content/lectures/01-intro-to-r.html#arithmetic-operators-examples",
    "title": "01-intro-to-r",
    "section": "Arithmetic Operators: Examples",
    "text": "Arithmetic Operators: Examples\n\n7 + 6  \n\n[1] 13\n\n2 - 3\n\n[1] -1\n\n4 * 2\n\n[1] 8\n\n9 / 2\n\n[1] 4.5"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#reminder",
    "href": "content/lectures/01-intro-to-r.html#reminder",
    "title": "01-intro-to-r",
    "section": "Reminder",
    "text": "Reminder\nOutput can be stored to a variable\n\nmy_addition <- 7 + 6\n\n\n\nmy_addition\n\n[1] 13"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#comparison-operators",
    "href": "content/lectures/01-intro-to-r.html#comparison-operators",
    "title": "01-intro-to-r",
    "section": "Comparison Operators",
    "text": "Comparison Operators\nThese operators return a Boolean.\n\n\n\nOperator\nDescription\n\n\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#comparison-operators-examples",
    "href": "content/lectures/01-intro-to-r.html#comparison-operators-examples",
    "title": "01-intro-to-r",
    "section": "Comparison Operators: Examples",
    "text": "Comparison Operators: Examples\n\n4 < 12\n\n[1] TRUE\n\n4 >= 3\n\n[1] TRUE\n\n6 == 6\n\n[1] TRUE\n\n7 != 6\n\n[1] TRUE"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#your-turn-1",
    "href": "content/lectures/01-intro-to-r.html#your-turn-1",
    "title": "01-intro-to-r",
    "section": "Your Turn",
    "text": "Your Turn\nUse arithmetic and comparison operators to store the value 30 in the variable var_30 and TRUE in the variable true_var.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#packages",
    "href": "content/lectures/01-intro-to-r.html#packages",
    "title": "01-intro-to-r",
    "section": "Packages",
    "text": "Packages\n\nPackages are installed with the install.packages function and loaded with the library function, once per session:\n\n\ninstall.packages(\"package_name\")\nlibrary(package_name)\n\n\nIn this course, most packages we’ll use have been installed for you already on datahub, so you will only have to load the package in (using library)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-sets-in-r",
    "href": "content/lectures/01-intro-to-r.html#data-sets-in-r",
    "title": "01-intro-to-r",
    "section": "Data “sets” in R",
    "text": "Data “sets” in R\n\n“set” is in quotation marks because it is not a formal data class\nA tidy data “set” can be one of the following types:\n\ntibble\ndata.frame\n\nWe’ll often work with tibbles:\n\nreadr package (e.g. read_csv function) loads data as a tibble by default\ntibbles are part of the tidyverse, so they work well with other packages we are using\nthey make minimal assumptions about your data, so are less likely to cause hard to track bugs in your code"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-frames",
    "href": "content/lectures/01-intro-to-r.html#data-frames",
    "title": "01-intro-to-r",
    "section": "Data frames",
    "text": "Data frames\n\nA data frame is the most commonly used data structure in R, they are list of equal length vectors (usually atomic, but can be generic). Each vector is treated as a column and elements of the vectors as rows.\nA tibble is a type of data frame that … makes your life (i.e. data analysis) easier.\nMost often a data frame will be constructed by reading in from a file, but we can create them from scratch.\n\n\ndf <- tibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\nclass(df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(df)\n\nRows: 3\nColumns: 2\n$ x <int> 1, 2, 3\n$ y <chr> \"a\", \"b\", \"c\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#data-frames-cont.",
    "href": "content/lectures/01-intro-to-r.html#data-frames-cont.",
    "title": "01-intro-to-r",
    "section": "Data frames (cont.)",
    "text": "Data frames (cont.)\n\nattributes(df)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n[1] 1 2 3\n\n$names\n[1] \"x\" \"y\"\n\n\n\nColumns (variables) in data frames are accessed with $:\n\ndataframe$var_name\n\n\n\n\nclass(df$x)  # access variable type for column\n\n[1] \"integer\"\n\nclass(df$y)  \n\n[1] \"character\""
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-types",
    "href": "content/lectures/01-intro-to-r.html#variable-types",
    "title": "01-intro-to-r",
    "section": "Variable Types",
    "text": "Variable Types\nData stored in columns can include different kinds of information…which would require a different type (class) of variable to be used in R.\n\n\n\n\nR Data Types:\n\nContinuous: numeric, integer\nDiscrete: factors (we haven’t talked about these yet, but will today!)\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#variable-types-cont.",
    "href": "content/lectures/01-intro-to-r.html#variable-types-cont.",
    "title": "01-intro-to-r",
    "section": "Variable Types (cont.)",
    "text": "Variable Types (cont.)\nSometimes data are non-numeric and store words. Even when that is the case, the data can be conveying different information.\n\n\n\n\nR Data Types:\n\nNominal: character\nOrdinal: factors\nBinary: logical OR numeric OR factors 😱\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#example-cat-lovers",
    "href": "content/lectures/01-intro-to-r.html#example-cat-lovers",
    "title": "01-intro-to-r",
    "section": "Example: Cat lovers",
    "text": "Example: Cat lovers\nA survey asked respondents their name and number of cats. The instructions said to enter the number of cats as a numerical value.\n\n🚨 There is code ahead that we’re not going to discuss in detail today, but we will in coming lectures.\n\ncat_lovers <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/cat-lovers.csv\")"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#the-data",
    "href": "content/lectures/01-intro-to-r.html#the-data",
    "title": "01-intro-to-r",
    "section": "The Data",
    "text": "The Data\n\ncat_lovers |>\n  datatable()"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#the-question",
    "href": "content/lectures/01-intro-to-r.html#the-question",
    "title": "01-intro-to-r",
    "section": "The Question",
    "text": "The Question\nHow many respondents have a below average number of cats?\n\nGiving it a first shot…\n\ncat_lovers |>\n  summarise(mean = mean(number_of_cats))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean = mean(number_of_cats)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n   mean\n  <dbl>\n1    NA\n\n\n\n\n💡 maybe there is missing data in the number_of_cats column!\nOh why will you still not work??!!\n\ncat_lovers |>\n  summarise(mean_cats = mean(number_of_cats, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `mean_cats = mean(number_of_cats, na.rm = TRUE)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\n\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1        NA\n\n\n\n\n💡What is the type of the number_of_cats variable?"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#take-a-breath-and-look-at-your-data",
    "href": "content/lectures/01-intro-to-r.html#take-a-breath-and-look-at-your-data",
    "title": "01-intro-to-r",
    "section": "Take a breath and look at your data",
    "text": "Take a breath and look at your data\n\n\nglimpse(cat_lovers)\n\nRows: 60\nColumns: 3\n$ name           <chr> \"Bernice Warren\", \"Woodrow Stone\", \"Willie Bass\", \"Tyro…\n$ number_of_cats <chr> \"0\", \"0\", \"1\", \"3\", \"3\", \"2\", \"1\", \"1\", \"0\", \"0\", \"0\", …\n$ handedness     <chr> \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\",…"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#lets-take-another-look",
    "href": "content/lectures/01-intro-to-r.html#lets-take-another-look",
    "title": "01-intro-to-r",
    "section": "Let’s take another look",
    "text": "Let’s take another look"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#sometimes-you-need-to-babysit-your-respondents",
    "href": "content/lectures/01-intro-to-r.html#sometimes-you-need-to-babysit-your-respondents",
    "title": "01-intro-to-r",
    "section": "Sometimes you need to babysit your respondents",
    "text": "Sometimes you need to babysit your respondents\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n    name == \"Ginger Clark\" ~ 2,\n    name == \"Doug Bass\"    ~ 3,\n    TRUE                   ~ as.numeric(number_of_cats))) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `number_of_cats = case_when(...)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\n# A tibble: 60 × 3\n   name           number_of_cats handedness\n   <chr>                   <dbl> <chr>     \n 1 Bernice Warren              0 left      \n 2 Woodrow Stone               0 left      \n 3 Willie Bass                 1 left      \n 4 Tyrone Estrada              3 left      \n 5 Alex Daniels                3 left      \n 6 Jane Bates                  2 left      \n 7 Latoya Simpson              1 left      \n 8 Darin Woods                 1 left      \n 9 Agnes Cobb                  0 left      \n10 Tabitha Grant               0 left      \n# … with 50 more rows"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#always-respect-check-data-types",
    "href": "content/lectures/01-intro-to-r.html#always-respect-check-data-types",
    "title": "01-intro-to-r",
    "section": "Always respect (& check!) data types",
    "text": "Always respect (& check!) data types\n\ncat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats)) |>\n  summarise(mean_cats = mean(number_of_cats))\n\n# A tibble: 1 × 1\n  mean_cats\n      <dbl>\n1     0.817"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#now-that-we-know-what-were-doing",
    "href": "content/lectures/01-intro-to-r.html#now-that-we-know-what-were-doing",
    "title": "01-intro-to-r",
    "section": "Now that we know what we’re doing…",
    "text": "Now that we know what we’re doing…\n\ncat_lovers <- cat_lovers |>\n  mutate(number_of_cats = case_when(\n         name == \"Ginger Clark\" ~ \"2\",\n         name == \"Doug Bass\"    ~ \"3\",\n         TRUE                   ~ number_of_cats),\n         number_of_cats = as.numeric(number_of_cats))\n\n… store your data in a variable (here we’re overwriting the old cat_lovers tibble)."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#moral-of-the-story",
    "href": "content/lectures/01-intro-to-r.html#moral-of-the-story",
    "title": "01-intro-to-r",
    "section": "Moral of the story",
    "text": "Moral of the story\n\nIf your data does not behave how you expect it to, type coercion upon reading in the data might be the reason.\nGo in and investigate your data, apply the fix, save your data, live happily ever after."
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#r-markdown-tour",
    "href": "content/lectures/01-intro-to-r.html#r-markdown-tour",
    "title": "01-intro-to-r",
    "section": "R Markdown: tour",
    "text": "R Markdown: tour\n\n[DEMO]\n\nBefore we move on…\n   What is the Bechdel test?\n\nThe Bechdel test asks whether a work of fiction features at least two women who talk to each other about something other than a man, and there must be two women named characters.\n\n\nConcepts introduced:\n\nKnitting documents\nR Markdown and (some) R syntax"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#giving-the-demo-a-go",
    "href": "content/lectures/01-intro-to-r.html#giving-the-demo-a-go",
    "title": "01-intro-to-r",
    "section": "Giving the demo a go…",
    "text": "Giving the demo a go…\n\nNavigate to the demo URL (on Canvas)\nAccept the “assignment” (this is NOT graded)\nClone the repo\nEdit the document\nKnit the document\nPush your changes\n\nTry to play around with this after finishing your lab tomorrow!"
  },
  {
    "objectID": "content/lectures/01-intro-to-r.html#recap",
    "href": "content/lectures/01-intro-to-r.html#recap",
    "title": "01-intro-to-r",
    "section": "Recap",
    "text": "Recap\n\nAlways best to think of data as part of a tibble\n\nThis plays nicely with the tidyverse as well\nRows are observations, columns are variables\n\nWhat are the common variable types in R\n\nHow do I create a variable of each type?\nWhen would I use each one?\n\nDo I know how to determine the class/type of a variable?\nCan I explain dynamic typing?\nCan I operate on variables and values using…\n\narithmetic operators?\ncomparison operators?\n\nWhat are dataframes/tibbles? and why are they useful?\nWhat is the difference between installing and loading a package?\nWhat are the components of an R Markdown file?"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#qa",
    "href": "content/lectures/04-ggplot2-slides.html#qa",
    "title": "04-ggplot2",
    "section": "Q&A",
    "text": "Q&A\n\nQ:Can we have a demo in class on how to clone our repo and check if our Github submissions submitted properly? I am having trouble doing this.\nA: Yes! Let’s do that today.\n\n\nQ: Does left_join(a, b) = right_join(b, a)?\nA: No. The total information would be equivalent but the order of columns would differ. In the left join, the columns from a would be first and in the right join, the colums from b would be listed first. But the actual rows that are joined/included would be the same.\n\n\nQ: I would still like to know if there is a way to save data sets more often when using functions on them, such as filter. I see we only saved data sets when we use functions like mutate, but why not other functions?\nA: You’re allowed to store variables into a new data frame whenever you’d like! Now, the more variables you create, the more to keep track of, so I would encourage you to store your output any time you make a “substantial” change to your data. So, say you do a filter, selection, and mutate to get your data into the format for analysis. I wouldn’t store after each step, but I would store it after I wrote the pipe doing all three. Follow up if this doesn’t clarify!\n\n\nQ: I think the left_join, right_join, and full_join was pretty confusing.\nA: You’re right! It is confusing. We’ll go through more examples and get more practice!"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#course-announcements",
    "href": "content/lectures/04-ggplot2-slides.html#course-announcements",
    "title": "04-ggplot2",
    "section": "Course Announcements",
    "text": "Course Announcements\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHappy New Year!\nGitHub assignment clone + submission demo\nLab02 Notes:\n\nLook at the answer key:\ngrouping by name and year\ncan add text outside of code chunks; discuss variable reference\nsolutions to optional exercises"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#ggplot2-in-tidyverse",
    "href": "content/lectures/04-ggplot2-slides.html#ggplot2-in-tidyverse",
    "title": "04-ggplot2",
    "section": "ggplot2 \\(\\in\\) tidyverse",
    "text": "ggplot2 \\(\\in\\) tidyverse\n\n\n\n\n\n\n\n\n\nggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#data-palmer-penguins",
    "href": "content/lectures/04-ggplot2-slides.html#data-palmer-penguins",
    "title": "04-ggplot2",
    "section": "Data: Palmer Penguins",
    "text": "Data: Palmer Penguins\nMeasurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#the-data",
    "href": "content/lectures/04-ggplot2-slides.html#the-data",
    "title": "04-ggplot2",
    "section": "The Data",
    "text": "The Data\n\npenguins |>\n  datatable()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#a-plot",
    "href": "content/lectures/04-ggplot2-slides.html#a-plot",
    "title": "04-ggplot2",
    "section": "A Plot",
    "text": "A Plot\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section",
    "href": "content/lectures/04-ggplot2-slides.html#section",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame\n\n\n\nggplot(data = penguins)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-1",
    "href": "content/lectures/04-ggplot2-slides.html#section-1",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-2",
    "href": "content/lectures/04-ggplot2-slides.html#section-2",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-3",
    "href": "content/lectures/04-ggplot2-slides.html#section-3",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-4",
    "href": "content/lectures/04-ggplot2-slides.html#section-4",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-5",
    "href": "content/lectures/04-ggplot2-slides.html#section-5",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-6",
    "href": "content/lectures/04-ggplot2-slides.html#section-6",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-7",
    "href": "content/lectures/04-ggplot2-slides.html#section-7",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-8",
    "href": "content/lectures/04-ggplot2-slides.html#section-8",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-9",
    "href": "content/lectures/04-ggplot2-slides.html#section-9",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-10",
    "href": "content/lectures/04-ggplot2-slides.html#section-10",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#coding-out-loud-1",
    "href": "content/lectures/04-ggplot2-slides.html#coding-out-loud-1",
    "title": "04-ggplot2",
    "section": "Coding out loud",
    "text": "Coding out loud\n\nCodePlotNarrative\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\n\n\nStart with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\nRepresent each observation with a point and map species to the color of each point.\nTitle the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\nFinally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#argument-names",
    "href": "content/lectures/04-ggplot2-slides.html#argument-names",
    "title": "04-ggplot2",
    "section": "Argument names",
    "text": "Argument names\n\n\n\n\n\n\nTip\n\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm,  \n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\nggplot(penguins, \n       aes(x = bill_depth_mm, \n           y = bill_depth_mm,\n           color = species)) +\n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a basic plot in ggplot2 using different variables than those in the last example (last example: bill_depth_mm & bill_depth_mm).\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#aesthetics-options",
    "href": "content/lectures/04-ggplot2-slides.html#aesthetics-options",
    "title": "04-ggplot2",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#color",
    "href": "content/lectures/04-ggplot2-slides.html#color",
    "title": "04-ggplot2",
    "section": "Color",
    "text": "Color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#shape",
    "href": "content/lectures/04-ggplot2-slides.html#shape",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = island)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#shape-1",
    "href": "content/lectures/04-ggplot2-slides.html#shape-1",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#size",
    "href": "content/lectures/04-ggplot2-slides.html#size",
    "title": "04-ggplot2",
    "section": "Size",
    "text": "Size\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#alpha",
    "href": "content/lectures/04-ggplot2-slides.html#alpha",
    "title": "04-ggplot2",
    "section": "Alpha",
    "text": "Alpha\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g,\n           alpha = flipper_length_mm)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting",
    "href": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting",
    "title": "04-ggplot2",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nMapping: Determine the size, alpha, etc. of points based on the values of a variable in the data\n\ngoes into aes()\n\nSetting: Determine the size, alpha, etc. of points not based on the values of a variable in the data\n\ngoes into geom_*() (this was geom_point() in the previous example, but we’ll learn about other geoms soon!)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting-example",
    "href": "content/lectures/04-ggplot2-slides.html#mapping-vs.-setting-example",
    "title": "04-ggplot2",
    "section": "Mapping vs. Setting (example)",
    "text": "Mapping vs. Setting (example)\n\n\nMapping\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm,\n           size = body_mass_g, \n           alpha = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\nSetting\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm)) + \n  geom_point(size = 2, alpha = 0.5)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn-1",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn-1",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nEdit the basic plot you created earlier to change something about its aesthetics.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#faceting-1",
    "href": "content/lectures/04-ggplot2-slides.html#faceting-1",
    "title": "04-ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\nSmaller plots that display different subsets of the data\nUseful for exploring conditional relationships and large data\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ island)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#various-ways-to-facet",
    "href": "content/lectures/04-ggplot2-slides.html#various-ways-to-facet",
    "title": "04-ggplot2",
    "section": "Various ways to facet",
    "text": "Various ways to facet\n🧠 In the next few slides describe what each plot displays. Think about how the code relates to the output.\n\n\n\n\n\n\nWarning\n\n\nThe plots in the next few slides do not have proper titles, axis labels, etc. because we want you to figure out what’s happening in the plots. But you should always label your plots!"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-11",
    "href": "content/lectures/04-ggplot2-slides.html#section-11",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ sex)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-12",
    "href": "content/lectures/04-ggplot2-slides.html#section-12",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(sex ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-13",
    "href": "content/lectures/04-ggplot2-slides.html#section-13",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-14",
    "href": "content/lectures/04-ggplot2-slides.html#section-14",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(. ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#section-15",
    "href": "content/lectures/04-ggplot2-slides.html#section-15",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species, ncol = 2)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#faceting-summary",
    "href": "content/lectures/04-ggplot2-slides.html#faceting-summary",
    "title": "04-ggplot2",
    "section": "Faceting summary",
    "text": "Faceting summary\n\nfacet_grid():\n\n2d grid\nrows ~ cols\nuse . for no split\n\nfacet_wrap(): 1d ribbon wrapped according to number of rows and columns specified or available plotting area"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#facet-and-color",
    "href": "content/lectures/04-ggplot2-slides.html#facet-and-color",
    "title": "04-ggplot2",
    "section": "Facet and color",
    "text": "Facet and color\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) + \n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#face-and-color-no-legend",
    "href": "content/lectures/04-ggplot2-slides.html#face-and-color-no-legend",
    "title": "04-ggplot2",
    "section": "Face and color, no legend",
    "text": "Face and color, no legend\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#common-geoms",
    "href": "content/lectures/04-ggplot2-slides.html#common-geoms",
    "title": "04-ggplot2",
    "section": "Common geoms",
    "text": "Common geoms\n\n\n\ngeom 1\nDescription 2\n\n\n\n\ngeom_point\nscatterplot\n\n\ngeom_bar\nbarplot\n\n\ngeom_line\nline plot\n\n\ngeom_density\ndensityplot\n\n\ngeom_histogram\nhistogram\n\n\ngeom_boxplot\nboxplot\n\n\n\nggplot2 geoms listed hereWhen each visualization is appropriate here"
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#your-turn-2",
    "href": "content/lectures/04-ggplot2-slides.html#your-turn-2",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a plot in ggplot2 using a different geom than what you did previously. Customize as much as you can before time is “up.”\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2-slides.html#suggested-reading",
    "href": "content/lectures/04-ggplot2-slides.html#suggested-reading",
    "title": "04-ggplot2",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/\n\n\n\n\nhttps://cogs137.github.io/website/"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html",
    "href": "content/lectures/04-ggplot2.html",
    "title": "04-ggplot2",
    "section": "",
    "text": "Q:Can we have a demo in class on how to clone our repo and check if our Github submissions submitted properly? I am having trouble doing this.\nA: Yes! Let’s do that today.\n\n\nQ: Does left_join(a, b) = right_join(b, a)?\nA: No. The total information would be equivalent but the order of columns would differ. In the left join, the columns from a would be first and in the right join, the colums from b would be listed first. But the actual rows that are joined/included would be the same.\n\n\nQ: I would still like to know if there is a way to save data sets more often when using functions on them, such as filter. I see we only saved data sets when we use functions like mutate, but why not other functions?\nA: You’re allowed to store variables into a new data frame whenever you’d like! Now, the more variables you create, the more to keep track of, so I would encourage you to store your output any time you make a “substantial” change to your data. So, say you do a filter, selection, and mutate to get your data into the format for analysis. I wouldn’t store after each step, but I would store it after I wrote the pipe doing all three. Follow up if this doesn’t clarify!\n\n\nQ: I think the left_join, right_join, and full_join was pretty confusing.\nA: You’re right! It is confusing. We’ll go through more examples and get more practice!\n\n\n\n\nDue Dates:\n\nLab 03 due Friday (1/27; 11:59 PM)\nLecture Participation survey “due” after class\n\nCourse Announcements:\n\nHappy New Year!\nGitHub assignment clone + submission demo\nLab02 Notes:\n\nLook at the answer key:\ngrouping by name and year\ncan add text outside of code chunks; discuss variable reference\nsolutions to optional exercises\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 is tidyverse’s data visualization package\nStructure of the code for plots can be summarized as\n\n\nggplot(data = [dataset], \n       mapping = aes(x = [x-variable], \n                     y = [y-variable])) +\n   geom_xxx() +\n   other options\n\n\n\n\n\n\nMeasurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\n\n\n\n\n\n\n\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               <fct> male, female, female, NA, female, male, female, male…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\nArtwork by @allison_horst \n\n\n\n\npenguins |>\n  datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm, y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section",
    "href": "content/lectures/04-ggplot2.html#section",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame\n\n\nggplot(data = penguins)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-1",
    "href": "content/lectures/04-ggplot2.html#section-1",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-2",
    "href": "content/lectures/04-ggplot2.html#section-2",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm))"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-3",
    "href": "content/lectures/04-ggplot2.html#section-3",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-4",
    "href": "content/lectures/04-ggplot2.html#section-4",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) + \n  geom_point()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-5",
    "href": "content/lectures/04-ggplot2.html#section-5",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-6",
    "href": "content/lectures/04-ggplot2.html#section-6",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-7",
    "href": "content/lectures/04-ggplot2.html#section-7",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-8",
    "href": "content/lectures/04-ggplot2.html#section-8",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-9",
    "href": "content/lectures/04-ggplot2.html#section-9",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\")"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-10",
    "href": "content/lectures/04-ggplot2.html#section-10",
    "title": "04-ggplot2",
    "section": "",
    "text": "Start with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis. Represent each observation with a point and map species to the color of each point. Title the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source. Finally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness.\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "href": "content/lectures/04-ggplot2.html#coding-out-loud-1",
    "title": "04-ggplot2",
    "section": "Coding out loud",
    "text": "Coding out loud\n\nCodePlotNarrative\n\n\n\nggplot(data = penguins,\n       mapping = aes(x = bill_depth_mm,\n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Bill depth and length\",\n       subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n       x = \"Bill depth (mm)\", y = \"Bill length (mm)\",\n       color = \"Species\",\n       caption = \"Source: Palmer Station LTER / palmerpenguins package\") +\n  scale_color_viridis_d()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nStart with the penguins data frame, map bill depth to the x-axis and map bill length to the y-axis.\nRepresent each observation with a point and map species to the color of each point.\nTitle the plot “Bill depth and length”, add the subtitle “Dimensions for Adelie, Chinstrap, and Gentoo Penguins”, label the x and y axes as “Bill depth (mm)” and “Bill length (mm)”, respectively, label the legend “Species”, and add a caption for the data source.\nFinally, use a discrete color scale that is designed to be perceived by viewers with common forms of color blindness."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#argument-names",
    "href": "content/lectures/04-ggplot2.html#argument-names",
    "title": "04-ggplot2",
    "section": "Argument names",
    "text": "Argument names\n\n\n\n\n\n\nTip\n\n\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\n\nggplot(data = penguins, \n       mapping = aes(x = bill_depth_mm,  \n                     y = bill_length_mm,\n                     color = species)) +\n  geom_point() +\n  scale_color_viridis_d()\n\n\n\nggplot(penguins, \n       aes(x = bill_depth_mm, \n           y = bill_depth_mm,\n           color = species)) +\n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn",
    "href": "content/lectures/04-ggplot2.html#your-turn",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a basic plot in ggplot2 using different variables than those in the last example (last example: bill_depth_mm & bill_depth_mm).\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#aesthetics-options",
    "href": "content/lectures/04-ggplot2.html#aesthetics-options",
    "title": "04-ggplot2",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#color",
    "href": "content/lectures/04-ggplot2.html#color",
    "title": "04-ggplot2",
    "section": "Color",
    "text": "Color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape",
    "href": "content/lectures/04-ggplot2.html#shape",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to a different variable than color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = island)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#shape-1",
    "href": "content/lectures/04-ggplot2.html#shape-1",
    "title": "04-ggplot2",
    "section": "Shape",
    "text": "Shape\nMapped to same variable as color\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#size",
    "href": "content/lectures/04-ggplot2.html#size",
    "title": "04-ggplot2",
    "section": "Size",
    "text": "Size\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#alpha",
    "href": "content/lectures/04-ggplot2.html#alpha",
    "title": "04-ggplot2",
    "section": "Alpha",
    "text": "Alpha\n\nggplot(penguins,\n       aes(x = bill_depth_mm, \n           y = bill_length_mm,\n           color = species,\n           shape = species,\n           size = body_mass_g,\n           alpha = flipper_length_mm)) + \n  geom_point() +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting",
    "title": "04-ggplot2",
    "section": "Mapping vs. setting",
    "text": "Mapping vs. setting\n\nMapping: Determine the size, alpha, etc. of points based on the values of a variable in the data\n\ngoes into aes()\n\nSetting: Determine the size, alpha, etc. of points not based on the values of a variable in the data\n\ngoes into geom_*() (this was geom_point() in the previous example, but we’ll learn about other geoms soon!)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "href": "content/lectures/04-ggplot2.html#mapping-vs.-setting-example",
    "title": "04-ggplot2",
    "section": "Mapping vs. Setting (example)",
    "text": "Mapping vs. Setting (example)\n\n\nMapping\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm,\n           size = body_mass_g, \n           alpha = flipper_length_mm)) + \n  geom_point()\n\n\n\n\n\nSetting\n\nggplot(penguins,\n       aes(x = bill_depth_mm,\n           y = bill_length_mm)) + \n  geom_point(size = 2, alpha = 0.5)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-1",
    "href": "content/lectures/04-ggplot2.html#your-turn-1",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nEdit the basic plot you created earlier to change something about its aesthetics.\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-1",
    "href": "content/lectures/04-ggplot2.html#faceting-1",
    "title": "04-ggplot2",
    "section": "Faceting",
    "text": "Faceting\n\nSmaller plots that display different subsets of the data\nUseful for exploring conditional relationships and large data\n\n\n\n\n\n\n\n\nggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ island) \n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "href": "content/lectures/04-ggplot2.html#various-ways-to-facet",
    "title": "04-ggplot2",
    "section": "Various ways to facet",
    "text": "Various ways to facet\n🧠 In the next few slides describe what each plot displays. Think about how the code relates to the output.\n\n\n\n\n\n\nWarning\n\n\n\nThe plots in the next few slides do not have proper titles, axis labels, etc. because we want you to figure out what’s happening in the plots. But you should always label your plots!"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-11",
    "href": "content/lectures/04-ggplot2.html#section-11",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(species ~ sex)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-12",
    "href": "content/lectures/04-ggplot2.html#section-12",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(sex ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-13",
    "href": "content/lectures/04-ggplot2.html#section-13",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-14",
    "href": "content/lectures/04-ggplot2.html#section-14",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_grid(. ~ species)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#section-15",
    "href": "content/lectures/04-ggplot2.html#section-15",
    "title": "04-ggplot2",
    "section": "",
    "text": "ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ species, ncol = 2)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#faceting-summary",
    "href": "content/lectures/04-ggplot2.html#faceting-summary",
    "title": "04-ggplot2",
    "section": "Faceting summary",
    "text": "Faceting summary\n\nfacet_grid():\n\n2d grid\nrows ~ cols\nuse . for no split\n\nfacet_wrap(): 1d ribbon wrapped according to number of rows and columns specified or available plotting area"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#facet-and-color",
    "href": "content/lectures/04-ggplot2.html#facet-and-color",
    "title": "04-ggplot2",
    "section": "Facet and color",
    "text": "Facet and color\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) + \n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d()"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "href": "content/lectures/04-ggplot2.html#face-and-color-no-legend",
    "title": "04-ggplot2",
    "section": "Face and color, no legend",
    "text": "Face and color, no legend\n\nggplot(\n  penguins, \n  aes(x = bill_depth_mm, \n      y = bill_length_mm, \n      color = species)) +\n  geom_point() +\n  facet_grid(species ~ sex) +\n  scale_color_viridis_d() +\n  guides(color = FALSE)"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#common-geoms",
    "href": "content/lectures/04-ggplot2.html#common-geoms",
    "title": "04-ggplot2",
    "section": "Common geoms",
    "text": "Common geoms\n\n\n\ngeom 1\nDescription 2\n\n\n\n\ngeom_point\nscatterplot\n\n\ngeom_bar\nbarplot\n\n\ngeom_line\nline plot\n\n\ngeom_density\ndensityplot\n\n\ngeom_histogram\nhistogram\n\n\ngeom_boxplot\nboxplot"
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#your-turn-2",
    "href": "content/lectures/04-ggplot2.html#your-turn-2",
    "title": "04-ggplot2",
    "section": "Your Turn",
    "text": "Your Turn\nGenerate a plot in ggplot2 using a different geom than what you did previously. Customize as much as you can before time is “up.”\n\n\nPut a green sticky on the front of your computer when you’re done. Put a pink if you want help/have a question."
  },
  {
    "objectID": "content/lectures/04-ggplot2.html#suggested-reading",
    "href": "content/lectures/04-ggplot2.html#suggested-reading",
    "title": "04-ggplot2",
    "section": "Suggested Reading",
    "text": "Suggested Reading\n\nR4DS Chapter 3: Data Visualization\nData to Viz: https://www.data-to-viz.com/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COGS 137: Practical Data Science in R",
    "section": "",
    "text": "Course Info\nPractical Data Science in R focuses on teaching students how to think rigorously throughout the data science process. To this end, through interaction with unique data sets and interesting questions, this course helps students 1) gain fluency in the R programming language, 2) effectively explore & visualize data, 3) use statistical thinking to analyze data and rigorously evaluate their conclusions, and 4) effectively communicate their results. Course objectives are accomplished through hands-on practice, using real-world data to learn via case studies, and project-based learning.\n\nDays & Times\n\n\n\n\n\n Lecture: Tu/Th 2-3:20 (Peterson Hall 104)\n Lab: Fri 1-1:50 (Peterson Hall 104)\n\nInstructional Staff & Office Hours\n\n\n\nInstructor\nShannon Ellis\nsellis@ucsd.edu\nWed 2-3\nVirtual (see canvas)\n\n\n\n\n\nTh 12:50-1:50 PM\nCSB 243\n\n\nTA\nShubham Kulkarni\n\nF 12-1PM\nCSB 114\n\n\nIAs\nChristian Kim\n\nTh 11AM-12PM\nCSB 114\n\n\n\n\n\n\n\nCourse Objectives\n\nProgram at the introductory level in the R statistical programming language\nEmploy the tidyverse suite of packages to interact with, wrangle, visualize, and model data\nExplain & apply statistical concepts (estimation, linear regression, logistic regression, etc.) for data analysis\nCommunicate data science projects through effective visualization, oral presentation, and written reports\n\n\n\nTexts\nTexts are freely available online:\n\n\n\nIntroduction to Modern Statistics\nÇetinkaya-Rundel and Hardin\nOpenIntro, 1st Edition, 2021\n\n\nR for Data Science\nGrolemund and Wickham\nO’Reilly, 1st edition, 2016\n\n\n\n\n\nMaterials\nYou should have access to a laptop and bring it to every class, fully charged (as possible).\nNote: If you do not have consistent access to the technology needed, please use this form to request a loaner laptop. (For any issues that you may have, please email vcsa@ucsd.edu, and they will work to assist you.)\n\n\nAcknowledgements\nI want to first recognize Dr. Mine Çetinkaya-Rundeland for her unparalleled efforts in support of education and educators in data science, statistics, and R programming. This course website was adapted from her course website. These course slides/labs/homework…also adapted from Mine’s course and the related datascienceinabox. I am so *very* indebted to Mine! I also want to thank the Open Case Studies team for their tireless work in putting together interesting and topical case studies, a handful of which we use throughout the course. And, finally, thanks to Allison Horst, whose artwork is inspiring, educational, and fun…and is used throughout this course. Further, thanks to the R (education) community generally; planning this course was really fun because I had so many awesome resources to choose from. Having these materials made course prep and planning is just another example of what sets the R community apart!"
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "COGS 137",
    "section": "",
    "text": "Class\nClass will include short lectures as well as interactive activities. The goal of lecture is to introduce the topics and information needed for the course. The goal of your time outside of lecture is to practice with topics that are introduced and deepen your understanding of material presented in class. Since so much of programming and statistical analysis is learned best by doing, we’ll prioritize that throughout the course, both in and outside of the classroom.\n\n\nDiversity & Inclusion\nMy goal is that every student, regardless of their background or perspective, will be well-served by this course. My philosophy is that the diversity of students in this class is a huge asset to our learning community; our differences provide opportunities for learning and understanding. I intend to present course materials that are conscious of and respectful to diversity (gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, politics, and culture); however, if I ever fall short or if you ever have suggestions for improvement, please do share with me! This feedback is always welcomed, and I am always in the process of learning and improving to this end. If you would like to provide that feedback anonymously, please use the anonymous Google Form.*\n\nWhat should you call me?\nMost students call me Professor/Prof Ellis, and that’s great! This is how I typically sign emails to students. I’m also totally OK with you addressing me as Shannon or Dr. Ellis.\n\n\nWhat I should call you?\nI should call you by your preferred name, with the correct pronunciation. Please correct me (in the moment or via email/Campuswire after the fact…however you’re most comfortable) if I ever make a mistake.\n\n\n\nDisability Access\nStudents requesting accommodations due to a disability should provide a current Authorization for Accommodation (AFA) letter. These letters are issued by the Office for Students with Disabilities (OSD), which is located in University Center 202 behind Center Hall. If you are struggling to get necessary accommodations or want to further discuss your accommodations, please feel free to reach out to Professor Ellis directly.\nContacting the OSD can help you further:\n858.534.4382 (phone)\nosd@ucsd.edu (email)\nhttp://disabilities.ucsd.edu\n\n\nHow to get help\nIt’s great that we have so many ways to communicate, but it can get tricky to figure out who to contact or where your question belongs or when to expect a response. These guidelines are to help you get your question answered as quickly as possible and to ensure that we’re able to get to everyone’s questions.\nThat said, to ensure that we’re respecting their time, TAs and IAs have been instructed they’re only encouraged to answer questions between standard working hours (M-F 9AM-5PM). Professor Ellis is also going to do her best to stick to these working hours and only go on Campuswire M-F each week. However, we know that’s not necessarily when you may be doing your work. So, please feel free to post whenever is best for you while knowing that if you post late at night or on a weekend, you may not get a response until the next weekday. As such, do your best not to wait until the last minute to ask a question.\nIf you have:\n\nquestions about course content - these are awesome! We want everyone to see them and have their questions answered too, so post these to Campuswire!\na technical assignment question - come to office hours (or post to Campuswire). Answering technical questions is often best accomplished ‘in person’ where we can discuss the question and talk through ideas. However, if that is not possible, post your question to Campuswire. Be as specific as you can in the question you ask. And, for those answering, help your classmates as much as you can without just giving the answer. Help guide them, point them in a direction, provide pseudo code, but do not provide code that answers assignment questions.\nbeen stuck on something for a while (>30min) and aren’t even really sure where to start - Programming can be frustrating and it may not always be obvious what is going wrong or why something isn’t working. That’s OK - we’ve all been there! If you are stuck, you can and should reach out for help, even if you aren’t exactly sure what your specific question is. To determine when to reach out, consider the 2-hour rule. This rule states that if you are stuck, work on that problem for an hour. Then, take a 30 minute break and do something else. When you come back after your break, try for another 30 minutes or so to solve your problem. If you are still completely stuck, stop and contact us (office hours, post on Campuswire). If you don’t have a specific question, include the information you have (what you’re stuck on, the code you’ve been trying that hasn’t been happening, and/or the error messages you’ve been getting).\nquestions about course logistics - first, check the course website. If you can’t find the answer there, first ask a classmate. If still unsure, post on Campuswire.\nquestions about a grade - Post on Campuswire with “regrades” tag in a private post to “Instructors & TAs”.\nsomething super cool to share related to class or want to talk about a topic in further depth - feel free to email Professor Ellis (sellis@ucsd.edu) or come to office hours. Please include COGS137 in the email subject line.\nsome feedback about the course you want to share anonymously - If you’ve been offended by an example in class, really liked or disliked a lesson, or wish there were something covered in class that wasn’t but would rather not share this publicly, etc., please fill out the anonymous Google Form*\n\n*This form can be taken down at any time if it’s not being used for its intended purpose; however, you all will be notified should that happen.\n\n\nAcademic integrity\nDon’t cheat.\nYou are generally encouraged to work together and help one another in this course. However, you are personally responsible for the work you submit. A helpful heuristic can b to ask yourself “Can I explain each piece of code and each analysis carried out in what I’m submitting? Could I reproduce this code/analysis on my own?”; you should be able to answer “Yes” to both questions for everything you submit in this course. For labs and assignments, you are allowed and encouraged to work together, but it is your responsibility to ensure you understand everything you’ve submitted. (For exams, all work has to be completed individually and communication with others about the exam is not allowed; this will be discussed more explicitly before exams.)\nA note on sharing / reusing code: The Internet is an excellent resource; there will be many times you find helpful information online. You should use available resources (e.g. StackOverflow), but you must explicitly cite any code you use directly or any code you use as inspiration. This can be done by including the URL/reference to the source directly in your code (as a code comment) or in accompanying text for a given assignment/exam/lab. You should never share code directly (e.g. copy + paste; share an send an answer to a classmate), but you can discuss code and work together on everything other than take-home exams.\nPlease review UCSD’s academic integrity policies here.\nCheating and plagiarism have been and will be strongly penalized. If, for whatever reason, Canvas or DataHub is down or something else prohibits you from being able to turn in an assignment on time, immediately contact Professor Ellis by emailing your assignment (sellis@ucsd.edu), or else it will be graded as late.\n\n\nCourse components\n\nLecture\nLectures will be your introduction to course topics and material. Lectures will be interactive, and you will be given time to practice with the lecture concepts during class. Attendance is not required, but is encouraged if you’re feeling well. To help incentivize coming to class, there will be a daily participation survey that will open at the end of lecture and close shortly after each Tues/Thurs lecture. Each time you fill out the lecture survey, you get a small % of credit toward your final project Completion of all surveys can provide up to 3.5% extra credit on your final project (not your final course grade).\n\nReadings\nReadings will be assigned for some class days and are best completed prior to the day’s lecture. These are meant to provide background and additional context for the upcoming day’s lecture topics. These can also be a good source after class when studying or reviewing topics discussed in class.\n\n\nPodcast\nIn case you miss class or would like to review the material covered in class, you can view the podcasts here.\n\n\n\nLabs (16%)\nLabs are meant to give you deeper understanding and hands-on experience with the technical and statistical topics introduced during lecture in a low-stakes environment. Lab sections will typically comprise of a short review and explanation of the lab and then time for you to complete the assigned weekly lab. Labs are submitted individually, but you are encouraged to work together during lab. You are free to ask and answer each others’ questions and discuss your work. Instructional staff will be present during lab to help further your understanding.\nLabs are graded for concerted effort. This is because when we learn something new, mistakes are going to happen! In fact, we learn a lot from the mistakes we make during the learning process. If your submission reflects ~50 min of work/effort, you will receive full credit for the week’s lab.\nLab attendance is not required, but is definitely encouraged if you are feeling well. While slides used are shared, lab sessions are not recorded, so being present is the best way to fully engage in the course.\n\n\nHomework (32%)\nAfter practice in lecture and labs, homework assignments are meant to demonstrate your solidified understanding of the course material. These are typically 2-4x longer and more involved than labs. Homework assignments are completed and submitted individually and are marked for correctness. You are allowed to work together on homework assignments, but academic integrity must be upheld.\n\n\nMidterm (15%)\nThere will be a single take-home exam due in week 4, and you will have at least 48 hours to complete it. This exam is meant to assess your understanding of the R programming language prior to us moving into focusing on case studies and full analyses. The exam will be completed individually and will be open-notes and open-Internet; however, you will not be permitted to ask questions of one another, the Internet, nor instructional staff while completing the take-home exam.\n\n\nTeams\nThere will be two case study mini-projects and a final project. Teams will be randomly assigned for the mini-projects but you will choose your final project groups. (By working with teammates throughout the course, you will also be able to use one another as a resource during labs and assignments.)\n\n\nCase Study Mini-projects (20%)\nStarting week 5, we will transition to a project-based course. This will allow us to use case studies to focus on deepening statistical knowledge and carrying out interesting analyses. In this, specific case studies and statistics topics will be discussed in class. In your teams and for each of the case studies, you will: 1) extend the analysis from class and 2) write up a full report of the case study for your team project.\n\n\nFinal Project (17%)\nThe final project will be completed in groups. There will be two different general final projects from which your group can choose, but the idea is that whichever you choose, you will be able to tackle it using and building upon the tools and techniques discussed in class. Briefly here, the two options will be: 1. Create a technical presentation on a statistics topic and/or an R package. 2. Carry out a data analysis.\nEach will require a written report and an oral presentation, but the specific requirements will differ between the two.\nFinal Projects will be due on Th 3/23 of finals week at 11:59 PM.\n\n\n\nGrading\nYour final grade will be comprised of the following:\n\n\n\nLabs (8)\n16%\n\n\nHomework (4)\n32%\n\n\nMidterm (1)\n15%\n\n\nCase Study Projects* (2)\n20%\n\n\nFinal Project(*) (1)\n17%\n\n\n\n* indicates group submission\n\nFinal Grades\nTo calculate final grades, I use the standard grading scale and do not round grades up (given the numerous extra credit opportunities offered):\n\n\n\n97-100%\nA+\n\n\n93-96%\nA\n\n\n90-92%\nA-\n\n\n87-89%\nB+\n\n\n83-86%\nB\n\n\n80-82%\nB-\n\n\n77-79%\nC+\n\n\n73-76%\nC\n\n\n70-72%\nC-\n\n\n67-69%\nD+\n\n\n63-66%\nD\n\n\n60-62%\nD-\n\n\n<60%\nF\n\n\n\n\n\n\nLate / missed work\nLate homework assignments and case study projects will be accepted up to 3 days (72 hours) after the assigned deadline. Late submissions will receive a 25% deduction.\nThere are no late deadlines for labs, the exam, or the final project.\nNote: Prof Ellis is a reasonable person; reach out to her if you have an extenuating circumstance at any point in the quarter.\n\n\nRegrade requests\nWe will work hard to grade everyone fairly and return assignments quickly. And, we know you also work hard and want you to receive the grade you’ve earned. Occasionally, grading mistakes do happen, and it’s important to us to correct them. If you think there is a mistake in your grade on an assignment, post privately on Campuswire to “Instructors” using the “regrades” tag within 72 hours. This post should include evidence of why you think your answer was correct and should point to the specific part of the assignment in question.\n\n\nProfessionalism\nPlease refrain from texting or using your computer for anything other than coursework during class. Not only is this distracting to you, but it can also be distracting to those around you. (Note that there is no consequence associated with this. I know it can be difficult, but I ask that you try your best!)"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "COGS 137",
    "section": "",
    "text": "Week\nDate\nTitle\nType\n\n\n\n\n1\nTu Jan 10\nWelcome & Tooling\nLecture\n\n\n1\nTh Jan 12\nIntro to R\nLecture\n\n\n1\nFri Jan 13\nLab 01: Intro to R\nLab\n\n\n2\nTu Jan 17\nData Wrangling: dplyr\nLecture\n\n\n2\nTh Jan 19\nData Wrangling: tidyr\nLecture\n\n\n2\nFri Jan 20\nLab 02: Data Wrangling\nLab\n\n\n3\nMon Jan 23\nHW01 due (11:59 PM)\nHW\n\n\n3\nTu Jan 24\nData Visualization: ggplot2 (day 1)\nLecture\n\n\n3\nTh Jan 26\nData Visualization: ggplot2 (day 2)\nLecture\n\n\n3\nFri Jan 27\nLab 03: Data Visualization\nLab\n\n\n4\nTu Jan 31\nData Analysis & Modeling\nLecture\n\n\n4\nTh Feb 2\nLinear Models Review\nLecture\n\n\n4\nFri Feb 3\nLab 04: Modeling\nLab\n\n\n5\nMon Feb 6\nHW02 due (11:59 PM)\nHW\n\n\n5\nTu Feb 7\nEffective Communication\nLecture\n\n\n5\nTh Feb 9\nCase Study & Final Project Info\nLecture\n\n\n5\nFri Feb 10\nLab used for midterm review\nLab\n\n\n6\nMon Feb 13\nMIDTERM EXAM (due 11:59 PM) \nExam\n\n\n6\nTu Feb 14\nMultiple Linear Regression\nLecture\n\n\n6\nTh Feb 16\nCase Study 01: Right to Carry (day 1)\nLecture\n\n\n6\nFri Feb 17\nLab 05: Multiple Linear Regression\nLab\n\n\n7\nTu Feb 21\nCase Study 01: Right to Carry (day 2)\nLecture\n\n\n7\nTh Feb 23\nCase Study 01: Right to Carry (day 3)\nLecture\n\n\n7\nFri Feb 24\nLab 06: CS01\nLab\n\n\n8\nMon Feb 27\nHW03 due (11:59 PM)\nHW\n\n\n8\nTu Feb 28\nLogistic Regression (day 1)\nLecture\n\n\n8\nTh Mar 2\nLogistic Regression (day 2)\nLecture\n\n\n8\nFri Mar 3\nLab 07: Logistic Regression\nLab\n\n\n9\nMon Mar 6\nCS01 Due (11:59 PM)\nCase Study\n\n\n9\nTu Mar 7\nCase Study 02: Vaping (day 1)\nLecture\n\n\n9\nTh Mar 9\nCase Study 02: Vaping (day 2)\nLecture\n\n\n9\nFri Mar 10\nLab 08: CS02\nLab\n\n\n10\nMon Mar 13\nHW04 due (11:59 PM)\nHW\n\n\n10\nTu Mar 14\nFinal Project Brainstorming\nLecture\n\n\n10\nTh Mar 16\nNext Steps\nLecture\n\n\n10\nFri Mar 17\nLab used for Final Project\nLab\n\n\nFinals\nMon Mar 20\nCS02 Due\nCase Study\n\n\nFinals\nTh Mar 23\nFinal Project Due (11:59 PM)\nLecture"
  }
]